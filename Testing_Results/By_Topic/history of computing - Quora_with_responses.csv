Authors,Time,Questions,Answers,api_response,originality_score
Danny Thorpe,Updated 8y,Why did Borland fail?,"Borland lost its way when executive management decided to change the company to pursue a different market.

The company was founded on the idea of making mass-market software - products that can be used by a large number of people in a variety of different scenarios, and at reasonable prices. Turbo Pascal did not become a worldwide sensation just because it compiled thousands of times faster than other compilers - it was also priced hundreds of dollars less than other compiler products, and provided a level of developer tool integration never before seen.

A few years after Borland went public, founder and CEO Philippe Kahn began to have increasing disagreements with the Borland board of directors. Kahn wanted to continue building products for the mass market, but the board of directors wanted to shift gears and pursue the ""enterprise"" software market. I get the impression that this difference of opinion simmered for years. Ultimately the board fired Kahn and threw the company headlong into the pursuit of the enterprise market.

I'm sure there were several motivations for this change of company direction. In the early 90's, Microsoft had a firm foothold in mass market software, but was still relatively unknown in the enterprise market. Was the board solely drawn to the high price tags of the enterprise market? Or were they also attempting to run away from Microsoft's shadow?

In the height of the enterprise transformation, I asked Del Yocam, one of many interim CEOs after Kahn, ""Are you saying you want to trade a million loyal $100 customers for a hundred $1 million customers?"" Yocam replied without hesitation ""Absolutely.""

And that, in my opinion, was the core of Borland's downfall. The executive team decided to take a talented development team with lots of experience building innovative consumer products and retask them to build enterprise software. On paper this may seem like a fairly minor adjustment, if you have the attitude (as Borland executive management had) that developers are a dime a dozen and any developer can be applied to any product or problem space. That may work for technical programming skills but it doesn't work for passion.

The reality is that the enterprise market is a radically different beast from the consumer market. ""Turning the ship"" to pursue enterprise customers requires a lot more than just changing the project requirements given to the development teams. It requires a different skillset and mindset in the development teams, a different sales force, and ultimately a different corporate culture.

For many of Borland's talented developers, enterprise software was the antithesis of their interests and passions. Hot rodders, hackers, and modern day makers were tasked with making things they had no passion for - building the software analogs of sewer systems, utility poles, or synthetic hairballs for ceramic cats.

Borland's long slow death spiral began when it turned away from what it knew best to chase a unicorn it knew nothing about.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2dkuim3yhcbe756p', 'title': 'Why did Borland fail?', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'Borland lost its way when executive management decided to change the company to pursue a different market.\n\nThe company was founded on the idea of making mass-market software - products that can be used by a large number of people in a variety of different scenarios, and at reasonable prices. Turbo Pascal did not become a worldwide sensation just because it compiled thousands of times faster than other compilers - it was also priced hundreds of dollars less than other compiler products, and provided a level of developer tool integration never before seen.\n\nA few years after Borland went public, founder and CEO Philippe Kahn began to have increasing disagreements with the Borland board of directors. Kahn wanted to continue building products for the mass market, but the board of directors wanted to shift gears and pursue the ""enterprise"" software market. I get the impression that this difference of opinion simmered for years. Ultimately the board fired Kahn and threw the company headlong into the pursuit of the enterprise market.\n\nI\'m sure there were several motivations for this change of company direction. In the early 90\'s, Microsoft had a firm foothold in mass market software, but was still relatively unknown in the enterprise market. Was the board solely drawn to the high price tags of the enterprise market? Or were they also attempting to run away from Microsoft\'s shadow?\n\nIn the height of the enterprise transformation, I asked Del Yocam, one of many interim CEOs after Kahn, ""Are you saying you want to trade a million loyal $100 customers for a hundred $1 million customers?"" Yocam replied without hesitation ""Absolutely.""\n\nAnd that, in my opinion, was the core of Borland\'s downfall. The executive team decided to take a talented development team with lots of experience building innovative consumer products and retask them to build enterprise software. On paper this may seem like a fairly minor adjustment, if you have the attitude (as Borland executive management had) that developers are a dime a dozen and any developer can be applied to any product or problem space. That may work for technical programming skills but it doesn\'t work for passion.\n\nThe reality is that the enterprise market is a radically different beast from the consumer market. ""Turning the ship"" to pursue enterprise customers requires a lot more than just changing the project requirements given to the development teams. It requires a different skillset and mindset in the development teams, a different sales force, and ultimately a different corporate culture.\n\nFor many of Borland\'s talented developers, enterprise software was the antithesis of their interests and passions. Hot rodders, hackers, and modern day makers were tasked with making things they had no passion for - building the software analogs of sewer systems, utility poles, or synthetic hairballs for ceramic cats.\n\nBorland\'s long slow death spiral began when it turned away from what it knew best to chase a unicorn it knew nothing about.', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995598, 'subscription': 0, 'content': 'Borland lost its way when executive management decided to change the company to pursue a different market.\n\nThe company was founded on the idea of making mass-market software - products that can be used by a large number of people in a variety of different scenarios, and at reasonable prices. Turbo Pascal did not become a worldwide sensation just because it compiled thousands of times faster than other compilers - it was also priced hundreds of dollars less than other compiler products, and provided a level of developer tool integration never before seen.\n\nA few years after Borland went public, founder and CEO Philippe Kahn began to have increasing disagreements with the Borland board of directors. Kahn wanted to continue building products for the mass market, but the board of directors wanted to shift gears and pursue the ""enterprise"" software market. I get the impression that this difference of opinion simmered for years. Ultimately the board fired Kahn and threw the company headlong into the pursuit of the enterprise market.\n\nI\'m sure there were several motivations for this change of company direction. In the early 90\'s, Microsoft had a firm foothold in mass market software, but was still relatively unknown in the enterprise market. Was the board solely drawn to the high price tags of the enterprise market? Or were they also attempting to run away from Microsoft\'s shadow?\n\nIn the height of the enterprise transformation, I asked Del Yocam, one of many interim CEOs after Kahn, ""Are you saying you want to trade a million loyal $100 customers for a hundred $1 million customers?"" Yocam replied without hesitation ""Absolutely.""\n\nAnd that, in my opinion, was the core of Borland\'s downfall. The executive team decided to take a talented development team with lots of experience building innovative consumer products and retask them to build enterprise software. On paper this may seem like a fairly minor adjustment, if you have the attitude (as Borland executive management had) that developers are a dime a dozen and any developer can be applied to any product or problem space. That may work for technical programming skills but it doesn\'t work for passion.\n\nThe reality is that the enterprise market is a radically different beast from the consumer market. ""Turning the ship"" to pursue enterprise customers requires a lot more than just changing the project requirements given to the development teams. It requires a different skillset and mindset in the development teams, a different sales force, and ultimately a different corporate culture.\n\nFor many of Borland\'s talented developers, enterprise software was the antithesis of their interests and passions. Hot rodders, hackers, and modern day makers were tasked with making things they had no passion for - building the software analogs of sewer systems, utility poles, or synthetic hairballs for ceramic cats.\n\nBorland\'s long slow death spiral began when it turned away from what it knew best to chase a unicorn it knew nothing about.', 'aiModelVersion': '1'}",0.9996
Alan Kay,Updated 3y,What was it like programming an IBM 1401 back in the 60’s?,"The IBM 1401 was the first computer I programmed as part of a real job (an enlisted man in the US Air Force at Air Training Command (ATG), Randolph AFB, San Antonio, Texas).

In the early 60s, the USAF had a quite difficult aptitude test made for them by IBM to screen potential programmers for any of their computers. I took it for fun in early 1962, passed it, and was assigned to Randolph AFB, and IBM training school for the 1401. The training was done in one intense week, wall to wall.

The range of HW architectures was much wider back then, but the 1401 was still thought to be “odd”, and today would be considered very unusual. It had been developed to gradually subsume “punched card accounting machine” (PCAM) tasks (for which IBM was the world’s largest company). As with many businesses back then, Air Training Command had a number of “shops” of such machines, with perhaps 50 to 100 machines each laid out over 0.5 to an acre.

— I can’t believe that Google won’t yield a picture of one of these large “shops” — perhaps someone can find one —

A very small PCAM “shop” — imagine with 50+ machines in an enormous room …

The 1401 computer was a “character machine”, with a memory of 5K to 16K characters of 6 info bits plus a “word mark” bit each — these were set to indicate ends of “words” (a sequence of characters in memory addressed by the low order character. A “data character” was (most generally) an encoding of a 12-row punched card column: 4 bits to encode 0–9 and the other two bits to encode the “zone row bits”. Note that this just gives you 40 combinations, but other tricks were used …

To add two numbers, the 1401 essentially did elementary school arithmetic, progressing character by character until one or both word marks were reached. (This meant that the 1401 could be set up to add one half of memory to the other half.)

The 1401’s memory cycle for one character was 11.5 microseconds … so by computer standards of today it was both tiny and a snail. By PCAM standards, and tape standards, it was quite fast enough.

It was essentially a two address machine, but the address registers persisted, so you could deal with “fields” on “cards” sequentially by e.g. doing one two address add — an opcode and two three digit addresses (which would be terminated by the word marks), and the next add would only require the opcode because it would use where the address registers wound up from the previous operation. (Instructions were processed in the left to right direction with word marks at the opcodes.)

The 1401 was thus essentially a variable sized word and variable sized instruction machine. Getting lots of stuff done was greatly rewarded by careful planning.

IBM 1401 - Wikipedia
 A nicely detailed article.

The IBM Reference Manual we used
 (the version here is accidentally from early 1962, just about exactly when I started programming the 1401).

The punched cards to be worked on were read by a card reader, converted to tapes, processed, the results went back to tapes, were printed, and/or punched back into card form.

IBM eventually made and sold well over 10,000 of these machines (the first mass produced computer in history).

The IBM 1401 “mainframe”.

1403 Printer

1402 Card reader/punch (good for putting program listings on top of)

IBM 729 Tape Drives — with vacuum tape loop buffering

The 1401 we worked on at ATC had 8K bytes of memory and six very fast tape drives for those days. There was no OS, and most debugging was “desk checking”. You could get “3 minutes” once a day to try a program, but you couldn’t touch the machine — you put your listing on top of the card reader and asked the operator to do this and that (making friends with the machine operators was critical).

The basic learning process was to do the one week hardware school, after which you were quite able to program anything on the 1401 (this was one of the fruits of the difficult aptitude test — it probably overscreened, but everyone who did pass it had absolutely no trouble learning a whole tricky machine and its machine code successfully in a week).

Then you spent some months “programming” — which was called “coding”. What was called “programming” back then was design and flowcharting the design. The PCAM machines were all flowcharted, and these were dataflow schemes. These had to be serialized a bit to be converted to computer processing.

A “coder” was thus a human “compiler” — whose job it was to convert the higher level programming language of flowcharts to working machine code.

After a few months of this “on the job training” and assessment one was allowed to do some design, programming, and then coding for new tasks. There were already seasoned (a year or so) programmers — also mostly enlisted men — who were generally very helpful.

There was another larger computer there — the Burroughs 220 — and learning it and doing similar kind of work on it improved the programming and coding for both machines.

A few other things I should mention.

It was possible to be extremely clever with the 1401, and it should be obvious that the wordmark scheme allowed clever memory allocation schemes, overlays, etc.

(It is well worth your time to take a look at Val Schorre’s 1964 “Meta II” system at UCLA,
 a compiler-compiler that would run in an 8K 1401. I wish I had known about this when I was a 1401er, but its invention happened after I went back to school.)

We were able to make a batch operating system that would run all the shop jobs, and which fit into the top few hundred characters of memory (and using tape to help).

IBM had an extremely capable “tailored macro” assembler — Autocoder — that allowed a wide range of expression and “conditional assembly” to produce highly optimized and compact code.

IBM already had a policy to “make everything run on everything”, and this meant that both the relatively new FORTRAN, and the even newer COBOL had to be compilable and runnable, even on the smaller configurations. The latter never made it into the ATC shop while I was there, but there was a FORTRAN “for curiosity’s sake” (this was because the compiler took about 100 passes (or “pulls” of the tape drive) i.e. a very long time).

Reports Program Generator (RPG) was used for a few jobs (I tried it a few times along with others). It made a few vanilla tasks quicker, but the macro library we had developed was generally more convenient, and only a little more dangerous.

The older larger slower vacuum tube Burroughs 220 did have something really interesting: the BALGOL compiler (Algol 58 with a few Burroughs touches). It had one of the most beautiful manuals, and this got me to read it. One of the officers had been a CalTech grad and had used it there. At ATC, it was also a “curiosity” but as a much “sweeter and cleaner” example of a higher level language.

The next machine in line for ATC was to be a Burroughs B5000, a truly amazing machine whose hardware was made directly for an Algolic language (Algol 58 really), and all of whose software — including the OS (it had one) — was written in higher level form. I learned this machine from the documentation, only understood about half of it, and was back to college before it showed up. (A few years later — by accident — one of my grad school profs was Bob Barton, the main inventor of this marvelous design.)

I was able to get back in school for the fall of 1963, and the AF let people out up to 100 days early to not miss a term. I was able to get a good part time job programming for the National Center for Atmospheric Research (NCAR) in Boulder, that would pay my tuition and room and board for the rest of my undergrad years at the U of Colorado.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/hkfxymje39i0u56r', 'title': 'What was it like programming an IBM 1401 back in the 60’s?', 'score': {'original': 0.92106666666667, 'ai': 0.078933333333333}, 'blocks': [{'text': 'The IBM 1401 was the first computer I programmed as part of a real job (an enlisted man in the US Air Force at Air Training Command (ATG), Randolph AFB, San Antonio, Texas).\n\nIn the early 60s, the USAF had a quite difficult aptitude test made for them by IBM to screen potential programmers for any of their computers. I took it for fun in early 1962, passed it, and was assigned to Randolph AFB, and IBM training school for the 1401. The training was done in one intense week, wall to wall.\n\nThe range of HW architectures was much wider back then, but the 1401 was still thought to be “odd”, and today would be considered very unusual. It had been developed to gradually subsume “punched card accounting machine” (PCAM) tasks (for which IBM was the world’s largest company). As with many businesses back then, Air Training Command had a number of “shops” of such machines, with perhaps 50 to 100 machines each laid out over 0.5 to an acre.\n\n— I can’t believe that Google won’t yield a picture of one of these large “shops” — perhaps someone can find one —\n\nA very small PCAM “shop” — imagine with 50+ machines in an enormous room …\n\nThe 1401 computer was a “character machine”, with a memory of 5K to 16K characters of 6 info bits plus a “word mark” bit each — these were set to indicate ends of “words” (a sequence of characters in memory addressed by the low order character. A “data character” was (most generally) an encoding of a 12-row punched card column: 4 bits to encode 0–9 and the other two bits to encode the “zone row bits”. Note that this just gives you 40 combinations, but other tricks were used …\n\nTo add two numbers, the 1401 essentially did elementary school arithmetic, progressing character by character until one or both word marks were reached. (This meant that the 1401 could be set up to add one half of memory to the other half.)\n\nThe 1401’s memory cycle for one character was 11.5 microseconds … so by computer standards of today it was both tiny and a snail. By PCAM standards, and tape standards, it was quite fast enough.\n\nIt was essentially a two address machine, but the address registers persisted, so you could deal with “fields” on “cards” sequentially by e.g. doing one two address add — an opcode and two three digit addresses (which would be terminated by the word marks), and the next add would only require the opcode because it would use where the address registers wound up from the previous operation. (Instructions were processed in the left to right direction with word marks at the opcodes.)\n\nThe 1401 was thus essentially a variable sized word and variable sized instruction machine. Getting lots of stuff done was greatly rewarded by careful planning.\n\nIBM 1401 - Wikipedia\n A nicely detailed article.\n\nThe IBM Reference Manual we used\n (the version here is accidentally from early 1962, just about exactly when I started programming the 1401).\n\nThe punched cards to be worked on were read by', 'result': {'fake': 0.0078, 'real': 0.9922}, 'status': 'success'}, {'text': 'a card reader, converted to tapes, processed, the results went back to tapes, were printed, and/or punched back into card form.\n\nIBM eventually made and sold well over 10,000 of these machines (the first mass produced computer in history).\n\nThe IBM 1401 “mainframe”.\n\n1403 Printer\n\n1402 Card reader/punch (good for putting program listings on top of)\n\nIBM 729 Tape Drives — with vacuum tape loop buffering\n\nThe 1401 we worked on at ATC had 8K bytes of memory and six very fast tape drives for those days. There was no OS, and most debugging was “desk checking”. You could get “3 minutes” once a day to try a program, but you couldn’t touch the machine — you put your listing on top of the card reader and asked the operator to do this and that (making friends with the machine operators was critical).\n\nThe basic learning process was to do the one week hardware school, after which you were quite able to program anything on the 1401 (this was one of the fruits of the difficult aptitude test — it probably overscreened, but everyone who did pass it had absolutely no trouble learning a whole tricky machine and its machine code successfully in a week).\n\nThen you spent some months “programming” — which was called “coding”. What was called “programming” back then was design and flowcharting the design. The PCAM machines were all flowcharted, and these were dataflow schemes. These had to be serialized a bit to be converted to computer processing.\n\nA “coder” was thus a human “compiler” — whose job it was to convert the higher level programming language of flowcharts to working machine code.\n\nAfter a few months of this “on the job training” and assessment one was allowed to do some design, programming, and then coding for new tasks. There were already seasoned (a year or so) programmers — also mostly enlisted men — who were generally very helpful.\n\nThere was another larger computer there — the Burroughs 220 — and learning it and doing similar kind of work on it improved the programming and coding for both machines.\n\nA few other things I should mention.\n\nIt was possible to be extremely clever with the 1401, and it should be obvious that the wordmark scheme allowed clever memory allocation schemes, overlays, etc.\n\n(It is well worth your time to take a look at Val Schorre’s 1964 “Meta II” system at UCLA,\n a compiler-compiler that would run in an 8K 1401. I wish I had known about this when I was a 1401er, but its invention happened after I went back to school.)\n\nWe were able to make a batch operating system that would run all the shop jobs, and which fit into the top few hundred characters of memory (and using tape to help).\n\nIBM had an extremely capable “tailored macro” assembler — Autocoder — that allowed a wide range of expression and “conditional assembly” to produce highly optimized and compact code.\n\nIBM already had a policy to “make everything run on everything”, and this meant that both the relatively new FORTRAN, and the even newer COBOL had to be compilable and runnable, even on', 'result': {'fake': 0.8075, 'real': 0.1925}, 'status': 'success'}, {'text': 'the smaller configurations. The latter never made it into the ATC shop while I was there, but there was a FORTRAN “for curiosity’s sake” (this was because the compiler took about 100 passes (or “pulls” of the tape drive) i.e. a very long time).\n\nReports Program Generator (RPG) was used for a few jobs (I tried it a few times along with others). It made a few vanilla tasks quicker, but the macro library we had developed was generally more convenient, and only a little more dangerous.\n\nThe older larger slower vacuum tube Burroughs 220 did have something really interesting: the BALGOL compiler (Algol 58 with a few Burroughs touches). It had one of the most beautiful manuals, and this got me to read it. One of the officers had been a CalTech grad and had used it there. At ATC, it was also a “curiosity” but as a much “sweeter and cleaner” example of a higher level language.\n\nThe next machine in line for ATC was to be a Burroughs B5000, a truly amazing machine whose hardware was made directly for an Algolic language (Algol 58 really), and all of whose software — including the OS (it had one) — was written in higher level form. I learned this machine from the documentation, only understood about half of it, and was back to college before it showed up. (A few years later — by accident — one of my grad school profs was Bob Barton, the main inventor of this marvelous design.)\n\nI was able to get back in school for the fall of 1963, and the AF let people out up to 100 days early to not miss a term. I was able to get a good part time job programming for the National Center for Atmospheric Research (NCAR) in Boulder, that would pay my tuition and room and board for the rest of my undergrad years at the U of Colorado.', 'result': {'fake': 0.632, 'real': 0.368}, 'status': 'success'}], 'credits_used': 14, 'credits': 1995584, 'subscription': 0, 'content': 'The IBM 1401 was the first computer I programmed as part of a real job (an enlisted man in the US Air Force at Air Training Command (ATG), Randolph AFB, San Antonio, Texas).\n\nIn the early 60s, the USAF had a quite difficult aptitude test made for them by IBM to screen potential programmers for any of their computers. I took it for fun in early 1962, passed it, and was assigned to Randolph AFB, and IBM training school for the 1401. The training was done in one intense week, wall to wall.\n\nThe range of HW architectures was much wider back then, but the 1401 was still thought to be “odd”, and today would be considered very unusual. It had been developed to gradually subsume “punched card accounting machine” (PCAM) tasks (for which IBM was the world’s largest company). As with many businesses back then, Air Training Command had a number of “shops” of such machines, with perhaps 50 to 100 machines each laid out over 0.5 to an acre.\n\n— I can’t believe that Google won’t yield a picture of one of these large “shops” — perhaps someone can find one —\n\nA very small PCAM “shop” — imagine with 50+ machines in an enormous room …\n\nThe 1401 computer was a “character machine”, with a memory of 5K to 16K characters of 6 info bits plus a “word mark” bit each — these were set to indicate ends of “words” (a sequence of characters in memory addressed by the low order character. A “data character” was (most generally) an encoding of a 12-row punched card column: 4 bits to encode 0–9 and the other two bits to encode the “zone row bits”. Note that this just gives you 40 combinations, but other tricks were used …\n\nTo add two numbers, the 1401 essentially did elementary school arithmetic, progressing character by character until one or both word marks were reached. (This meant that the 1401 could be set up to add one half of memory to the other half.)\n\nThe 1401’s memory cycle for one character was 11.5 microseconds … so by computer standards of today it was both tiny and a snail. By PCAM standards, and tape standards, it was quite fast enough.\n\nIt was essentially a two address machine, but the address registers persisted, so you could deal with “fields” on “cards” sequentially by e.g. doing one two address add — an opcode and two three digit addresses (which would be terminated by the word marks), and the next add would only require the opcode because it would use where the address registers wound up from the previous operation. (Instructions were processed in the left to right direction with word marks at the opcodes.)\n\nThe 1401 was thus essentially a variable sized word and variable sized instruction machine. Getting lots of stuff done was greatly rewarded by careful planning.\n\nIBM 1401 - Wikipedia\n A nicely detailed article.\n\nThe IBM Reference Manual we used\n (the version here is accidentally from early 1962, just about exactly when I started programming the 1401).\n\nThe punched cards to be worked on were read by a card reader, converted to tapes, processed, the results went back to tapes, were printed, and/or punched back into card form.\n\nIBM eventually made and sold well over 10,000 of these machines (the first mass produced computer in history).\n\nThe IBM 1401 “mainframe”.\n\n1403 Printer\n\n1402 Card reader/punch (good for putting program listings on top of)\n\nIBM 729 Tape Drives — with vacuum tape loop buffering\n\nThe 1401 we worked on at ATC had 8K bytes of memory and six very fast tape drives for those days. There was no OS, and most debugging was “desk checking”. You could get “3 minutes” once a day to try a program, but you couldn’t touch the machine — you put your listing on top of the card reader and asked the operator to do this and that (making friends with the machine operators was critical).\n\nThe basic learning process was to do the one week hardware school, after which you were quite able to program anything on the 1401 (this was one of the fruits of the difficult aptitude test — it probably overscreened, but everyone who did pass it had absolutely no trouble learning a whole tricky machine and its machine code successfully in a week).\n\nThen you spent some months “programming” — which was called “coding”. What was called “programming” back then was design and flowcharting the design. The PCAM machines were all flowcharted, and these were dataflow schemes. These had to be serialized a bit to be converted to computer processing.\n\nA “coder” was thus a human “compiler” — whose job it was to convert the higher level programming language of flowcharts to working machine code.\n\nAfter a few months of this “on the job training” and assessment one was allowed to do some design, programming, and then coding for new tasks. There were already seasoned (a year or so) programmers — also mostly enlisted men — who were generally very helpful.\n\nThere was another larger computer there — the Burroughs 220 — and learning it and doing similar kind of work on it improved the programming and coding for both machines.\n\nA few other things I should mention.\n\nIt was possible to be extremely clever with the 1401, and it should be obvious that the wordmark scheme allowed clever memory allocation schemes, overlays, etc.\n\n(It is well worth your time to take a look at Val Schorre’s 1964 “Meta II” system at UCLA,\n a compiler-compiler that would run in an 8K 1401. I wish I had known about this when I was a 1401er, but its invention happened after I went back to school.)\n\nWe were able to make a batch operating system that would run all the shop jobs, and which fit into the top few hundred characters of memory (and using tape to help).\n\nIBM had an extremely capable “tailored macro” assembler — Autocoder — that allowed a wide range of expression and “conditional assembly” to produce highly optimized and compact code.\n\nIBM already had a policy to “make everything run on everything”, and this meant that both the relatively new FORTRAN, and the even newer COBOL had to be compilable and runnable, even on the smaller configurations. The latter never made it into the ATC shop while I was there, but there was a FORTRAN “for curiosity’s sake” (this was because the compiler took about 100 passes (or “pulls” of the tape drive) i.e. a very long time).\n\nReports Program Generator (RPG) was used for a few jobs (I tried it a few times along with others). It made a few vanilla tasks quicker, but the macro library we had developed was generally more convenient, and only a little more dangerous.\n\nThe older larger slower vacuum tube Burroughs 220 did have something really interesting: the BALGOL compiler (Algol 58 with a few Burroughs touches). It had one of the most beautiful manuals, and this got me to read it. One of the officers had been a CalTech grad and had used it there. At ATC, it was also a “curiosity” but as a much “sweeter and cleaner” example of a higher level language.\n\nThe next machine in line for ATC was to be a Burroughs B5000, a truly amazing machine whose hardware was made directly for an Algolic language (Algol 58 really), and all of whose software — including the OS (it had one) — was written in higher level form. I learned this machine from the documentation, only understood about half of it, and was back to college before it showed up. (A few years later — by accident — one of my grad school profs was Bob Barton, the main inventor of this marvelous design.)\n\nI was able to get back in school for the fall of 1963, and the AF let people out up to 100 days early to not miss a term. I was able to get a good part time job programming for the National Center for Atmospheric Research (NCAR) in Boulder, that would pay my tuition and room and board for the rest of my undergrad years at the U of Colorado.', 'aiModelVersion': '1'}",0.92106666666667
Alan Kay,1y,What is the origin of model-view-controller?,"Ivan Sutherland — the inventor-originator of interactive computer graphics — had the idea of a clipping window to see a part of the very large Sketchpad world. This wound up being a program structure roughly like a camera which had an aperture that could be of any size relative to the world, and would show that portion of the world on the screen. Each Sketchpad object had a method that would render the object in the world, and the windowing apparatus did the job of transforming world coordinates to window coordinates (this was done as part of the rendering operation).

Ivan Sutherland in 1962 at the building-sized TX-2 computer working with Sketchpad on a truss bridge design with the screen showing part of the whole bridge that has been zoomed in on. He is holding a light pen used for pointing and graphical input. This year is the 60th anniversary of Sketchpad.

The next version of Sketchpad was a 3D system (done by Timothy Johnson), and it supported 3D multiple views of the world.

Ivan’s next major project was the first virtual reality headmounted display, and this required tracking the head of the wearer, and using this to sample from the interior 3D world for each eye.

So there was a similar software structure like a telescope or camera that had one end in the virtual world, and had the other on the display face. Both the to be visualized objects and the “cameras” had six degrees of freedom. The virtual world side was sometimes called the “camera”, and sometimes called the “window”. The real world side was often called the “viewpoint” (at Xerox Parc, this was renamed the “window” to help explanations to Xerox).

When I was a grad student at the U of Utah, I overlapped with Ivan, the HMD project, and its approach (meaning: there is a world with virtual objects, and these include camera objects whose purpose is to look at parts of the virtual world and send these to be displayed on windows on a display). The simple linear equation for coordinate transformations was now a many degrees of freedom matrix that required quite a bit of special HW support (that had to be built from scratch).

At some point I started thinking about other than graphical virtual objects in the “everything is an object” ideas I was pursuing. The idea was that every object should know how to render itself, and this should be combined with a windowing-viewing mechanism. A fun thing was the idea to use multiple windows to show different aspects of the objects in the view. I first saw this in Sketchpad III, then in the Engelbart system in the late 60s (it was mostly used for filtering detail), and used the idea in the FLEX Machine that Ed Cheadle and I did.

The earliest versions of Smalltalk were simple and slow, and included multiple windows and “turtles” that could be part of a window (that would then clip what the turtle drew).

A Smalltalk-72 screen showing an editable view of an Elvish character, which is used in one of the paragraphs in “the galley editor” (an early experiment in desktop publishing). The galley is a collection of views of both paragraphs of text and a painting view with a sketch of Frodo and Gandalf. The user interface for a painting view pops up around it, and it in turn is made from views of the controls.

Another early plan — not done until later — was to have a slippage scheme, because the simulation “frames” from the model could often be computed much faster than graphical rendering of the view, and thus one would like to decouple the updating of the semantics from the updating of the display. This was later actually done in a principled way when MVC was implemented.

Shortly after Smalltalk-76 was done, we were visited by Trygve Reenskaug from Norway, who was very interested in large scale planning (such as for the Norwegian ship building industry). He was indefatigable, and decided to do a real planning system in the new Smalltalk. I attribute the first formal and working examples of MVC to Trygve, who among other things used them to view and change a complex changing schedule and to have different windows show lists, Gannt and Pert charts, etc. each of which could be edited directly and would automatically cause the model to update itself.

Trygve’s first planning system ca 1976–7 showing three different dynamic views of the same plan with the same item (A21) selected and editable in each view. This is arguably the first principled use of MVC.

Adele Goldberg and Dave Robson got very interested in this and took it further to the general idea that there is a “model” system, which has the semantics of the system, a “viewing system” which can get the needed parts from a model for each view, and do the screen painting, and a “controller” system that deals with the input devices employed by the user, does the sometimes difficult inverse transformation on the view to find what is being interacted with, and informs the model about needed changes.

This more formal system was put into Smalltalk at some point — maybe not until Smalltalk-80 — and both overkilled and underkilled. Part of the underkill was that originally there was a class “Model” with a protocol for things to be viewed by members of class “View”. But since you wanted to view everything, the model protocol should be part of class Object. Etc. This eventually happened.

And so forth. For the “simple things should be simple” part, there needed to be default views that would be automatically included in any class definition. One of these later that was much used—Morphic—was done by John Maloney — originally for Self — and then for Smalltalk. Similarly, having default controllers was very useful.

The whole framework was a very useful way to separate concerns (and I think even more should be done pragmatically with the details of creating MVC subsystems).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/32kzqe5xwldob9ua', 'title': 'What is the origin of model-view-controller?', 'score': {'original': 0.9773, 'ai': 0.0227}, 'blocks': [{'text': 'Ivan Sutherland — the inventor-originator of interactive computer graphics — had the idea of a clipping window to see a part of the very large Sketchpad world. This wound up being a program structure roughly like a camera which had an aperture that could be of any size relative to the world, and would show that portion of the world on the screen. Each Sketchpad object had a method that would render the object in the world, and the windowing apparatus did the job of transforming world coordinates to window coordinates (this was done as part of the rendering operation).\n\nIvan Sutherland in 1962 at the building-sized TX-2 computer working with Sketchpad on a truss bridge design with the screen showing part of the whole bridge that has been zoomed in on. He is holding a light pen used for pointing and graphical input. This year is the 60th anniversary of Sketchpad.\n\nThe next version of Sketchpad was a 3D system (done by Timothy Johnson), and it supported 3D multiple views of the world.\n\nIvan’s next major project was the first virtual reality headmounted display, and this required tracking the head of the wearer, and using this to sample from the interior 3D world for each eye.\n\nSo there was a similar software structure like a telescope or camera that had one end in the virtual world, and had the other on the display face. Both the to be visualized objects and the “cameras” had six degrees of freedom. The virtual world side was sometimes called the “camera”, and sometimes called the “window”. The real world side was often called the “viewpoint” (at Xerox Parc, this was renamed the “window” to help explanations to Xerox).\n\nWhen I was a grad student at the U of Utah, I overlapped with Ivan, the HMD project, and its approach (meaning: there is a world with virtual objects, and these include camera objects whose purpose is to look at parts of the virtual world and send these to be displayed on windows on a display). The simple linear equation for coordinate transformations was now a many degrees of freedom matrix that required quite a bit of special HW support (that had to be built from scratch).\n\nAt some point I started thinking about other than graphical virtual objects in the “everything is an object” ideas I was pursuing. The idea was that every object should know how to render itself, and this should be combined with a windowing-viewing mechanism. A fun thing was the idea to use multiple windows to show different aspects of the objects in the view. I first saw this in Sketchpad III, then in the Engelbart system in the late 60s (it was mostly used for filtering detail), and used the idea in the FLEX Machine that Ed Cheadle and I did.\n\nThe earliest versions of Smalltalk were simple and slow, and included multiple windows and “turtles” that could be part of a window (that would then clip what the turtle drew).\n\nA Smalltalk-72 screen showing an editable view of an Elvish character, which is used in one of the', 'result': {'fake': 0.054, 'real': 0.946}, 'status': 'success'}, {'text': 'paragraphs in “the galley editor” (an early experiment in desktop publishing). The galley is a collection of views of both paragraphs of text and a painting view with a sketch of Frodo and Gandalf. The user interface for a painting view pops up around it, and it in turn is made from views of the controls.\n\nAnother early plan — not done until later — was to have a slippage scheme, because the simulation “frames” from the model could often be computed much faster than graphical rendering of the view, and thus one would like to decouple the updating of the semantics from the updating of the display. This was later actually done in a principled way when MVC was implemented.\n\nShortly after Smalltalk-76 was done, we were visited by Trygve Reenskaug from Norway, who was very interested in large scale planning (such as for the Norwegian ship building industry). He was indefatigable, and decided to do a real planning system in the new Smalltalk. I attribute the first formal and working examples of MVC to Trygve, who among other things used them to view and change a complex changing schedule and to have different windows show lists, Gannt and Pert charts, etc. each of which could be edited directly and would automatically cause the model to update itself.\n\nTrygve’s first planning system ca 1976–7 showing three different dynamic views of the same plan with the same item (A21) selected and editable in each view. This is arguably the first principled use of MVC.\n\nAdele Goldberg and Dave Robson got very interested in this and took it further to the general idea that there is a “model” system, which has the semantics of the system, a “viewing system” which can get the needed parts from a model for each view, and do the screen painting, and a “controller” system that deals with the input devices employed by the user, does the sometimes difficult inverse transformation on the view to find what is being interacted with, and informs the model about needed changes.\n\nThis more formal system was put into Smalltalk at some point — maybe not until Smalltalk-80 — and both overkilled and underkilled. Part of the underkill was that originally there was a class “Model” with a protocol for things to be viewed by members of class “View”. But since you wanted to view everything, the model protocol should be part of class Object. Etc. This eventually happened.\n\nAnd so forth. For the “simple things should be simple” part, there needed to be default views that would be automatically included in any class definition. One of these later that was much used—Morphic—was done by John Maloney — originally for Self — and then for Smalltalk. Similarly, having default controllers was very useful.\n\nThe whole framework was a very useful way to separate concerns (and I think even more should be done pragmatically with the details of creating MVC subsystems).', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 11, 'credits': 1995573, 'subscription': 0, 'content': 'Ivan Sutherland — the inventor-originator of interactive computer graphics — had the idea of a clipping window to see a part of the very large Sketchpad world. This wound up being a program structure roughly like a camera which had an aperture that could be of any size relative to the world, and would show that portion of the world on the screen. Each Sketchpad object had a method that would render the object in the world, and the windowing apparatus did the job of transforming world coordinates to window coordinates (this was done as part of the rendering operation).\n\nIvan Sutherland in 1962 at the building-sized TX-2 computer working with Sketchpad on a truss bridge design with the screen showing part of the whole bridge that has been zoomed in on. He is holding a light pen used for pointing and graphical input. This year is the 60th anniversary of Sketchpad.\n\nThe next version of Sketchpad was a 3D system (done by Timothy Johnson), and it supported 3D multiple views of the world.\n\nIvan’s next major project was the first virtual reality headmounted display, and this required tracking the head of the wearer, and using this to sample from the interior 3D world for each eye.\n\nSo there was a similar software structure like a telescope or camera that had one end in the virtual world, and had the other on the display face. Both the to be visualized objects and the “cameras” had six degrees of freedom. The virtual world side was sometimes called the “camera”, and sometimes called the “window”. The real world side was often called the “viewpoint” (at Xerox Parc, this was renamed the “window” to help explanations to Xerox).\n\nWhen I was a grad student at the U of Utah, I overlapped with Ivan, the HMD project, and its approach (meaning: there is a world with virtual objects, and these include camera objects whose purpose is to look at parts of the virtual world and send these to be displayed on windows on a display). The simple linear equation for coordinate transformations was now a many degrees of freedom matrix that required quite a bit of special HW support (that had to be built from scratch).\n\nAt some point I started thinking about other than graphical virtual objects in the “everything is an object” ideas I was pursuing. The idea was that every object should know how to render itself, and this should be combined with a windowing-viewing mechanism. A fun thing was the idea to use multiple windows to show different aspects of the objects in the view. I first saw this in Sketchpad III, then in the Engelbart system in the late 60s (it was mostly used for filtering detail), and used the idea in the FLEX Machine that Ed Cheadle and I did.\n\nThe earliest versions of Smalltalk were simple and slow, and included multiple windows and “turtles” that could be part of a window (that would then clip what the turtle drew).\n\nA Smalltalk-72 screen showing an editable view of an Elvish character, which is used in one of the paragraphs in “the galley editor” (an early experiment in desktop publishing). The galley is a collection of views of both paragraphs of text and a painting view with a sketch of Frodo and Gandalf. The user interface for a painting view pops up around it, and it in turn is made from views of the controls.\n\nAnother early plan — not done until later — was to have a slippage scheme, because the simulation “frames” from the model could often be computed much faster than graphical rendering of the view, and thus one would like to decouple the updating of the semantics from the updating of the display. This was later actually done in a principled way when MVC was implemented.\n\nShortly after Smalltalk-76 was done, we were visited by Trygve Reenskaug from Norway, who was very interested in large scale planning (such as for the Norwegian ship building industry). He was indefatigable, and decided to do a real planning system in the new Smalltalk. I attribute the first formal and working examples of MVC to Trygve, who among other things used them to view and change a complex changing schedule and to have different windows show lists, Gannt and Pert charts, etc. each of which could be edited directly and would automatically cause the model to update itself.\n\nTrygve’s first planning system ca 1976–7 showing three different dynamic views of the same plan with the same item (A21) selected and editable in each view. This is arguably the first principled use of MVC.\n\nAdele Goldberg and Dave Robson got very interested in this and took it further to the general idea that there is a “model” system, which has the semantics of the system, a “viewing system” which can get the needed parts from a model for each view, and do the screen painting, and a “controller” system that deals with the input devices employed by the user, does the sometimes difficult inverse transformation on the view to find what is being interacted with, and informs the model about needed changes.\n\nThis more formal system was put into Smalltalk at some point — maybe not until Smalltalk-80 — and both overkilled and underkilled. Part of the underkill was that originally there was a class “Model” with a protocol for things to be viewed by members of class “View”. But since you wanted to view everything, the model protocol should be part of class Object. Etc. This eventually happened.\n\nAnd so forth. For the “simple things should be simple” part, there needed to be default views that would be automatically included in any class definition. One of these later that was much used—Morphic—was done by John Maloney — originally for Self — and then for Smalltalk. Similarly, having default controllers was very useful.\n\nThe whole framework was a very useful way to separate concerns (and I think even more should be done pragmatically with the details of creating MVC subsystems).', 'aiModelVersion': '1'}",0.9773
Kevin McClear,1y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","I used to spend HOURS getting the best use out of every KB of ram:

That’s kilobyte. As in 1,000 bytes.

My current computer has 16 GB of ram. As in 16,000,000,000 bytes (actually more, since computer metric makes no sense*).

My friends and I would swap screenshots of just how tightly we used RAM. That’s just… not a thing anymore.

*If you are curious about the not-flippant reason for these numbers, please see Jerason Banes’s comment below.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/a4wm63p8jl7i1yzg', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': 'I used to spend HOURS getting the best use out of every KB of ram:\n\nThat’s kilobyte. As in 1,000 bytes.\n\nMy current computer has 16 GB of ram. As in 16,000,000,000 bytes (actually more, since computer metric makes no sense*).\n\nMy friends and I would swap screenshots of just how tightly we used RAM. That’s just… not a thing anymore.\n\n*If you are curious about the not-flippant reason for these numbers, please see Jerason Banes’s comment below.', 'result': {'fake': 0.0007, 'real': 0.9993}, 'status': 'success'}], 'credits_used': 1, 'credits': 1995572, 'subscription': 0, 'content': 'I used to spend HOURS getting the best use out of every KB of ram:\n\nThat’s kilobyte. As in 1,000 bytes.\n\nMy current computer has 16 GB of ram. As in 16,000,000,000 bytes (actually more, since computer metric makes no sense*).\n\nMy friends and I would swap screenshots of just how tightly we used RAM. That’s just… not a thing anymore.\n\n*If you are curious about the not-flippant reason for these numbers, please see Jerason Banes’s comment below.', 'aiModelVersion': '1'}",0.9993
Ted Mayberry,Updated 1y,How would a 1980s computer scientist react to today’s modern computers?,"In the early 80’s, I used IBM punch cards to run my programs through the computer.

I remember the instructor holding one up and stating these will become obsolete and we should stick one in a drawer sometime because years later, people won’t even know what it is. I had a hard time believing him.

Well, I don’t remember what capacity that mainframe I used back in the day that parsed out jobs from hundreds of different users, but I suspect my desktop likely exceeds that computer’s capacity.

In other words, if I visited myself in the 80’s with just my smart phone, well, I’d say the 80’s version of me would be blown away.

Edit: Had no idea so many would comment.

I’m pretty sure my cell phone has more computing capacity over the mainframe I used in the 80’s. But, since I don’t really know what the specs of the mainframe back in the 80’s, well, I figured softening my statement was for the best. People on this board can get brutal when you don’t have your facts straight. Then again, many can be brutal because they’re idiots.
Reading through the comments, reminded me of the parameter that we set in case we accidentally programed an infinite loop. Back then, the standard was 2 seconds, if the program took more than 2 seconds, it would halt the job and spit out whatever you got. My dad got his engineering degree in the early 70’s and used the same interface, punch cards to a mainframe. His infinite loop parameter was 2 minutes.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/p610hao5ixunszl2', 'title': 'How would a 1980s computer scientist react to today’s modern computers?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'In the early 80’s, I used IBM punch cards to run my programs through the computer.\n\nI remember the instructor holding one up and stating these will become obsolete and we should stick one in a drawer sometime because years later, people won’t even know what it is. I had a hard time believing him.\n\nWell, I don’t remember what capacity that mainframe I used back in the day that parsed out jobs from hundreds of different users, but I suspect my desktop likely exceeds that computer’s capacity.\n\nIn other words, if I visited myself in the 80’s with just my smart phone, well, I’d say the 80’s version of me would be blown away.\n\nEdit: Had no idea so many would comment.\n\nI’m pretty sure my cell phone has more computing capacity over the mainframe I used in the 80’s. But, since I don’t really know what the specs of the mainframe back in the 80’s, well, I figured softening my statement was for the best. People on this board can get brutal when you don’t have your facts straight. Then again, many can be brutal because they’re idiots.\nReading through the comments, reminded me of the parameter that we set in case we accidentally programed an infinite loop. Back then, the standard was 2 seconds, if the program took more than 2 seconds, it would halt the job and spit out whatever you got. My dad got his engineering degree in the early 70’s and used the same interface, punch cards to a mainframe. His infinite loop parameter was 2 minutes.', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995569, 'subscription': 0, 'content': 'In the early 80’s, I used IBM punch cards to run my programs through the computer.\n\nI remember the instructor holding one up and stating these will become obsolete and we should stick one in a drawer sometime because years later, people won’t even know what it is. I had a hard time believing him.\n\nWell, I don’t remember what capacity that mainframe I used back in the day that parsed out jobs from hundreds of different users, but I suspect my desktop likely exceeds that computer’s capacity.\n\nIn other words, if I visited myself in the 80’s with just my smart phone, well, I’d say the 80’s version of me would be blown away.\n\nEdit: Had no idea so many would comment.\n\nI’m pretty sure my cell phone has more computing capacity over the mainframe I used in the 80’s. But, since I don’t really know what the specs of the mainframe back in the 80’s, well, I figured softening my statement was for the best. People on this board can get brutal when you don’t have your facts straight. Then again, many can be brutal because they’re idiots.\nReading through the comments, reminded me of the parameter that we set in case we accidentally programed an infinite loop. Back then, the standard was 2 seconds, if the program took more than 2 seconds, it would halt the job and spit out whatever you got. My dad got his engineering degree in the early 70’s and used the same interface, punch cards to a mainframe. His infinite loop parameter was 2 minutes.', 'aiModelVersion': '1'}",0.9999
Jeff Sturm,11mo,Why was it so hard for Unix to run on a 386 CPU before Linux?,"It wasn’t. There were commercial ports to the i386 available before 1990. When I worked in a Radio Shack retail store in the late 1980’s, our store operating system ran on a Tandy 4000 computer running SCO Xenix, a flavor of Unix. It supported two POS terminals connected by a serial RS-232 link and the main console where we would enter the day’s cash balances at the close of business, among other management tasks.

Essentially, one of these:

https://worldradiohistory.com/hd2/IDX-CATALOGS/IDX/Radio-Shack/Radio-Shack-1988-OCR-Page-0175.pdf

The Tandy 4000 had one megabyte RAM standard and could be upgraded to a maximum of 16MB. But these, and the other early 80386 systems from PC clone manufacturers, are what made Unix possible on the desktop for “reasonable” cost (probably around $4k all in).

Altos sold 80386 systems running AT&T System V Unix, and later 80486 which were software compatible but higher performance (up to 33 MHz). I supported these early in my career with up to 20 users running accounting software.

But the commercial Unix releases were usually out of reach of the home computer hobbyist. From memory, a base license for SCO Xenix could be $795 retail (EDIT: Xenix was available for $595 per the Radio Shack ad above), and that’s without upgrades for TCP/IP networking or development tools.

Many hobbyists in the 1980’s were frustrated with the limitations of MS-DOS and wanted a true multitasking, multiuser operating system on their home computers. Most wouldn’t pay for the commercial licenses from AT&T licensees, so there was a small, but growing interest in having a Unix clone OS available for inexpensive home computers.

That was how Linux got its start, when Linus Torvalds, then a computer science student in Finland, began work on his kernel project that would eventually allow him to host GNU ports of common Unix utilities and software from the Free Software Foundation. But Linux wasn’t the first—though as an ambitious project, it was among the best. Linux importantly had support for virtual memory on the 80386, allowing for a unified buffer cache and demand-paged memory that yielded high performance of applications ported from Unix to GNU/Linux, even on small memory systems.

Before Linux there was Minix, which also ran on the 8088-based IBM PC, but it was more of a teaching tool than a practical operating system.

The Mark-Williams company saw an opportunity and created a commercial Unix clone called Coherent, initially on the 16-bit 80286 CPU, later ported to the 80386. Coherent had inadequate support for virtual memory and lacked a sufficient developer community as it was closed source. That company went out of business as they could not compete with Linux or any of the BSD ports then.

Speaking of BSD, the 386BSD project was a port of the Berkeley Software Distribution’s Unix system (created and maintained initially at the University of California, Berkeley campus) to the 80386, and it could be freely downloaded by hobbyists in 1992 around the time Linux was in early development. I can remember downloading a boot floppy that could run a live 386BSD distribution, downloading software packages over a dial-up modem. Very very slow to download, but it worked!

(386BSD has been discontinued as a project. It split into other projects such as NetBSD or FreeBSD which are still maintained today. However, unlike Linux developers, the BSD team had access to AT&T licensed source code for Unix. AT&T filed a lawsuit against the BSDi, a company the produced one of the early ports of BSD to the 80386, alleging that the free code contained proprietary software licensed by Unix System Laboratories, a subsidiary of AT&T. The lawsuits were settled by 1994, but in the meantime, Linux got a sizeable head start, as it was unencumbered by the lawsuits.)

Strictly speaking the various Linux and BSD releases could not be called “Unix” as they were not a licensed product. But they all strove to be Unix compatible and made it easy to port Unix software often without changes. In some cases, they were even binary compatible with Unix, allowing a Linux user to run software released for Unix without modification.

By the mid-1990’s the BSD distributions along with GNU/Linux were stable and successful on Intel-based personal computers. I ran a Linux distribution on my 80486-based home computer and used it every day. It wasn’t called “Unix” but it was close enough for us, and the name “Unix” has began to fade with history as today it is commonly (and incorrectly) simply called “Linux”.

In short, Linux wasn’t the first or only port to the 80386, but it saw early success due to a combination of factors such as a high-quality implementation that would rival commercial releases, and luck (due to the AT&T lawsuit).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/947zmfwedslvap0c', 'title': 'Why was it so hard for Unix to run on a 386 CPU before Linux?', 'score': {'original': 0.92885, 'ai': 0.07115}, 'blocks': [{'text': 'It wasn’t. There were commercial ports to the i386 available before 1990. When I worked in a Radio Shack retail store in the late 1980’s, our store operating system ran on a Tandy 4000 computer running SCO Xenix, a flavor of Unix. It supported two POS terminals connected by a serial RS-232 link and the main console where we would enter the day’s cash balances at the close of business, among other management tasks.\n\nEssentially, one of these:\n\nhttps://worldradiohistory.com/hd2/IDX-CATALOGS/IDX/Radio-Shack/Radio-Shack-1988-OCR-Page-0175.pdf\n\nThe Tandy 4000 had one megabyte RAM standard and could be upgraded to a maximum of 16MB. But these, and the other early 80386 systems from PC clone manufacturers, are what made Unix possible on the desktop for “reasonable” cost (probably around $4k all in).\n\nAltos sold 80386 systems running AT&T System V Unix, and later 80486 which were software compatible but higher performance (up to 33 MHz). I supported these early in my career with up to 20 users running accounting software.\n\nBut the commercial Unix releases were usually out of reach of the home computer hobbyist. From memory, a base license for SCO Xenix could be $795 retail (EDIT: Xenix was available for $595 per the Radio Shack ad above), and that’s without upgrades for TCP/IP networking or development tools.\n\nMany hobbyists in the 1980’s were frustrated with the limitations of MS-DOS and wanted a true multitasking, multiuser operating system on their home computers. Most wouldn’t pay for the commercial licenses from AT&T licensees, so there was a small, but growing interest in having a Unix clone OS available for inexpensive home computers.\n\nThat was how Linux got its start, when Linus Torvalds, then a computer science student in Finland, began work on his kernel project that would eventually allow him to host GNU ports of common Unix utilities and software from the Free Software Foundation. But Linux wasn’t the first—though as an ambitious project, it was among the best. Linux importantly had support for virtual memory on the 80386, allowing for a unified buffer cache and demand-paged memory that yielded high performance of applications ported from Unix to GNU/Linux, even on small memory systems.\n\nBefore Linux there was Minix, which also ran on the 8088-based IBM PC, but it was more of a teaching tool than a practical operating system.\n\nThe Mark-Williams company saw an opportunity and created a commercial Unix clone called Coherent, initially on the 16-bit 80286 CPU, later ported to the 80386. Coherent had inadequate support for virtual memory and lacked a sufficient developer community as it was closed source. That company went out of business as they could not compete with Linux or any of the BSD ports then.\n\nSpeaking of BSD, the 386BSD project was a port of the Berkeley Software Distribution’s Unix system (created and maintained initially at the University of California, Berkeley campus) to the 80386, and it could be freely downloaded by hobbyists in 1992 around the time Linux was in early development. I can remember downloading a boot floppy that could run a live 386BSD distribution, downloading software packages over a dial-up modem. Very very slow to download, but it worked!\n\n(386BSD', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}, {'text': 'has been discontinued as a project. It split into other projects such as NetBSD or FreeBSD which are still maintained today. However, unlike Linux developers, the BSD team had access to AT&T licensed source code for Unix. AT&T filed a lawsuit against the BSDi, a company the produced one of the early ports of BSD to the 80386, alleging that the free code contained proprietary software licensed by Unix System Laboratories, a subsidiary of AT&T. The lawsuits were settled by 1994, but in the meantime, Linux got a sizeable head start, as it was unencumbered by the lawsuits.)\n\nStrictly speaking the various Linux and BSD releases could not be called “Unix” as they were not a licensed product. But they all strove to be Unix compatible and made it easy to port Unix software often without changes. In some cases, they were even binary compatible with Unix, allowing a Linux user to run software released for Unix without modification.\n\nBy the mid-1990’s the BSD distributions along with GNU/Linux were stable and successful on Intel-based personal computers. I ran a Linux distribution on my 80486-based home computer and used it every day. It wasn’t called “Unix” but it was close enough for us, and the name “Unix” has began to fade with history as today it is commonly (and incorrectly) simply called “Linux”.\n\nIn short, Linux wasn’t the first or only port to the 80386, but it saw early success due to a combination of factors such as a high-quality implementation that would rival commercial releases, and luck (due to the AT&T lawsuit).', 'result': {'fake': 0.1565, 'real': 0.8435}, 'status': 'success'}], 'credits_used': 8, 'credits': 1995561, 'subscription': 0, 'content': 'It wasn’t. There were commercial ports to the i386 available before 1990. When I worked in a Radio Shack retail store in the late 1980’s, our store operating system ran on a Tandy 4000 computer running SCO Xenix, a flavor of Unix. It supported two POS terminals connected by a serial RS-232 link and the main console where we would enter the day’s cash balances at the close of business, among other management tasks.\n\nEssentially, one of these:\n\nhttps://worldradiohistory.com/hd2/IDX-CATALOGS/IDX/Radio-Shack/Radio-Shack-1988-OCR-Page-0175.pdf\n\nThe Tandy 4000 had one megabyte RAM standard and could be upgraded to a maximum of 16MB. But these, and the other early 80386 systems from PC clone manufacturers, are what made Unix possible on the desktop for “reasonable” cost (probably around $4k all in).\n\nAltos sold 80386 systems running AT&T System V Unix, and later 80486 which were software compatible but higher performance (up to 33 MHz). I supported these early in my career with up to 20 users running accounting software.\n\nBut the commercial Unix releases were usually out of reach of the home computer hobbyist. From memory, a base license for SCO Xenix could be $795 retail (EDIT: Xenix was available for $595 per the Radio Shack ad above), and that’s without upgrades for TCP/IP networking or development tools.\n\nMany hobbyists in the 1980’s were frustrated with the limitations of MS-DOS and wanted a true multitasking, multiuser operating system on their home computers. Most wouldn’t pay for the commercial licenses from AT&T licensees, so there was a small, but growing interest in having a Unix clone OS available for inexpensive home computers.\n\nThat was how Linux got its start, when Linus Torvalds, then a computer science student in Finland, began work on his kernel project that would eventually allow him to host GNU ports of common Unix utilities and software from the Free Software Foundation. But Linux wasn’t the first—though as an ambitious project, it was among the best. Linux importantly had support for virtual memory on the 80386, allowing for a unified buffer cache and demand-paged memory that yielded high performance of applications ported from Unix to GNU/Linux, even on small memory systems.\n\nBefore Linux there was Minix, which also ran on the 8088-based IBM PC, but it was more of a teaching tool than a practical operating system.\n\nThe Mark-Williams company saw an opportunity and created a commercial Unix clone called Coherent, initially on the 16-bit 80286 CPU, later ported to the 80386. Coherent had inadequate support for virtual memory and lacked a sufficient developer community as it was closed source. That company went out of business as they could not compete with Linux or any of the BSD ports then.\n\nSpeaking of BSD, the 386BSD project was a port of the Berkeley Software Distribution’s Unix system (created and maintained initially at the University of California, Berkeley campus) to the 80386, and it could be freely downloaded by hobbyists in 1992 around the time Linux was in early development. I can remember downloading a boot floppy that could run a live 386BSD distribution, downloading software packages over a dial-up modem. Very very slow to download, but it worked!\n\n(386BSD has been discontinued as a project. It split into other projects such as NetBSD or FreeBSD which are still maintained today. However, unlike Linux developers, the BSD team had access to AT&T licensed source code for Unix. AT&T filed a lawsuit against the BSDi, a company the produced one of the early ports of BSD to the 80386, alleging that the free code contained proprietary software licensed by Unix System Laboratories, a subsidiary of AT&T. The lawsuits were settled by 1994, but in the meantime, Linux got a sizeable head start, as it was unencumbered by the lawsuits.)\n\nStrictly speaking the various Linux and BSD releases could not be called “Unix” as they were not a licensed product. But they all strove to be Unix compatible and made it easy to port Unix software often without changes. In some cases, they were even binary compatible with Unix, allowing a Linux user to run software released for Unix without modification.\n\nBy the mid-1990’s the BSD distributions along with GNU/Linux were stable and successful on Intel-based personal computers. I ran a Linux distribution on my 80486-based home computer and used it every day. It wasn’t called “Unix” but it was close enough for us, and the name “Unix” has began to fade with history as today it is commonly (and incorrectly) simply called “Linux”.\n\nIn short, Linux wasn’t the first or only port to the 80386, but it saw early success due to a combination of factors such as a high-quality implementation that would rival commercial releases, and luck (due to the AT&T lawsuit).', 'aiModelVersion': '1'}",0.92885
Brian Bi,11y,Have there been any new brilliant computer science algorithms in last 10 years?,"I believe just last year it was found that the maximum network flow problem can be solved in O(VE)O(VE)O(VE) time. This bound was first achieved for dense graphs, i.e., E∈Ω(V2)E∈Ω(V2)E \in \Omega(V^2), by the relabel-to-front algorithm, a particular version of the push-preflow method, which runs in O(V3)O(V3)O(V^3) time. This algorithm was published by Goldberg and Tarjan in 1986, and you can find it in CLRS. In 1994, King, Rao, and Tarjan gave an algorithm that runs in O(VElog(V)/log(EVlogV))O(VElog⁡(V)/log⁡(EVlog⁡V))O\left(VE \log(V)/\log\left(\frac{E}{V \log V}\right)\right) time. This reduces to O(VE)O(VE)O(VE) for all graphs that are not too sparse, i.e., E∈Ω(V1+ϵ)E∈Ω(V1+ϵ)E \in \Omega(V^{1+\epsilon}). (Graphs with constant or logarithmic average degree are still a bit too sparse.) Finally, in 2012, Orlin put the final piece in place, finding an algorithm that runs in O(VE+E31/16log2V)O(VE+E31/16log2⁡V)O(VE + E^{31/16} \log^2 V). For E∈O(V16/15−ϵ)E∈O(V16/15−ϵ)E \in O(V^{16/15-\epsilon}), that is, graphs that are sparse enough, this reduces to O(VE)O(VE)O(VE). By combining the algorithm of King, Rao, and Tarjan, and Orlin's algorithm, we see that maximum network flow can be solved in O(VE)O(VE)O(VE) time. I think that's pretty awesome.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/nkwyx521sb9mridf', 'title': 'Have there been any new brilliant computer science algorithms in last 10 years?', 'score': {'original': 0.0317, 'ai': 0.9683}, 'blocks': [{'text': ""I believe just last year it was found that the maximum network flow problem can be solved in O(VE)O(VE)O(VE) time. This bound was first achieved for dense graphs, i.e., E∈Ω(V2)E∈Ω(V2)E \\in \\Omega(V^2), by the relabel-to-front algorithm, a particular version of the push-preflow method, which runs in O(V3)O(V3)O(V^3) time. This algorithm was published by Goldberg and Tarjan in 1986, and you can find it in CLRS. In 1994, King, Rao, and Tarjan gave an algorithm that runs in O(VElog(V)/log(EVlogV))O(VElog\u2061(V)/log\u2061(EVlog\u2061V))O\\left(VE \\log(V)/\\log\\left(\\frac{E}{V \\log V}\\right)\\right) time. This reduces to O(VE)O(VE)O(VE) for all graphs that are not too sparse, i.e., E∈Ω(V1+ϵ)E∈Ω(V1+ϵ)E \\in \\Omega(V^{1+\\epsilon}). (Graphs with constant or logarithmic average degree are still a bit too sparse.) Finally, in 2012, Orlin put the final piece in place, finding an algorithm that runs in O(VE+E31/16log2V)O(VE+E31/16log2\u2061V)O(VE + E^{31/16} \\log^2 V). For E∈O(V16/15−ϵ)E∈O(V16/15−ϵ)E \\in O(V^{16/15-\\epsilon}), that is, graphs that are sparse enough, this reduces to O(VE)O(VE)O(VE). By combining the algorithm of King, Rao, and Tarjan, and Orlin's algorithm, we see that maximum network flow can be solved in O(VE)O(VE)O(VE) time. I think that's pretty awesome."", 'result': {'fake': 0.9683, 'real': 0.0317}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995558, 'subscription': 0, 'content': ""I believe just last year it was found that the maximum network flow problem can be solved in O(VE)O(VE)O(VE) time. This bound was first achieved for dense graphs, i.e., E∈Ω(V2)E∈Ω(V2)E \\in \\Omega(V^2), by the relabel-to-front algorithm, a particular version of the push-preflow method, which runs in O(V3)O(V3)O(V^3) time. This algorithm was published by Goldberg and Tarjan in 1986, and you can find it in CLRS. In 1994, King, Rao, and Tarjan gave an algorithm that runs in O(VElog(V)/log(EVlogV))O(VElog\u2061(V)/log\u2061(EVlog\u2061V))O\\left(VE \\log(V)/\\log\\left(\\frac{E}{V \\log V}\\right)\\right) time. This reduces to O(VE)O(VE)O(VE) for all graphs that are not too sparse, i.e., E∈Ω(V1+ϵ)E∈Ω(V1+ϵ)E \\in \\Omega(V^{1+\\epsilon}). (Graphs with constant or logarithmic average degree are still a bit too sparse.) Finally, in 2012, Orlin put the final piece in place, finding an algorithm that runs in O(VE+E31/16log2V)O(VE+E31/16log2\u2061V)O(VE + E^{31/16} \\log^2 V). For E∈O(V16/15−ϵ)E∈O(V16/15−ϵ)E \\in O(V^{16/15-\\epsilon}), that is, graphs that are sparse enough, this reduces to O(VE)O(VE)O(VE). By combining the algorithm of King, Rao, and Tarjan, and Orlin's algorithm, we see that maximum network flow can be solved in O(VE)O(VE)O(VE) time. I think that's pretty awesome."", 'aiModelVersion': '1'}",0.0317
Alan Kay,Updated 5y,What are the Seven Wonders of computer science?,"I love this question — in no small part because I don’t think a really correct/exclusive answer is possible.

I have a particular set of meanings for the term “computer science” — the questioner might have meant to say “computing”. A few of my items might be taken from “computing” rather than from “computer science” per se.

The things I have found to be astonishing and amazing (and shocking) are:

Turing’s notion of machines that can simulate machines completely by interpreting their descriptions (exhibiting the programmable computer as “a language machine” and a “meta-language machine” — along with this is the simplicity of what is required to do so (a great book is Marvin Minsky’s “Computation: Finite and Infinite Machines”). Turing’s approach is much more of a “real CS” approach compared to Goedel’s earlier methods, and soon led to a large number of important next steps.
<this one is tentative at this point> How simple (a) it is to design a whole computer from just one kind of logical element (e.g. “NOT-BOTH”), especially when compared (b) to how Russell and Whitehead struggled to “bootstrap mathematics, etc., from logic at the turn of the last century. (This is one of those “Point of View is Worth 80 IQ Points” …)
Lisp, and McCarthy’s general approach to “mathematical theories of computation” and having languages that can act as their own metalanguage. One of the great cornucopias of our field.
Sketchpad by Ivan Sutherland for so many reasons, including: the approach to interactive computer graphics and the simulations of the graphic relationships, the “object-oriented” approach to definition and deriving new kinds of things (including “masters” and making instances from masters), enormous virtual worlds that are windowed on the display, the use of goal-directed programming with the system solving the simultaneous goals in real-time, etc. And more, including the demonstration that a simulated computer on a computer need look nothing like the underlying hardware or any “normal” idea of “computer”.
The big Shannon et al. ideas about how to have imperfect things be organized in systems that are much more perfectly behaved even if the organizational mechanisms are themselves noisy. Includes all forms of “noise”, “representations”, “communications”, “machines”, etc. and poking deeply into Biology and how living things work. Nice implications for “stochastic computing” of many kinds which are needed more and more as things scale.
The deep implications of “symbolic computation” (now a very un-funded area) for being able to move from the trivialities of “data” (no matter how voluminous”) to the profundities and powers of “Meaning”. This used to be called “AI” and now has to be called “real AI” or “strong AI” (it would be much better under a less loaded term: how about “Flexible Competence”?)
The Internet. Certainly the best thing done by my research community, and the first real essay into the kinds of scaling and stabilities that all computer science should be trying to understand and improve. This was a great invention and development process in all ways, and — by looking at Biology, which inspired but we really couldn’t use — it had a reasonable chance to work. That it was able to scale stably over more than 10 (maybe 11) orders of magnitude, as indeed planned, is still kind of amazing to me (even though it should have). Judging from most software systems today not being organized like the Internet, one is forced into the opinion that most computerists don’t understand it, why it is great (and maybe don’t even think of it as the fruits of “real computer science” because it just works so much better and more reliably than most other attempted artifacts in the field).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pwoa9hlqd4kcung1', 'title': 'What are the Seven Wonders of computer science?', 'score': {'original': 0.8118, 'ai': 0.1882}, 'blocks': [{'text': 'I love this question — in no small part because I don’t think a really correct/exclusive answer is possible.\n\nI have a particular set of meanings for the term “computer science” — the questioner might have meant to say “computing”. A few of my items might be taken from “computing” rather than from “computer science” per se.\n\nThe things I have found to be astonishing and amazing (and shocking) are:\n\nTuring’s notion of machines that can simulate machines completely by interpreting their descriptions (exhibiting the programmable computer as “a language machine” and a “meta-language machine” — along with this is the simplicity of what is required to do so (a great book is Marvin Minsky’s “Computation: Finite and Infinite Machines”). Turing’s approach is much more of a “real CS” approach compared to Goedel’s earlier methods, and soon led to a large number of important next steps.\n<this one is tentative at this point> How simple (a) it is to design a whole computer from just one kind of logical element (e.g. “NOT-BOTH”), especially when compared (b) to how Russell and Whitehead struggled to “bootstrap mathematics, etc., from logic at the turn of the last century. (This is one of those “Point of View is Worth 80 IQ Points” …)\nLisp, and McCarthy’s general approach to “mathematical theories of computation” and having languages that can act as their own metalanguage. One of the great cornucopias of our field.\nSketchpad by Ivan Sutherland for so many reasons, including: the approach to interactive computer graphics and the simulations of the graphic relationships, the “object-oriented” approach to definition and deriving new kinds of things (including “masters” and making instances from masters), enormous virtual worlds that are windowed on the display, the use of goal-directed programming with the system solving the simultaneous goals in real-time, etc. And more, including the demonstration that a simulated computer on a computer need look nothing like the underlying hardware or any “normal” idea of “computer”.\nThe big Shannon et al. ideas about how to have imperfect things be organized in systems that are much more perfectly behaved even if the organizational mechanisms are themselves noisy. Includes all forms of “noise”, “representations”, “communications”, “machines”, etc. and poking deeply into Biology and how living things work. Nice implications for “stochastic computing” of many kinds which are needed more and more as things scale.\nThe deep implications of “symbolic computation” (now a very un-funded area) for being able to move from the trivialities of “data” (no matter how voluminous”) to the profundities and powers of “Meaning”. This used to be called “AI” and now has to be called “real AI” or “strong AI” (it would be much better under a less loaded term: how about “Flexible Competence”?)\nThe Internet. Certainly the best thing done by my research community, and the first real essay into the kinds of scaling and stabilities that all computer science should be trying to understand and improve. This was a great invention and development process in all ways, and — by looking at Biology, which inspired but we really couldn’t use — it had a reasonable chance to work.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}, {'text': 'That it was able to scale stably over more than 10 (maybe 11) orders of magnitude, as indeed planned, is still kind of amazing to me (even though it should have). Judging from most software systems today not being organized like the Internet, one is forced into the opinion that most computerists don’t understand it, why it is great (and maybe don’t even think of it as the fruits of “real computer science” because it just works so much better and more reliably than most other attempted artifacts in the field).', 'result': {'fake': 0.0044, 'real': 0.9956}, 'status': 'success'}], 'credits_used': 7, 'credits': 1995551, 'subscription': 0, 'content': 'I love this question — in no small part because I don’t think a really correct/exclusive answer is possible.\n\nI have a particular set of meanings for the term “computer science” — the questioner might have meant to say “computing”. A few of my items might be taken from “computing” rather than from “computer science” per se.\n\nThe things I have found to be astonishing and amazing (and shocking) are:\n\nTuring’s notion of machines that can simulate machines completely by interpreting their descriptions (exhibiting the programmable computer as “a language machine” and a “meta-language machine” — along with this is the simplicity of what is required to do so (a great book is Marvin Minsky’s “Computation: Finite and Infinite Machines”). Turing’s approach is much more of a “real CS” approach compared to Goedel’s earlier methods, and soon led to a large number of important next steps.\n<this one is tentative at this point> How simple (a) it is to design a whole computer from just one kind of logical element (e.g. “NOT-BOTH”), especially when compared (b) to how Russell and Whitehead struggled to “bootstrap mathematics, etc., from logic at the turn of the last century. (This is one of those “Point of View is Worth 80 IQ Points” …)\nLisp, and McCarthy’s general approach to “mathematical theories of computation” and having languages that can act as their own metalanguage. One of the great cornucopias of our field.\nSketchpad by Ivan Sutherland for so many reasons, including: the approach to interactive computer graphics and the simulations of the graphic relationships, the “object-oriented” approach to definition and deriving new kinds of things (including “masters” and making instances from masters), enormous virtual worlds that are windowed on the display, the use of goal-directed programming with the system solving the simultaneous goals in real-time, etc. And more, including the demonstration that a simulated computer on a computer need look nothing like the underlying hardware or any “normal” idea of “computer”.\nThe big Shannon et al. ideas about how to have imperfect things be organized in systems that are much more perfectly behaved even if the organizational mechanisms are themselves noisy. Includes all forms of “noise”, “representations”, “communications”, “machines”, etc. and poking deeply into Biology and how living things work. Nice implications for “stochastic computing” of many kinds which are needed more and more as things scale.\nThe deep implications of “symbolic computation” (now a very un-funded area) for being able to move from the trivialities of “data” (no matter how voluminous”) to the profundities and powers of “Meaning”. This used to be called “AI” and now has to be called “real AI” or “strong AI” (it would be much better under a less loaded term: how about “Flexible Competence”?)\nThe Internet. Certainly the best thing done by my research community, and the first real essay into the kinds of scaling and stabilities that all computer science should be trying to understand and improve. This was a great invention and development process in all ways, and — by looking at Biology, which inspired but we really couldn’t use — it had a reasonable chance to work. That it was able to scale stably over more than 10 (maybe 11) orders of magnitude, as indeed planned, is still kind of amazing to me (even though it should have). Judging from most software systems today not being organized like the Internet, one is forced into the opinion that most computerists don’t understand it, why it is great (and maybe don’t even think of it as the fruits of “real computer science” because it just works so much better and more reliably than most other attempted artifacts in the field).', 'aiModelVersion': '1'}",0.8118
Dave Griffin,2y,Why were many 20th century computer programs designed to store only the last two digits of the year (thus giving rise to the infamous Y2K problem)? Didn't people realise that it would cause confusion at the turn of the century?,"Because of this baby right here:

Gather round young ones… this is what a “database table” looked like from probably the 1930s to well into the 1980s (so, most of the 20th century): the 80 column punched card. Whether it was a census record, student record, property record, medical record, you name it - the information about it was crammed into 80 bytes. If you want to add the century to the dates you had to take two bytes (2.5%) out of something else.

When others talk about memory limitations, which are valid, I think this was the limiting factor. Once the data was transferred to magnetic tape or disk drives a large majority of the programs written would be fine with 4 digit years (it’s not like we read the entire file into memory and chomped on it - it was read in one block at a time (with multiple records per block) or even one record at a time. But to get that information on to those tape or disk drives you, 90% of the time you went through this particular bottleneck.

It’s also worth noting that until tapes and disks were common, these were the physical manifestation of the data - and a LOT of it predated what you would consider computers. “Automatic Data Processing” machines, like card sorters, “programmable” reproducing punches, accounting tabulators, etc. were used to do all sorts of interesting “programs” on the data encoded in the holes of these cards. Literally rooms of file cabinets held this information and that legacy would take decades to break free from.

I arrived on the scene in the mid 70’s and while I did most of my work on data that resided on disks and tapes, most of my programs were on these cards as well (thousands of them) and I did my fair share of running sets of data through equipment designed and built in the 1940s that relied on wiring up boards (yes, dozens of pegs with colored wires) to “compress” or “rewrite” data from one dataset to another… ka-chunk, ka-chunk, ka-chunk….","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/yfx810rbikct5sqe', 'title': ""Why were many 20th century computer programs designed to store only the last two digits of the year (thus giving rise to the infamous Y2K problem)? Didn't people realise that it would cause confusion at the turn of the century?"", 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Because of this baby right here:\n\nGather round young ones… this is what a “database table” looked like from probably the 1930s to well into the 1980s (so, most of the 20th century): the 80 column punched card. Whether it was a census record, student record, property record, medical record, you name it - the information about it was crammed into 80 bytes. If you want to add the century to the dates you had to take two bytes (2.5%) out of something else.\n\nWhen others talk about memory limitations, which are valid, I think this was the limiting factor. Once the data was transferred to magnetic tape or disk drives a large majority of the programs written would be fine with 4 digit years (it’s not like we read the entire file into memory and chomped on it - it was read in one block at a time (with multiple records per block) or even one record at a time. But to get that information on to those tape or disk drives you, 90% of the time you went through this particular bottleneck.\n\nIt’s also worth noting that until tapes and disks were common, these were the physical manifestation of the data - and a LOT of it predated what you would consider computers. “Automatic Data Processing” machines, like card sorters, “programmable” reproducing punches, accounting tabulators, etc. were used to do all sorts of interesting “programs” on the data encoded in the holes of these cards. Literally rooms of file cabinets held this information and that legacy would take decades to break free from.\n\nI arrived on the scene in the mid 70’s and while I did most of my work on data that resided on disks and tapes, most of my programs were on these cards as well (thousands of them) and I did my fair share of running sets of data through equipment designed and built in the 1940s that relied on wiring up boards (yes, dozens of pegs with colored wires) to “compress” or “rewrite” data from one dataset to another… ka-chunk, ka-chunk, ka-chunk….', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 4, 'credits': 1995547, 'subscription': 0, 'content': 'Because of this baby right here:\n\nGather round young ones… this is what a “database table” looked like from probably the 1930s to well into the 1980s (so, most of the 20th century): the 80 column punched card. Whether it was a census record, student record, property record, medical record, you name it - the information about it was crammed into 80 bytes. If you want to add the century to the dates you had to take two bytes (2.5%) out of something else.\n\nWhen others talk about memory limitations, which are valid, I think this was the limiting factor. Once the data was transferred to magnetic tape or disk drives a large majority of the programs written would be fine with 4 digit years (it’s not like we read the entire file into memory and chomped on it - it was read in one block at a time (with multiple records per block) or even one record at a time. But to get that information on to those tape or disk drives you, 90% of the time you went through this particular bottleneck.\n\nIt’s also worth noting that until tapes and disks were common, these were the physical manifestation of the data - and a LOT of it predated what you would consider computers. “Automatic Data Processing” machines, like card sorters, “programmable” reproducing punches, accounting tabulators, etc. were used to do all sorts of interesting “programs” on the data encoded in the holes of these cards. Literally rooms of file cabinets held this information and that legacy would take decades to break free from.\n\nI arrived on the scene in the mid 70’s and while I did most of my work on data that resided on disks and tapes, most of my programs were on these cards as well (thousands of them) and I did my fair share of running sets of data through equipment designed and built in the 1940s that relied on wiring up boards (yes, dozens of pegs with colored wires) to “compress” or “rewrite” data from one dataset to another… ka-chunk, ka-chunk, ka-chunk….', 'aiModelVersion': '1'}",0.9998
Jerry Rufener,Updated 3y,How was the first programming language created without an operating system?,"Computers don’t necessarily have to have operating systems!!!!

An extremely popular computer was the PDP8 series of computers introduced in 1965 and a development the earlier PDP5. They were made by Digital Equipment Corporation (DEC) of Maynard, MA. They were THE minicomputer for many years. It had several OS’s available but were often sold/used without an OS - in fact without a disk. I have personally used them that way.

Here is a picture of one of the more popular versions, the PDP8/e. (By Florian Schäffer - Own work, CC BY-SA 4.0, File:Digital pdp8-e2.jpg
)

Notice the switches. To use it you sat in front of it and, using those switches, toggled in the RIM (Read In Mode) loader which was (as I recall) about 20 instructions long. You entered the 12 bit address using the switches, you pressed LOAD ADDR, you entered your first 12 bit instruction, you pressed DEP - that instruction was loaded at the specified memory address and address was incremented. You loaded your next instruction, you pressed DEP again … repeated until all the instructions were loaded. Now you entered the starting address of the loader, pressed LOAD ADDR and then pressed RUN. The RIM loader would execute. Its job was to load the BIN (Binary) loader, on paper tape, from your ASR-33 teletype (33 character per second). The BIN loaded would then load a program from a binary paper tape from the ASR-33.

If it was a cold start you probably read in editor - which allowed you to enter your source code at the, again, ASR-33. The editor would then print out a listing of your program on the ASR-33 and a paper tape of the source code. The source code was most likely in assembler. You then loaded the assembly program using - you guessed it - the ASR33.

The assembler read the source code from the paper tape using the ASR33. If all went well, the assembler would punch out a paper tape in object format. More likely - the first few times - you got errors and had to go back to the editor to fix them. You goal was to produce a paper tape with object code on it.

You would then load the linker and link all of the object modules you produced together - along with any libraries. This could produce more errors which sent you back several steps.

Once you had a clean binary, you loaded in using the BIN loader and the debugging fun began! No symbolic debug - you put debugging instructions into your own code.

Fun huh???

There were disks available - very expensive - very small. Also available was “drum” memory - it is like a disk but built like an Edison cylindrical phonograph. More common were tapes - both 7 and 9 track IBM compatible mag-tapes could be had - if you had the money. More common was something called DECtape - which was actually pretty slick and found on, I think, most PPD8’s. You could get an high speed paper tape reader (100 cps). You could get a ROM loader with RIM on it, but it took and entire slot in the computer, to save the toggling in process. Why didn’t everyone get these items? The computer alone cost $10,000 - for a base model. A disk would at least double the price. You can see where that goes.

Now if you were with a large organization they would have one machine with all the bells and whistles for the developer. They then distributed their application (and updates) on paper tape to machines in the field that were bare bones. I worked on one veneer processing system that was exactly that way. I was with DEC - the OEM (the company that developed the software) bought PDP8s from DEC - they built the electrical/electronic interface to control the veneer manufacturing equipment. They sold it to the end user along with the software.

While you could buy a computer directly from DEC, most were sold via the OEM route. They were installed and used without an OS.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ndlqk59csj7xt4a0', 'title': 'How was the first programming language created without an operating system?', 'score': {'original': 0.99245, 'ai': 0.00755}, 'blocks': [{'text': 'Computers don’t necessarily have to have operating systems!!!!\n\nAn extremely popular computer was the PDP8 series of computers introduced in 1965 and a development the earlier PDP5. They were made by Digital Equipment Corporation (DEC) of Maynard, MA. They were THE minicomputer for many years. It had several OS’s available but were often sold/used without an OS - in fact without a disk. I have personally used them that way.\n\nHere is a picture of one of the more popular versions, the PDP8/e. (By Florian Schäffer - Own work, CC BY-SA 4.0, File:Digital pdp8-e2.jpg\n)\n\nNotice the switches. To use it you sat in front of it and, using those switches, toggled in the RIM (Read In Mode) loader which was (as I recall) about 20 instructions long. You entered the 12 bit address using the switches, you pressed LOAD ADDR, you entered your first 12 bit instruction, you pressed DEP - that instruction was loaded at the specified memory address and address was incremented. You loaded your next instruction, you pressed DEP again … repeated until all the instructions were loaded. Now you entered the starting address of the loader, pressed LOAD ADDR and then pressed RUN. The RIM loader would execute. Its job was to load the BIN (Binary) loader, on paper tape, from your ASR-33 teletype (33 character per second). The BIN loaded would then load a program from a binary paper tape from the ASR-33.\n\nIf it was a cold start you probably read in editor - which allowed you to enter your source code at the, again, ASR-33. The editor would then print out a listing of your program on the ASR-33 and a paper tape of the source code. The source code was most likely in assembler. You then loaded the assembly program using - you guessed it - the ASR33.\n\nThe assembler read the source code from the paper tape using the ASR33. If all went well, the assembler would punch out a paper tape in object format. More likely - the first few times - you got errors and had to go back to the editor to fix them. You goal was to produce a paper tape with object code on it.\n\nYou would then load the linker and link all of the object modules you produced together - along with any libraries. This could produce more errors which sent you back several steps.\n\nOnce you had a clean binary, you loaded in using the BIN loader and the debugging fun began! No symbolic debug - you put debugging instructions into your own code.\n\nFun huh???\n\nThere were disks available - very expensive - very small. Also available was “drum” memory - it is like a disk but built like an Edison cylindrical phonograph. More common were tapes - both 7 and 9 track IBM compatible mag-tapes could be had - if you had the money. More common was something called DECtape - which was actually pretty slick and found on, I think, most PPD8’s. You could get an high speed paper tape reader (100 cps). You could get a ROM loader with RIM', 'result': {'fake': 0.0167, 'real': 0.9833}, 'status': 'success'}, {'text': 'on it, but it took and entire slot in the computer, to save the toggling in process. Why didn’t everyone get these items? The computer alone cost $10,000 - for a base model. A disk would at least double the price. You can see where that goes.\n\nNow if you were with a large organization they would have one machine with all the bells and whistles for the developer. They then distributed their application (and updates) on paper tape to machines in the field that were bare bones. I worked on one veneer processing system that was exactly that way. I was with DEC - the OEM (the company that developed the software) bought PDP8s from DEC - they built the electrical/electronic interface to control the veneer manufacturing equipment. They sold it to the end user along with the software.\n\nWhile you could buy a computer directly from DEC, most were sold via the OEM route. They were installed and used without an OS.', 'result': {'fake': 0.0717, 'real': 0.9283}, 'status': 'success'}], 'credits_used': 7, 'credits': 1995540, 'subscription': 0, 'content': 'Computers don’t necessarily have to have operating systems!!!!\n\nAn extremely popular computer was the PDP8 series of computers introduced in 1965 and a development the earlier PDP5. They were made by Digital Equipment Corporation (DEC) of Maynard, MA. They were THE minicomputer for many years. It had several OS’s available but were often sold/used without an OS - in fact without a disk. I have personally used them that way.\n\nHere is a picture of one of the more popular versions, the PDP8/e. (By Florian Schäffer - Own work, CC BY-SA 4.0, File:Digital pdp8-e2.jpg\n)\n\nNotice the switches. To use it you sat in front of it and, using those switches, toggled in the RIM (Read In Mode) loader which was (as I recall) about 20 instructions long. You entered the 12 bit address using the switches, you pressed LOAD ADDR, you entered your first 12 bit instruction, you pressed DEP - that instruction was loaded at the specified memory address and address was incremented. You loaded your next instruction, you pressed DEP again … repeated until all the instructions were loaded. Now you entered the starting address of the loader, pressed LOAD ADDR and then pressed RUN. The RIM loader would execute. Its job was to load the BIN (Binary) loader, on paper tape, from your ASR-33 teletype (33 character per second). The BIN loaded would then load a program from a binary paper tape from the ASR-33.\n\nIf it was a cold start you probably read in editor - which allowed you to enter your source code at the, again, ASR-33. The editor would then print out a listing of your program on the ASR-33 and a paper tape of the source code. The source code was most likely in assembler. You then loaded the assembly program using - you guessed it - the ASR33.\n\nThe assembler read the source code from the paper tape using the ASR33. If all went well, the assembler would punch out a paper tape in object format. More likely - the first few times - you got errors and had to go back to the editor to fix them. You goal was to produce a paper tape with object code on it.\n\nYou would then load the linker and link all of the object modules you produced together - along with any libraries. This could produce more errors which sent you back several steps.\n\nOnce you had a clean binary, you loaded in using the BIN loader and the debugging fun began! No symbolic debug - you put debugging instructions into your own code.\n\nFun huh???\n\nThere were disks available - very expensive - very small. Also available was “drum” memory - it is like a disk but built like an Edison cylindrical phonograph. More common were tapes - both 7 and 9 track IBM compatible mag-tapes could be had - if you had the money. More common was something called DECtape - which was actually pretty slick and found on, I think, most PPD8’s. You could get an high speed paper tape reader (100 cps). You could get a ROM loader with RIM on it, but it took and entire slot in the computer, to save the toggling in process. Why didn’t everyone get these items? The computer alone cost $10,000 - for a base model. A disk would at least double the price. You can see where that goes.\n\nNow if you were with a large organization they would have one machine with all the bells and whistles for the developer. They then distributed their application (and updates) on paper tape to machines in the field that were bare bones. I worked on one veneer processing system that was exactly that way. I was with DEC - the OEM (the company that developed the software) bought PDP8s from DEC - they built the electrical/electronic interface to control the veneer manufacturing equipment. They sold it to the end user along with the software.\n\nWhile you could buy a computer directly from DEC, most were sold via the OEM route. They were installed and used without an OS.', 'aiModelVersion': '1'}",0.99245
Stan Hanks,Updated 5y,Why did BSD almost disappear into oblivion with Linux's arrival? We don't seem to hear much about BSD and its variants as we hear about Linux. What could be the reasons for that?,"As one of the ""Mentally Contaminated""
 guys in Unix Systems Laboratories Inc. vs. Berkeley Software Design Inc.
 (and in one of those twists of fate, having been short-listed to run USL the year before), let me apply some color. There were really three prime drivers.

First, the law suit, and some background.

4.3bsd
 had a big problem. After deployment in 1986, it became clear that it was really the end of the line for the DEC
 VAX
. Bill Joy
 and others had departed to join Sun Microsystems
 in 1982, and while the BSD team continued to do amazing stuff, the battlefront shifted. The new best-and-highest use case was no longer a quarter-million dollar machine that occupied a couple hundred square feet of raised-floor conditioned space, it was this thing that sat on your desk, and gave you, an individual, all the power that had previously been shared with up to a couple hundred other people, for about $25k. 10-to-1 price compression, 100-to-1 performance increase. Heady stuff.

So, the development for 4.4 was initially re-focused to the Power 6/32 computer from Computer Consoles Inc.
 -- a decision soon abandoned, but not before the team split the machine dependent code from the machine independent code in the OS. No one had really done that before.

The resulting code, 4.3-Tahoe, from June 1988, only had limited distribution, but it made it possible for many, many other people to port 4.3 to other platforms. That was big. But what was huge was a decision to release the networking code -- none of which was covered under the AT&T
 source license -- in a special, separate release called Net/1 in June 1989.

That expanded when Keith Bostic
 suggested releasing not only the networking code, but ALL of the code that was not covered under the AT&T source license. That happened in the Net/2 release in June 1991.

Where Net/1 was just networking code, and required not only 4bsd knowledge but also knowledge of a target OS to be of use in creating a new TCP/IP stack, Net/2 was ALMOST a complete OS.

In fact, it is the immediate lineal predecessor of 386BSD
, and of BSD/OS
 from Berkeley Software Design
. 386BSD was short-lived, but its children, NetBSD
 and FreeBSD
 live on today.

BSD/OS, that's where the trouble started... See, William Jolitz
 really wasn't out to make a buck on 386BSD - he just wanted software that ran on $1000 machines versus $10000 machines. But the guys at BSDi, they were out for profit....

And that's where AT&T stepped in, and the lawsuit happened. Why? Well, it turns out that ALL of the BSDi guys were ""Mentally Contaminated"" - they'd all been up to their elbows in the actual AT&T licensed, proprietary, secret source code while at UCB. And all the fiddly bits they wrote to ""finish"" Net/2 and turn it into a real distribution? Yeah, they'd seen how AT&T did it in V6, V7, 32V, System III, System V... and it was really, really hard to prove that ""no, really that's the only way to do this"" was a valid argument against ""but you copied it from US (insert Darth Vader noises here)"".

And that lawsuit, by itself, completely shut down literally everyone working on anything based on Net/2 that had ever seen AT&T source. And that was literally just about everyone working in the space. If you even breathed ""UNIX sold here"" - unless you had the requisite AT&T Source License and the even more ridiculously expensive AT&T UNIX OEM License - you got an injunction. Bam. Done.

Which pretty much parked 4bsd based systems until the suit played out.

Second, while Bill Joy was a huge proponent of the work done at Berkeley, he was a much larger proponent of the work done at Sun, and in particular, better monetizing the opportunity. In 1986, discussions started on what became known at UNIX System V Release 4
. What wasn't widely known was that while it was a completely cooperative effort between Unix Systems Laboratories and Sun Microsystems, as part of the package, AT&T/USL took a stake in Sun, and as part of that, Bill personally sold stock to AT&T. I know that last bit because he was in my office, and used my copier to duplicate execution copies of the agreement, the day that transaction closed.

So, Sun's ship was now sailing in a different direction. It was no longer all-BSD, all the time; it was ""Yeah, well, SunOS
 was like friggin' awesome and all but there's this new Solaris
 thing...""

And with that, commercial 4BSD systems started to fade away... by 1993, Solaris was the default OS, and by 1995, support for SunOS officially ended. Moment of silence, please.

The third factor was our man, Linus Torvalds
. Linus saw UNIX for the first time in 1990, on a MicroVAX
 running Ultrix
 (also 4bsd based). I don't know why he didn't run screaming from the room, as the MicroVAX is one of those products that never, ever should have shipped, but he thought it was cool, and lead to his M. Sc. thesis ""Linux
: A Portable Operating System"".

Now, because Ultrix was a binary-only system (unless you not only had AT&T Source Licenses, but the even harder to get DEC Source License), and because UNIX of all sorts was export-restricted during that time frame, there was zero chance that Linus has seen the UNIX sources. There's some chance that he might have tripped across Lions' Commentary on UNIX 6th Edition, with Source Code
, but even that was a stretch - and not clear that, as it was published and widely available, would count in an intellectual property dispute.

The appetite was there in the market: with Intel processors nearing original VAX performance, people wanted some kind of UNIX on their desks, and didn't want to pay Sun (or competitor) prices.

So, there you have it: AT&T sued the bejezus out of the people trying to make affordable BSD happen, Sun moved away from BSD to further cement their deal with AT&T, and Linus just happened to be in the right place at the right time...

The end result: Linux, being essentially un-restricted and outside the scope of the AT&T suit, was freely available. The BSD twins (FreeBSD and NetBSD) were grounded for the duration, as was BSDi (OpenBSD wouldn't appear until after the suit). And thousands and thousands of people wanted something...","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/l7apbk2e3srufxvj', 'title': ""Why did BSD almost disappear into oblivion with Linux's arrival? We don't seem to hear much about BSD and its variants as we hear about Linux. What could be the reasons for that?"", 'score': {'original': 0.927, 'ai': 0.073}, 'blocks': [{'text': 'As one of the ""Mentally Contaminated""\n guys in Unix Systems Laboratories Inc. vs. Berkeley Software Design Inc.\n (and in one of those twists of fate, having been short-listed to run USL the year before), let me apply some color. There were really three prime drivers.\n\nFirst, the law suit, and some background.\n\n4.3bsd\n had a big problem. After deployment in 1986, it became clear that it was really the end of the line for the DEC\n VAX\n. Bill Joy\n and others had departed to join Sun Microsystems\n in 1982, and while the BSD team continued to do amazing stuff, the battlefront shifted. The new best-and-highest use case was no longer a quarter-million dollar machine that occupied a couple hundred square feet of raised-floor conditioned space, it was this thing that sat on your desk, and gave you, an individual, all the power that had previously been shared with up to a couple hundred other people, for about $25k. 10-to-1 price compression, 100-to-1 performance increase. Heady stuff.\n\nSo, the development for 4.4 was initially re-focused to the Power 6/32 computer from Computer Consoles Inc.\n -- a decision soon abandoned, but not before the team split the machine dependent code from the machine independent code in the OS. No one had really done that before.\n\nThe resulting code, 4.3-Tahoe, from June 1988, only had limited distribution, but it made it possible for many, many other people to port 4.3 to other platforms. That was big. But what was huge was a decision to release the networking code -- none of which was covered under the AT&T\n source license -- in a special, separate release called Net/1 in June 1989.\n\nThat expanded when Keith Bostic\n suggested releasing not only the networking code, but ALL of the code that was not covered under the AT&T source license. That happened in the Net/2 release in June 1991.\n\nWhere Net/1 was just networking code, and required not only 4bsd knowledge but also knowledge of a target OS to be of use in creating a new TCP/IP stack, Net/2 was ALMOST a complete OS.\n\nIn fact, it is the immediate lineal predecessor of 386BSD\n, and of BSD/OS\n from Berkeley Software Design\n. 386BSD was short-lived, but its children, NetBSD\n and FreeBSD\n live on today.\n\nBSD/OS, that\'s where the trouble started... See, William Jolitz\n really wasn\'t out to make a buck on 386BSD - he just wanted software that ran on $1000 machines versus $10000 machines. But the guys at BSDi, they were out for profit....\n\nAnd that\'s where AT&T stepped in, and the lawsuit happened. Why? Well, it turns out that ALL of the BSDi guys were ""Mentally Contaminated"" - they\'d all been up to their elbows in the actual AT&T licensed, proprietary, secret source code while at UCB. And all the fiddly bits they wrote to ""finish"" Net/2 and turn it into a real distribution? Yeah, they\'d seen how AT&T did it in V6, V7, 32V, System III, System V... and it was really, really hard to prove that ""no, really that\'s the only way to do this"" was a valid argument against ""but you copied it from', 'result': {'fake': 0.1225, 'real': 0.8775}, 'status': 'success'}, {'text': 'US (insert Darth Vader noises here)"".\n\nAnd that lawsuit, by itself, completely shut down literally everyone working on anything based on Net/2 that had ever seen AT&T source. And that was literally just about everyone working in the space. If you even breathed ""UNIX sold here"" - unless you had the requisite AT&T Source License and the even more ridiculously expensive AT&T UNIX OEM License - you got an injunction. Bam. Done.\n\nWhich pretty much parked 4bsd based systems until the suit played out.\n\nSecond, while Bill Joy was a huge proponent of the work done at Berkeley, he was a much larger proponent of the work done at Sun, and in particular, better monetizing the opportunity. In 1986, discussions started on what became known at UNIX System V Release 4\n. What wasn\'t widely known was that while it was a completely cooperative effort between Unix Systems Laboratories and Sun Microsystems, as part of the package, AT&T/USL took a stake in Sun, and as part of that, Bill personally sold stock to AT&T. I know that last bit because he was in my office, and used my copier to duplicate execution copies of the agreement, the day that transaction closed.\n\nSo, Sun\'s ship was now sailing in a different direction. It was no longer all-BSD, all the time; it was ""Yeah, well, SunOS\n was like friggin\' awesome and all but there\'s this new Solaris\n thing...""\n\nAnd with that, commercial 4BSD systems started to fade away... by 1993, Solaris was the default OS, and by 1995, support for SunOS officially ended. Moment of silence, please.\n\nThe third factor was our man, Linus Torvalds\n. Linus saw UNIX for the first time in 1990, on a MicroVAX\n running Ultrix\n (also 4bsd based). I don\'t know why he didn\'t run screaming from the room, as the MicroVAX is one of those products that never, ever should have shipped, but he thought it was cool, and lead to his M. Sc. thesis ""Linux\n: A Portable Operating System"".\n\nNow, because Ultrix was a binary-only system (unless you not only had AT&T Source Licenses, but the even harder to get DEC Source License), and because UNIX of all sorts was export-restricted during that time frame, there was zero chance that Linus has seen the UNIX sources. There\'s some chance that he might have tripped across Lions\' Commentary on UNIX 6th Edition, with Source Code\n, but even that was a stretch - and not clear that, as it was published and widely available, would count in an intellectual property dispute.\n\nThe appetite was there in the market: with Intel processors nearing original VAX performance, people wanted some kind of UNIX on their desks, and didn\'t want to pay Sun (or competitor) prices.\n\nSo, there you have it: AT&T sued the bejezus out of the people trying to make affordable BSD happen, Sun moved away from BSD to further cement their deal with AT&T, and Linus just happened to be in the right place at the right time...\n\nThe end result: Linux, being essentially un-restricted and outside the scope of the AT&T suit, was freely available. The BSD twins (FreeBSD and', 'result': {'fake': 0.3845, 'real': 0.6155}, 'status': 'success'}, {'text': ""NetBSD) were grounded for the duration, as was BSDi (OpenBSD wouldn't appear until after the suit). And thousands and thousands of people wanted something..."", 'result': {'fake': 0.2062, 'real': 0.7938}, 'status': 'success'}], 'credits_used': 11, 'credits': 1995529, 'subscription': 0, 'content': 'As one of the ""Mentally Contaminated""\n guys in Unix Systems Laboratories Inc. vs. Berkeley Software Design Inc.\n (and in one of those twists of fate, having been short-listed to run USL the year before), let me apply some color. There were really three prime drivers.\n\nFirst, the law suit, and some background.\n\n4.3bsd\n had a big problem. After deployment in 1986, it became clear that it was really the end of the line for the DEC\n VAX\n. Bill Joy\n and others had departed to join Sun Microsystems\n in 1982, and while the BSD team continued to do amazing stuff, the battlefront shifted. The new best-and-highest use case was no longer a quarter-million dollar machine that occupied a couple hundred square feet of raised-floor conditioned space, it was this thing that sat on your desk, and gave you, an individual, all the power that had previously been shared with up to a couple hundred other people, for about $25k. 10-to-1 price compression, 100-to-1 performance increase. Heady stuff.\n\nSo, the development for 4.4 was initially re-focused to the Power 6/32 computer from Computer Consoles Inc.\n -- a decision soon abandoned, but not before the team split the machine dependent code from the machine independent code in the OS. No one had really done that before.\n\nThe resulting code, 4.3-Tahoe, from June 1988, only had limited distribution, but it made it possible for many, many other people to port 4.3 to other platforms. That was big. But what was huge was a decision to release the networking code -- none of which was covered under the AT&T\n source license -- in a special, separate release called Net/1 in June 1989.\n\nThat expanded when Keith Bostic\n suggested releasing not only the networking code, but ALL of the code that was not covered under the AT&T source license. That happened in the Net/2 release in June 1991.\n\nWhere Net/1 was just networking code, and required not only 4bsd knowledge but also knowledge of a target OS to be of use in creating a new TCP/IP stack, Net/2 was ALMOST a complete OS.\n\nIn fact, it is the immediate lineal predecessor of 386BSD\n, and of BSD/OS\n from Berkeley Software Design\n. 386BSD was short-lived, but its children, NetBSD\n and FreeBSD\n live on today.\n\nBSD/OS, that\'s where the trouble started... See, William Jolitz\n really wasn\'t out to make a buck on 386BSD - he just wanted software that ran on $1000 machines versus $10000 machines. But the guys at BSDi, they were out for profit....\n\nAnd that\'s where AT&T stepped in, and the lawsuit happened. Why? Well, it turns out that ALL of the BSDi guys were ""Mentally Contaminated"" - they\'d all been up to their elbows in the actual AT&T licensed, proprietary, secret source code while at UCB. And all the fiddly bits they wrote to ""finish"" Net/2 and turn it into a real distribution? Yeah, they\'d seen how AT&T did it in V6, V7, 32V, System III, System V... and it was really, really hard to prove that ""no, really that\'s the only way to do this"" was a valid argument against ""but you copied it from US (insert Darth Vader noises here)"".\n\nAnd that lawsuit, by itself, completely shut down literally everyone working on anything based on Net/2 that had ever seen AT&T source. And that was literally just about everyone working in the space. If you even breathed ""UNIX sold here"" - unless you had the requisite AT&T Source License and the even more ridiculously expensive AT&T UNIX OEM License - you got an injunction. Bam. Done.\n\nWhich pretty much parked 4bsd based systems until the suit played out.\n\nSecond, while Bill Joy was a huge proponent of the work done at Berkeley, he was a much larger proponent of the work done at Sun, and in particular, better monetizing the opportunity. In 1986, discussions started on what became known at UNIX System V Release 4\n. What wasn\'t widely known was that while it was a completely cooperative effort between Unix Systems Laboratories and Sun Microsystems, as part of the package, AT&T/USL took a stake in Sun, and as part of that, Bill personally sold stock to AT&T. I know that last bit because he was in my office, and used my copier to duplicate execution copies of the agreement, the day that transaction closed.\n\nSo, Sun\'s ship was now sailing in a different direction. It was no longer all-BSD, all the time; it was ""Yeah, well, SunOS\n was like friggin\' awesome and all but there\'s this new Solaris\n thing...""\n\nAnd with that, commercial 4BSD systems started to fade away... by 1993, Solaris was the default OS, and by 1995, support for SunOS officially ended. Moment of silence, please.\n\nThe third factor was our man, Linus Torvalds\n. Linus saw UNIX for the first time in 1990, on a MicroVAX\n running Ultrix\n (also 4bsd based). I don\'t know why he didn\'t run screaming from the room, as the MicroVAX is one of those products that never, ever should have shipped, but he thought it was cool, and lead to his M. Sc. thesis ""Linux\n: A Portable Operating System"".\n\nNow, because Ultrix was a binary-only system (unless you not only had AT&T Source Licenses, but the even harder to get DEC Source License), and because UNIX of all sorts was export-restricted during that time frame, there was zero chance that Linus has seen the UNIX sources. There\'s some chance that he might have tripped across Lions\' Commentary on UNIX 6th Edition, with Source Code\n, but even that was a stretch - and not clear that, as it was published and widely available, would count in an intellectual property dispute.\n\nThe appetite was there in the market: with Intel processors nearing original VAX performance, people wanted some kind of UNIX on their desks, and didn\'t want to pay Sun (or competitor) prices.\n\nSo, there you have it: AT&T sued the bejezus out of the people trying to make affordable BSD happen, Sun moved away from BSD to further cement their deal with AT&T, and Linus just happened to be in the right place at the right time...\n\nThe end result: Linux, being essentially un-restricted and outside the scope of the AT&T suit, was freely available. The BSD twins (FreeBSD and NetBSD) were grounded for the duration, as was BSDi (OpenBSD wouldn\'t appear until after the suit). And thousands and thousands of people wanted something...', 'aiModelVersion': '1'}",0.927
Martin La Grange,Updated 4y,Why were old games programmed in assembly when higher level languages existed?,"Hello there,

There is a yet top-notch flight simulation which was hand-coded in 32-bit Intel i386 Assembly language, and has a number of pilots flying it even yet - including me.

It’s this :

Su-27 Flanker for Windows 95

Don’t let its age stop you - this is a crisp jet flight combat simulator with detail enough to be fascinating even now - and its story contains a twist or two that should demonstrate why assembly coding a game remains an admired high art, even into the present day.

Why was it coded in Assembly language? Well, after you see the features list, the reasons make the application - on modernish hardware and software - admirable.

Modelled Features - all good btw.

Comprehensive, fluid-dynamic modelling of the Su-27 for a very accurate simulation of flight and physics. Your Flanker even pulls contrails, and gas-flow dynamics for accurate stall, turn and flight simulation are delivered in real-time - quite an achievement for a Pentium-1 class machine for which the software was designed.
Equally detailed modelling of other aircraft, ships, ground units, SAMs. All flight surfaces move, the wheel steering works - in short, all possible detail is modelled with dramatic accuracy.
The operational area, the Crimean peninsula, is modelled down to the level of powerlines and cars on the road.
Lighting is dynamic, for different times of day, and year. Clouds, while basic, are compelling to look at and fly.
Real time Goraud-shaded polygonal graphics, which while ancient today, remain crisp and rapid to render on even the most basic hardware back then.
The cockpit features a set of instruments which work with the detail of the real thing.
Most important of all - there is NO lag, and NO impact to speed when running possibly hundreds of objects simultaneously! It’s full speed all the way - and to the sim pilot this is endearing and impressive.

Now for the twists :

Because it is 32-bit Assembly coded, the entire program is TINY - only 12 Megabytes in total ! This is dramatic, insofar as all its competitors were at the time huge - the smallest near-equivalent was literally 10 times the size, disk wise!
Its origin should give you pause - the reason that SAM and Weapon performance is so incredibly accurate is that it began life as a Weapons Simulation Trainer for the Russian Air Force! Yup, this software is a piece of Surplus MILITARY Software!

And entire Su-27 v1.5 installation occupies a princely 30 megabytes of disk space - over half of which is the manual! Then, the mission files themselves (and they can be very dense in terms of units etc.) are kilobytes in size, thanks again to them being encoded in an assembly-language compatible format for processing.

Finally - I can and do run it seamlessly on my Windows 10 PC, as the application is directly addressing the CPU in i386 mode, for all the performance it needs.

It is, to the best of my knowledge, one of the last hand-coded games made in Assembly Language, and one of the most impressive, as it clearly utilising this program stategy for maximum performance and impact.

Its legacy is with us yet in the form of Lock-On: Modern Air Combat, and of course DCS World - which are its lineal descendants.

My 2c worth, and kind regards.

Addendum -

For all those interested, it is possible today to get hold of the whole shooting match of Su-27 releases as hosted on the Archive.

The image hosted is of the CD-ROM containing the videos, missions , etc, as well as versions 1.1 and 1.2 that were released prior to the definitive v 1.5 Squadron Commander’s Edition.

The disc image is 237MB big - big enough to fit on a CD-ROM, of course.

That said, as noted previously - most of this is video in AVI file format, the graphically rich manual (which is big), as well as all sorts of extras. The core simulation is - dinky at 12MB (Sim + mission files)

Feel free to get it here : Su-27 Flanker : Free Download, Borrow, and Streaming : Internet Archive","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0gmj6a2fk45sc983', 'title': 'Why were old games programmed in assembly when higher level languages existed?', 'score': {'original': 0.7571, 'ai': 0.2429}, 'blocks': [{'text': 'Hello there,\n\nThere is a yet top-notch flight simulation which was hand-coded in 32-bit Intel i386 Assembly language, and has a number of pilots flying it even yet - including me.\n\nIt’s this :\n\nSu-27 Flanker for Windows 95\n\nDon’t let its age stop you - this is a crisp jet flight combat simulator with detail enough to be fascinating even now - and its story contains a twist or two that should demonstrate why assembly coding a game remains an admired high art, even into the present day.\n\nWhy was it coded in Assembly language? Well, after you see the features list, the reasons make the application - on modernish hardware and software - admirable.\n\nModelled Features - all good btw.\n\nComprehensive, fluid-dynamic modelling of the Su-27 for a very accurate simulation of flight and physics. Your Flanker even pulls contrails, and gas-flow dynamics for accurate stall, turn and flight simulation are delivered in real-time - quite an achievement for a Pentium-1 class machine for which the software was designed.\nEqually detailed modelling of other aircraft, ships, ground units, SAMs. All flight surfaces move, the wheel steering works - in short, all possible detail is modelled with dramatic accuracy.\nThe operational area, the Crimean peninsula, is modelled down to the level of powerlines and cars on the road.\nLighting is dynamic, for different times of day, and year. Clouds, while basic, are compelling to look at and fly.\nReal time Goraud-shaded polygonal graphics, which while ancient today, remain crisp and rapid to render on even the most basic hardware back then.\nThe cockpit features a set of instruments which work with the detail of the real thing.\nMost important of all - there is NO lag, and NO impact to speed when running possibly hundreds of objects simultaneously! It’s full speed all the way - and to the sim pilot this is endearing and impressive.\n\nNow for the twists :\n\nBecause it is 32-bit Assembly coded, the entire program is TINY - only 12 Megabytes in total ! This is dramatic, insofar as all its competitors were at the time huge - the smallest near-equivalent was literally 10 times the size, disk wise!\nIts origin should give you pause - the reason that SAM and Weapon performance is so incredibly accurate is that it began life as a Weapons Simulation Trainer for the Russian Air Force! Yup, this software is a piece of Surplus MILITARY Software!\n\nAnd entire Su-27 v1.5 installation occupies a princely 30 megabytes of disk space - over half of which is the manual! Then, the mission files themselves (and they can be very dense in terms of units etc.) are kilobytes in size, thanks again to them being encoded in an assembly-language compatible format for processing.\n\nFinally - I can and do run it seamlessly on my Windows 10 PC, as the application is directly addressing the CPU in i386 mode, for all the performance it needs.\n\nIt is, to the best of my knowledge, one of the last hand-coded games made in Assembly Language, and one of the most impressive, as it clearly utilising this program stategy for maximum performance and impact.\n\nIts legacy is with', 'result': {'fake': 0.2226, 'real': 0.7774}, 'status': 'success'}, {'text': 'us yet in the form of Lock-On: Modern Air Combat, and of course DCS World - which are its lineal descendants.\n\nMy 2c worth, and kind regards.\n\nAddendum -\n\nFor all those interested, it is possible today to get hold of the whole shooting match of Su-27 releases as hosted on the Archive.\n\nThe image hosted is of the CD-ROM containing the videos, missions , etc, as well as versions 1.1 and 1.2 that were released prior to the definitive v 1.5 Squadron Commander’s Edition.\n\nThe disc image is 237MB big - big enough to fit on a CD-ROM, of course.\n\nThat said, as noted previously - most of this is video in AVI file format, the graphically rich manual (which is big), as well as all sorts of extras. The core simulation is - dinky at 12MB (Sim + mission files)\n\nFeel free to get it here : Su-27 Flanker : Free Download, Borrow, and Streaming : Internet Archive', 'result': {'fake': 0.2924, 'real': 0.7076}, 'status': 'success'}], 'credits_used': 7, 'credits': 1995522, 'subscription': 0, 'content': 'Hello there,\n\nThere is a yet top-notch flight simulation which was hand-coded in 32-bit Intel i386 Assembly language, and has a number of pilots flying it even yet - including me.\n\nIt’s this :\n\nSu-27 Flanker for Windows 95\n\nDon’t let its age stop you - this is a crisp jet flight combat simulator with detail enough to be fascinating even now - and its story contains a twist or two that should demonstrate why assembly coding a game remains an admired high art, even into the present day.\n\nWhy was it coded in Assembly language? Well, after you see the features list, the reasons make the application - on modernish hardware and software - admirable.\n\nModelled Features - all good btw.\n\nComprehensive, fluid-dynamic modelling of the Su-27 for a very accurate simulation of flight and physics. Your Flanker even pulls contrails, and gas-flow dynamics for accurate stall, turn and flight simulation are delivered in real-time - quite an achievement for a Pentium-1 class machine for which the software was designed.\nEqually detailed modelling of other aircraft, ships, ground units, SAMs. All flight surfaces move, the wheel steering works - in short, all possible detail is modelled with dramatic accuracy.\nThe operational area, the Crimean peninsula, is modelled down to the level of powerlines and cars on the road.\nLighting is dynamic, for different times of day, and year. Clouds, while basic, are compelling to look at and fly.\nReal time Goraud-shaded polygonal graphics, which while ancient today, remain crisp and rapid to render on even the most basic hardware back then.\nThe cockpit features a set of instruments which work with the detail of the real thing.\nMost important of all - there is NO lag, and NO impact to speed when running possibly hundreds of objects simultaneously! It’s full speed all the way - and to the sim pilot this is endearing and impressive.\n\nNow for the twists :\n\nBecause it is 32-bit Assembly coded, the entire program is TINY - only 12 Megabytes in total ! This is dramatic, insofar as all its competitors were at the time huge - the smallest near-equivalent was literally 10 times the size, disk wise!\nIts origin should give you pause - the reason that SAM and Weapon performance is so incredibly accurate is that it began life as a Weapons Simulation Trainer for the Russian Air Force! Yup, this software is a piece of Surplus MILITARY Software!\n\nAnd entire Su-27 v1.5 installation occupies a princely 30 megabytes of disk space - over half of which is the manual! Then, the mission files themselves (and they can be very dense in terms of units etc.) are kilobytes in size, thanks again to them being encoded in an assembly-language compatible format for processing.\n\nFinally - I can and do run it seamlessly on my Windows 10 PC, as the application is directly addressing the CPU in i386 mode, for all the performance it needs.\n\nIt is, to the best of my knowledge, one of the last hand-coded games made in Assembly Language, and one of the most impressive, as it clearly utilising this program stategy for maximum performance and impact.\n\nIts legacy is with us yet in the form of Lock-On: Modern Air Combat, and of course DCS World - which are its lineal descendants.\n\nMy 2c worth, and kind regards.\n\nAddendum -\n\nFor all those interested, it is possible today to get hold of the whole shooting match of Su-27 releases as hosted on the Archive.\n\nThe image hosted is of the CD-ROM containing the videos, missions , etc, as well as versions 1.1 and 1.2 that were released prior to the definitive v 1.5 Squadron Commander’s Edition.\n\nThe disc image is 237MB big - big enough to fit on a CD-ROM, of course.\n\nThat said, as noted previously - most of this is video in AVI file format, the graphically rich manual (which is big), as well as all sorts of extras. The core simulation is - dinky at 12MB (Sim + mission files)\n\nFeel free to get it here : Su-27 Flanker : Free Download, Borrow, and Streaming : Internet Archive', 'aiModelVersion': '1'}",0.7571
Sean Kernan,4y,What were the most laughably large first iterations of technology?,"This is the first VCR:

(sidenote: why did dudes pull their pants up so high back then?)

It required a team of engineers, and a small fortune to build. You could have one of these if you wanted. But it would run you $50,000.

In 1956 dollars. You do the math.

With most technology, particularly when scanning the last 50 years to now, you typically see way oversized hardware, that gra...

Access this answer and support the author as a Quora+ subscriber
Access all answers reserved by 
Sean Kernan
 for Quora+ subscribers
Access exclusive answers from thousands more participating creators in Quora+
Browse ad‑free and support creators
Start free trial
Learn more","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6cpgt8w5fsmnjlvx', 'title': 'What were the most laughably large first iterations of technology?', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'This is the first VCR:\n\n(sidenote: why did dudes pull their pants up so high back then?)\n\nIt required a team of engineers, and a small fortune to build. You could have one of these if you wanted. But it would run you $50,000.\n\nIn 1956 dollars. You do the math.\n\nWith most technology, particularly when scanning the last 50 years to now, you typically see way oversized hardware, that gra...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nSean Kernan\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995520, 'subscription': 0, 'content': 'This is the first VCR:\n\n(sidenote: why did dudes pull their pants up so high back then?)\n\nIt required a team of engineers, and a small fortune to build. You could have one of these if you wanted. But it would run you $50,000.\n\nIn 1956 dollars. You do the math.\n\nWith most technology, particularly when scanning the last 50 years to now, you typically see way oversized hardware, that gra...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nSean Kernan\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'aiModelVersion': '1'}",0.9996
Fred Mitchell,Updated 4y,Why was the Amiga Computer a failure when spec wise it was way beyond anything else that was out at the time?,"Since I was there, and I saw this up-front and in the raw, I can tell you precisely why the Amiga failed.

Upper Management.

We engineers had big plans for the Amiga, like 24-bit graphics with a blitter per bitplane, a DSP, and other goodies that we dubbed with the code name: AAA.

We were working on this AAA chip set, which would’ve taken the Amiga to a whole new plane of existence for its time, but upper management smacked it down and ended development of that, and told us to do something “simpler”. And so we did — code name? AA.

Even the AA chipset didn’t suck too badly, and was still ahead of what was available at the time. But technology was never an issue for the Amiga. It was poor marketing decisions.

For example, Commodore once had a golden opportunity of getting the Amiga into Sears. Commodore “marketing” assed that up. Why? Sears wanted to list the Amiga in their catalogue, like they do all their products, but Commodore Marketing claimed it would make the Amiga look like “a toy”, and so forbade it.

As well, Sears didn’t need the Amiga as much as the Amiga needed Sears, and so it was dropped.

This is one of the biggest blunders Commodore made, in a long list of blunders that may have saved the company. Had the Amiga been introduced into Sears, that would’ve placed the Amiga in front of millions of people that never even heard of the Amiga before. It would’ve been there on display for them to play with and be amazed. And so the Amiga would’ve flown off the shelves. And Commodore would’ve lived to see another day.

Irving Gould drove Commodore into the ground. See: Irving Gould - The Money Man
.

I had a real passion for the Amiga, and it was the one thing that liberated me from Windows 3.11 and Intel tech of the time.

Right around the time Commodore closed their doors, Windows NT came out, and I jumped on that. I was hoping to get into Unix development, but the one drive I had Unix — AmigaUNIX! — on, died. Yes, We had ported SVR5 Unix to the Amiga, which was also killed by upper management. This was a bonehead move, because at the time, students needing Unix machines for college could afford the much-cheaper Amiga than the other options available at the time — and said students would’ve carried the Amiga right into the business arena once they graduated and move on — you know, the same thing that happened with Apple.

Oh, that’s another story, because Commodore had the educational market at one point with their Commodore PET computer. They almost literally handed that market to Apple, and we all know what happened after that.

It’s scary to think where Commodore would be today had it not made such bone-headed decisions. It would probably be where Apple is today, and Apple would’ve become the footnote in history.

I have fond memories of the Amiga and its impressive graphics. And today, I have the RTX 2070 in my desktop computer. Yes, it can do real-time Ray Tracing. Back in my Commodore Amiga days, ray tracing was all the rage — but it generally took an hour or more to render a single ray-traced still for the smallish monitors of the time — 640x400. Now I have this beast that can do it in real-time to a UHD / 4K display. Who knows? Maybe they would be where Nvidia is today. Who knows?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/8hj42mqe9lr6kpvu', 'title': 'Why was the Amiga Computer a failure when spec wise it was way beyond anything else that was out at the time?', 'score': {'original': 0.98065, 'ai': 0.01935}, 'blocks': [{'text': 'Since I was there, and I saw this up-front and in the raw, I can tell you precisely why the Amiga failed.\n\nUpper Management.\n\nWe engineers had big plans for the Amiga, like 24-bit graphics with a blitter per bitplane, a DSP, and other goodies that we dubbed with the code name: AAA.\n\nWe were working on this AAA chip set, which would’ve taken the Amiga to a whole new plane of existence for its time, but upper management smacked it down and ended development of that, and told us to do something “simpler”. And so we did — code name? AA.\n\nEven the AA chipset didn’t suck too badly, and was still ahead of what was available at the time. But technology was never an issue for the Amiga. It was poor marketing decisions.\n\nFor example, Commodore once had a golden opportunity of getting the Amiga into Sears. Commodore “marketing” assed that up. Why? Sears wanted to list the Amiga in their catalogue, like they do all their products, but Commodore Marketing claimed it would make the Amiga look like “a toy”, and so forbade it.\n\nAs well, Sears didn’t need the Amiga as much as the Amiga needed Sears, and so it was dropped.\n\nThis is one of the biggest blunders Commodore made, in a long list of blunders that may have saved the company. Had the Amiga been introduced into Sears, that would’ve placed the Amiga in front of millions of people that never even heard of the Amiga before. It would’ve been there on display for them to play with and be amazed. And so the Amiga would’ve flown off the shelves. And Commodore would’ve lived to see another day.\n\nIrving Gould drove Commodore into the ground. See: Irving Gould - The Money Man\n.\n\nI had a real passion for the Amiga, and it was the one thing that liberated me from Windows 3.11 and Intel tech of the time.\n\nRight around the time Commodore closed their doors, Windows NT came out, and I jumped on that. I was hoping to get into Unix development, but the one drive I had Unix — AmigaUNIX! — on, died. Yes, We had ported SVR5 Unix to the Amiga, which was also killed by upper management. This was a bonehead move, because at the time, students needing Unix machines for college could afford the much-cheaper Amiga than the other options available at the time — and said students would’ve carried the Amiga right into the business arena once they graduated and move on — you know, the same thing that happened with Apple.\n\nOh, that’s another story, because Commodore had the educational market at one point with their Commodore PET computer. They almost literally handed that market to Apple, and we all know what happened after that.\n\nIt’s scary to think where Commodore would be today had it not made such bone-headed decisions. It would probably be where Apple is today, and Apple would’ve become the footnote in history.\n\nI have fond memories of the Amiga and its impressive graphics. And today, I have the RTX 2070 in my desktop computer. Yes, it can', 'result': {'fake': 0.0105, 'real': 0.9895}, 'status': 'success'}, {'text': 'do real-time Ray Tracing. Back in my Commodore Amiga days, ray tracing was all the rage — but it generally took an hour or more to render a single ray-traced still for the smallish monitors of the time — 640x400. Now I have this beast that can do it in real-time to a UHD / 4K display. Who knows? Maybe they would be where Nvidia is today. Who knows?', 'result': {'fake': 0.2166, 'real': 0.7834}, 'status': 'success'}], 'credits_used': 6, 'credits': 1995514, 'subscription': 0, 'content': 'Since I was there, and I saw this up-front and in the raw, I can tell you precisely why the Amiga failed.\n\nUpper Management.\n\nWe engineers had big plans for the Amiga, like 24-bit graphics with a blitter per bitplane, a DSP, and other goodies that we dubbed with the code name: AAA.\n\nWe were working on this AAA chip set, which would’ve taken the Amiga to a whole new plane of existence for its time, but upper management smacked it down and ended development of that, and told us to do something “simpler”. And so we did — code name? AA.\n\nEven the AA chipset didn’t suck too badly, and was still ahead of what was available at the time. But technology was never an issue for the Amiga. It was poor marketing decisions.\n\nFor example, Commodore once had a golden opportunity of getting the Amiga into Sears. Commodore “marketing” assed that up. Why? Sears wanted to list the Amiga in their catalogue, like they do all their products, but Commodore Marketing claimed it would make the Amiga look like “a toy”, and so forbade it.\n\nAs well, Sears didn’t need the Amiga as much as the Amiga needed Sears, and so it was dropped.\n\nThis is one of the biggest blunders Commodore made, in a long list of blunders that may have saved the company. Had the Amiga been introduced into Sears, that would’ve placed the Amiga in front of millions of people that never even heard of the Amiga before. It would’ve been there on display for them to play with and be amazed. And so the Amiga would’ve flown off the shelves. And Commodore would’ve lived to see another day.\n\nIrving Gould drove Commodore into the ground. See: Irving Gould - The Money Man\n.\n\nI had a real passion for the Amiga, and it was the one thing that liberated me from Windows 3.11 and Intel tech of the time.\n\nRight around the time Commodore closed their doors, Windows NT came out, and I jumped on that. I was hoping to get into Unix development, but the one drive I had Unix — AmigaUNIX! — on, died. Yes, We had ported SVR5 Unix to the Amiga, which was also killed by upper management. This was a bonehead move, because at the time, students needing Unix machines for college could afford the much-cheaper Amiga than the other options available at the time — and said students would’ve carried the Amiga right into the business arena once they graduated and move on — you know, the same thing that happened with Apple.\n\nOh, that’s another story, because Commodore had the educational market at one point with their Commodore PET computer. They almost literally handed that market to Apple, and we all know what happened after that.\n\nIt’s scary to think where Commodore would be today had it not made such bone-headed decisions. It would probably be where Apple is today, and Apple would’ve become the footnote in history.\n\nI have fond memories of the Amiga and its impressive graphics. And today, I have the RTX 2070 in my desktop computer. Yes, it can do real-time Ray Tracing. Back in my Commodore Amiga days, ray tracing was all the rage — but it generally took an hour or more to render a single ray-traced still for the smallish monitors of the time — 640x400. Now I have this beast that can do it in real-time to a UHD / 4K display. Who knows? Maybe they would be where Nvidia is today. Who knows?', 'aiModelVersion': '1'}",0.98065
Alan Kay,Updated 4y,How was the Xerox Alto done in only 3 months?,"Basically: Chuck Thacker was simply amazing.

I recounted some of the history of the Alto in “The Early History of Smalltalk”
, and both Butler Lampson and Chuck also wrote histories of that time that included accounts of the Alto development (all can be found via the ACM, and I think they are online also).

It was part of the ARPA research M.O. to “compute in the future rather than in the present” by either using existing supercomputers, or by making a kind of supercomputer. There was enough science and engineering substantiation to allow a reasonable faith that the supercomputers of the present would be the commodity computers of the future. Beside needing to try out ideas and designs in many areas, there was also the brute fact that inventing and implementing new software ideas took quite a while to do — with really big dreams, a 10–15 year horizon was needed.

I’ve written elsewhere in Quora a few things about the Alto, so I won’t repeat here.

Was the Xerox Alto a prototype or a finished product?

To what extent did the Xerox PARC Alto inspire the Apple Macintosh?

I think I might have also written about “MAXC”, the emulated PDP-10 that was the first project done by the computer researchers at Parc (in 1971). Xerox wouldn’t allow us to buy one (it was in the process of becoming the standard TSS computer on the ARPAnet) — instead they wanted us to use a Xerox Data Systems computer — so everyone almost walked out right at the beginning.

We had in Butler Lampson very likely the best OS designer in the world — so we could do our own OS on the XDS machine — but even really good people need 3 or so years to do a good and reliable OS, so that was out. Then the discussion turned to “but we could make an emulated PDP-10 in less than a year” — and — “we could use and learn about the new VLSI DRAM chips from Intel”, etc….

Every thing about MAXC (Multiple Access Xerox Computer) was quite different than the DEC PDP-10 (including being much more reliable). This project was headed by Chuck Thacker, and required most of the computer researcher resources then at Parc. The result — in just 9 months — was both a successfully running mainframe machine plus technological leverage, especially with regard to the new DRAM.

In parallel, the idea was floating that we would do some kind of a personal computer look-ahead to have an “Interim Dynabook”, to explore serious UIs and local applications, to try distributed computing using local area networks, etc. We decided that it would be worth sacrificing RAM to have a dynamic bitmap display (because then we could show “anything”). We wanted “printing quality fonts” and bitmap painting. I wanted dynamic 2.5D animation and 12 polytimbral voices of music synthesis, etc. We built HW in early 1972 that allowed simulation of all five of these aims. This gave us a pretty crisp understanding of what kind of HW power would be required.

We knew that we wanted the microcode of the computers to be able to efficiently emulate byte-codes as fast as main memory could cycle — because we were in the process of inventing a variety of new languages that would be implemented this way (Smalltalk, Mesa, etc.). The parties involved decided in the summer of 1972 on a scheme as to how that could be done nicely.

We had all agreed that we would always “engineer for 100 users”, so if we did a personal machine, we had to be able to manufacture 100 of them (in fact, close to 2000 of them were eventually made).

The actual project wound up being started — in late Nov 1972 — secretly because of some politics, and — as with a few other projects at Parc — as the result of a bet: Chuck said that a futuristic computer could be done “in three months” and a Xerox exec bet him a case of wine that it couldn’t be done.

Even though the DRAM was now understood, and the prior simulations established figures of merit for everything, there was still a lot of work to be done in just 3 months! (For example: designing and building and debugging the first machine!)

The Alto was put together partly using wire-wrap technology in one 19 inch rack. Most of the boards were DRAM (128K bytes organized as 16 bit words), and a very fast couple of processor boards which held the 16 program counter zero-overhead-task-switched microcoded CPU and register file. The backplane was also wire-wrapped.

Most of the detailed design and much of the building was done by Chuck*. He had two technicians to help (Larry Clark and Mike Overton). Ed McCreight (another of the truly amazing multitalent computerists at Parc, designed the disk interface).

Three months later: voila! We used to say: “Chuck just threw the parts at the wall and they fell down Alto!”

From left to right: Larry Clark, Chuck Thacker, moi, Ed McCreight, and on the screen the Alto’s first image, a bit-map painting of the Cookie Monster on “Bilbo”, the first Alto. (Mike Overton, not pictured)

A closer look at Bilbo at birth, the Cookie Monster, and the board monster

The Alto could easily emulate 12 high quality polytimbral voices in real-time (and 8 FM voices), and could handle two keyboards and pedals organ console input.

The Alto could animate about 120 square inches of bit-map images at 10 frames per second in 2.5D.

The happy user in his office at Parc.

Chuck with his 2009 Turing Award bowl. He is one of those special people you can’t praise too highly in all respects of his life.

We were all shocked when Chuck went away and left us in 2017. It is a vast understatement to say that we miss him greatly

* From the Alto Hardware Manual:

“The Alto was originally designed by Charles P. Thacker and Edward M. McCreight, and was based on requirements and ideas contributed by Alan Kay, Butler Lampson, and other members of PARC’s Computer Sciences Laboratory and Systems Science Laboratory”","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2pl4b5u9idmog8sh', 'title': 'How was the Xerox Alto done in only 3 months?', 'score': {'original': 0.34833333333333, 'ai': 0.65166666666667}, 'blocks': [{'text': 'Basically: Chuck Thacker was simply amazing.\n\nI recounted some of the history of the Alto in “The Early History of Smalltalk”\n, and both Butler Lampson and Chuck also wrote histories of that time that included accounts of the Alto development (all can be found via the ACM, and I think they are online also).\n\nIt was part of the ARPA research M.O. to “compute in the future rather than in the present” by either using existing supercomputers, or by making a kind of supercomputer. There was enough science and engineering substantiation to allow a reasonable faith that the supercomputers of the present would be the commodity computers of the future. Beside needing to try out ideas and designs in many areas, there was also the brute fact that inventing and implementing new software ideas took quite a while to do — with really big dreams, a 10–15 year horizon was needed.\n\nI’ve written elsewhere in Quora a few things about the Alto, so I won’t repeat here.\n\nWas the Xerox Alto a prototype or a finished product?\n\nTo what extent did the Xerox PARC Alto inspire the Apple Macintosh?\n\nI think I might have also written about “MAXC”, the emulated PDP-10 that was the first project done by the computer researchers at Parc (in 1971). Xerox wouldn’t allow us to buy one (it was in the process of becoming the standard TSS computer on the ARPAnet) — instead they wanted us to use a Xerox Data Systems computer — so everyone almost walked out right at the beginning.\n\nWe had in Butler Lampson very likely the best OS designer in the world — so we could do our own OS on the XDS machine — but even really good people need 3 or so years to do a good and reliable OS, so that was out. Then the discussion turned to “but we could make an emulated PDP-10 in less than a year” — and — “we could use and learn about the new VLSI DRAM chips from Intel”, etc….\n\nEvery thing about MAXC (Multiple Access Xerox Computer) was quite different than the DEC PDP-10 (including being much more reliable). This project was headed by Chuck Thacker, and required most of the computer researcher resources then at Parc. The result — in just 9 months — was both a successfully running mainframe machine plus technological leverage, especially with regard to the new DRAM.\n\nIn parallel, the idea was floating that we would do some kind of a personal computer look-ahead to have an “Interim Dynabook”, to explore serious UIs and local applications, to try distributed computing using local area networks, etc. We decided that it would be worth sacrificing RAM to have a dynamic bitmap display (because then we could show “anything”). We wanted “printing quality fonts” and bitmap painting. I wanted dynamic 2.5D animation and 12 polytimbral voices of music synthesis, etc. We built HW in early 1972 that allowed simulation of all five of these aims. This gave us a pretty crisp understanding of what kind of HW power would be required.\n\nWe knew that we wanted the microcode of', 'result': {'fake': 0.1086, 'real': 0.8914}, 'status': 'success'}, {'text': 'the computers to be able to efficiently emulate byte-codes as fast as main memory could cycle — because we were in the process of inventing a variety of new languages that would be implemented this way (Smalltalk, Mesa, etc.). The parties involved decided in the summer of 1972 on a scheme as to how that could be done nicely.\n\nWe had all agreed that we would always “engineer for 100 users”, so if we did a personal machine, we had to be able to manufacture 100 of them (in fact, close to 2000 of them were eventually made).\n\nThe actual project wound up being started — in late Nov 1972 — secretly because of some politics, and — as with a few other projects at Parc — as the result of a bet: Chuck said that a futuristic computer could be done “in three months” and a Xerox exec bet him a case of wine that it couldn’t be done.\n\nEven though the DRAM was now understood, and the prior simulations established figures of merit for everything, there was still a lot of work to be done in just 3 months! (For example: designing and building and debugging the first machine!)\n\nThe Alto was put together partly using wire-wrap technology in one 19 inch rack. Most of the boards were DRAM (128K bytes organized as 16 bit words), and a very fast couple of processor boards which held the 16 program counter zero-overhead-task-switched microcoded CPU and register file. The backplane was also wire-wrapped.\n\nMost of the detailed design and much of the building was done by Chuck*. He had two technicians to help (Larry Clark and Mike Overton). Ed McCreight (another of the truly amazing multitalent computerists at Parc, designed the disk interface).\n\nThree months later: voila! We used to say: “Chuck just threw the parts at the wall and they fell down Alto!”\n\nFrom left to right: Larry Clark, Chuck Thacker, moi, Ed McCreight, and on the screen the Alto’s first image, a bit-map painting of the Cookie Monster on “Bilbo”, the first Alto. (Mike Overton, not pictured)\n\nA closer look at Bilbo at birth, the Cookie Monster, and the board monster\n\nThe Alto could easily emulate 12 high quality polytimbral voices in real-time (and 8 FM voices), and could handle two keyboards and pedals organ console input.\n\nThe Alto could animate about 120 square inches of bit-map images at 10 frames per second in 2.5D.\n\nThe happy user in his office at Parc.\n\nChuck with his 2009 Turing Award bowl. He is one of those special people you can’t praise too highly in all respects of his life.\n\nWe were all shocked when Chuck went away and left us in 2017. It is a vast understatement to say that we miss him greatly\n\n* From the Alto Hardware Manual:\n\n“The Alto was originally designed by Charles P. Thacker and Edward M. McCreight, and was based on requirements and ideas contributed by Alan Kay, Butler Lampson, and other members of PARC’s Computer Sciences Laboratory and Systems Science Laboratory”', 'result': {'fake': 0.0139, 'real': 0.9861}, 'status': 'success'}], 'credits_used': 11, 'credits': 1995503, 'subscription': 0, 'content': 'Basically: Chuck Thacker was simply amazing.\n\nI recounted some of the history of the Alto in “The Early History of Smalltalk”\n, and both Butler Lampson and Chuck also wrote histories of that time that included accounts of the Alto development (all can be found via the ACM, and I think they are online also).\n\nIt was part of the ARPA research M.O. to “compute in the future rather than in the present” by either using existing supercomputers, or by making a kind of supercomputer. There was enough science and engineering substantiation to allow a reasonable faith that the supercomputers of the present would be the commodity computers of the future. Beside needing to try out ideas and designs in many areas, there was also the brute fact that inventing and implementing new software ideas took quite a while to do — with really big dreams, a 10–15 year horizon was needed.\n\nI’ve written elsewhere in Quora a few things about the Alto, so I won’t repeat here.\n\nWas the Xerox Alto a prototype or a finished product?\n\nTo what extent did the Xerox PARC Alto inspire the Apple Macintosh?\n\nI think I might have also written about “MAXC”, the emulated PDP-10 that was the first project done by the computer researchers at Parc (in 1971). Xerox wouldn’t allow us to buy one (it was in the process of becoming the standard TSS computer on the ARPAnet) — instead they wanted us to use a Xerox Data Systems computer — so everyone almost walked out right at the beginning.\n\nWe had in Butler Lampson very likely the best OS designer in the world — so we could do our own OS on the XDS machine — but even really good people need 3 or so years to do a good and reliable OS, so that was out. Then the discussion turned to “but we could make an emulated PDP-10 in less than a year” — and — “we could use and learn about the new VLSI DRAM chips from Intel”, etc….\n\nEvery thing about MAXC (Multiple Access Xerox Computer) was quite different than the DEC PDP-10 (including being much more reliable). This project was headed by Chuck Thacker, and required most of the computer researcher resources then at Parc. The result — in just 9 months — was both a successfully running mainframe machine plus technological leverage, especially with regard to the new DRAM.\n\nIn parallel, the idea was floating that we would do some kind of a personal computer look-ahead to have an “Interim Dynabook”, to explore serious UIs and local applications, to try distributed computing using local area networks, etc. We decided that it would be worth sacrificing RAM to have a dynamic bitmap display (because then we could show “anything”). We wanted “printing quality fonts” and bitmap painting. I wanted dynamic 2.5D animation and 12 polytimbral voices of music synthesis, etc. We built HW in early 1972 that allowed simulation of all five of these aims. This gave us a pretty crisp understanding of what kind of HW power would be required.\n\nWe knew that we wanted the microcode of the computers to be able to efficiently emulate byte-codes as fast as main memory could cycle — because we were in the process of inventing a variety of new languages that would be implemented this way (Smalltalk, Mesa, etc.). The parties involved decided in the summer of 1972 on a scheme as to how that could be done nicely.\n\nWe had all agreed that we would always “engineer for 100 users”, so if we did a personal machine, we had to be able to manufacture 100 of them (in fact, close to 2000 of them were eventually made).\n\nThe actual project wound up being started — in late Nov 1972 — secretly because of some politics, and — as with a few other projects at Parc — as the result of a bet: Chuck said that a futuristic computer could be done “in three months” and a Xerox exec bet him a case of wine that it couldn’t be done.\n\nEven though the DRAM was now understood, and the prior simulations established figures of merit for everything, there was still a lot of work to be done in just 3 months! (For example: designing and building and debugging the first machine!)\n\nThe Alto was put together partly using wire-wrap technology in one 19 inch rack. Most of the boards were DRAM (128K bytes organized as 16 bit words), and a very fast couple of processor boards which held the 16 program counter zero-overhead-task-switched microcoded CPU and register file. The backplane was also wire-wrapped.\n\nMost of the detailed design and much of the building was done by Chuck*. He had two technicians to help (Larry Clark and Mike Overton). Ed McCreight (another of the truly amazing multitalent computerists at Parc, designed the disk interface).\n\nThree months later: voila! We used to say: “Chuck just threw the parts at the wall and they fell down Alto!”\n\nFrom left to right: Larry Clark, Chuck Thacker, moi, Ed McCreight, and on the screen the Alto’s first image, a bit-map painting of the Cookie Monster on “Bilbo”, the first Alto. (Mike Overton, not pictured)\n\nA closer look at Bilbo at birth, the Cookie Monster, and the board monster\n\nThe Alto could easily emulate 12 high quality polytimbral voices in real-time (and 8 FM voices), and could handle two keyboards and pedals organ console input.\n\nThe Alto could animate about 120 square inches of bit-map images at 10 frames per second in 2.5D.\n\nThe happy user in his office at Parc.\n\nChuck with his 2009 Turing Award bowl. He is one of those special people you can’t praise too highly in all respects of his life.\n\nWe were all shocked when Chuck went away and left us in 2017. It is a vast understatement to say that we miss him greatly\n\n* From the Alto Hardware Manual:\n\n“The Alto was originally designed by Charles P. Thacker and Edward M. McCreight, and was based on requirements and ideas contributed by Alan Kay, Butler Lampson, and other members of PARC’s Computer Sciences Laboratory and Systems Science Laboratory”', 'aiModelVersion': '1'}",0.34833333333333
Blake McBride,1y,Why did Borland fail?,"Microsoft killed Borland just like they killed hundreds of other companies. Let’s take their C compiler for example. Borland sold their C compiler for $99. Microsoft bought a C compiler, re-branded it, and sold it for $89 because they were subsidised by their OS sales. After they drove Borland out of business they immediately changed the price of their C compiler to $399. It soon went to $799.

That is Microsoft’s MO. Give the product away at a high loss year after year until you drive the competition ouf of business. Then you can charge whatever you like.

Microsoft has used this strategy with many, many of their products including Excel, Word, Microsoft SQL, etc. The list goes on and on.

This has all been to the significant detriment to the industry. This is the reason we need stronger anti-trust laws and enforcement.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/5jm9qhdzeru4lp17', 'title': 'Why did Borland fail?', 'score': {'original': 0.9987, 'ai': 0.0013}, 'blocks': [{'text': 'Microsoft killed Borland just like they killed hundreds of other companies. Let’s take their C compiler for example. Borland sold their C compiler for $99. Microsoft bought a C compiler, re-branded it, and sold it for $89 because they were subsidised by their OS sales. After they drove Borland out of business they immediately changed the price of their C compiler to $399. It soon went to $799.\n\nThat is Microsoft’s MO. Give the product away at a high loss year after year until you drive the competition ouf of business. Then you can charge whatever you like.\n\nMicrosoft has used this strategy with many, many of their products including Excel, Word, Microsoft SQL, etc. The list goes on and on.\n\nThis has all been to the significant detriment to the industry. This is the reason we need stronger anti-trust laws and enforcement.', 'result': {'fake': 0.0013, 'real': 0.9987}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995501, 'subscription': 0, 'content': 'Microsoft killed Borland just like they killed hundreds of other companies. Let’s take their C compiler for example. Borland sold their C compiler for $99. Microsoft bought a C compiler, re-branded it, and sold it for $89 because they were subsidised by their OS sales. After they drove Borland out of business they immediately changed the price of their C compiler to $399. It soon went to $799.\n\nThat is Microsoft’s MO. Give the product away at a high loss year after year until you drive the competition ouf of business. Then you can charge whatever you like.\n\nMicrosoft has used this strategy with many, many of their products including Excel, Word, Microsoft SQL, etc. The list goes on and on.\n\nThis has all been to the significant detriment to the industry. This is the reason we need stronger anti-trust laws and enforcement.', 'aiModelVersion': '1'}",0.9987
Tarun Chitra,9y,Have there been any new brilliant computer science algorithms in last 10 years?,"Fully Homomorphic Encryption (FHE)Imagine if you were a hospital that needed to process/analyze data on EC2, but were worried about privacy. In an ideal world, one would like:To encrypt his or her dataSend the encrypted data to the cloud and have the cloud servers operate on the encypted data (yes, I mean that)Sending you back the encrypted results, that you can easily decryptThis probably seems like magic — after all, how can someone operate on garbled data, creating more garbled data and then send me back some junk that is guaranteed to decrypt to the correct answer? It turns out that since most cryptosystems inherit symmetries/operations from the mathematical objects that they are based on (groups, rings, lattices, algebraic number fields), we can sometimes use these operations without breaking our encryption.  Before giving some details about FHE, let's look at an example. Suppose that I give you an RSA public key (N,e)(N,e)(N,e)  and suppose that the secret key is d≡e−1 mod ϕ(N)d≡e−1 mod ϕ(N) d \equiv e^{-1} \text{ mod } \phi(N). Then two plaintexts π1,π2∈Zπ1,π2∈Z\pi_1, \pi_2 \in \Z would become ciphertexts ψ1,ψ2ψ1,ψ2\psi_1, \psi_2, where ψ1=πe1 mod N,ψ2=πe2 mod Nψ1=π1e mod N,ψ2=π2e mod N \psi_1 = \pi_1^e \text{ mod } N, \psi_2 = \pi_2^e \text{ mod } N Note that, π1π2 mod N=(ψd1 mod N)(ψd2 mod N)=π1π2 mod N=(ψ1d mod N)(ψ2d mod N)= \pi_1 \pi_2 \text{ mod } N = (\psi_1^d \text{ mod } N) (\psi_2^d \text{ mod } N) = (ψ1ψ2)d mod N(ψ1ψ2)d mod N (\psi_1 \psi_2)^d  \text{ mod } Nso that the decryption of the product of two ciphertexts is equivalent (mod N) to the product of the two plaintexts. Our goal is to make a system that is both multiplicatively homomorphic (like RSA) as well as additively homomorphic (which RSA is not, as you can see from the binomial formula). Note that in this about process, the cloud servers never see your raw data. Fully homomorphic encrypytion makes this happen. The landmark paper/thesis [0] of Gentry (2009) led to some astonishing revelations about simple encryption schemes:As long as a scheme can evaluate its own decryption circuit as well as a single NAND gate, one can bootstrap this scheme to a fully homomorphic schemeTechniques used to prove guarantees of post-quantum security turn out to be useful for proving that a bootstrappable homomorphic encryption scheme existsEven very simple integer schemes can workIn particular, van Dijk, Gentry, Halevi and Vaikunthan (DGHV) proved in [1] that a cryptography scheme that is simpler than RSA (!!!) isHomomorphic for virtually unlimited number of additive gates, homomorphic for small numbers of multiplicative gatesBootstrappableSo simple and clean (relative to other cryptosystems, e.g. NTRU) that it was casually proposed on a message board in 2000 by Bram CohenLater, Vaikunthan and Brakerski proved that one can construct non-bootstrappable homomorphic encryption schemes [2] that base their security on stronger, post-quantum problems (e.g. Learning with Errors). This scheme appears to be more practical than the original bootstrapping schemes (although it still uses bootstrapping as 'an optimization').  A nice non-technical summary can be found on the MIT tech review [3] [0] Craig Gentry's PhD Thesis[1] Page on microsoft.com[2] Page on iacr.org[3] Homomorphic Encryption - MIT Technology Review","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6ei8lu9rb2swcfjk', 'title': 'Have there been any new brilliant computer science algorithms in last 10 years?', 'score': {'original': 0.9986, 'ai': 0.0014}, 'blocks': [{'text': ""Fully Homomorphic Encryption (FHE)Imagine if you were a hospital that needed to process/analyze data on EC2, but were worried about privacy. In an ideal world, one would like:To encrypt his or her dataSend the encrypted data to the cloud and have the cloud servers operate on the encypted data (yes, I mean that)Sending you back the encrypted results, that you can easily decryptThis probably seems like magic — after all, how can someone operate on garbled data, creating more garbled data and then send me back some junk that is guaranteed to decrypt to the correct answer? It turns out that since most cryptosystems inherit symmetries/operations from the mathematical objects that they are based on (groups, rings, lattices, algebraic number fields), we can sometimes use these operations without breaking our encryption.  Before giving some details about FHE, let's look at an example. Suppose that I give you an RSA public key (N,e)(N,e)(N,e)  and suppose that the secret key is d≡e−1\xa0mod\xa0ϕ(N)d≡e−1\xa0mod\xa0ϕ(N) d \\equiv e^{-1} \\text{ mod } \\phi(N). Then two plaintexts π1,π2∈Zπ1,π2∈Z\\pi_1, \\pi_2 \\in \\Z would become ciphertexts ψ1,ψ2ψ1,ψ2\\psi_1, \\psi_2, where ψ1=πe1\xa0mod\xa0N,ψ2=πe2\xa0mod\xa0Nψ1=π1e\xa0mod\xa0N,ψ2=π2e\xa0mod\xa0N \\psi_1 = \\pi_1^e \\text{ mod } N, \\psi_2 = \\pi_2^e \\text{ mod } N Note that, π1π2\xa0mod\xa0N=(ψd1\xa0mod\xa0N)(ψd2\xa0mod\xa0N)=π1π2\xa0mod\xa0N=(ψ1d\xa0mod\xa0N)(ψ2d\xa0mod\xa0N)= \\pi_1 \\pi_2 \\text{ mod } N = (\\psi_1^d \\text{ mod } N) (\\psi_2^d \\text{ mod } N) = (ψ1ψ2)d\xa0mod\xa0N(ψ1ψ2)d\xa0mod\xa0N (\\psi_1 \\psi_2)^d  \\text{ mod } Nso that the decryption of the product of two ciphertexts is equivalent (mod N) to the product of the two plaintexts. Our goal is to make a system that is both multiplicatively homomorphic (like RSA) as well as additively homomorphic (which RSA is not, as you can see from the binomial formula). Note that in this about process, the cloud servers never see your raw data. Fully homomorphic encrypytion makes this happen. The landmark paper/thesis [0] of Gentry (2009) led to some astonishing revelations about simple encryption schemes:As long as a scheme can evaluate its own decryption circuit as well as a single NAND gate, one can bootstrap this scheme to a fully homomorphic schemeTechniques used to prove guarantees of post-quantum security turn out to be useful for proving that a bootstrappable homomorphic encryption scheme existsEven very simple integer schemes can workIn particular, van Dijk, Gentry, Halevi and Vaikunthan (DGHV) proved in [1] that a cryptography scheme that is simpler than RSA (!!!) isHomomorphic for virtually unlimited number of additive gates, homomorphic for small numbers of multiplicative gatesBootstrappableSo simple and clean (relative to other cryptosystems, e.g. NTRU) that it was casually proposed on a message board in 2000 by Bram CohenLater, Vaikunthan and Brakerski proved that one can construct non-bootstrappable homomorphic encryption schemes [2] that base their security on stronger, post-quantum problems (e.g. Learning with Errors). This scheme appears to be more practical than the original bootstrapping schemes (although it still uses bootstrapping as 'an optimization').  A nice non-technical summary can be found on the MIT tech review [3] [0] Craig Gentry's PhD Thesis[1] Page on microsoft.com[2] Page on iacr.org[3] Homomorphic Encryption - MIT Technology Review"", 'result': {'fake': 0.0014, 'real': 0.9986}, 'status': 'success'}], 'credits_used': 6, 'credits': 1995495, 'subscription': 0, 'content': ""Fully Homomorphic Encryption (FHE)Imagine if you were a hospital that needed to process/analyze data on EC2, but were worried about privacy. In an ideal world, one would like:To encrypt his or her dataSend the encrypted data to the cloud and have the cloud servers operate on the encypted data (yes, I mean that)Sending you back the encrypted results, that you can easily decryptThis probably seems like magic — after all, how can someone operate on garbled data, creating more garbled data and then send me back some junk that is guaranteed to decrypt to the correct answer? It turns out that since most cryptosystems inherit symmetries/operations from the mathematical objects that they are based on (groups, rings, lattices, algebraic number fields), we can sometimes use these operations without breaking our encryption.  Before giving some details about FHE, let's look at an example. Suppose that I give you an RSA public key (N,e)(N,e)(N,e)  and suppose that the secret key is d≡e−1\xa0mod\xa0ϕ(N)d≡e−1\xa0mod\xa0ϕ(N) d \\equiv e^{-1} \\text{ mod } \\phi(N). Then two plaintexts π1,π2∈Zπ1,π2∈Z\\pi_1, \\pi_2 \\in \\Z would become ciphertexts ψ1,ψ2ψ1,ψ2\\psi_1, \\psi_2, where ψ1=πe1\xa0mod\xa0N,ψ2=πe2\xa0mod\xa0Nψ1=π1e\xa0mod\xa0N,ψ2=π2e\xa0mod\xa0N \\psi_1 = \\pi_1^e \\text{ mod } N, \\psi_2 = \\pi_2^e \\text{ mod } N Note that, π1π2\xa0mod\xa0N=(ψd1\xa0mod\xa0N)(ψd2\xa0mod\xa0N)=π1π2\xa0mod\xa0N=(ψ1d\xa0mod\xa0N)(ψ2d\xa0mod\xa0N)= \\pi_1 \\pi_2 \\text{ mod } N = (\\psi_1^d \\text{ mod } N) (\\psi_2^d \\text{ mod } N) = (ψ1ψ2)d\xa0mod\xa0N(ψ1ψ2)d\xa0mod\xa0N (\\psi_1 \\psi_2)^d  \\text{ mod } Nso that the decryption of the product of two ciphertexts is equivalent (mod N) to the product of the two plaintexts. Our goal is to make a system that is both multiplicatively homomorphic (like RSA) as well as additively homomorphic (which RSA is not, as you can see from the binomial formula). Note that in this about process, the cloud servers never see your raw data. Fully homomorphic encrypytion makes this happen. The landmark paper/thesis [0] of Gentry (2009) led to some astonishing revelations about simple encryption schemes:As long as a scheme can evaluate its own decryption circuit as well as a single NAND gate, one can bootstrap this scheme to a fully homomorphic schemeTechniques used to prove guarantees of post-quantum security turn out to be useful for proving that a bootstrappable homomorphic encryption scheme existsEven very simple integer schemes can workIn particular, van Dijk, Gentry, Halevi and Vaikunthan (DGHV) proved in [1] that a cryptography scheme that is simpler than RSA (!!!) isHomomorphic for virtually unlimited number of additive gates, homomorphic for small numbers of multiplicative gatesBootstrappableSo simple and clean (relative to other cryptosystems, e.g. NTRU) that it was casually proposed on a message board in 2000 by Bram CohenLater, Vaikunthan and Brakerski proved that one can construct non-bootstrappable homomorphic encryption schemes [2] that base their security on stronger, post-quantum problems (e.g. Learning with Errors). This scheme appears to be more practical than the original bootstrapping schemes (although it still uses bootstrapping as 'an optimization').  A nice non-technical summary can be found on the MIT tech review [3] [0] Craig Gentry's PhD Thesis[1] Page on microsoft.com[2] Page on iacr.org[3] Homomorphic Encryption - MIT Technology Review"", 'aiModelVersion': '1'}",0.9986
Torben Ægidius Mogensen,4y,Why were old games programmed in assembly when higher level languages existed?,"Here is an image from the game Elite:

Elite was a first-person, real-time 3D space game originally made for the BBC Microcomputer, which had an 8-bit processor running at 2MHz and 32kB of memory for everything including the screen image. The game used 10kB for the screen image, which leaves 22kB for code and game data. And 2MHz on an 8-bit processor is not exactly high speed, so the programmers needed to get all of the processing power they could out of the machine, while keeping the code as small as possible.

The machine came equipped with a BASIC interpreter in ROM, so the programmers could use that. Interpreted BASIC is a lot slower than assembly, but (thanks to a compact tokenised representation) BASIC code took up less space than machine code. So BASIC was (IIRC) used for things that did not need to go fast, such as trading at space stations, but everything else was done in machine code. The result was a game that few people believed would even be possible on such a small and slow machine.

There were a few compilers available for the BBC Micro. One was a compiler for BASIC, another was a compiler for Pascal. But both of these generated code that was much bigger and much slower than what you could get from hand-written machine code. BBC BASIC actually had a built-in assembler, so you could write your code in symbolic assembler and compile this to binary machine code. So a common strategy was to write your code in symbolic assembler, compile it, delete the BASIC program from memory, and then run the machine code. Alternatively, storing the machine code to tape or disk and then loading several pieces of machine code before running this, so you could build machine-code programs larger than what you could generate from a single BASIC program.

Oh, and one more thing: Look at the screen image above. Notice that the top part is in black and white and the bottom part is in four colours. Notice, also, that the vertical lines in the bottom part are wider than in the top part. This is because the top part uses a 320-pixel wide b/w screen mode, where the bottom part uses a 160-pixel wide four-colour screen mode. Normally, you could choose either one of these modes, but the program set up a timer to interrupt when the screen refresh was about three quarters down the screen, and then changed the screen mode at that time. The alternative would be to use a 320-pixel wide four-colour mode (which was possible), but that would take up 20kB instead of the 10kB for the split mode, leaving only 12kB for the game, which is far too little.

So, to answer the original question: Old games used machine code because that was the only way to get games at acceptable speed into the small memory available on the machines at the time.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/c2xunwifdy4pgh6e', 'title': 'Why were old games programmed in assembly when higher level languages existed?', 'score': {'original': 0.999, 'ai': 0.001}, 'blocks': [{'text': 'Here is an image from the game Elite:\n\nElite was a first-person, real-time 3D space game originally made for the BBC Microcomputer, which had an 8-bit processor running at 2MHz and 32kB of memory for everything including the screen image. The game used 10kB for the screen image, which leaves 22kB for code and game data. And 2MHz on an 8-bit processor is not exactly high speed, so the programmers needed to get all of the processing power they could out of the machine, while keeping the code as small as possible.\n\nThe machine came equipped with a BASIC interpreter in ROM, so the programmers could use that. Interpreted BASIC is a lot slower than assembly, but (thanks to a compact tokenised representation) BASIC code took up less space than machine code. So BASIC was (IIRC) used for things that did not need to go fast, such as trading at space stations, but everything else was done in machine code. The result was a game that few people believed would even be possible on such a small and slow machine.\n\nThere were a few compilers available for the BBC Micro. One was a compiler for BASIC, another was a compiler for Pascal. But both of these generated code that was much bigger and much slower than what you could get from hand-written machine code. BBC BASIC actually had a built-in assembler, so you could write your code in symbolic assembler and compile this to binary machine code. So a common strategy was to write your code in symbolic assembler, compile it, delete the BASIC program from memory, and then run the machine code. Alternatively, storing the machine code to tape or disk and then loading several pieces of machine code before running this, so you could build machine-code programs larger than what you could generate from a single BASIC program.\n\nOh, and one more thing: Look at the screen image above. Notice that the top part is in black and white and the bottom part is in four colours. Notice, also, that the vertical lines in the bottom part are wider than in the top part. This is because the top part uses a 320-pixel wide b/w screen mode, where the bottom part uses a 160-pixel wide four-colour screen mode. Normally, you could choose either one of these modes, but the program set up a timer to interrupt when the screen refresh was about three quarters down the screen, and then changed the screen mode at that time. The alternative would be to use a 320-pixel wide four-colour mode (which was possible), but that would take up 20kB instead of the 10kB for the split mode, leaving only 12kB for the game, which is far too little.\n\nSo, to answer the original question: Old games used machine code because that was the only way to get games at acceptable speed into the small memory available on the machines at the time.', 'result': {'fake': 0.001, 'real': 0.999}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995490, 'subscription': 0, 'content': 'Here is an image from the game Elite:\n\nElite was a first-person, real-time 3D space game originally made for the BBC Microcomputer, which had an 8-bit processor running at 2MHz and 32kB of memory for everything including the screen image. The game used 10kB for the screen image, which leaves 22kB for code and game data. And 2MHz on an 8-bit processor is not exactly high speed, so the programmers needed to get all of the processing power they could out of the machine, while keeping the code as small as possible.\n\nThe machine came equipped with a BASIC interpreter in ROM, so the programmers could use that. Interpreted BASIC is a lot slower than assembly, but (thanks to a compact tokenised representation) BASIC code took up less space than machine code. So BASIC was (IIRC) used for things that did not need to go fast, such as trading at space stations, but everything else was done in machine code. The result was a game that few people believed would even be possible on such a small and slow machine.\n\nThere were a few compilers available for the BBC Micro. One was a compiler for BASIC, another was a compiler for Pascal. But both of these generated code that was much bigger and much slower than what you could get from hand-written machine code. BBC BASIC actually had a built-in assembler, so you could write your code in symbolic assembler and compile this to binary machine code. So a common strategy was to write your code in symbolic assembler, compile it, delete the BASIC program from memory, and then run the machine code. Alternatively, storing the machine code to tape or disk and then loading several pieces of machine code before running this, so you could build machine-code programs larger than what you could generate from a single BASIC program.\n\nOh, and one more thing: Look at the screen image above. Notice that the top part is in black and white and the bottom part is in four colours. Notice, also, that the vertical lines in the bottom part are wider than in the top part. This is because the top part uses a 320-pixel wide b/w screen mode, where the bottom part uses a 160-pixel wide four-colour screen mode. Normally, you could choose either one of these modes, but the program set up a timer to interrupt when the screen refresh was about three quarters down the screen, and then changed the screen mode at that time. The alternative would be to use a 320-pixel wide four-colour mode (which was possible), but that would take up 20kB instead of the 10kB for the split mode, leaving only 12kB for the game, which is far too little.\n\nSo, to answer the original question: Old games used machine code because that was the only way to get games at acceptable speed into the small memory available on the machines at the time.', 'aiModelVersion': '1'}",0.999
Aaron Christianson,4y,"Which programming language has the most interesting ""backstory""?","I’m not a fan of JavaScript, and have been a rather noisy critic of the JS ecosystem, but the story of JavaScript is interesting.

Netscape hired Brendan Eich to embed a programming language in the web browser so pages could have some dynamic content. Eich and Netscape made the excellent, tasteful decision to use Scheme. At the same time, Sun was doing the big marketing push on Java, and partnered up with Netscape to put Java in the browser, which they did, but it was a security nightmare and most browsers removed Java years ago.

Anyway, because they were suddenly going to be shoving Java in the browser, Netscape (probably under the influence of Sun) decided they wanted their DOM scripting language to be more like Java, and oh, P.S. Brendan, you have ten days to implement the language.

So Eich rushed. He ended up with a mutant. C-style syntax (like Java), exotic prototypical inheritance from Self, high-order functions like Scheme, and troublingly Perl-like semantics around variables and type coercion.

JavaScript, the language that refuses to crash. JavaScript, the language that has as much to do with Java as hamsters do with ham. JavaScript, the language everyone keeps calling “Lisp in C’s clothing” because it has high-order functions and they don’t know what macros are.

Anyway, I think it’s a good story.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/fetbpdh81x540gwl', 'title': 'Which programming language has the most interesting ""backstory""?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'I’m not a fan of JavaScript, and have been a rather noisy critic of the JS ecosystem, but the story of JavaScript is interesting.\n\nNetscape hired Brendan Eich to embed a programming language in the web browser so pages could have some dynamic content. Eich and Netscape made the excellent, tasteful decision to use Scheme. At the same time, Sun was doing the big marketing push on Java, and partnered up with Netscape to put Java in the browser, which they did, but it was a security nightmare and most browsers removed Java years ago.\n\nAnyway, because they were suddenly going to be shoving Java in the browser, Netscape (probably under the influence of Sun) decided they wanted their DOM scripting language to be more like Java, and oh, P.S. Brendan, you have ten days to implement the language.\n\nSo Eich rushed. He ended up with a mutant. C-style syntax (like Java), exotic prototypical inheritance from Self, high-order functions like Scheme, and troublingly Perl-like semantics around variables and type coercion.\n\nJavaScript, the language that refuses to crash. JavaScript, the language that has as much to do with Java as hamsters do with ham. JavaScript, the language everyone keeps calling “Lisp in C’s clothing” because it has high-order functions and they don’t know what macros are.\n\nAnyway, I think it’s a good story.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995487, 'subscription': 0, 'content': 'I’m not a fan of JavaScript, and have been a rather noisy critic of the JS ecosystem, but the story of JavaScript is interesting.\n\nNetscape hired Brendan Eich to embed a programming language in the web browser so pages could have some dynamic content. Eich and Netscape made the excellent, tasteful decision to use Scheme. At the same time, Sun was doing the big marketing push on Java, and partnered up with Netscape to put Java in the browser, which they did, but it was a security nightmare and most browsers removed Java years ago.\n\nAnyway, because they were suddenly going to be shoving Java in the browser, Netscape (probably under the influence of Sun) decided they wanted their DOM scripting language to be more like Java, and oh, P.S. Brendan, you have ten days to implement the language.\n\nSo Eich rushed. He ended up with a mutant. C-style syntax (like Java), exotic prototypical inheritance from Self, high-order functions like Scheme, and troublingly Perl-like semantics around variables and type coercion.\n\nJavaScript, the language that refuses to crash. JavaScript, the language that has as much to do with Java as hamsters do with ham. JavaScript, the language everyone keeps calling “Lisp in C’s clothing” because it has high-order functions and they don’t know what macros are.\n\nAnyway, I think it’s a good story.', 'aiModelVersion': '1'}",0.9997
Staffan Sandström,1y,The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?,"We, don’t do that any more. But there is a problem coming up that’s called the Y2K38 problem. It haven’t gotten as much publicity as the Y2K problem as it’s further away and 3:14:07 UTC on 19 January 2038 isn’t as interesting as a new years eve. That problem is probably a bit harder to understand for laymen. In many systems the time is stored as seconds from 1970. And it’s stored as a 32-bit integer so we’re running out of seconds. Some have switched to using a 64-bit integer instead and that should work for 292 billion years.

In 2108 the timestamps for DOS files will not work. But that’s hardly a major issue.

JavaScript uses milliseconds since 1 January 1970 and they will not work past 3 September, 275,760. But that gives some time to come up with something.

So there are some other issues that are more relevant than the Y10K bug.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rgubj81wqtlhdn43', 'title': 'The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?', 'score': {'original': 0.9231, 'ai': 0.0769}, 'blocks': [{'text': 'We, don’t do that any more. But there is a problem coming up that’s called the Y2K38 problem. It haven’t gotten as much publicity as the Y2K problem as it’s further away and 3:14:07 UTC on 19 January 2038 isn’t as interesting as a new years eve. That problem is probably a bit harder to understand for laymen. In many systems the time is stored as seconds from 1970. And it’s stored as a 32-bit integer so we’re running out of seconds. Some have switched to using a 64-bit integer instead and that should work for 292 billion years.\n\nIn 2108 the timestamps for DOS files will not work. But that’s hardly a major issue.\n\nJavaScript uses milliseconds since 1 January 1970 and they will not work past 3 September, 275,760. But that gives some time to come up with something.\n\nSo there are some other issues that are more relevant than the Y10K bug.', 'result': {'fake': 0.0769, 'real': 0.9231}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995485, 'subscription': 0, 'content': 'We, don’t do that any more. But there is a problem coming up that’s called the Y2K38 problem. It haven’t gotten as much publicity as the Y2K problem as it’s further away and 3:14:07 UTC on 19 January 2038 isn’t as interesting as a new years eve. That problem is probably a bit harder to understand for laymen. In many systems the time is stored as seconds from 1970. And it’s stored as a 32-bit integer so we’re running out of seconds. Some have switched to using a 64-bit integer instead and that should work for 292 billion years.\n\nIn 2108 the timestamps for DOS files will not work. But that’s hardly a major issue.\n\nJavaScript uses milliseconds since 1 January 1970 and they will not work past 3 September, 275,760. But that gives some time to come up with something.\n\nSo there are some other issues that are more relevant than the Y10K bug.', 'aiModelVersion': '1'}",0.9231
Tom Crosley,Updated 2y,Why were punch cards used for programming? Didn't computer screens and keyboards already exist by the time programmers used them?,"As odd as it might seem today, commercial computers “back in the day” (1950s - early 1970s) did not have keyboards or screens (except for the operators) because users didn’t have direct access to the computers. These were mainframes, which took up entire rooms. This situation persisted until at least the mid 1960s, when terminals started to appear.

IBM 360/50 with tape drives in the background, and a rack of 9-track tapes in the foreground. Each could hold only 45 MB of data on a 2400′ reel!

You would typically write your program out on a coding sheet, and turn it into the keypunch department where a number of operators — usually women — would use a keypunch to punch a card for each line of your program, whether it be FORTRAN, COBOL, etc. — the languages common back then. It was unusual for a programmer to keypunch their own program, but their might be one keypunch outside the computer room where a person could punch a few cards to correct mistakes in their program, before resubmitting.

Typically the jobs, if fairly large, were submitted in a box. Each box could hold up to 2000 80-column cards, like this one:

For small programs, say 50 cards or so, you could just submit a deck of cards surrounded by a rubber band.

The program decks would then be read into a card reader attached to the computer, and executed as “batch jobs”, run one after another. Each program was preceded by a few cards with JCL (Job Control Language) which specified what resources the program needed (such as tape drives) and invoked the necessary compilers and linkers. The cards for your source program would be followed by any cards containing pre-built libraries needed by your program (in binary format), followed by additional cards (if any) with the data needed by your program.

When the program was finished running, perhaps minutes, hours, or even a day after you submitted it, both the original deck and your printout would be available in on a shelf in an output area. Less often, your program might punch cards as additional output.

Card decks for different jobs could be stacked in a reader by the operator, since each one had its own JCL header. On most large systems, the cards would not be read by the computer directly, as that would be too slow but instead would be read by a separate device and their image written to a tape drive.

When minicomputers appeared in the 1960s, paper tape was often used instead of punch cards for smaller installations.

By the 1970s, computer terminal access to mainframes became more widespread, and punch card usage dwindled off. Then personal computers appeared, with their individual keyboards and screens in the mid to late 1970s.

IBM 3270 terminal, used to access mainframes using TSO (Time Sharing Option).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rfzycqw6anm2ivxp', 'title': ""Why were punch cards used for programming? Didn't computer screens and keyboards already exist by the time programmers used them?"", 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'As odd as it might seem today, commercial computers “back in the day” (1950s - early 1970s) did not have keyboards or screens (except for the operators) because users didn’t have direct access to the computers. These were mainframes, which took up entire rooms. This situation persisted until at least the mid 1960s, when terminals started to appear.\n\nIBM 360/50 with tape drives in the background, and a rack of 9-track tapes in the foreground. Each could hold only 45 MB of data on a 2400′ reel!\n\nYou would typically write your program out on a coding sheet, and turn it into the keypunch department where a number of operators — usually women — would use a keypunch to punch a card for each line of your program, whether it be FORTRAN, COBOL, etc. — the languages common back then. It was unusual for a programmer to keypunch their own program, but their might be one keypunch outside the computer room where a person could punch a few cards to correct mistakes in their program, before resubmitting.\n\nTypically the jobs, if fairly large, were submitted in a box. Each box could hold up to 2000 80-column cards, like this one:\n\nFor small programs, say 50 cards or so, you could just submit a deck of cards surrounded by a rubber band.\n\nThe program decks would then be read into a card reader attached to the computer, and executed as “batch jobs”, run one after another. Each program was preceded by a few cards with JCL (Job Control Language) which specified what resources the program needed (such as tape drives) and invoked the necessary compilers and linkers. The cards for your source program would be followed by any cards containing pre-built libraries needed by your program (in binary format), followed by additional cards (if any) with the data needed by your program.\n\nWhen the program was finished running, perhaps minutes, hours, or even a day after you submitted it, both the original deck and your printout would be available in on a shelf in an output area. Less often, your program might punch cards as additional output.\n\nCard decks for different jobs could be stacked in a reader by the operator, since each one had its own JCL header. On most large systems, the cards would not be read by the computer directly, as that would be too slow but instead would be read by a separate device and their image written to a tape drive.\n\nWhen minicomputers appeared in the 1960s, paper tape was often used instead of punch cards for smaller installations.\n\nBy the 1970s, computer terminal access to mainframes became more widespread, and punch card usage dwindled off. Then personal computers appeared, with their individual keyboards and screens in the mid to late 1970s.\n\nIBM 3270 terminal, used to access mainframes using TSO (Time Sharing Option).', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995480, 'subscription': 0, 'content': 'As odd as it might seem today, commercial computers “back in the day” (1950s - early 1970s) did not have keyboards or screens (except for the operators) because users didn’t have direct access to the computers. These were mainframes, which took up entire rooms. This situation persisted until at least the mid 1960s, when terminals started to appear.\n\nIBM 360/50 with tape drives in the background, and a rack of 9-track tapes in the foreground. Each could hold only 45 MB of data on a 2400′ reel!\n\nYou would typically write your program out on a coding sheet, and turn it into the keypunch department where a number of operators — usually women — would use a keypunch to punch a card for each line of your program, whether it be FORTRAN, COBOL, etc. — the languages common back then. It was unusual for a programmer to keypunch their own program, but their might be one keypunch outside the computer room where a person could punch a few cards to correct mistakes in their program, before resubmitting.\n\nTypically the jobs, if fairly large, were submitted in a box. Each box could hold up to 2000 80-column cards, like this one:\n\nFor small programs, say 50 cards or so, you could just submit a deck of cards surrounded by a rubber band.\n\nThe program decks would then be read into a card reader attached to the computer, and executed as “batch jobs”, run one after another. Each program was preceded by a few cards with JCL (Job Control Language) which specified what resources the program needed (such as tape drives) and invoked the necessary compilers and linkers. The cards for your source program would be followed by any cards containing pre-built libraries needed by your program (in binary format), followed by additional cards (if any) with the data needed by your program.\n\nWhen the program was finished running, perhaps minutes, hours, or even a day after you submitted it, both the original deck and your printout would be available in on a shelf in an output area. Less often, your program might punch cards as additional output.\n\nCard decks for different jobs could be stacked in a reader by the operator, since each one had its own JCL header. On most large systems, the cards would not be read by the computer directly, as that would be too slow but instead would be read by a separate device and their image written to a tape drive.\n\nWhen minicomputers appeared in the 1960s, paper tape was often used instead of punch cards for smaller installations.\n\nBy the 1970s, computer terminal access to mainframes became more widespread, and punch card usage dwindled off. Then personal computers appeared, with their individual keyboards and screens in the mid to late 1970s.\n\nIBM 3270 terminal, used to access mainframes using TSO (Time Sharing Option).', 'aiModelVersion': '1'}",0.9998
Ira J Perlow,Updated 1y,Why was a mediocre OS like MS-DOS able to become so dominant?,"Why was a mediocre OS like MS-DOS able to become so dominant?

That would be my fault!
Although, for the time of 1981, MS-DOS (or PC-DOS) was not much different from others operating systems of the time, so calling it mediocre may be an overstatement.

Short History of MS-DOS
Back in 1981, IBM needed an Operating System (OS) for their entry into the “home” computer market, with the slant to small business computer market. They approached Gary Kildall at Digital Research who had written CP/M, the current popular OS for the 8 bit 8080 CPU for a version to run on x86 systems. They also spoke with Microsoft’s Bill Gates who didn’t have one, but he ran down the street to Seattle Computer Products, and bought rights from Tim Paterson for his 86-DOS (originally labeled QDOS) product, which was CP/M-like for his company x86 hardware.

Long story
 short, Bill Gates won the bid, and IBM sold PC-DOS. He also had worked out in the deal the ability to sell a DOS version for others with no royalties to IBM, which was called MS-DOS. Microsoft had arranged for Phoenix Software (prior name of Phoenix Technologies) to provide configuration of MS-DOS to manufacturers, using a consulting firm Sigma Digital Labs to actually perform the configuration work.

There were many MS-DOS systems (but not PC clones) that came to market, but there were relatively few apps that were screen-oriented without being customized for each system.

Compaq Clones the PC
In 1982, Compaq created the 1st “legal” clone of the IBM PC. The hardware was not patented, and with Microsoft providing MS-DOS, everything was available except the copyrighted ROM BIOS that IBM wrote. Compaq created a functional copy of the IBM PC BIOS using a clean room technique (so programmers who never saw IBM’s version wrote it, and thus could not be copied, although functionally worked the same.

This now made two computers on the market that could run identical screen-oriented apps, but was not enough to establish dominance. IBM prices were high, and Compaq’s prices, while not as high (and offered a “luggable” PC that IBM did not have), the availability was limited.

Phoenix Technologies Clones the BIOS for Everyone
In 1983 Phoenix Software was looking at legally cloning the PC BIOS as Compaq had done, except they wanted to license it to any and all companies. They did some initial work on it to see if the idea was practical.

I was a member of the New England Computer Society (NECS) since 1978 which was a home-brew computer group, and a member of the Boston Computer Society
 (BCS) PC Tech SIG (Special Interest Group) since about 1982. It was at the BCS I met two people from Sigma Digital Labs that would connect Phoenix Software to hiring me (as a consultant) to write their BIOS. I started talking to Phoenix sometime around February or March 1984 about the work.

And So the Age of Legal PC Clones Begins
In May 1984, I started the consulting job with Phoenix Software to write the PC BIOS. I was a perfect choice for them. I had never seen the PC BIOS source code, I was an expert in 8080 code (which was similar to the 8086 and 8088 CPUs code that a PC uses), I had configured dozens of CP/M and MP/M BIOSes, as well as a lot of configuration files for Modem 7, and was a computer hardware (CPU Architect) designer (meaning I would understand hardware nuances), and had written a small amount of x86 code for a missile tracker while at Raytheon Missile Systems Division.

It took 4 months to write the PC BIOS, another 2 months for the PC-XT BIOS and another 6 months for the PC-AT BIOS. By May 1985, Phoenix Technologies (I believe they changed the name in early 1988) had the BIOS market wrapped up and the clone industry exploded.

What if the Phoenix BIOS was Bad?
It’s important to understand that IF the first third-party BIOS clone was poorly written/coded, the entire PC market would probably not have taken off (we’d all be using Apple products and macOS !). Nor would the johnny-come-latelies to the legal third-party BIOS market (like Award, AMI, etc) ever happen.

MS-DOS was, initially anyway, essentially a file system laid on top of the BIOS disk functions and a code wrapper around most of the BIOS functions. As memory management and drivers started to be made to enhance MS-DOS, it started to be more than what it was originally.

The Takeaway
The market penetration of the PC clones (which means XT and AT clones too) placed 100% compatible hardware everywhere. This allowed ONE version of MS-DOS (or for that matter, any app) to be placed on every single PC clone. Dominance was assured for many years.

What made the ability for any OS to dominate was a common hardware platform that required zero configuration of the program or OS. We have since eliminated that identical hardware need today (to some extent),

A side effect of this hardware standardization was motherboard chipsets, larger and larger integration of electronics, and the PC hardware boomed. The PC growth had the side effect of growth using those same hardware technologies into other electronics, like VCR’s, DVD players, MP3 players, cell phones etc.

While Windows 3.0 and 3.1 in the beginning of the 1990’s would start to make inroads to the graphic OS on the PC clones, it would not be until about 2000 when the underlying MS-DOS in Windows would start to be eliminated.

Other operating systems existed at the time (1980’s) and people could easily run them both (especially before hard drive systems became prevalent). Eventually someone would have won out, or we’d have both, somewhat akin to macOS vs Windows.

Nowadays, you have a driver API for the OS level that adapts new hardware to the software. But none of this would happen till the hardware got significantly faster such that this extra hardware to software driver level would not slow down the application and the OSes added an API driver level.

And because we now have really fast systems and standard cross-OS graphic libraries, you can recompile an application for multiple OSes, e.g. macOS, Windows and Linux.

Notes:
Just to be clear, the effort of creating the Phoenix BIOS was not just me. Any major project takes a team. There were about 5 people who disassembled the IBM BIOS and wrote the specifications. Then there were people in Marketing, Promotion/Press, Legal, and maybe a few others I’ve forgotten. While the PC BIOS was coded completely by me, Al Weiner (consultant for Sigma Digital Labs) wrote the XT BIOS hard disk driver using a skeleton framework I gave him, and Stan Lyness (Phoenix Employee) wrote portions of the AT BIOS, including the Hard Drive code and some initialization code.

And the Halt and Catch Fire TV series (which I was asked about in a comment, which I then watched a bit of) has no relation to anything of reality. Nothing even remotely happened that way!

Info based on Comments:

Some additional history

In January 1975, I was sitting in the student IEEE lounge at my college having lunch with my friends. We were discussing the newest Popular Electronics issue describing the MITS Altair 8800 home computer kit (based around the Intel 8080 CPU). For $400, you can have your own computer!

I remember to this day saying to them, “The mainframe is dead”. Most of them just couldn’t see it. But I knew putting a small 8080 or better under every person’s fingertips is going to allow much more computing power than time-sharing on a mainframe. By 1990, maybe earlier, I think I was proved right.

But I never saw the Arpanet evolving into the internet, but that was a time of dial-up, acoustic coupled modems running at most 300 baud! By 1986 we had Compuserve (which was in many countries world wide allowing transfer of data across borders), and by 1992 (when the internet seemed to really take off) the Arpanet had morphed into a world-wide interconnected network. Well, I guess you can’t prognosticate everything! (But it’s possible I could have, see the 2nd half of my answer If you entered the computer technology industry in the days of punchcards and huge computers, did you foresee any of the technology we have today? Did anyone think you were crazy?)
The x86 language and register segmentation didn’t really create issues from the point of writing the BIOS.
The Phoenix BIOS had some compatibility issues that I had to tackle from bad apps that jumped straight into the BIOS expecting the segment register to be set the same as IBM’s. It was stuff like that that made understanding of the poorly written programs the definition of what compatibility meant. Some apps originally coded around Compaq differences, so Compaq left it alone. On the other hand, I coded for allowing these truly bizarre usages of the BIOS to work. Only occasionally would Phoenix have to decide between which competing bad usages to support.
Almost every function of the Phoenix BIOS was either a lot faster or at least as fast compared to IBM and other BIOSes.
When I started the Phoenix BIOS coding, no one, including myself, thought that the importance would be relevant in history. Only by looking back can you really say that. But I guess you can say that about a lot of things.
When I think about historical things, I think about how important a crux it was to history. I consider whether it would have eventually happened with someone else there. There were many people involved in other aspects of creating the Phoenix BIOS, but based upon seeing BIOS code from other companies later when I was designing motherboards, I felt that history was lucky (and myself lucky to be a part of it) for me to be that person to do the process of coding the 1st third party clone BIOS. It was just the serendipity of being that right person in the right place at the right time with the right skills.

References:

The legality of copying and the concept of intellectual property (IP) and copying the IBM BIOS is discussed in The Invisible Edge: Taking Your Strategy to the Next Level
 book.
For a more complete answer on writing how BIOS were written see:
How is the BIOS for a computer motherboard written? How was the first BIOS written?
To see a discussion of my work on creating the Phoenix BIOS, see
What is the coolest thing you have ever created alone as a programmer?
This describes the early needs for a “turbo” button, my design of the AST 286 Premium motherboard (fastest PC computer of 1987), and why the average person cares about memory timing for their PC
History of Computing: What did the ""turbo"" button on early 90s PCs do?
This explains why the only systems left are PCs, Apple Macintoshes, and ARM processors
Why did Digital Equipment Corporation (DEC) fail?
This explains the early PC history and the creation of the first S-100 computer system, the MITS Altair computer
Did the sales of computers rise after the invention of video games?
This discusses why IBM lost market share to the PC clones and eventually sold their PC division. Also, why IBM’s Micro Channel Architecture (MCA) based systems failed.
Why did IBM abandon the PC?
Who should get more credit for the rise of the PC: anonymous engineers at IBM and Xerox or Wozniak, Jobs, Allen and Gates?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/mj4ory0khlxut58e', 'title': 'Why was a mediocre OS like MS-DOS able to become so dominant?', 'score': {'original': 0.9351, 'ai': 0.0649}, 'blocks': [{'text': 'Why was a mediocre OS like MS-DOS able to become so dominant?\n\nThat would be my fault!\nAlthough, for the time of 1981, MS-DOS (or PC-DOS) was not much different from others operating systems of the time, so calling it mediocre may be an overstatement.\n\nShort History of MS-DOS\nBack in 1981, IBM needed an Operating System (OS) for their entry into the “home” computer market, with the slant to small business computer market. They approached Gary Kildall at Digital Research who had written CP/M, the current popular OS for the 8 bit 8080 CPU for a version to run on x86 systems. They also spoke with Microsoft’s Bill Gates who didn’t have one, but he ran down the street to Seattle Computer Products, and bought rights from Tim Paterson for his 86-DOS (originally labeled QDOS) product, which was CP/M-like for his company x86 hardware.\n\nLong story\n short, Bill Gates won the bid, and IBM sold PC-DOS. He also had worked out in the deal the ability to sell a DOS version for others with no royalties to IBM, which was called MS-DOS. Microsoft had arranged for Phoenix Software (prior name of Phoenix Technologies) to provide configuration of MS-DOS to manufacturers, using a consulting firm Sigma Digital Labs to actually perform the configuration work.\n\nThere were many MS-DOS systems (but not PC clones) that came to market, but there were relatively few apps that were screen-oriented without being customized for each system.\n\nCompaq Clones the PC\nIn 1982, Compaq created the 1st “legal” clone of the IBM PC. The hardware was not patented, and with Microsoft providing MS-DOS, everything was available except the copyrighted ROM BIOS that IBM wrote. Compaq created a functional copy of the IBM PC BIOS using a clean room technique (so programmers who never saw IBM’s version wrote it, and thus could not be copied, although functionally worked the same.\n\nThis now made two computers on the market that could run identical screen-oriented apps, but was not enough to establish dominance. IBM prices were high, and Compaq’s prices, while not as high (and offered a “luggable” PC that IBM did not have), the availability was limited.\n\nPhoenix Technologies Clones the BIOS for Everyone\nIn 1983 Phoenix Software was looking at legally cloning the PC BIOS as Compaq had done, except they wanted to license it to any and all companies. They did some initial work on it to see if the idea was practical.\n\nI was a member of the New England Computer Society (NECS) since 1978 which was a home-brew computer group, and a member of the Boston Computer Society\n (BCS) PC Tech SIG (Special Interest Group) since about 1982. It was at the BCS I met two people from Sigma Digital Labs that would connect Phoenix Software to hiring me (as a consultant) to write their BIOS. I started talking to Phoenix sometime around February or March 1984 about the work.\n\nAnd So the Age of Legal PC Clones Begins\nIn May 1984, I started the consulting job with Phoenix Software to write the PC BIOS. I was a perfect choice for them. I had never seen the PC BIOS', 'result': {'fake': 0.0338, 'real': 0.9662}, 'status': 'success'}, {'text': 'source code, I was an expert in 8080 code (which was similar to the 8086 and 8088 CPUs code that a PC uses), I had configured dozens of CP/M and MP/M BIOSes, as well as a lot of configuration files for Modem 7, and was a computer hardware (CPU Architect) designer (meaning I would understand hardware nuances), and had written a small amount of x86 code for a missile tracker while at Raytheon Missile Systems Division.\n\nIt took 4 months to write the PC BIOS, another 2 months for the PC-XT BIOS and another 6 months for the PC-AT BIOS. By May 1985, Phoenix Technologies (I believe they changed the name in early 1988) had the BIOS market wrapped up and the clone industry exploded.\n\nWhat if the Phoenix BIOS was Bad?\nIt’s important to understand that IF the first third-party BIOS clone was poorly written/coded, the entire PC market would probably not have taken off (we’d all be using Apple products and macOS !). Nor would the johnny-come-latelies to the legal third-party BIOS market (like Award, AMI, etc) ever happen.\n\nMS-DOS was, initially anyway, essentially a file system laid on top of the BIOS disk functions and a code wrapper around most of the BIOS functions. As memory management and drivers started to be made to enhance MS-DOS, it started to be more than what it was originally.\n\nThe Takeaway\nThe market penetration of the PC clones (which means XT and AT clones too) placed 100% compatible hardware everywhere. This allowed ONE version of MS-DOS (or for that matter, any app) to be placed on every single PC clone. Dominance was assured for many years.\n\nWhat made the ability for any OS to dominate was a common hardware platform that required zero configuration of the program or OS. We have since eliminated that identical hardware need today (to some extent),\n\nA side effect of this hardware standardization was motherboard chipsets, larger and larger integration of electronics, and the PC hardware boomed. The PC growth had the side effect of growth using those same hardware technologies into other electronics, like VCR’s, DVD players, MP3 players, cell phones etc.\n\nWhile Windows 3.0 and 3.1 in the beginning of the 1990’s would start to make inroads to the graphic OS on the PC clones, it would not be until about 2000 when the underlying MS-DOS in Windows would start to be eliminated.\n\nOther operating systems existed at the time (1980’s) and people could easily run them both (especially before hard drive systems became prevalent). Eventually someone would have won out, or we’d have both, somewhat akin to macOS vs Windows.\n\nNowadays, you have a driver API for the OS level that adapts new hardware to the software. But none of this would happen till the hardware got significantly faster such that this extra hardware to software driver level would not slow down the application and the OSes added an API driver level.\n\nAnd because we now have really fast systems and standard cross-OS graphic libraries, you can recompile an application for multiple OSes, e.g. macOS, Windows and Linux.\n\nNotes:\nJust to be clear, the effort of creating the', 'result': {'fake': 0.2134, 'real': 0.7866}, 'status': 'success'}, {'text': 'Phoenix BIOS was not just me. Any major project takes a team. There were about 5 people who disassembled the IBM BIOS and wrote the specifications. Then there were people in Marketing, Promotion/Press, Legal, and maybe a few others I’ve forgotten. While the PC BIOS was coded completely by me, Al Weiner (consultant for Sigma Digital Labs) wrote the XT BIOS hard disk driver using a skeleton framework I gave him, and Stan Lyness (Phoenix Employee) wrote portions of the AT BIOS, including the Hard Drive code and some initialization code.\n\nAnd the Halt and Catch Fire TV series (which I was asked about in a comment, which I then watched a bit of) has no relation to anything of reality. Nothing even remotely happened that way!\n\nInfo based on Comments:\n\nSome additional history\n\nIn January 1975, I was sitting in the student IEEE lounge at my college having lunch with my friends. We were discussing the newest Popular Electronics issue describing the MITS Altair 8800 home computer kit (based around the Intel 8080 CPU). For $400, you can have your own computer!\n\nI remember to this day saying to them, “The mainframe is dead”. Most of them just couldn’t see it. But I knew putting a small 8080 or better under every person’s fingertips is going to allow much more computing power than time-sharing on a mainframe. By 1990, maybe earlier, I think I was proved right.\n\nBut I never saw the Arpanet evolving into the internet, but that was a time of dial-up, acoustic coupled modems running at most 300 baud! By 1986 we had Compuserve (which was in many countries world wide allowing transfer of data across borders), and by 1992 (when the internet seemed to really take off) the Arpanet had morphed into a world-wide interconnected network. Well, I guess you can’t prognosticate everything! (But it’s possible I could have, see the 2nd half of my answer If you entered the computer technology industry in the days of punchcards and huge computers, did you foresee any of the technology we have today? Did anyone think you were crazy?)\nThe x86 language and register segmentation didn’t really create issues from the point of writing the BIOS.\nThe Phoenix BIOS had some compatibility issues that I had to tackle from bad apps that jumped straight into the BIOS expecting the segment register to be set the same as IBM’s. It was stuff like that that made understanding of the poorly written programs the definition of what compatibility meant. Some apps originally coded around Compaq differences, so Compaq left it alone. On the other hand, I coded for allowing these truly bizarre usages of the BIOS to work. Only occasionally would Phoenix have to decide between which competing bad usages to support.\nAlmost every function of the Phoenix BIOS was either a lot faster or at least as fast compared to IBM and other BIOSes.\nWhen I started the Phoenix BIOS coding, no one, including myself, thought that the importance would be relevant in history. Only by looking back can you really say that. But I guess you can say that', 'result': {'fake': 0.0009, 'real': 0.9991}, 'status': 'success'}, {'text': 'about a lot of things.\nWhen I think about historical things, I think about how important a crux it was to history. I consider whether it would have eventually happened with someone else there. There were many people involved in other aspects of creating the Phoenix BIOS, but based upon seeing BIOS code from other companies later when I was designing motherboards, I felt that history was lucky (and myself lucky to be a part of it) for me to be that person to do the process of coding the 1st third party clone BIOS. It was just the serendipity of being that right person in the right place at the right time with the right skills.\n\nReferences:\n\nThe legality of copying and the concept of intellectual property (IP) and copying the IBM BIOS is discussed in The Invisible Edge: Taking Your Strategy to the Next Level\n book.\nFor a more complete answer on writing how BIOS were written see:\nHow is the BIOS for a computer motherboard written? How was the first BIOS written?\nTo see a discussion of my work on creating the Phoenix BIOS, see\nWhat is the coolest thing you have ever created alone as a programmer?\nThis describes the early needs for a “turbo” button, my design of the AST 286 Premium motherboard (fastest PC computer of 1987), and why the average person cares about memory timing for their PC\nHistory of Computing: What did the ""turbo"" button on early 90s PCs do?\nThis explains why the only systems left are PCs, Apple Macintoshes, and ARM processors\nWhy did Digital Equipment Corporation (DEC) fail?\nThis explains the early PC history and the creation of the first S-100 computer system, the MITS Altair computer\nDid the sales of computers rise after the invention of video games?\nThis discusses why IBM lost market share to the PC clones and eventually sold their PC division. Also, why IBM’s Micro Channel Architecture (MCA) based systems failed.\nWhy did IBM abandon the PC?\nWho should get more credit for the rise of the PC: anonymous engineers at IBM and Xerox or Wozniak, Jobs, Allen and Gates?', 'result': {'fake': 0.1358, 'real': 0.8642}, 'status': 'success'}], 'credits_used': 20, 'credits': 1995460, 'subscription': 0, 'content': 'Why was a mediocre OS like MS-DOS able to become so dominant?\n\nThat would be my fault!\nAlthough, for the time of 1981, MS-DOS (or PC-DOS) was not much different from others operating systems of the time, so calling it mediocre may be an overstatement.\n\nShort History of MS-DOS\nBack in 1981, IBM needed an Operating System (OS) for their entry into the “home” computer market, with the slant to small business computer market. They approached Gary Kildall at Digital Research who had written CP/M, the current popular OS for the 8 bit 8080 CPU for a version to run on x86 systems. They also spoke with Microsoft’s Bill Gates who didn’t have one, but he ran down the street to Seattle Computer Products, and bought rights from Tim Paterson for his 86-DOS (originally labeled QDOS) product, which was CP/M-like for his company x86 hardware.\n\nLong story\n short, Bill Gates won the bid, and IBM sold PC-DOS. He also had worked out in the deal the ability to sell a DOS version for others with no royalties to IBM, which was called MS-DOS. Microsoft had arranged for Phoenix Software (prior name of Phoenix Technologies) to provide configuration of MS-DOS to manufacturers, using a consulting firm Sigma Digital Labs to actually perform the configuration work.\n\nThere were many MS-DOS systems (but not PC clones) that came to market, but there were relatively few apps that were screen-oriented without being customized for each system.\n\nCompaq Clones the PC\nIn 1982, Compaq created the 1st “legal” clone of the IBM PC. The hardware was not patented, and with Microsoft providing MS-DOS, everything was available except the copyrighted ROM BIOS that IBM wrote. Compaq created a functional copy of the IBM PC BIOS using a clean room technique (so programmers who never saw IBM’s version wrote it, and thus could not be copied, although functionally worked the same.\n\nThis now made two computers on the market that could run identical screen-oriented apps, but was not enough to establish dominance. IBM prices were high, and Compaq’s prices, while not as high (and offered a “luggable” PC that IBM did not have), the availability was limited.\n\nPhoenix Technologies Clones the BIOS for Everyone\nIn 1983 Phoenix Software was looking at legally cloning the PC BIOS as Compaq had done, except they wanted to license it to any and all companies. They did some initial work on it to see if the idea was practical.\n\nI was a member of the New England Computer Society (NECS) since 1978 which was a home-brew computer group, and a member of the Boston Computer Society\n (BCS) PC Tech SIG (Special Interest Group) since about 1982. It was at the BCS I met two people from Sigma Digital Labs that would connect Phoenix Software to hiring me (as a consultant) to write their BIOS. I started talking to Phoenix sometime around February or March 1984 about the work.\n\nAnd So the Age of Legal PC Clones Begins\nIn May 1984, I started the consulting job with Phoenix Software to write the PC BIOS. I was a perfect choice for them. I had never seen the PC BIOS source code, I was an expert in 8080 code (which was similar to the 8086 and 8088 CPUs code that a PC uses), I had configured dozens of CP/M and MP/M BIOSes, as well as a lot of configuration files for Modem 7, and was a computer hardware (CPU Architect) designer (meaning I would understand hardware nuances), and had written a small amount of x86 code for a missile tracker while at Raytheon Missile Systems Division.\n\nIt took 4 months to write the PC BIOS, another 2 months for the PC-XT BIOS and another 6 months for the PC-AT BIOS. By May 1985, Phoenix Technologies (I believe they changed the name in early 1988) had the BIOS market wrapped up and the clone industry exploded.\n\nWhat if the Phoenix BIOS was Bad?\nIt’s important to understand that IF the first third-party BIOS clone was poorly written/coded, the entire PC market would probably not have taken off (we’d all be using Apple products and macOS !). Nor would the johnny-come-latelies to the legal third-party BIOS market (like Award, AMI, etc) ever happen.\n\nMS-DOS was, initially anyway, essentially a file system laid on top of the BIOS disk functions and a code wrapper around most of the BIOS functions. As memory management and drivers started to be made to enhance MS-DOS, it started to be more than what it was originally.\n\nThe Takeaway\nThe market penetration of the PC clones (which means XT and AT clones too) placed 100% compatible hardware everywhere. This allowed ONE version of MS-DOS (or for that matter, any app) to be placed on every single PC clone. Dominance was assured for many years.\n\nWhat made the ability for any OS to dominate was a common hardware platform that required zero configuration of the program or OS. We have since eliminated that identical hardware need today (to some extent),\n\nA side effect of this hardware standardization was motherboard chipsets, larger and larger integration of electronics, and the PC hardware boomed. The PC growth had the side effect of growth using those same hardware technologies into other electronics, like VCR’s, DVD players, MP3 players, cell phones etc.\n\nWhile Windows 3.0 and 3.1 in the beginning of the 1990’s would start to make inroads to the graphic OS on the PC clones, it would not be until about 2000 when the underlying MS-DOS in Windows would start to be eliminated.\n\nOther operating systems existed at the time (1980’s) and people could easily run them both (especially before hard drive systems became prevalent). Eventually someone would have won out, or we’d have both, somewhat akin to macOS vs Windows.\n\nNowadays, you have a driver API for the OS level that adapts new hardware to the software. But none of this would happen till the hardware got significantly faster such that this extra hardware to software driver level would not slow down the application and the OSes added an API driver level.\n\nAnd because we now have really fast systems and standard cross-OS graphic libraries, you can recompile an application for multiple OSes, e.g. macOS, Windows and Linux.\n\nNotes:\nJust to be clear, the effort of creating the Phoenix BIOS was not just me. Any major project takes a team. There were about 5 people who disassembled the IBM BIOS and wrote the specifications. Then there were people in Marketing, Promotion/Press, Legal, and maybe a few others I’ve forgotten. While the PC BIOS was coded completely by me, Al Weiner (consultant for Sigma Digital Labs) wrote the XT BIOS hard disk driver using a skeleton framework I gave him, and Stan Lyness (Phoenix Employee) wrote portions of the AT BIOS, including the Hard Drive code and some initialization code.\n\nAnd the Halt and Catch Fire TV series (which I was asked about in a comment, which I then watched a bit of) has no relation to anything of reality. Nothing even remotely happened that way!\n\nInfo based on Comments:\n\nSome additional history\n\nIn January 1975, I was sitting in the student IEEE lounge at my college having lunch with my friends. We were discussing the newest Popular Electronics issue describing the MITS Altair 8800 home computer kit (based around the Intel 8080 CPU). For $400, you can have your own computer!\n\nI remember to this day saying to them, “The mainframe is dead”. Most of them just couldn’t see it. But I knew putting a small 8080 or better under every person’s fingertips is going to allow much more computing power than time-sharing on a mainframe. By 1990, maybe earlier, I think I was proved right.\n\nBut I never saw the Arpanet evolving into the internet, but that was a time of dial-up, acoustic coupled modems running at most 300 baud! By 1986 we had Compuserve (which was in many countries world wide allowing transfer of data across borders), and by 1992 (when the internet seemed to really take off) the Arpanet had morphed into a world-wide interconnected network. Well, I guess you can’t prognosticate everything! (But it’s possible I could have, see the 2nd half of my answer If you entered the computer technology industry in the days of punchcards and huge computers, did you foresee any of the technology we have today? Did anyone think you were crazy?)\nThe x86 language and register segmentation didn’t really create issues from the point of writing the BIOS.\nThe Phoenix BIOS had some compatibility issues that I had to tackle from bad apps that jumped straight into the BIOS expecting the segment register to be set the same as IBM’s. It was stuff like that that made understanding of the poorly written programs the definition of what compatibility meant. Some apps originally coded around Compaq differences, so Compaq left it alone. On the other hand, I coded for allowing these truly bizarre usages of the BIOS to work. Only occasionally would Phoenix have to decide between which competing bad usages to support.\nAlmost every function of the Phoenix BIOS was either a lot faster or at least as fast compared to IBM and other BIOSes.\nWhen I started the Phoenix BIOS coding, no one, including myself, thought that the importance would be relevant in history. Only by looking back can you really say that. But I guess you can say that about a lot of things.\nWhen I think about historical things, I think about how important a crux it was to history. I consider whether it would have eventually happened with someone else there. There were many people involved in other aspects of creating the Phoenix BIOS, but based upon seeing BIOS code from other companies later when I was designing motherboards, I felt that history was lucky (and myself lucky to be a part of it) for me to be that person to do the process of coding the 1st third party clone BIOS. It was just the serendipity of being that right person in the right place at the right time with the right skills.\n\nReferences:\n\nThe legality of copying and the concept of intellectual property (IP) and copying the IBM BIOS is discussed in The Invisible Edge: Taking Your Strategy to the Next Level\n book.\nFor a more complete answer on writing how BIOS were written see:\nHow is the BIOS for a computer motherboard written? How was the first BIOS written?\nTo see a discussion of my work on creating the Phoenix BIOS, see\nWhat is the coolest thing you have ever created alone as a programmer?\nThis describes the early needs for a “turbo” button, my design of the AST 286 Premium motherboard (fastest PC computer of 1987), and why the average person cares about memory timing for their PC\nHistory of Computing: What did the ""turbo"" button on early 90s PCs do?\nThis explains why the only systems left are PCs, Apple Macintoshes, and ARM processors\nWhy did Digital Equipment Corporation (DEC) fail?\nThis explains the early PC history and the creation of the first S-100 computer system, the MITS Altair computer\nDid the sales of computers rise after the invention of video games?\nThis discusses why IBM lost market share to the PC clones and eventually sold their PC division. Also, why IBM’s Micro Channel Architecture (MCA) based systems failed.\nWhy did IBM abandon the PC?\nWho should get more credit for the rise of the PC: anonymous engineers at IBM and Xerox or Wozniak, Jobs, Allen and Gates?', 'aiModelVersion': '1'}",0.9351
Ian Murphy,1y,Why did Borland fail?,"There seem to be quite a few people posting answers here saying that Borland were killed by Microsoft. I’m afraid I have to disagree. I was a faithful borland user, having learned Pascal and C using the tiny compact Turbo versions of each and I loved both. They were used in my university course and were great.

At the time the MS compiler was a classic edit, save, compile, link, run collection and it was sloooow. By comparison the (first) borland compiler was tiny and you could run an application by hitting a single key and be running your application is seconds.

But, Borland didn’t pay enough attention to quality control.

I happen to have been a programmer just when Ms started improving their compiler and Borland brought out the bigger bulked up integrated environment. Can’t remember which version that was. It no longer fitted on a floppy, that was certain.

At that time I was a junior programmer and had been handed an application with something like 250k lines of C which contained some mysterious bugs which needed tracking down. It compiled in both the MS compiler and the Borland one and we had copies of both, so naturally I went for the Borland ‘IDE’.

I spent quite some time trying to track down bugs, stepping through libraries, adding debugging code, checks on return values, all sorts of stuff, but I would just keep on getting nonsensical results, which varied depending on the run. I eventually ended up stepping into the assembly code as I was convinced some include library was doing something weird. After a fair amount of wasted time I found I was able to step into the same block of assembler and watch as the debugger itself trashed part of the code while I executed a single instruction (an add or something similar, which couldn’t be overwriting memory).

After realising that the compiler didn’t produce the same results compiling the same code twice and finding that the debugger was buggy I had to just drop Borland.

After switching back to the MS compiler all the weird behaviours disappeared and we went back to tracking the actual bugs in the application.

I’m sure there were other reasons, but I never used Borland again.

I hated that job, debugging other peoples undocumented code is the worst.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/kj64n8qmvrb2yutg', 'title': 'Why did Borland fail?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'There seem to be quite a few people posting answers here saying that Borland were killed by Microsoft. I’m afraid I have to disagree. I was a faithful borland user, having learned Pascal and C using the tiny compact Turbo versions of each and I loved both. They were used in my university course and were great.\n\nAt the time the MS compiler was a classic edit, save, compile, link, run collection and it was sloooow. By comparison the (first) borland compiler was tiny and you could run an application by hitting a single key and be running your application is seconds.\n\nBut, Borland didn’t pay enough attention to quality control.\n\nI happen to have been a programmer just when Ms started improving their compiler and Borland brought out the bigger bulked up integrated environment. Can’t remember which version that was. It no longer fitted on a floppy, that was certain.\n\nAt that time I was a junior programmer and had been handed an application with something like 250k lines of C which contained some mysterious bugs which needed tracking down. It compiled in both the MS compiler and the Borland one and we had copies of both, so naturally I went for the Borland ‘IDE’.\n\nI spent quite some time trying to track down bugs, stepping through libraries, adding debugging code, checks on return values, all sorts of stuff, but I would just keep on getting nonsensical results, which varied depending on the run. I eventually ended up stepping into the assembly code as I was convinced some include library was doing something weird. After a fair amount of wasted time I found I was able to step into the same block of assembler and watch as the debugger itself trashed part of the code while I executed a single instruction (an add or something similar, which couldn’t be overwriting memory).\n\nAfter realising that the compiler didn’t produce the same results compiling the same code twice and finding that the debugger was buggy I had to just drop Borland.\n\nAfter switching back to the MS compiler all the weird behaviours disappeared and we went back to tracking the actual bugs in the application.\n\nI’m sure there were other reasons, but I never used Borland again.\n\nI hated that job, debugging other peoples undocumented code is the worst.', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 4, 'credits': 1995456, 'subscription': 0, 'content': 'There seem to be quite a few people posting answers here saying that Borland were killed by Microsoft. I’m afraid I have to disagree. I was a faithful borland user, having learned Pascal and C using the tiny compact Turbo versions of each and I loved both. They were used in my university course and were great.\n\nAt the time the MS compiler was a classic edit, save, compile, link, run collection and it was sloooow. By comparison the (first) borland compiler was tiny and you could run an application by hitting a single key and be running your application is seconds.\n\nBut, Borland didn’t pay enough attention to quality control.\n\nI happen to have been a programmer just when Ms started improving their compiler and Borland brought out the bigger bulked up integrated environment. Can’t remember which version that was. It no longer fitted on a floppy, that was certain.\n\nAt that time I was a junior programmer and had been handed an application with something like 250k lines of C which contained some mysterious bugs which needed tracking down. It compiled in both the MS compiler and the Borland one and we had copies of both, so naturally I went for the Borland ‘IDE’.\n\nI spent quite some time trying to track down bugs, stepping through libraries, adding debugging code, checks on return values, all sorts of stuff, but I would just keep on getting nonsensical results, which varied depending on the run. I eventually ended up stepping into the assembly code as I was convinced some include library was doing something weird. After a fair amount of wasted time I found I was able to step into the same block of assembler and watch as the debugger itself trashed part of the code while I executed a single instruction (an add or something similar, which couldn’t be overwriting memory).\n\nAfter realising that the compiler didn’t produce the same results compiling the same code twice and finding that the debugger was buggy I had to just drop Borland.\n\nAfter switching back to the MS compiler all the weird behaviours disappeared and we went back to tracking the actual bugs in the application.\n\nI’m sure there were other reasons, but I never used Borland again.\n\nI hated that job, debugging other peoples undocumented code is the worst.', 'aiModelVersion': '1'}",0.9999
Geoff Caplan,Updated 2y,How tedious was working with early computers?,"But they weren’t “early computers”. They were the latest thing and wildly exciting.

In the early ’70s I wrote my first programs on punch cards, as part of a school outreach initiative from a local University. A van turned up once a week to pick up our cards and deliver the most recent output.

It was an exciting introduction to a revolutionary technology, even though the Read-Eval-Print-Loop took 168 hours!

So getting my hands on my friend’s BBC Acorn with a tape drive was a huge step forwards. Nowadays we’d consider the load times glacial, but they were orders of magnitude quicker than accessing a remote mainframe. And we soon had floppy disk drives like these and things speeded up even more.

Then I set up my first business and laid out a king’s ransom on an Atari Mega ST, with an exotic 2 megs of RAM and extravagant 20 meg hard drive (from memory the drive alone cost £450 which was serious moolah in those days).

What you probably don’t realise is that this was a highly functional machine. Back then, developers knew how to code for limited resources, and it ran like the wind.

When I saw a demo of Windows 1 at a show it appeared laughably sluggish - the Amiga, Apricot, Atari and Mac were far superior - particularly the Apricot, which was to die for…

No edition of Windows has approached the responsiveness of any of these machines - they were a joy to use. I was wildly in love with my Atari. If anything, the user experience has gone backwards. The dominance of Windows is a triumph of marketing, not of technology.

My little Atari marvel ran a spreadsheet, accounting program, SQL database, FTP, Telnet, email, bulletin boards, the best WP I’ve ever used, a highly capable DTP package, vector and bitmap graphics packages, games, MIDI music… And there was good choice of compilers - I was able to play with Assembler, Basic and C.

With the exception of the WWW, which hadn’t been invented, my mid-80s Atari could do pretty much everything I do today. Since then improvement has been incremental rather than revolutionary, and today’s bloated software actually runs slower, despite my high-end HP Workstation.

And then I managed to blag a cheap ex-display NextStation - Steve Jobs’s wet dream and probably the sexiest machine ever built. It was so far ahead of its time it was mind-blowing.

Did I feel deprived? What do you think? I thought I had entered computer heaven.

Then one fine day I was browsing the CERN ftp site - they acted as a repository for the NextStep community - and spotted a mysterious folder called “WWW”.

I downloaded a file named something like WorldWideWeb-0.2 and fired up a prototype web browser. It was just a proof-of-concept, so there wasn’t a public web to access at that stage. But the web was developed on the NextStation, and we were ahead of the curve. So the first point of presence that opened in the UK we shared the requisite Perl scripts and logged on. And on 1 June 1992 I was one of the first people in the word to load up a website…

I was on Yahoo the day it launched. And Alta Vista. And Amazon. And Google…

Does this sound “tedious” to you?

These pioneering days are long gone. Nowadays I’m using a 10 year old workstation that’s more than I really need. With the exception of video streaming and video calls, nothing much has changed for a decade or two and progress has slowed to a crawl.

The current generation have missed out on the real excitement.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/uvb5ewdas1xp804k', 'title': 'How tedious was working with early computers?', 'score': {'original': 0.5153, 'ai': 0.4847}, 'blocks': [{'text': 'But they weren’t “early computers”. They were the latest thing and wildly exciting.\n\nIn the early ’70s I wrote my first programs on punch cards, as part of a school outreach initiative from a local University. A van turned up once a week to pick up our cards and deliver the most recent output.\n\nIt was an exciting introduction to a revolutionary technology, even though the Read-Eval-Print-Loop took 168 hours!\n\nSo getting my hands on my friend’s BBC Acorn with a tape drive was a huge step forwards. Nowadays we’d consider the load times glacial, but they were orders of magnitude quicker than accessing a remote mainframe. And we soon had floppy disk drives like these and things speeded up even more.\n\nThen I set up my first business and laid out a king’s ransom on an Atari Mega ST, with an exotic 2 megs of RAM and extravagant 20 meg hard drive (from memory the drive alone cost £450 which was serious moolah in those days).\n\nWhat you probably don’t realise is that this was a highly functional machine. Back then, developers knew how to code for limited resources, and it ran like the wind.\n\nWhen I saw a demo of Windows 1 at a show it appeared laughably sluggish - the Amiga, Apricot, Atari and Mac were far superior - particularly the Apricot, which was to die for…\n\nNo edition of Windows has approached the responsiveness of any of these machines - they were a joy to use. I was wildly in love with my Atari. If anything, the user experience has gone backwards. The dominance of Windows is a triumph of marketing, not of technology.\n\nMy little Atari marvel ran a spreadsheet, accounting program, SQL database, FTP, Telnet, email, bulletin boards, the best WP I’ve ever used, a highly capable DTP package, vector and bitmap graphics packages, games, MIDI music… And there was good choice of compilers - I was able to play with Assembler, Basic and C.\n\nWith the exception of the WWW, which hadn’t been invented, my mid-80s Atari could do pretty much everything I do today. Since then improvement has been incremental rather than revolutionary, and today’s bloated software actually runs slower, despite my high-end HP Workstation.\n\nAnd then I managed to blag a cheap ex-display NextStation - Steve Jobs’s wet dream and probably the sexiest machine ever built. It was so far ahead of its time it was mind-blowing.\n\nDid I feel deprived? What do you think? I thought I had entered computer heaven.\n\nThen one fine day I was browsing the CERN ftp site - they acted as a repository for the NextStep community - and spotted a mysterious folder called “WWW”.\n\nI downloaded a file named something like WorldWideWeb-0.2 and fired up a prototype web browser. It was just a proof-of-concept, so there wasn’t a public web to access at that stage. But the web was developed on the NextStation, and we were ahead of the curve. So the first point of presence that opened in the UK we shared the requisite Perl scripts and logged on. And on 1 June 1992 I was one of', 'result': {'fake': 0.0094, 'real': 0.9906}, 'status': 'success'}, {'text': 'the first people in the word to load up a website…\n\nI was on Yahoo the day it launched. And Alta Vista. And Amazon. And Google…\n\nDoes this sound “tedious” to you?\n\nThese pioneering days are long gone. Nowadays I’m using a 10 year old workstation that’s more than I really need. With the exception of video streaming and video calls, nothing much has changed for a decade or two and progress has slowed to a crawl.\n\nThe current generation have missed out on the real excitement.', 'result': {'fake': 0.7675, 'real': 0.2325}, 'status': 'success'}], 'credits_used': 7, 'credits': 1995449, 'subscription': 0, 'content': 'But they weren’t “early computers”. They were the latest thing and wildly exciting.\n\nIn the early ’70s I wrote my first programs on punch cards, as part of a school outreach initiative from a local University. A van turned up once a week to pick up our cards and deliver the most recent output.\n\nIt was an exciting introduction to a revolutionary technology, even though the Read-Eval-Print-Loop took 168 hours!\n\nSo getting my hands on my friend’s BBC Acorn with a tape drive was a huge step forwards. Nowadays we’d consider the load times glacial, but they were orders of magnitude quicker than accessing a remote mainframe. And we soon had floppy disk drives like these and things speeded up even more.\n\nThen I set up my first business and laid out a king’s ransom on an Atari Mega ST, with an exotic 2 megs of RAM and extravagant 20 meg hard drive (from memory the drive alone cost £450 which was serious moolah in those days).\n\nWhat you probably don’t realise is that this was a highly functional machine. Back then, developers knew how to code for limited resources, and it ran like the wind.\n\nWhen I saw a demo of Windows 1 at a show it appeared laughably sluggish - the Amiga, Apricot, Atari and Mac were far superior - particularly the Apricot, which was to die for…\n\nNo edition of Windows has approached the responsiveness of any of these machines - they were a joy to use. I was wildly in love with my Atari. If anything, the user experience has gone backwards. The dominance of Windows is a triumph of marketing, not of technology.\n\nMy little Atari marvel ran a spreadsheet, accounting program, SQL database, FTP, Telnet, email, bulletin boards, the best WP I’ve ever used, a highly capable DTP package, vector and bitmap graphics packages, games, MIDI music… And there was good choice of compilers - I was able to play with Assembler, Basic and C.\n\nWith the exception of the WWW, which hadn’t been invented, my mid-80s Atari could do pretty much everything I do today. Since then improvement has been incremental rather than revolutionary, and today’s bloated software actually runs slower, despite my high-end HP Workstation.\n\nAnd then I managed to blag a cheap ex-display NextStation - Steve Jobs’s wet dream and probably the sexiest machine ever built. It was so far ahead of its time it was mind-blowing.\n\nDid I feel deprived? What do you think? I thought I had entered computer heaven.\n\nThen one fine day I was browsing the CERN ftp site - they acted as a repository for the NextStep community - and spotted a mysterious folder called “WWW”.\n\nI downloaded a file named something like WorldWideWeb-0.2 and fired up a prototype web browser. It was just a proof-of-concept, so there wasn’t a public web to access at that stage. But the web was developed on the NextStation, and we were ahead of the curve. So the first point of presence that opened in the UK we shared the requisite Perl scripts and logged on. And on 1 June 1992 I was one of the first people in the word to load up a website…\n\nI was on Yahoo the day it launched. And Alta Vista. And Amazon. And Google…\n\nDoes this sound “tedious” to you?\n\nThese pioneering days are long gone. Nowadays I’m using a 10 year old workstation that’s more than I really need. With the exception of video streaming and video calls, nothing much has changed for a decade or two and progress has slowed to a crawl.\n\nThe current generation have missed out on the real excitement.', 'aiModelVersion': '1'}",0.5153
Alan Kay,3y,What are the most important elements of computing history not widely known by current CS graduates or working developers?,"I think this question is much too large for a Quora sized answer (in part because of a combination of a lack of curiosity in the field generally, and a general lack of emphasis on history in schooling).

Just to pick four here in no special order — and from about the same period (~ 1962) — how about:

1. John McCarthy’s approach to computation and reasoning that allowed large complex interrelated “situations” to progress forward in states but retained the ability to use logic to reason about causality and relatedness (“fluents” etc.) Adopting this idea instead of the much weaker notions of “semaphores” would have made a big difference in every part of computing including much of programming, systems designs, and AI.

Anyone in computing should have read and understood everything that McCarthy did and wrote (this doesn’t mean he was right about everything, but “interesting people are interesting whether they are right or wrong”, and John was one of the most interesting in his field and century).

cf. McCarthy, J. (1963). Situations, actions and causal laws. Stanford Artificial Intelligence Project: Memo 2
. Also in “Semantic Information Processing” (Minsky ed.).

2. Doug Engelbart’s actual goals and plans (as expressed in his original proposal to the Air Force in 1962). Understanding and acting on this would have allowed Moore’s Law revolution into personal computing to have taken full advantage of the deepest ideas about it by Doug, and some of those after him who developed these ideas further. (The current state of affairs is such a botch as to not even address most of the important ideas at all, let alone doing them badly).

cf. Douglas Engelbart (1962) Augmenting Human Intellect — A Conceptual Framework

3. Ivan Sutherland’s “Sketchpad” system — done in 1962 — which was not just about the invention of interactive computer graphics, CAD, object-oriented design and relationships, but also showed how programming really needed to be developed forward in computing, for all programming, and especially for end-users.

cf Ivan Sutherland (1963) “Sketchpad: A Man-Machine Interactive System”

4. Bob Barton’s “A new approach to the functional design of a computer system” (~ 1961) which showed how hardware architecture should start to seriously cater to the advent and great need for much higher level programming languages and environments. This led to the Burroughs B5000/5500/6500 etc but had very little impact on what most people thought mainstream HW and SW was about (and until this day quite unfortunately).

cf. Robert S. Barton: “A new approach to the functional design of a computer system”, Proc. WJCC, 1961

Note: All four of the above early sixties ideas and insights by out and out geniuses had a great influence on subsequent work in the ARPA community, and especially at Xerox Parc.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/u2mgc0hx81fp5o39', 'title': 'What are the most important elements of computing history not widely known by current CS graduates or working developers?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'I think this question is much too large for a Quora sized answer (in part because of a combination of a lack of curiosity in the field generally, and a general lack of emphasis on history in schooling).\n\nJust to pick four here in no special order — and from about the same period (~ 1962) — how about:\n\n1. John McCarthy’s approach to computation and reasoning that allowed large complex interrelated “situations” to progress forward in states but retained the ability to use logic to reason about causality and relatedness (“fluents” etc.) Adopting this idea instead of the much weaker notions of “semaphores” would have made a big difference in every part of computing including much of programming, systems designs, and AI.\n\nAnyone in computing should have read and understood everything that McCarthy did and wrote (this doesn’t mean he was right about everything, but “interesting people are interesting whether they are right or wrong”, and John was one of the most interesting in his field and century).\n\ncf. McCarthy, J. (1963). Situations, actions and causal laws. Stanford Artificial Intelligence Project: Memo 2\n. Also in “Semantic Information Processing” (Minsky ed.).\n\n2. Doug Engelbart’s actual goals and plans (as expressed in his original proposal to the Air Force in 1962). Understanding and acting on this would have allowed Moore’s Law revolution into personal computing to have taken full advantage of the deepest ideas about it by Doug, and some of those after him who developed these ideas further. (The current state of affairs is such a botch as to not even address most of the important ideas at all, let alone doing them badly).\n\ncf. Douglas Engelbart (1962) Augmenting Human Intellect — A Conceptual Framework\n\n3. Ivan Sutherland’s “Sketchpad” system — done in 1962 — which was not just about the invention of interactive computer graphics, CAD, object-oriented design and relationships, but also showed how programming really needed to be developed forward in computing, for all programming, and especially for end-users.\n\ncf Ivan Sutherland (1963) “Sketchpad: A Man-Machine Interactive System”\n\n4. Bob Barton’s “A new approach to the functional design of a computer system” (~ 1961) which showed how hardware architecture should start to seriously cater to the advent and great need for much higher level programming languages and environments. This led to the Burroughs B5000/5500/6500 etc but had very little impact on what most people thought mainstream HW and SW was about (and until this day quite unfortunately).\n\ncf. Robert S. Barton: “A new approach to the functional design of a computer system”, Proc. WJCC, 1961\n\nNote: All four of the above early sixties ideas and insights by out and out geniuses had a great influence on subsequent work in the ARPA community, and especially at Xerox Parc.', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995444, 'subscription': 0, 'content': 'I think this question is much too large for a Quora sized answer (in part because of a combination of a lack of curiosity in the field generally, and a general lack of emphasis on history in schooling).\n\nJust to pick four here in no special order — and from about the same period (~ 1962) — how about:\n\n1. John McCarthy’s approach to computation and reasoning that allowed large complex interrelated “situations” to progress forward in states but retained the ability to use logic to reason about causality and relatedness (“fluents” etc.) Adopting this idea instead of the much weaker notions of “semaphores” would have made a big difference in every part of computing including much of programming, systems designs, and AI.\n\nAnyone in computing should have read and understood everything that McCarthy did and wrote (this doesn’t mean he was right about everything, but “interesting people are interesting whether they are right or wrong”, and John was one of the most interesting in his field and century).\n\ncf. McCarthy, J. (1963). Situations, actions and causal laws. Stanford Artificial Intelligence Project: Memo 2\n. Also in “Semantic Information Processing” (Minsky ed.).\n\n2. Doug Engelbart’s actual goals and plans (as expressed in his original proposal to the Air Force in 1962). Understanding and acting on this would have allowed Moore’s Law revolution into personal computing to have taken full advantage of the deepest ideas about it by Doug, and some of those after him who developed these ideas further. (The current state of affairs is such a botch as to not even address most of the important ideas at all, let alone doing them badly).\n\ncf. Douglas Engelbart (1962) Augmenting Human Intellect — A Conceptual Framework\n\n3. Ivan Sutherland’s “Sketchpad” system — done in 1962 — which was not just about the invention of interactive computer graphics, CAD, object-oriented design and relationships, but also showed how programming really needed to be developed forward in computing, for all programming, and especially for end-users.\n\ncf Ivan Sutherland (1963) “Sketchpad: A Man-Machine Interactive System”\n\n4. Bob Barton’s “A new approach to the functional design of a computer system” (~ 1961) which showed how hardware architecture should start to seriously cater to the advent and great need for much higher level programming languages and environments. This led to the Burroughs B5000/5500/6500 etc but had very little impact on what most people thought mainstream HW and SW was about (and until this day quite unfortunately).\n\ncf. Robert S. Barton: “A new approach to the functional design of a computer system”, Proc. WJCC, 1961\n\nNote: All four of the above early sixties ideas and insights by out and out geniuses had a great influence on subsequent work in the ARPA community, and especially at Xerox Parc.', 'aiModelVersion': '1'}",0.9999
Mark Harrison,Updated 4y,The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?,"No, and not (just) because we don’t expect software written today to still be in use in 8,000 years time.

To be fair, the developers of the 1960s thought the same about 40 year time horizons…

In most cases, though, we DON’T store years in 4 digits. Instead, we store timestamps in some underlying format, and rely on our programming languages to work out how to give us extra time functions from them.

A common format for internal representation is “seconds since 00:00 on 1st January 1970 in the UTC time zone.”

On 32 bit systems, that means we can only count up until 19th January 2038. This is “soon enough” that those of us involved in things like loan calculation software (who worry about, say, 25-year date frames), had to move away from 32 bit systems a few years ago.

Fortunately, in the wake of the Y2k issues, we’d learned, and were coding with the expectation of slotting in 64-bit systems when the main “vendors” got around to releasing them. [1]

That buys us over 290 billion years :-)

[1] There’s an interesting question raised in the comments by Ben Yeomans about why the move to 64-bit made a difference. My experience was in the Linux world, and it seems that the posix type definitions in the linux kernel define the type time_t by reference to kernel_long_t, which is in turn defined by reference to the number of bits in the kernel… most of the software I was worried about sat a long way above the kernel, but relied on the underlying Posix types defined therein. [I think that this paragraph is true in the detail… The real fact is that, at the time, it didn’t matter WHY upgrading to 64-bit versions worked, the key point was that that upgrade fixed all the issues we had :-) ]","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/bgd0amt98ifjyskq', 'title': 'The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'No, and not (just) because we don’t expect software written today to still be in use in 8,000 years time.\n\nTo be fair, the developers of the 1960s thought the same about 40 year time horizons…\n\nIn most cases, though, we DON’T store years in 4 digits. Instead, we store timestamps in some underlying format, and rely on our programming languages to work out how to give us extra time functions from them.\n\nA common format for internal representation is “seconds since 00:00 on 1st January 1970 in the UTC time zone.”\n\nOn 32 bit systems, that means we can only count up until 19th January 2038. This is “soon enough” that those of us involved in things like loan calculation software (who worry about, say, 25-year date frames), had to move away from 32 bit systems a few years ago.\n\nFortunately, in the wake of the Y2k issues, we’d learned, and were coding with the expectation of slotting in 64-bit systems when the main “vendors” got around to releasing them. [1]\n\nThat buys us over 290 billion years :-)\n\n[1] There’s an interesting question raised in the comments by Ben Yeomans about why the move to 64-bit made a difference. My experience was in the Linux world, and it seems that the posix type definitions in the linux kernel define the type time_t by reference to kernel_long_t, which is in turn defined by reference to the number of bits in the kernel… most of the software I was worried about sat a long way above the kernel, but relied on the underlying Posix types defined therein. [I think that this paragraph is true in the detail… The real fact is that, at the time, it didn’t matter WHY upgrading to 64-bit versions worked, the key point was that that upgrade fixed all the issues we had :-) ]', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 4, 'credits': 1995440, 'subscription': 0, 'content': 'No, and not (just) because we don’t expect software written today to still be in use in 8,000 years time.\n\nTo be fair, the developers of the 1960s thought the same about 40 year time horizons…\n\nIn most cases, though, we DON’T store years in 4 digits. Instead, we store timestamps in some underlying format, and rely on our programming languages to work out how to give us extra time functions from them.\n\nA common format for internal representation is “seconds since 00:00 on 1st January 1970 in the UTC time zone.”\n\nOn 32 bit systems, that means we can only count up until 19th January 2038. This is “soon enough” that those of us involved in things like loan calculation software (who worry about, say, 25-year date frames), had to move away from 32 bit systems a few years ago.\n\nFortunately, in the wake of the Y2k issues, we’d learned, and were coding with the expectation of slotting in 64-bit systems when the main “vendors” got around to releasing them. [1]\n\nThat buys us over 290 billion years :-)\n\n[1] There’s an interesting question raised in the comments by Ben Yeomans about why the move to 64-bit made a difference. My experience was in the Linux world, and it seems that the posix type definitions in the linux kernel define the type time_t by reference to kernel_long_t, which is in turn defined by reference to the number of bits in the kernel… most of the software I was worried about sat a long way above the kernel, but relied on the underlying Posix types defined therein. [I think that this paragraph is true in the detail… The real fact is that, at the time, it didn’t matter WHY upgrading to 64-bit versions worked, the key point was that that upgrade fixed all the issues we had :-) ]', 'aiModelVersion': '1'}",0.9998
Alan Kay,Updated 1y,What is the significance of Butler Lampson's work?,"A good reply is much too large for a Quora sized reply.

Basically: Butler is one of the most significant computer scientists in history — and has been one since he moved from physics to computing almost 60 years ago. He was always a “principal scientist, investigator, and thought leader” even as a grad student. He is a quintessential example of the word “brilliant” (and the term seems to have been coined for him).

In the 60s while hearing a rapidly delivered perfectly formed Butler talk, a fellow grad student turned to me and said “You know, it’s just a privilege to know someone like Butler”. When asked about “computer science” and “computer scientist” I’ve said “Look at Butler to see an example of a top computer scientist — it’s hard to put into exact words, but he is one”.

The list of his accomplishments is long, and so is the list of awards. Much of his significance has been in how he was able to both understand, invent, and influence the large world of systems of many kinds. These include a number of significant operating systems (including SDS-940 and Tenex), computers (including the Xerox Parc MAXC and Alto), networks (including the Ethernet and the Internet), and programming languages (including Mesa).

He is both a great thinker and a great doer, and both of these plus his larger than life personality almost literally dragged the whole field forward, and made all the rest of us into better computer scientists. I.e. a kind of “magical person”.

I will return here periodically with a little history and a few stories, but they are really commercials for readers to look at larger accounts of Butler’s larger than life history.

The citations and bio for his 1992 ACM Turing Award (the highest award in computing) are good to start with:

Butler W Lampson
Butler Lampson was born in Washington DC and educated at The Lawrenceville School , an elite boarding school 6 miles from Princeton New Jersey. Butler’s first computer was an underutilized IBM 650 at Princeton, which an enterprising high school classmate found and got permission to use while it was idle. It had a drum memory with 2000 words of ten decimal digits, and the only I/O aside from the console was a card reader/punch, controlled partly by the computer and partly by a plug board. As an undergraduate at Harvard, Lampson learned the APL programming language from Ken Iverson , who spent a year on sabbatical there. At that time there was no implementation of the language, and Ken didn’t want one because he was sure that it would make compromises that would wreck the language. While still an undergraduate Lampson also programmed a PDP-1 to analyze spark chamber photographs from the Cambridge Electron Accelerator and wrote a display editor for the PDP-1’s one-point-at-a-time display. Butler went to the University of California at Berkeley in the fall of 1964 as a graduate student in physics. At the Fall Joint Computer Conference in San Francisco that year he ran across Steve Russell from MIT, who told him about the Genie project hidden behind an unmarked door in Cory Hall. There he met Peter Deutsch and Mel Pirtle, the Principal Investigator, and was quickly seduced away from physics to study computers. The Genie project modified a Scientific Data Systems SDS 930 minicomputer to create the first time-sharing system with character-by-character interaction. Later SDS marketed it as the SDS 940, the first commercially available general purpose time-sharing system. Butler wrote parts of the operating system and several programming languages, notably Cal, an interactive language for numerical computation derived from Cliff Shaw’s Joss , and the QSPL system programming language done with Peter Deutsch. At Berkeley he also designed the Cal time-sharing system for a CDC 6400 in the computer center, together with Jim Gray , Charles Simonyi , Howard Sturgis and Bruce Lindsay , who all went on to later fame. This was the first capability-based system to have a real user community. It pioneered the ideas of shadow pages and redo logs, but also taught us that capabilities are not a good basis for long-term security. The Genie project researchers couldn’t figure out how to build the much more grandiose second system at Berkeley, so in 1969 they started Berkeley Computer Corporation (BCC) to do it. After two years it burned through $4,000,000 and built just one working system before folding. Butler designed and coded most of the microcoded parts of the operating system and worked on the SPL system programming language. Around this time he also devised the access matrix model for computer security, unifying the ideas of capabilities and access control lists. Luckily, as BCC was ending the Xerox Palo Alto Research Center (PARC) was getting started, and Bob Tayl
https://amturing.acm.org/award_winners/lampson_1142421.cfm

Next, the Wikipedia article —

Butler Lampson - Wikipedia
American computer scientist Butler W. Lampson , ForMemRS , (born December 23, 1943) is an American computer scientist best known for his contributions to the development and implementation of distributed personal computing . Education and early life [ edit ] After graduating from the Lawrenceville School (where in 2009 he was awarded the Aldo Leopold Award, also known as the Lawrenceville Medal, Lawrenceville's highest award to alumni), Lampson received an A.B. in physics ( magna cum laude with highest honors in the discipline) from Harvard University in 1964 and a PhD in electrical engineering and computer science from the University of California, Berkeley in 1967. Career and research [ edit ] Professional Developers Conference 2009 Technical Leaders Panel During the 1960s, Lampson and others were part of Project GENIE at UC Berkeley. In 1965, several Project GENIE members, specifically Lampson and Peter Deutsch , developed the Berkeley Timesharing System for Scientific Data Systems ' SDS 940 computer. After completing his doctorate, Lampson stayed on at UC Berkeley as an assistant professor (1967–1970) and associate professor (1970–1971) of computer science. For a period of time, he concurrently served as director of system development for the Berkeley Computer Corporation (1969–1971). In 1971, Lampson became one of the founding members of Xerox PARC , where he worked in the Computer Science Laboratory (CSL) as a principal scientist (1971–1975) and senior research fellow (1975–1983). His now-famous vision of a personal computer was captured in the 1972 memo entitled ""Why Alto?"". [1] In 1973, the Xerox Alto , with its three-button mouse and full-page-sized monitor , was born. [2] It is now considered to be the first actual personal computer in terms of what has become the ""canonical"" GUI mode of operation. All the subsequent computers built at Xerox PARC except for the ""Dolphin"" (used in the Xerox 1100 LISP machine) and the ""Dorado"" (used in the Xerox 1132 LISP machine) followed a general blueprint called ""Wildflower"", written by Lampson, and this included the D-Series Machines: the ""Dandelion"" (used in the Xerox Star and Xerox 1108 LISP machine), ""Dandetiger"" (used in the Xerox 1109 LISP machine), ""Daybreak"" ( Xerox 6085 ), and ""Dicentra"" (used internally to control various specialized hardware devices). At PARC, Lampson helped work on many other revolutionary technologies, such as laser printer design; two-phase commit protocols ; Bravo , the first WYSIWYG text formatting program; and Ethernet , the first high-speed local area network (LAN). He designed several influential programming languages such as Euclid . Following the acrimonious resignation of Xerox PARC CSL manager Bob Taylor in 1983, Lampson and Chuck Thacker followed their longtime colleague to Digital Equipment Corporation 's Systems Research Center . There, he was a senior consulting engineer (1984–1986), corporate consulting engineer (1986–1993) and senior corporate consulting
https://en.wikipedia.org/wiki/Butler_Lampson

Pretty much all of his more than 100 papers are not just well worth reading for content, but also to savor his crystal clear style.

Microsoft researchers and engineers working around the world
Get to know Microsoft researchers and engineers around the world who are tackling complex problems across a wide range of disciplines.
https://www.microsoft.com/en-us/research/people/blampson/publications/

Early Butler Projects

Butler got an undergrad Physics degree at Harvard ca 1964, and traveled to UC Berkeley for a PhD. As he later put it, he “walked in the wrong door” and found students (one of them the legendary Peter Deutsch) debugging what would be called the “Project Genie” time-shared computer and OS sponsored by ARPA-IPTO. He immediately pitched in, and essentially never left this room. Soon, he was the main designer of the OS, a powerful “thought-force” for the HW modifications and whole project, and quickly wound up as an ARPA co-PI.

The resulting system was so successful that ARPA pushed SDS to commercialize it so that others could buy it (as the SDS-940). It was the computer used by Engelbart for “the mother of all demos” in 1968, and by TYMSHARE Corp to launch the first time-sharing business. And the design was recapped again for the next generation as “TENEX OS by BBN) for the DEC PDP-10, and again heavily used in the ARPA community.

The design and approach to the design were both highly significant. The base machine was about 0.5MIP and 64K 24 bit words and a bit shaky (as so much of the HW was then), so the OS had to be both comprehensive, lightweight, and as fail safe as “lightweight” could be. I had started grad school in late 1966, and found the system to be a revelation of “sweet parsimony and balance”.

Also, Butler’s “hobby project” on this system was “CAL” an optimized version of JOSS, that constituted a “sweet spot” for interactive computing by the larger public.

All this was essentially Butler’s PhD project. Via his obvious brilliance, and in the midst of the ARPA-IPTO community he was let to run free by his advisors: supreme computerists Harry Huskey

Harry Huskey - Wikipedia
American computer design pioneer Harry Douglas Huskey (January 19, 1916 – April 9, 2017) was an American computer design pioneer. Early life and career [ edit ] Huskey was born in Whittier , in the Smoky Mountains region of North Carolina and grew up in Idaho . He received his bachelor's degree in mathematics and physics at the University of Idaho . He was the first member of his family to attend college. He gained his Master's and then his PhD in 1943 from the Ohio State University on Contributions to the Problem of Geöcze . [2] Huskey taught mathematics to U.S. Navy students at the University of Pennsylvania and then worked part-time on the early ENIAC and EDVAC computers in 1945. This work represented his first formal introduction to computers, according to his obituary in The New York Times . [3] He visited the National Physical Laboratory (NPL) in the United Kingdom for a year and worked on the Pilot ACE computer with Alan Turing and others. He was also involved with the EDVAC and SEAC computer projects. Huskey designed and managed the construction of the Standards Western Automatic Computer (SWAC) at the National Bureau of Standards in Los Angeles (1949–1953). He also designed the G-15 computer for Bendix Aviation Corporation , a 950 pounds (430 kg) machine, operable by one person. [4] He had one at his home that is now in the Smithsonian Institution in Washington, D.C. After five years at the National Bureau of Standards, Huskey joined the faculty of the University of California, Berkeley in 1954 and then University of California, Santa Cruz from 1966. He cofounded the computer and information science program at UC Santa Cruz in 1967. He became director of its computer center. In 1986, UC Santa Cruz named him professor emeritus. While at Berkeley, he supervised the research of pioneering programming language designer Niklaus Wirth , who gained his PhD in 1963. During 1963-1964 Prof. Huskey participated in establishing the Computer Center at IIT Kanpur and convened a meeting there with many pioneers of computing technology. [5] Participants included Forman Acton of Princeton University , Robert Archer of Case Institute of Technology , S. Barton of CDC, Australia, S. Beltran from the Centro de Calculo [6] in Mexico City , John Makepeace Bennett of the University of Sydney , Launor Carter of SDC - author of the subsequent Carter Report on Computer Technology for Schools, [7] David Evans of UC Berkeley , Bruce Gilchrist of IBM-SBC, Clay Perry of UC San Diego , Sigeiti Moriguti of the University of Tokyo , Gio Wiederhold , also of UC Berkeley, Adriaan van Wijngaarden of the Mathematisch Centrum in Amsterdam , Maurice Wilkes of Cambridge University . Huskey was Professor Emeritus at the University of California after his retirement at the age of 70 in 1986. In 1994 he was inducted as a Fellow of the Association for Computing Machinery . Dag Spicer, senior curator at the Computer History Museum in Mountain View, California , ""described Dr. H
https://en.wikipedia.org/wiki/Harry_Huskey

and Dave Evans

David C. Evans - Wikipedia
American computer scientist David Cannon Evans (February 24, 1924 – October 3, 1998) was the founder of the computer science department at the University of Utah and co-founder (with Ivan Sutherland ) of Evans & Sutherland , a pioneering firm in computer graphics hardware . [2] Biography [ edit ] Evans was born in Salt Lake City . He attended the University of Utah and studied electrical engineering; he earned his Bachelor of Science in Physics in 1949 and his Doctorate in Physics in 1953. [3] Evans first worked at the Bendix aviation electronics company, where he acted as project manager in 1955 to develop what some describe as an early personal computer that ran on an interpretive operating system. The Bendix G-15 was a bulky unit about the size of a two-door refrigerator. He stayed with the company just long enough to manage the G-20 project. [4] Evans became a faculty member of the University of California, Berkeley . His first important work with graphics dates from that period, when he did several experiments on an IDIOM display hooked up to a Digital Equipment Corporation PDP -5. [ citation needed ] In 1963, he was co- Principal Investigator (with Harry Huskey ) for project Genie to produce an early multi-user timesharing system. Students from this period include Butler Lampson and L. Peter Deutsch . [5] The system, which included key developments in the field of virtual memory , was sponsored by the US Defense Department's Advanced Research Projects Agency . [4] In 1965, the University of Utah recruited him back to start their own computer science department. When he was building up the University of Utah department in 1968 he managed to convince Ivan Sutherland (who had funded Evans' DARPA research) to come to Utah , accepting the condition that they start a computer graphics company together. Evans retired from the company in 1994. Evans's students at Utah included Alan Kay , Edwin Catmull , James H. Clark , John Warnock , Alan Ashton , Frank Crow , Jim Blinn , Bui Tuong Phong , Gary Hodgman , and Henri Gouraud . Evans was a member of the Church of Jesus Christ of Latter-day Saints (LDS Church). He served as a branch president , a counselor in bishoprics and stake presidencies , and as a scout master for a total of 27 years. [6] Evans was awarded the Silver Beaver for his role in scouting. [4] Evans married Joy Frewin. They had ten children, only seven of which lived to adulthood. One of these is David F. Evans , who became a general authority in the LDS Church. From 1984 to 1990, Joy Evans was a counselor in the general presidency of the Relief Society to Barbara W. Winder . At the time of his death on October 12, 1998, Evans had 39 living grandchildren and great grandchildren. In 1996, Brigham Young University established the David C. Evans Chair of Computer Engineering and Graphics. Evans was at the ceremony where the founding of a chair in his honor was announced, but due to his suffering from Alzheimer's disease , did not make any
https://en.wikipedia.org/wiki/David_C._Evans

who a year or so later at Utah birthed modern 3D graphics (at Utah I was also lucky to have Dave as my PhD designer).

A lot can be learned about design from looking at the Project Genie OS and subsequent work (hint: look at his succession of papers and commentaries). I’m trying to work on the “significance” part of the question, so I’ll mention just two of the many interesting insights and features in this system.

The first is that I think it was pretty much the first OS that “was designed to fail”, in that contrary to most code that was assumed to work (and didn’t), Butler knew that there would be both SW and HW bugs that would cause crashes (and this is really a bad thing for a time-shared system). So, a percentage of the already meager resources of the HW were used to both mark pages as “dirty” or “clean”, and to continuously try to write out clean pages to the secondary storage device. This meant (a) that a need for a written out page could almost always find clean storage in core to put it (without having to write out beforehand), and (b) that a crash generally had most of the end user’s pages in a clean and written out state, and this along with other provisions almost always made crash recovery very swift.

The second had to do with “training programmers” to pay more attention to taking care of their working set. The scheme had to do with the HW maps forcing a working set to be smaller than the physical core storage. This allowed the next user’s working set to also be in core ready to go. The wrinkle was that a programmer could predict to the OS what pages would be in their next working set time quantum, and if true, the OS would reward by running on the fast queue. If the prediction failed (and an unpredicted page fetch was required) then the programmer was punished by being run on the much slower slow queue.

Both of these — along with other special characteristics — allowed the 940s to have remarkably efficient performance. It could run several dozens of users with 0.5 MIP and 192Kbytes of RAM — and it was also able to be the workhorse for the Engelbart’s group amazing “Mother of all demos” in 1968.

Next

If I were to write a next section here, I think I’d talk a bit about Butler’s work and influence on design, safety, security, confinement, and meaning in systems. An important idea he emphasized from early on is “In computing, design rules change dramatically every few years”. A lot of this is from both Moore’s Law scalings, and from the amount of learning that can happen in a few years.

This means that trying to carry old ideas and techniques forward is fraught with disaster, and if done, has to be done very carefully. Butler followed his own “hints” and tried to do new designs that fit the new situations for each new system he worked on. Often this had to be done with less than ideal HW from manufacturers. With the SDS 930 that became the 940, for Project Genie, they were able to make a new memory mapping system, and found very successful ways to get considerable power from what seems like meager resources. A User Machine In A Time-Sharing Environment
 covers some of the most important ideas and insights here.

On the later CDC 6400 at Berkeley, they were pretty much restricted to using the existing HW while trying to come up with a radically new way to structure, protect and make an OS that used the idea of “capabilities” — a secure unforgeable reference privilege with limits — but without having HW support.

On Reliable And Extendable Operating Systems (1969)
 gives a clear account of the thinking and resultant designs of this unusual system design.

<more to come>","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/o12snluce6j9qw7g', 'title': ""What is the significance of Butler Lampson's work?"", 'score': {'original': 0.91905714285714, 'ai': 0.080942857142857}, 'blocks': [{'text': 'A good reply is much too large for a Quora sized reply.\n\nBasically: Butler is one of the most significant computer scientists in history — and has been one since he moved from physics to computing almost 60 years ago. He was always a “principal scientist, investigator, and thought leader” even as a grad student. He is a quintessential example of the word “brilliant” (and the term seems to have been coined for him).\n\nIn the 60s while hearing a rapidly delivered perfectly formed Butler talk, a fellow grad student turned to me and said “You know, it’s just a privilege to know someone like Butler”. When asked about “computer science” and “computer scientist” I’ve said “Look at Butler to see an example of a top computer scientist — it’s hard to put into exact words, but he is one”.\n\nThe list of his accomplishments is long, and so is the list of awards. Much of his significance has been in how he was able to both understand, invent, and influence the large world of systems of many kinds. These include a number of significant operating systems (including SDS-940 and Tenex), computers (including the Xerox Parc MAXC and Alto), networks (including the Ethernet and the Internet), and programming languages (including Mesa).\n\nHe is both a great thinker and a great doer, and both of these plus his larger than life personality almost literally dragged the whole field forward, and made all the rest of us into better computer scientists. I.e. a kind of “magical person”.\n\nI will return here periodically with a little history and a few stories, but they are really commercials for readers to look at larger accounts of Butler’s larger than life history.\n\nThe citations and bio for his 1992 ACM Turing Award (the highest award in computing) are good to start with:\n\nButler W Lampson\nButler Lampson was born in Washington DC and educated at The Lawrenceville School , an elite boarding school 6 miles from Princeton New Jersey. Butler’s first computer was an underutilized IBM 650 at Princeton, which an enterprising high school classmate found and got permission to use while it was idle. It had a drum memory with 2000 words of ten decimal digits, and the only I/O aside from the console was a card reader/punch, controlled partly by the computer and partly by a plug board. As an undergraduate at Harvard, Lampson learned the APL programming language from Ken Iverson , who spent a year on sabbatical there. At that time there was no implementation of the language, and Ken didn’t want one because he was sure that it would make compromises that would wreck the language. While still an undergraduate Lampson also programmed a PDP-1 to analyze spark chamber photographs from the Cambridge Electron Accelerator and wrote a display editor for the PDP-1’s one-point-at-a-time display. Butler went to the University of California at Berkeley in the fall of 1964 as a graduate student in physics. At the Fall Joint Computer Conference in San Francisco that year he ran across Steve Russell from MIT, who told him about the Genie project hidden', 'result': {'fake': 0.1099, 'real': 0.8901}, 'status': 'success'}, {'text': ""behind an unmarked door in Cory Hall. There he met Peter Deutsch and Mel Pirtle, the Principal Investigator, and was quickly seduced away from physics to study computers. The Genie project modified a Scientific Data Systems SDS 930 minicomputer to create the first time-sharing system with character-by-character interaction. Later SDS marketed it as the SDS 940, the first commercially available general purpose time-sharing system. Butler wrote parts of the operating system and several programming languages, notably Cal, an interactive language for numerical computation derived from Cliff Shaw’s Joss , and the QSPL system programming language done with Peter Deutsch. At Berkeley he also designed the Cal time-sharing system for a CDC 6400 in the computer center, together with Jim Gray , Charles Simonyi , Howard Sturgis and Bruce Lindsay , who all went on to later fame. This was the first capability-based system to have a real user community. It pioneered the ideas of shadow pages and redo logs, but also taught us that capabilities are not a good basis for long-term security. The Genie project researchers couldn’t figure out how to build the much more grandiose second system at Berkeley, so in 1969 they started Berkeley Computer Corporation (BCC) to do it. After two years it burned through $4,000,000 and built just one working system before folding. Butler designed and coded most of the microcoded parts of the operating system and worked on the SPL system programming language. Around this time he also devised the access matrix model for computer security, unifying the ideas of capabilities and access control lists. Luckily, as BCC was ending the Xerox Palo Alto Research Center (PARC) was getting started, and Bob Tayl\nhttps://amturing.acm.org/award_winners/lampson_1142421.cfm\n\nNext, the Wikipedia article —\n\nButler Lampson - Wikipedia\nAmerican computer scientist Butler W. Lampson , ForMemRS , (born December 23, 1943) is an American computer scientist best known for his contributions to the development and implementation of distributed personal computing . Education and early life [ edit ] After graduating from the Lawrenceville School (where in 2009 he was awarded the Aldo Leopold Award, also known as the Lawrenceville Medal, Lawrenceville's highest award to alumni), Lampson received an A.B. in physics ( magna cum laude with highest honors in the discipline) from Harvard University in 1964 and a PhD in electrical engineering and computer science from the University of California, Berkeley in 1967. Career and research [ edit ] Professional Developers Conference 2009 Technical Leaders Panel During the 1960s, Lampson and others were part of Project GENIE at UC Berkeley. In 1965, several Project GENIE members, specifically Lampson and Peter Deutsch , developed the Berkeley Timesharing System for Scientific Data Systems ' SDS 940 computer. After completing his doctorate, Lampson stayed on at UC Berkeley as an assistant professor (1967–1970) and associate professor (1970–1971) of computer science. For a period of time, he concurrently served as director of system development for the Berkeley Computer Corporation (1969–1971). In 1971, Lampson became one of the founding members of Xerox PARC , where he worked in the Computer Science Laboratory (CSL) as a principal scientist (1971–1975) and senior"", 'result': {'fake': 0.1229, 'real': 0.8771}, 'status': 'success'}, {'text': 'research fellow (1975–1983). His now-famous vision of a personal computer was captured in the 1972 memo entitled ""Why Alto?"". [1] In 1973, the Xerox Alto , with its three-button mouse and full-page-sized monitor , was born. [2] It is now considered to be the first actual personal computer in terms of what has become the ""canonical"" GUI mode of operation. All the subsequent computers built at Xerox PARC except for the ""Dolphin"" (used in the Xerox 1100 LISP machine) and the ""Dorado"" (used in the Xerox 1132 LISP machine) followed a general blueprint called ""Wildflower"", written by Lampson, and this included the D-Series Machines: the ""Dandelion"" (used in the Xerox Star and Xerox 1108 LISP machine), ""Dandetiger"" (used in the Xerox 1109 LISP machine), ""Daybreak"" ( Xerox 6085 ), and ""Dicentra"" (used internally to control various specialized hardware devices). At PARC, Lampson helped work on many other revolutionary technologies, such as laser printer design; two-phase commit protocols ; Bravo , the first WYSIWYG text formatting program; and Ethernet , the first high-speed local area network (LAN). He designed several influential programming languages such as Euclid . Following the acrimonious resignation of Xerox PARC CSL manager Bob Taylor in 1983, Lampson and Chuck Thacker followed their longtime colleague to Digital Equipment Corporation \'s Systems Research Center . There, he was a senior consulting engineer (1984–1986), corporate consulting engineer (1986–1993) and senior corporate consulting\nhttps://en.wikipedia.org/wiki/Butler_Lampson\n\nPretty much all of his more than 100 papers are not just well worth reading for content, but also to savor his crystal clear style.\n\nMicrosoft researchers and engineers working around the world\nGet to know Microsoft researchers and engineers around the world who are tackling complex problems across a wide range of disciplines.\nhttps://www.microsoft.com/en-us/research/people/blampson/publications/\n\nEarly Butler Projects\n\nButler got an undergrad Physics degree at Harvard ca 1964, and traveled to UC Berkeley for a PhD. As he later put it, he “walked in the wrong door” and found students (one of them the legendary Peter Deutsch) debugging what would be called the “Project Genie” time-shared computer and OS sponsored by ARPA-IPTO. He immediately pitched in, and essentially never left this room. Soon, he was the main designer of the OS, a powerful “thought-force” for the HW modifications and whole project, and quickly wound up as an ARPA co-PI.\n\nThe resulting system was so successful that ARPA pushed SDS to commercialize it so that others could buy it (as the SDS-940). It was the computer used by Engelbart for “the mother of all demos” in 1968, and by TYMSHARE Corp to launch the first time-sharing business. And the design was recapped again for the next generation as “TENEX OS by BBN) for the DEC PDP-10, and again heavily used in the ARPA community.\n\nThe design and approach to the design were both highly significant. The base machine was about 0.5MIP and 64K 24 bit words and a bit shaky (as so much of the HW was then), so the OS had to be both comprehensive, lightweight, and as fail safe as “lightweight” could be. I had started grad school in late 1966, and found the system to be', 'result': {'fake': 0.0573, 'real': 0.9427}, 'status': 'success'}, {'text': ""a revelation of “sweet parsimony and balance”.\n\nAlso, Butler’s “hobby project” on this system was “CAL” an optimized version of JOSS, that constituted a “sweet spot” for interactive computing by the larger public.\n\nAll this was essentially Butler’s PhD project. Via his obvious brilliance, and in the midst of the ARPA-IPTO community he was let to run free by his advisors: supreme computerists Harry Huskey\n\nHarry Huskey - Wikipedia\nAmerican computer design pioneer Harry Douglas Huskey (January 19, 1916 – April 9, 2017) was an American computer design pioneer. Early life and career [ edit ] Huskey was born in Whittier , in the Smoky Mountains region of North Carolina and grew up in Idaho . He received his bachelor's degree in mathematics and physics at the University of Idaho . He was the first member of his family to attend college. He gained his Master's and then his PhD in 1943 from the Ohio State University on Contributions to the Problem of Geöcze . [2] Huskey taught mathematics to U.S. Navy students at the University of Pennsylvania and then worked part-time on the early ENIAC and EDVAC computers in 1945. This work represented his first formal introduction to computers, according to his obituary in The New York Times . [3] He visited the National Physical Laboratory (NPL) in the United Kingdom for a year and worked on the Pilot ACE computer with Alan Turing and others. He was also involved with the EDVAC and SEAC computer projects. Huskey designed and managed the construction of the Standards Western Automatic Computer (SWAC) at the National Bureau of Standards in Los Angeles (1949–1953). He also designed the G-15 computer for Bendix Aviation Corporation , a 950 pounds (430\xa0kg) machine, operable by one person. [4] He had one at his home that is now in the Smithsonian Institution in Washington, D.C. After five years at the National Bureau of Standards, Huskey joined the faculty of the University of California, Berkeley in 1954 and then University of California, Santa Cruz from 1966. He cofounded the computer and information science program at UC Santa Cruz in 1967. He became director of its computer center. In 1986, UC Santa Cruz named him professor emeritus. While at Berkeley, he supervised the research of pioneering programming language designer Niklaus Wirth , who gained his PhD in 1963. During 1963-1964 Prof. Huskey participated in establishing the Computer Center at IIT Kanpur and convened a meeting there with many pioneers of computing technology. [5] Participants included Forman Acton of Princeton University , Robert Archer of Case Institute of Technology , S. Barton of CDC, Australia, S. Beltran from the Centro de Calculo [6] in Mexico City , John Makepeace Bennett of the University of Sydney , Launor Carter of SDC - author of the subsequent Carter Report on Computer Technology for Schools, [7] David Evans of UC Berkeley , Bruce Gilchrist of IBM-SBC, Clay Perry of UC San Diego , Sigeiti Moriguti of the University of Tokyo , Gio Wiederhold , also of UC Berkeley, Adriaan van Wijngaarden of the Mathematisch Centrum in Amsterdam , Maurice"", 'result': {'fake': 0.1611, 'real': 0.8389}, 'status': 'success'}, {'text': 'Wilkes of Cambridge University . Huskey was Professor Emeritus at the University of California after his retirement at the age of 70 in 1986. In 1994 he was inducted as a Fellow of the Association for Computing Machinery . Dag Spicer, senior curator at the Computer History Museum in Mountain View, California , ""described Dr. H\nhttps://en.wikipedia.org/wiki/Harry_Huskey\n\nand Dave Evans\n\nDavid C. Evans - Wikipedia\nAmerican computer scientist David Cannon Evans (February 24, 1924 – October 3, 1998) was the founder of the computer science department at the University of Utah and co-founder (with Ivan Sutherland ) of Evans & Sutherland , a pioneering firm in computer graphics hardware . [2] Biography [ edit ] Evans was born in Salt Lake City . He attended the University of Utah and studied electrical engineering; he earned his Bachelor of Science in Physics in 1949 and his Doctorate in Physics in 1953. [3] Evans first worked at the Bendix aviation electronics company, where he acted as project manager in 1955 to develop what some describe as an early personal computer that ran on an interpretive operating system. The Bendix G-15 was a bulky unit about the size of a two-door refrigerator. He stayed with the company just long enough to manage the G-20 project. [4] Evans became a faculty member of the University of California, Berkeley . His first important work with graphics dates from that period, when he did several experiments on an IDIOM display hooked up to a Digital Equipment Corporation PDP -5. [ citation needed ] In 1963, he was co- Principal Investigator (with Harry Huskey ) for project Genie to produce an early multi-user timesharing system. Students from this period include Butler Lampson and L. Peter Deutsch . [5] The system, which included key developments in the field of virtual memory , was sponsored by the US Defense Department\'s Advanced Research Projects Agency . [4] In 1965, the University of Utah recruited him back to start their own computer science department. When he was building up the University of Utah department in 1968 he managed to convince Ivan Sutherland (who had funded Evans\' DARPA research) to come to Utah , accepting the condition that they start a computer graphics company together. Evans retired from the company in 1994. Evans\'s students at Utah included Alan Kay , Edwin Catmull , James H. Clark , John Warnock , Alan Ashton , Frank Crow , Jim Blinn , Bui Tuong Phong , Gary Hodgman , and Henri Gouraud . Evans was a member of the Church of Jesus Christ of Latter-day Saints (LDS Church). He served as a branch president , a counselor in bishoprics and stake presidencies , and as a scout master for a total of 27 years. [6] Evans was awarded the Silver Beaver for his role in scouting. [4] Evans married Joy Frewin. They had ten children, only seven of which lived to adulthood. One of these is David F. Evans , who became a general authority in the LDS Church. From 1984 to 1990, Joy Evans was a counselor in the general', 'result': {'fake': 0.054, 'real': 0.946}, 'status': 'success'}, {'text': ""presidency of the Relief Society to Barbara W. Winder . At the time of his death on October 12, 1998, Evans had 39 living grandchildren and great grandchildren. In 1996, Brigham Young University established the David C. Evans Chair of Computer Engineering and Graphics. Evans was at the ceremony where the founding of a chair in his honor was announced, but due to his suffering from Alzheimer's disease , did not make any\nhttps://en.wikipedia.org/wiki/David_C._Evans\n\nwho a year or so later at Utah birthed modern 3D graphics (at Utah I was also lucky to have Dave as my PhD designer).\n\nA lot can be learned about design from looking at the Project Genie OS and subsequent work (hint: look at his succession of papers and commentaries). I’m trying to work on the “significance” part of the question, so I’ll mention just two of the many interesting insights and features in this system.\n\nThe first is that I think it was pretty much the first OS that “was designed to fail”, in that contrary to most code that was assumed to work (and didn’t), Butler knew that there would be both SW and HW bugs that would cause crashes (and this is really a bad thing for a time-shared system). So, a percentage of the already meager resources of the HW were used to both mark pages as “dirty” or “clean”, and to continuously try to write out clean pages to the secondary storage device. This meant (a) that a need for a written out page could almost always find clean storage in core to put it (without having to write out beforehand), and (b) that a crash generally had most of the end user’s pages in a clean and written out state, and this along with other provisions almost always made crash recovery very swift.\n\nThe second had to do with “training programmers” to pay more attention to taking care of their working set. The scheme had to do with the HW maps forcing a working set to be smaller than the physical core storage. This allowed the next user’s working set to also be in core ready to go. The wrinkle was that a programmer could predict to the OS what pages would be in their next working set time quantum, and if true, the OS would reward by running on the fast queue. If the prediction failed (and an unpredicted page fetch was required) then the programmer was punished by being run on the much slower slow queue.\n\nBoth of these — along with other special characteristics — allowed the 940s to have remarkably efficient performance. It could run several dozens of users with 0.5 MIP and 192Kbytes of RAM — and it was also able to be the workhorse for the Engelbart’s group amazing “Mother of all demos” in 1968.\n\nNext\n\nIf I were to write a next section here, I think I’d talk a bit about Butler’s work and influence on design, safety, security, confinement, and meaning in systems. An important idea he emphasized from early on is “In computing, design rules change dramatically every few years”."", 'result': {'fake': 0.0244, 'real': 0.9756}, 'status': 'success'}, {'text': 'A lot of this is from both Moore’s Law scalings, and from the amount of learning that can happen in a few years.\n\nThis means that trying to carry old ideas and techniques forward is fraught with disaster, and if done, has to be done very carefully. Butler followed his own “hints” and tried to do new designs that fit the new situations for each new system he worked on. Often this had to be done with less than ideal HW from manufacturers. With the SDS 930 that became the 940, for Project Genie, they were able to make a new memory mapping system, and found very successful ways to get considerable power from what seems like meager resources. A User Machine In A Time-Sharing Environment\n covers some of the most important ideas and insights here.\n\nOn the later CDC 6400 at Berkeley, they were pretty much restricted to using the existing HW while trying to come up with a radically new way to structure, protect and make an OS that used the idea of “capabilities” — a secure unforgeable reference privilege with limits — but without having HW support.\n\nOn Reliable And Extendable Operating Systems (1969)\n gives a clear account of the thinking and resultant designs of this unusual system design.\n\n<more to come>', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 33, 'credits': 1995407, 'subscription': 0, 'content': 'A good reply is much too large for a Quora sized reply.\n\nBasically: Butler is one of the most significant computer scientists in history — and has been one since he moved from physics to computing almost 60 years ago. He was always a “principal scientist, investigator, and thought leader” even as a grad student. He is a quintessential example of the word “brilliant” (and the term seems to have been coined for him).\n\nIn the 60s while hearing a rapidly delivered perfectly formed Butler talk, a fellow grad student turned to me and said “You know, it’s just a privilege to know someone like Butler”. When asked about “computer science” and “computer scientist” I’ve said “Look at Butler to see an example of a top computer scientist — it’s hard to put into exact words, but he is one”.\n\nThe list of his accomplishments is long, and so is the list of awards. Much of his significance has been in how he was able to both understand, invent, and influence the large world of systems of many kinds. These include a number of significant operating systems (including SDS-940 and Tenex), computers (including the Xerox Parc MAXC and Alto), networks (including the Ethernet and the Internet), and programming languages (including Mesa).\n\nHe is both a great thinker and a great doer, and both of these plus his larger than life personality almost literally dragged the whole field forward, and made all the rest of us into better computer scientists. I.e. a kind of “magical person”.\n\nI will return here periodically with a little history and a few stories, but they are really commercials for readers to look at larger accounts of Butler’s larger than life history.\n\nThe citations and bio for his 1992 ACM Turing Award (the highest award in computing) are good to start with:\n\nButler W Lampson\nButler Lampson was born in Washington DC and educated at The Lawrenceville School , an elite boarding school 6 miles from Princeton New Jersey. Butler’s first computer was an underutilized IBM 650 at Princeton, which an enterprising high school classmate found and got permission to use while it was idle. It had a drum memory with 2000 words of ten decimal digits, and the only I/O aside from the console was a card reader/punch, controlled partly by the computer and partly by a plug board. As an undergraduate at Harvard, Lampson learned the APL programming language from Ken Iverson , who spent a year on sabbatical there. At that time there was no implementation of the language, and Ken didn’t want one because he was sure that it would make compromises that would wreck the language. While still an undergraduate Lampson also programmed a PDP-1 to analyze spark chamber photographs from the Cambridge Electron Accelerator and wrote a display editor for the PDP-1’s one-point-at-a-time display. Butler went to the University of California at Berkeley in the fall of 1964 as a graduate student in physics. At the Fall Joint Computer Conference in San Francisco that year he ran across Steve Russell from MIT, who told him about the Genie project hidden behind an unmarked door in Cory Hall. There he met Peter Deutsch and Mel Pirtle, the Principal Investigator, and was quickly seduced away from physics to study computers. The Genie project modified a Scientific Data Systems SDS 930 minicomputer to create the first time-sharing system with character-by-character interaction. Later SDS marketed it as the SDS 940, the first commercially available general purpose time-sharing system. Butler wrote parts of the operating system and several programming languages, notably Cal, an interactive language for numerical computation derived from Cliff Shaw’s Joss , and the QSPL system programming language done with Peter Deutsch. At Berkeley he also designed the Cal time-sharing system for a CDC 6400 in the computer center, together with Jim Gray , Charles Simonyi , Howard Sturgis and Bruce Lindsay , who all went on to later fame. This was the first capability-based system to have a real user community. It pioneered the ideas of shadow pages and redo logs, but also taught us that capabilities are not a good basis for long-term security. The Genie project researchers couldn’t figure out how to build the much more grandiose second system at Berkeley, so in 1969 they started Berkeley Computer Corporation (BCC) to do it. After two years it burned through $4,000,000 and built just one working system before folding. Butler designed and coded most of the microcoded parts of the operating system and worked on the SPL system programming language. Around this time he also devised the access matrix model for computer security, unifying the ideas of capabilities and access control lists. Luckily, as BCC was ending the Xerox Palo Alto Research Center (PARC) was getting started, and Bob Tayl\nhttps://amturing.acm.org/award_winners/lampson_1142421.cfm\n\nNext, the Wikipedia article —\n\nButler Lampson - Wikipedia\nAmerican computer scientist Butler W. Lampson , ForMemRS , (born December 23, 1943) is an American computer scientist best known for his contributions to the development and implementation of distributed personal computing . Education and early life [ edit ] After graduating from the Lawrenceville School (where in 2009 he was awarded the Aldo Leopold Award, also known as the Lawrenceville Medal, Lawrenceville\'s highest award to alumni), Lampson received an A.B. in physics ( magna cum laude with highest honors in the discipline) from Harvard University in 1964 and a PhD in electrical engineering and computer science from the University of California, Berkeley in 1967. Career and research [ edit ] Professional Developers Conference 2009 Technical Leaders Panel During the 1960s, Lampson and others were part of Project GENIE at UC Berkeley. In 1965, several Project GENIE members, specifically Lampson and Peter Deutsch , developed the Berkeley Timesharing System for Scientific Data Systems \' SDS 940 computer. After completing his doctorate, Lampson stayed on at UC Berkeley as an assistant professor (1967–1970) and associate professor (1970–1971) of computer science. For a period of time, he concurrently served as director of system development for the Berkeley Computer Corporation (1969–1971). In 1971, Lampson became one of the founding members of Xerox PARC , where he worked in the Computer Science Laboratory (CSL) as a principal scientist (1971–1975) and senior research fellow (1975–1983). His now-famous vision of a personal computer was captured in the 1972 memo entitled ""Why Alto?"". [1] In 1973, the Xerox Alto , with its three-button mouse and full-page-sized monitor , was born. [2] It is now considered to be the first actual personal computer in terms of what has become the ""canonical"" GUI mode of operation. All the subsequent computers built at Xerox PARC except for the ""Dolphin"" (used in the Xerox 1100 LISP machine) and the ""Dorado"" (used in the Xerox 1132 LISP machine) followed a general blueprint called ""Wildflower"", written by Lampson, and this included the D-Series Machines: the ""Dandelion"" (used in the Xerox Star and Xerox 1108 LISP machine), ""Dandetiger"" (used in the Xerox 1109 LISP machine), ""Daybreak"" ( Xerox 6085 ), and ""Dicentra"" (used internally to control various specialized hardware devices). At PARC, Lampson helped work on many other revolutionary technologies, such as laser printer design; two-phase commit protocols ; Bravo , the first WYSIWYG text formatting program; and Ethernet , the first high-speed local area network (LAN). He designed several influential programming languages such as Euclid . Following the acrimonious resignation of Xerox PARC CSL manager Bob Taylor in 1983, Lampson and Chuck Thacker followed their longtime colleague to Digital Equipment Corporation \'s Systems Research Center . There, he was a senior consulting engineer (1984–1986), corporate consulting engineer (1986–1993) and senior corporate consulting\nhttps://en.wikipedia.org/wiki/Butler_Lampson\n\nPretty much all of his more than 100 papers are not just well worth reading for content, but also to savor his crystal clear style.\n\nMicrosoft researchers and engineers working around the world\nGet to know Microsoft researchers and engineers around the world who are tackling complex problems across a wide range of disciplines.\nhttps://www.microsoft.com/en-us/research/people/blampson/publications/\n\nEarly Butler Projects\n\nButler got an undergrad Physics degree at Harvard ca 1964, and traveled to UC Berkeley for a PhD. As he later put it, he “walked in the wrong door” and found students (one of them the legendary Peter Deutsch) debugging what would be called the “Project Genie” time-shared computer and OS sponsored by ARPA-IPTO. He immediately pitched in, and essentially never left this room. Soon, he was the main designer of the OS, a powerful “thought-force” for the HW modifications and whole project, and quickly wound up as an ARPA co-PI.\n\nThe resulting system was so successful that ARPA pushed SDS to commercialize it so that others could buy it (as the SDS-940). It was the computer used by Engelbart for “the mother of all demos” in 1968, and by TYMSHARE Corp to launch the first time-sharing business. And the design was recapped again for the next generation as “TENEX OS by BBN) for the DEC PDP-10, and again heavily used in the ARPA community.\n\nThe design and approach to the design were both highly significant. The base machine was about 0.5MIP and 64K 24 bit words and a bit shaky (as so much of the HW was then), so the OS had to be both comprehensive, lightweight, and as fail safe as “lightweight” could be. I had started grad school in late 1966, and found the system to be a revelation of “sweet parsimony and balance”.\n\nAlso, Butler’s “hobby project” on this system was “CAL” an optimized version of JOSS, that constituted a “sweet spot” for interactive computing by the larger public.\n\nAll this was essentially Butler’s PhD project. Via his obvious brilliance, and in the midst of the ARPA-IPTO community he was let to run free by his advisors: supreme computerists Harry Huskey\n\nHarry Huskey - Wikipedia\nAmerican computer design pioneer Harry Douglas Huskey (January 19, 1916 – April 9, 2017) was an American computer design pioneer. Early life and career [ edit ] Huskey was born in Whittier , in the Smoky Mountains region of North Carolina and grew up in Idaho . He received his bachelor\'s degree in mathematics and physics at the University of Idaho . He was the first member of his family to attend college. He gained his Master\'s and then his PhD in 1943 from the Ohio State University on Contributions to the Problem of Geöcze . [2] Huskey taught mathematics to U.S. Navy students at the University of Pennsylvania and then worked part-time on the early ENIAC and EDVAC computers in 1945. This work represented his first formal introduction to computers, according to his obituary in The New York Times . [3] He visited the National Physical Laboratory (NPL) in the United Kingdom for a year and worked on the Pilot ACE computer with Alan Turing and others. He was also involved with the EDVAC and SEAC computer projects. Huskey designed and managed the construction of the Standards Western Automatic Computer (SWAC) at the National Bureau of Standards in Los Angeles (1949–1953). He also designed the G-15 computer for Bendix Aviation Corporation , a 950 pounds (430\xa0kg) machine, operable by one person. [4] He had one at his home that is now in the Smithsonian Institution in Washington, D.C. After five years at the National Bureau of Standards, Huskey joined the faculty of the University of California, Berkeley in 1954 and then University of California, Santa Cruz from 1966. He cofounded the computer and information science program at UC Santa Cruz in 1967. He became director of its computer center. In 1986, UC Santa Cruz named him professor emeritus. While at Berkeley, he supervised the research of pioneering programming language designer Niklaus Wirth , who gained his PhD in 1963. During 1963-1964 Prof. Huskey participated in establishing the Computer Center at IIT Kanpur and convened a meeting there with many pioneers of computing technology. [5] Participants included Forman Acton of Princeton University , Robert Archer of Case Institute of Technology , S. Barton of CDC, Australia, S. Beltran from the Centro de Calculo [6] in Mexico City , John Makepeace Bennett of the University of Sydney , Launor Carter of SDC - author of the subsequent Carter Report on Computer Technology for Schools, [7] David Evans of UC Berkeley , Bruce Gilchrist of IBM-SBC, Clay Perry of UC San Diego , Sigeiti Moriguti of the University of Tokyo , Gio Wiederhold , also of UC Berkeley, Adriaan van Wijngaarden of the Mathematisch Centrum in Amsterdam , Maurice Wilkes of Cambridge University . Huskey was Professor Emeritus at the University of California after his retirement at the age of 70 in 1986. In 1994 he was inducted as a Fellow of the Association for Computing Machinery . Dag Spicer, senior curator at the Computer History Museum in Mountain View, California , ""described Dr. H\nhttps://en.wikipedia.org/wiki/Harry_Huskey\n\nand Dave Evans\n\nDavid C. Evans - Wikipedia\nAmerican computer scientist David Cannon Evans (February 24, 1924 – October 3, 1998) was the founder of the computer science department at the University of Utah and co-founder (with Ivan Sutherland ) of Evans & Sutherland , a pioneering firm in computer graphics hardware . [2] Biography [ edit ] Evans was born in Salt Lake City . He attended the University of Utah and studied electrical engineering; he earned his Bachelor of Science in Physics in 1949 and his Doctorate in Physics in 1953. [3] Evans first worked at the Bendix aviation electronics company, where he acted as project manager in 1955 to develop what some describe as an early personal computer that ran on an interpretive operating system. The Bendix G-15 was a bulky unit about the size of a two-door refrigerator. He stayed with the company just long enough to manage the G-20 project. [4] Evans became a faculty member of the University of California, Berkeley . His first important work with graphics dates from that period, when he did several experiments on an IDIOM display hooked up to a Digital Equipment Corporation PDP -5. [ citation needed ] In 1963, he was co- Principal Investigator (with Harry Huskey ) for project Genie to produce an early multi-user timesharing system. Students from this period include Butler Lampson and L. Peter Deutsch . [5] The system, which included key developments in the field of virtual memory , was sponsored by the US Defense Department\'s Advanced Research Projects Agency . [4] In 1965, the University of Utah recruited him back to start their own computer science department. When he was building up the University of Utah department in 1968 he managed to convince Ivan Sutherland (who had funded Evans\' DARPA research) to come to Utah , accepting the condition that they start a computer graphics company together. Evans retired from the company in 1994. Evans\'s students at Utah included Alan Kay , Edwin Catmull , James H. Clark , John Warnock , Alan Ashton , Frank Crow , Jim Blinn , Bui Tuong Phong , Gary Hodgman , and Henri Gouraud . Evans was a member of the Church of Jesus Christ of Latter-day Saints (LDS Church). He served as a branch president , a counselor in bishoprics and stake presidencies , and as a scout master for a total of 27 years. [6] Evans was awarded the Silver Beaver for his role in scouting. [4] Evans married Joy Frewin. They had ten children, only seven of which lived to adulthood. One of these is David F. Evans , who became a general authority in the LDS Church. From 1984 to 1990, Joy Evans was a counselor in the general presidency of the Relief Society to Barbara W. Winder . At the time of his death on October 12, 1998, Evans had 39 living grandchildren and great grandchildren. In 1996, Brigham Young University established the David C. Evans Chair of Computer Engineering and Graphics. Evans was at the ceremony where the founding of a chair in his honor was announced, but due to his suffering from Alzheimer\'s disease , did not make any\nhttps://en.wikipedia.org/wiki/David_C._Evans\n\nwho a year or so later at Utah birthed modern 3D graphics (at Utah I was also lucky to have Dave as my PhD designer).\n\nA lot can be learned about design from looking at the Project Genie OS and subsequent work (hint: look at his succession of papers and commentaries). I’m trying to work on the “significance” part of the question, so I’ll mention just two of the many interesting insights and features in this system.\n\nThe first is that I think it was pretty much the first OS that “was designed to fail”, in that contrary to most code that was assumed to work (and didn’t), Butler knew that there would be both SW and HW bugs that would cause crashes (and this is really a bad thing for a time-shared system). So, a percentage of the already meager resources of the HW were used to both mark pages as “dirty” or “clean”, and to continuously try to write out clean pages to the secondary storage device. This meant (a) that a need for a written out page could almost always find clean storage in core to put it (without having to write out beforehand), and (b) that a crash generally had most of the end user’s pages in a clean and written out state, and this along with other provisions almost always made crash recovery very swift.\n\nThe second had to do with “training programmers” to pay more attention to taking care of their working set. The scheme had to do with the HW maps forcing a working set to be smaller than the physical core storage. This allowed the next user’s working set to also be in core ready to go. The wrinkle was that a programmer could predict to the OS what pages would be in their next working set time quantum, and if true, the OS would reward by running on the fast queue. If the prediction failed (and an unpredicted page fetch was required) then the programmer was punished by being run on the much slower slow queue.\n\nBoth of these — along with other special characteristics — allowed the 940s to have remarkably efficient performance. It could run several dozens of users with 0.5 MIP and 192Kbytes of RAM — and it was also able to be the workhorse for the Engelbart’s group amazing “Mother of all demos” in 1968.\n\nNext\n\nIf I were to write a next section here, I think I’d talk a bit about Butler’s work and influence on design, safety, security, confinement, and meaning in systems. An important idea he emphasized from early on is “In computing, design rules change dramatically every few years”. A lot of this is from both Moore’s Law scalings, and from the amount of learning that can happen in a few years.\n\nThis means that trying to carry old ideas and techniques forward is fraught with disaster, and if done, has to be done very carefully. Butler followed his own “hints” and tried to do new designs that fit the new situations for each new system he worked on. Often this had to be done with less than ideal HW from manufacturers. With the SDS 930 that became the 940, for Project Genie, they were able to make a new memory mapping system, and found very successful ways to get considerable power from what seems like meager resources. A User Machine In A Time-Sharing Environment\n covers some of the most important ideas and insights here.\n\nOn the later CDC 6400 at Berkeley, they were pretty much restricted to using the existing HW while trying to come up with a radically new way to structure, protect and make an OS that used the idea of “capabilities” — a secure unforgeable reference privilege with limits — but without having HW support.\n\nOn Reliable And Extendable Operating Systems (1969)\n gives a clear account of the thinking and resultant designs of this unusual system design.\n\n<more to come>', 'aiModelVersion': '1'}",0.91905714285714
Ron Kolinie,2y,What was the purpose of all of the blinking lights on the early computers (think Univac and Eniac) of the 1950s & 1960s? What information did they convey?,"What was the purpose of all of the blinking lights on the early computers (think Univac and Eniac) of the 1950s & 1960s? What information did they convey?

I am almost 78 now but I started on computers in the early 60s. I was working for IBM at the System Development Corporation (SDC) in Santa Monica, California in 1965. I supported the AN/FSQ-32, which was the largest transistor (solid state) computer ever made and it weight 60.3 metric tons (132,960 lb). You could walk around inside the computer system. I had to learn every pulse within this system. It took me 9 months at 40 hours a week to learn the entire system so that I could trouble shoot any hardware problem.

I also worked on the IBM AN/FSQ-31 SAC Data Processing System at Offutt Air Force Base near Omaha, Nebraska. Both the Q31 and Q32 needed 24x7 support by electronic technicians or as I was called an IBM Field Engineer assigned to a job site. I did hardware support for the 8 years I supported these systems before going into programming full time. Below is a picture to what I saw for 8 years. Many of the lights show the binary value of each register with in the system. If you know how the system works, then you can look to see if the register has the correct binary information! The man in the picture is touching a console that has a possibility of four instructions. You had to put your instruction in binary because each row of instructions had an on/off switch for a one or zero. Many times I would put an instruction (Like LDA for load accumulator) in the first row, then a branch back to that LDA instruction in the next row. I may be able to look at the blink lights and determine a problem because one bit was not able to change from a one to a zero. I often would put the computer in a loop using the switch console, then go find where I thought the problem was located and then use an oscilloscope to finish the trouble shooting. If I found the problem I may have to change out a “flip-flop” circuit so that the machine would now run correctly. Most of the time, the lights were used by the hardware field engineers for troubleshooting but some time by the systems programmers and very seldom by the application programmers. Note: about 30% of the hardware in the Q31 and Q32 was error checking hardware. This system used parity bits for all data transfers, therefore if any bit failed out 24, the parity bit would be in the wrong position and an error would be sent. You could set the computer so that as soon as the error occurred, the system would completely stop with the registers showing the status of the entire system. Again, if you know how the system worked, you could look at the registers and make a reasonable assessment.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/qorvmb2ice1sn476', 'title': 'What was the purpose of all of the blinking lights on the early computers (think Univac and Eniac) of the 1950s & 1960s? What information did they convey?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'What was the purpose of all of the blinking lights on the early computers (think Univac and Eniac) of the 1950s & 1960s? What information did they convey?\n\nI am almost 78 now but I started on computers in the early 60s. I was working for IBM at the System Development Corporation (SDC) in Santa Monica, California in 1965. I supported the AN/FSQ-32, which was the largest transistor (solid state) computer ever made and it weight 60.3 metric tons (132,960 lb). You could walk around inside the computer system. I had to learn every pulse within this system. It took me 9 months at 40 hours a week to learn the entire system so that I could trouble shoot any hardware problem.\n\nI also worked on the IBM AN/FSQ-31 SAC Data Processing System at Offutt Air Force Base near Omaha, Nebraska. Both the Q31 and Q32 needed 24x7 support by electronic technicians or as I was called an IBM Field Engineer assigned to a job site. I did hardware support for the 8 years I supported these systems before going into programming full time. Below is a picture to what I saw for 8 years. Many of the lights show the binary value of each register with in the system. If you know how the system works, then you can look to see if the register has the correct binary information! The man in the picture is touching a console that has a possibility of four instructions. You had to put your instruction in binary because each row of instructions had an on/off switch for a one or zero. Many times I would put an instruction (Like LDA for load accumulator) in the first row, then a branch back to that LDA instruction in the next row. I may be able to look at the blink lights and determine a problem because one bit was not able to change from a one to a zero. I often would put the computer in a loop using the switch console, then go find where I thought the problem was located and then use an oscilloscope to finish the trouble shooting. If I found the problem I may have to change out a “flip-flop” circuit so that the machine would now run correctly. Most of the time, the lights were used by the hardware field engineers for troubleshooting but some time by the systems programmers and very seldom by the application programmers. Note: about 30% of the hardware in the Q31 and Q32 was error checking hardware. This system used parity bits for all data transfers, therefore if any bit failed out 24, the parity bit would be in the wrong position and an error would be sent. You could set the computer so that as soon as the error occurred, the system would completely stop with the registers showing the status of the entire system. Again, if you know how the system worked, you could look at the registers and make a reasonable assessment.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995402, 'subscription': 0, 'content': 'What was the purpose of all of the blinking lights on the early computers (think Univac and Eniac) of the 1950s & 1960s? What information did they convey?\n\nI am almost 78 now but I started on computers in the early 60s. I was working for IBM at the System Development Corporation (SDC) in Santa Monica, California in 1965. I supported the AN/FSQ-32, which was the largest transistor (solid state) computer ever made and it weight 60.3 metric tons (132,960 lb). You could walk around inside the computer system. I had to learn every pulse within this system. It took me 9 months at 40 hours a week to learn the entire system so that I could trouble shoot any hardware problem.\n\nI also worked on the IBM AN/FSQ-31 SAC Data Processing System at Offutt Air Force Base near Omaha, Nebraska. Both the Q31 and Q32 needed 24x7 support by electronic technicians or as I was called an IBM Field Engineer assigned to a job site. I did hardware support for the 8 years I supported these systems before going into programming full time. Below is a picture to what I saw for 8 years. Many of the lights show the binary value of each register with in the system. If you know how the system works, then you can look to see if the register has the correct binary information! The man in the picture is touching a console that has a possibility of four instructions. You had to put your instruction in binary because each row of instructions had an on/off switch for a one or zero. Many times I would put an instruction (Like LDA for load accumulator) in the first row, then a branch back to that LDA instruction in the next row. I may be able to look at the blink lights and determine a problem because one bit was not able to change from a one to a zero. I often would put the computer in a loop using the switch console, then go find where I thought the problem was located and then use an oscilloscope to finish the trouble shooting. If I found the problem I may have to change out a “flip-flop” circuit so that the machine would now run correctly. Most of the time, the lights were used by the hardware field engineers for troubleshooting but some time by the systems programmers and very seldom by the application programmers. Note: about 30% of the hardware in the Q31 and Q32 was error checking hardware. This system used parity bits for all data transfers, therefore if any bit failed out 24, the parity bit would be in the wrong position and an error would be sent. You could set the computer so that as soon as the error occurred, the system would completely stop with the registers showing the status of the entire system. Again, if you know how the system worked, you could look at the registers and make a reasonable assessment.', 'aiModelVersion': '1'}",0.9998
Stan Hanks,2y,In 1973 did the entire internet only consist of 43 computers?,"Mmmm… maybe.

Here’s the 1969 map

and here’s the 1972 map

and here’s the 1973 map

Note that the 1969 map calls out “nodes” - which would be “computers with multiple message interfaces that pass messages” as opposed to “hosts” - computers with a single message interface on which people have accounts (i.e. “users”)

From the perspective of the guys building networks, what was interesting was the number of “nodes” - Interface Message Processors, IMPs (here’s an IMP, btw)

These suckers were the first “routers” - hardware manufactured explicitly to route network packets.

So when you say “43 computers”… that sounds about right. If you think that “computer” means either “host” or “node”.

But the number of “computers where people have user accounts and are doing stuff”?? Probably a lot less than that.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/h9k1ay26t4foelzv', 'title': 'In 1973 did the entire internet only consist of 43 computers?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': 'Mmmm… maybe.\n\nHere’s the 1969 map\n\nand here’s the 1972 map\n\nand here’s the 1973 map\n\nNote that the 1969 map calls out “nodes” - which would be “computers with multiple message interfaces that pass messages” as opposed to “hosts” - computers with a single message interface on which people have accounts (i.e. “users”)\n\nFrom the perspective of the guys building networks, what was interesting was the number of “nodes” - Interface Message Processors, IMPs (here’s an IMP, btw)\n\nThese suckers were the first “routers” - hardware manufactured explicitly to route network packets.\n\nSo when you say “43 computers”… that sounds about right. If you think that “computer” means either “host” or “node”.\n\nBut the number of “computers where people have user accounts and are doing stuff”?? Probably a lot less than that.', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995400, 'subscription': 0, 'content': 'Mmmm… maybe.\n\nHere’s the 1969 map\n\nand here’s the 1972 map\n\nand here’s the 1973 map\n\nNote that the 1969 map calls out “nodes” - which would be “computers with multiple message interfaces that pass messages” as opposed to “hosts” - computers with a single message interface on which people have accounts (i.e. “users”)\n\nFrom the perspective of the guys building networks, what was interesting was the number of “nodes” - Interface Message Processors, IMPs (here’s an IMP, btw)\n\nThese suckers were the first “routers” - hardware manufactured explicitly to route network packets.\n\nSo when you say “43 computers”… that sounds about right. If you think that “computer” means either “host” or “node”.\n\nBut the number of “computers where people have user accounts and are doing stuff”?? Probably a lot less than that.', 'aiModelVersion': '1'}",0.9995
Varun Gokhale,8y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","Well, I considered this ""cool"" when I was in 7th grade. We had the software Microsoft Word that year.

I love to read a lot and I had recently read Chetan Bhagat's One night @ the call center at that time. In that book, there was mention of a bug in Word. If you type in,

=rand(200,99)

It pours out a few thousand pages of text. It looks something like this.




​
​

This is the trick. It seemed pretty cool back then. But what happened in class was pretty funny.

Our teacher used to teach the class on a projector and one day he was teaching text formatting. He asked me to type in any paragraph while he ducked out of class for some work or something. I sat down in front of the keyboard while the teacher left. I typed this code in and pressed enter. The whole class didn't notice because who pays attention to the chalk board when the teacher's not in the class, right?

A few minutes layer, the teacher walked in and the expression on his face was amazingly astounded. He asked me how I did it and I replied, ""Magic!""

It was worth getting kicked out of class.
​","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/3ji5ahdw6ocg1vu0', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'Well, I considered this ""cool"" when I was in 7th grade. We had the software Microsoft Word that year.\n\nI love to read a lot and I had recently read Chetan Bhagat\'s One night @ the call center at that time. In that book, there was mention of a bug in Word. If you type in,\n\n=rand(200,99)\n\nIt pours out a few thousand pages of text. It looks something like this.\n\n\n\n\n\u200b\n\u200b\n\nThis is the trick. It seemed pretty cool back then. But what happened in class was pretty funny.\n\nOur teacher used to teach the class on a projector and one day he was teaching text formatting. He asked me to type in any paragraph while he ducked out of class for some work or something. I sat down in front of the keyboard while the teacher left. I typed this code in and pressed enter. The whole class didn\'t notice because who pays attention to the chalk board when the teacher\'s not in the class, right?\n\nA few minutes layer, the teacher walked in and the expression on his face was amazingly astounded. He asked me how I did it and I replied, ""Magic!""\n\nIt was worth getting kicked out of class.\n\u200b', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995397, 'subscription': 0, 'content': 'Well, I considered this ""cool"" when I was in 7th grade. We had the software Microsoft Word that year.\n\nI love to read a lot and I had recently read Chetan Bhagat\'s One night @ the call center at that time. In that book, there was mention of a bug in Word. If you type in,\n\n=rand(200,99)\n\nIt pours out a few thousand pages of text. It looks something like this.\n\n\n\n\n\u200b\n\u200b\n\nThis is the trick. It seemed pretty cool back then. But what happened in class was pretty funny.\n\nOur teacher used to teach the class on a projector and one day he was teaching text formatting. He asked me to type in any paragraph while he ducked out of class for some work or something. I sat down in front of the keyboard while the teacher left. I typed this code in and pressed enter. The whole class didn\'t notice because who pays attention to the chalk board when the teacher\'s not in the class, right?\n\nA few minutes layer, the teacher walked in and the expression on his face was amazingly astounded. He asked me how I did it and I replied, ""Magic!""\n\nIt was worth getting kicked out of class.\n\u200b', 'aiModelVersion': '1'}",0.9999
Steven Haddock,2y,What was the actual reason why computers were made?,"The first thing we might think of as a computer was the automatic tabulator machine

It was designed to deal with a logistical nightmare - the U.S. Census of 1900.

The U.S. Census of 1880 was a nightmare. The population of the country had grown tremendously in the previous ten years. The people hired to count all the forms worked overtime, but it took the counters seven years to tabulate the results. The population grew even more by 1890, and the estimate was that, using human beings, it would take about 15 years to complete.

So a guy named Herman Hollerith designed the machine above. When the forms were received, the data was transferred to cards like this

The card was put into a reader with wires on top and a pool of mercury beneath. When wire touched mercury, it moved a counter forward one space and rung a bell to let the operator know. It also opened up a compartment to hold the card for later use. Using this method, the 1890 census was completed in 18 months.

However, by 1940, this sort of tabulation system was again taking too long. At this point, IBM developed an electronic computer that had no mechanical components and could keep the count electronically.

The computer turned out to have other uses, like calculating these ballistic tables, which used to take about a month each

But the real commercial breakthrough happened in the United Kingdom. A large bakery company with a massive workforce was having trouble working out payroll deductions. It usually took them an entire week to work out each week’s paycheques. So, they developed their own computer to do the accounting for them. The machine was so successful, they also managed to build more of them to sell to others. They soon spun off the computer company

LEO (computer) - Wikipedia
1951 British computer The LEO ( Lyons Electronic Office ) was a series of early computer systems created by J. Lyons and Co. The first in the series, the LEO I, was the first computer used for commercial business applications. The prototype LEO I was modelled closely on the Cambridge EDSAC . Its construction was overseen by Oliver Standingford, Raymond Thompson and David Caminer of J. Lyons and Co. LEO I ran its first business application in 1951. In 1954 Lyons formed LEO Computers Ltd to market LEO I and its successors LEO II and LEO III to other companies. LEO Computers eventually became part of English Electric Company (EEL), (EELM), then English Electric Computers (EEC), where the same team developed the faster LEO 360 and even faster LEO 326 models. It then passed to International Computers Limited (ICL) and ultimately Fujitsu . LEO series computers were still in use until 1981. Origins and initial design [ edit ] J. Lyons and Co. was one of the UK's leading catering and food manufacturing companies in the first half of the 20th century. In 1947, two of its senior managers, Oliver Standingford and Raymond Thompson, were sent to the United States to look at new business methods developed during World War II . During the visit, they met Herman Goldstine who was one of the original developers of ENIAC , the first general-purpose electronic computer. Standingford and Thompson saw the potential of computers to help solve the problem of administering a major business enterprise. They also learned from Goldstine that, back in the UK, Douglas Hartree and Maurice Wilkes were actually building another such machine, the pioneering EDSAC computer, at the University of Cambridge . [1] On their return to the UK, Standingford and Thompson visited Hartree and Wilkes in Cambridge and were favourably impressed with their technical expertise and vision. Hartree and Wilkes estimated that EDSAC was 12–18 months from completion, but said that this interval could be shortened by additional funding. Standingford and Thompson wrote a report to the Lyons' board recommending that Lyons should acquire or build a computer to meet their business needs. The board agreed that, as a first step, Lyons would provide Hartree and Wilkes with £2,500 for the EDSAC project, and would also provide them with the services of a Lyons electrical engineer, Ernest Lenaerts. EDSAC was completed and ran its first program in May 1949. [2] Following the successful completion of EDSAC, the Lyons board agreed to start the construction of their own machine, expanding on the EDSAC design. The LEO computer room, which took up around 2,500 square feet of floor space, was at Cadby Hall in Hammersmith. [3] The Lyons machine was christened Lyons Electronic Office, or LEO. On the recommendation of Wilkes, Lyons recruited John Pinkerton , a radar engineer and research student at Cambridge, as team leader for the project. Lenaerts returned to Lyons to work on the project, and Wilkes provided training
https://en.wikipedia.org/wiki/LEO_(computer)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ks6b9xqr85nfu1zc', 'title': 'What was the actual reason why computers were made?', 'score': {'original': 0.74975, 'ai': 0.25025}, 'blocks': [{'text': ""The first thing we might think of as a computer was the automatic tabulator machine\n\nIt was designed to deal with a logistical nightmare - the U.S. Census of 1900.\n\nThe U.S. Census of 1880 was a nightmare. The population of the country had grown tremendously in the previous ten years. The people hired to count all the forms worked overtime, but it took the counters seven years to tabulate the results. The population grew even more by 1890, and the estimate was that, using human beings, it would take about 15 years to complete.\n\nSo a guy named Herman Hollerith designed the machine above. When the forms were received, the data was transferred to cards like this\n\nThe card was put into a reader with wires on top and a pool of mercury beneath. When wire touched mercury, it moved a counter forward one space and rung a bell to let the operator know. It also opened up a compartment to hold the card for later use. Using this method, the 1890 census was completed in 18 months.\n\nHowever, by 1940, this sort of tabulation system was again taking too long. At this point, IBM developed an electronic computer that had no mechanical components and could keep the count electronically.\n\nThe computer turned out to have other uses, like calculating these ballistic tables, which used to take about a month each\n\nBut the real commercial breakthrough happened in the United Kingdom. A large bakery company with a massive workforce was having trouble working out payroll deductions. It usually took them an entire week to work out each week’s paycheques. So, they developed their own computer to do the accounting for them. The machine was so successful, they also managed to build more of them to sell to others. They soon spun off the computer company\n\nLEO (computer) - Wikipedia\n1951 British computer The LEO ( Lyons Electronic Office ) was a series of early computer systems created by J. Lyons and Co. The first in the series, the LEO I, was the first computer used for commercial business applications. The prototype LEO I was modelled closely on the Cambridge EDSAC . Its construction was overseen by Oliver Standingford, Raymond Thompson and David Caminer of J. Lyons and Co. LEO I ran its first business application in 1951. In 1954 Lyons formed LEO Computers Ltd to market LEO\xa0I and its successors LEO\xa0II and LEO\xa0III to other companies. LEO Computers eventually became part of English Electric Company (EEL), (EELM), then English Electric Computers (EEC), where the same team developed the faster LEO\xa0360 and even faster LEO\xa0326 models. It then passed to International Computers Limited (ICL) and ultimately Fujitsu . LEO series computers were still in use until 1981. Origins and initial design [ edit ] J. Lyons and Co. was one of the UK's leading catering and food manufacturing companies in the first half of the 20th century. In 1947, two of its senior managers, Oliver Standingford and Raymond Thompson, were sent to the United States to look at new business methods developed during World War II . During the visit, they"", 'result': {'fake': 0.2607, 'real': 0.7393}, 'status': 'success'}, {'text': ""met Herman Goldstine who was one of the original developers of ENIAC , the first general-purpose electronic computer. Standingford and Thompson saw the potential of computers to help solve the problem of administering a major business enterprise. They also learned from Goldstine that, back in the UK, Douglas Hartree and Maurice Wilkes were actually building another such machine, the pioneering EDSAC computer, at the University of Cambridge . [1] On their return to the UK, Standingford and Thompson visited Hartree and Wilkes in Cambridge and were favourably impressed with their technical expertise and vision. Hartree and Wilkes estimated that EDSAC was 12–18 months from completion, but said that this interval could be shortened by additional funding. Standingford and Thompson wrote a report to the Lyons' board recommending that Lyons should acquire or build a computer to meet their business needs. The board agreed that, as a first step, Lyons would provide Hartree and Wilkes with £2,500 for the EDSAC project, and would also provide them with the services of a Lyons electrical engineer, Ernest Lenaerts. EDSAC was completed and ran its first program in May 1949. [2] Following the successful completion of EDSAC, the Lyons board agreed to start the construction of their own machine, expanding on the EDSAC design. The LEO computer room, which took up around 2,500 square feet of floor space, was at Cadby Hall in Hammersmith. [3] The Lyons machine was christened Lyons Electronic Office, or LEO. On the recommendation of Wilkes, Lyons recruited John Pinkerton , a radar engineer and research student at Cambridge, as team leader for the project. Lenaerts returned to Lyons to work on the project, and Wilkes provided training\nhttps://en.wikipedia.org/wiki/LEO_(computer)"", 'result': {'fake': 0.0037, 'real': 0.9963}, 'status': 'success'}], 'credits_used': 8, 'credits': 1995389, 'subscription': 0, 'content': ""The first thing we might think of as a computer was the automatic tabulator machine\n\nIt was designed to deal with a logistical nightmare - the U.S. Census of 1900.\n\nThe U.S. Census of 1880 was a nightmare. The population of the country had grown tremendously in the previous ten years. The people hired to count all the forms worked overtime, but it took the counters seven years to tabulate the results. The population grew even more by 1890, and the estimate was that, using human beings, it would take about 15 years to complete.\n\nSo a guy named Herman Hollerith designed the machine above. When the forms were received, the data was transferred to cards like this\n\nThe card was put into a reader with wires on top and a pool of mercury beneath. When wire touched mercury, it moved a counter forward one space and rung a bell to let the operator know. It also opened up a compartment to hold the card for later use. Using this method, the 1890 census was completed in 18 months.\n\nHowever, by 1940, this sort of tabulation system was again taking too long. At this point, IBM developed an electronic computer that had no mechanical components and could keep the count electronically.\n\nThe computer turned out to have other uses, like calculating these ballistic tables, which used to take about a month each\n\nBut the real commercial breakthrough happened in the United Kingdom. A large bakery company with a massive workforce was having trouble working out payroll deductions. It usually took them an entire week to work out each week’s paycheques. So, they developed their own computer to do the accounting for them. The machine was so successful, they also managed to build more of them to sell to others. They soon spun off the computer company\n\nLEO (computer) - Wikipedia\n1951 British computer The LEO ( Lyons Electronic Office ) was a series of early computer systems created by J. Lyons and Co. The first in the series, the LEO I, was the first computer used for commercial business applications. The prototype LEO I was modelled closely on the Cambridge EDSAC . Its construction was overseen by Oliver Standingford, Raymond Thompson and David Caminer of J. Lyons and Co. LEO I ran its first business application in 1951. In 1954 Lyons formed LEO Computers Ltd to market LEO\xa0I and its successors LEO\xa0II and LEO\xa0III to other companies. LEO Computers eventually became part of English Electric Company (EEL), (EELM), then English Electric Computers (EEC), where the same team developed the faster LEO\xa0360 and even faster LEO\xa0326 models. It then passed to International Computers Limited (ICL) and ultimately Fujitsu . LEO series computers were still in use until 1981. Origins and initial design [ edit ] J. Lyons and Co. was one of the UK's leading catering and food manufacturing companies in the first half of the 20th century. In 1947, two of its senior managers, Oliver Standingford and Raymond Thompson, were sent to the United States to look at new business methods developed during World War II . During the visit, they met Herman Goldstine who was one of the original developers of ENIAC , the first general-purpose electronic computer. Standingford and Thompson saw the potential of computers to help solve the problem of administering a major business enterprise. They also learned from Goldstine that, back in the UK, Douglas Hartree and Maurice Wilkes were actually building another such machine, the pioneering EDSAC computer, at the University of Cambridge . [1] On their return to the UK, Standingford and Thompson visited Hartree and Wilkes in Cambridge and were favourably impressed with their technical expertise and vision. Hartree and Wilkes estimated that EDSAC was 12–18 months from completion, but said that this interval could be shortened by additional funding. Standingford and Thompson wrote a report to the Lyons' board recommending that Lyons should acquire or build a computer to meet their business needs. The board agreed that, as a first step, Lyons would provide Hartree and Wilkes with £2,500 for the EDSAC project, and would also provide them with the services of a Lyons electrical engineer, Ernest Lenaerts. EDSAC was completed and ran its first program in May 1949. [2] Following the successful completion of EDSAC, the Lyons board agreed to start the construction of their own machine, expanding on the EDSAC design. The LEO computer room, which took up around 2,500 square feet of floor space, was at Cadby Hall in Hammersmith. [3] The Lyons machine was christened Lyons Electronic Office, or LEO. On the recommendation of Wilkes, Lyons recruited John Pinkerton , a radar engineer and research student at Cambridge, as team leader for the project. Lenaerts returned to Lyons to work on the project, and Wilkes provided training\nhttps://en.wikipedia.org/wiki/LEO_(computer)"", 'aiModelVersion': '1'}",0.74975
Trevor Zylstra,2y,"If a floppy disk in the 1980s could hold a gigabyte of memory, how long would it have taken to save a gigabyte file into it?","OK, I’ll first state my assumptions, then show my calculations.

First assumption: our magical floppy disk that can hold a GB doesn’t get any speed-up from the increased density of data. It’s magical.

Second assumption: our magical floppy drive from the 80’s is specifically a Commodore 1541, with no FastLoad. Fact, the 1541 with no FastLoad loads files at 403 bytes per second. But more importantly for this question, it saves files at 370 bytes per second.

Third assumption: by gigabyte, we mean exactly 1,073,741,824 bytes.

OK, that’s enough for our calculations. If the drive saves at 370 bytes per second, and has 1,073,741,824 bytes to save, then we just divide 1,073,741,824 / 370 to get the number of seconds required.

1,073,741,824 ÷ 370 = 2,902,004.9 seconds = 48,366.7 minutes = 806.1 hours =

33.6 days","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/t20vn9a1olfbjcmw', 'title': 'If a floppy disk in the 1980s could hold a gigabyte of memory, how long would it have taken to save a gigabyte file into it?', 'score': {'original': 0.9967, 'ai': 0.0033}, 'blocks': [{'text': 'OK, I’ll first state my assumptions, then show my calculations.\n\nFirst assumption: our magical floppy disk that can hold a GB doesn’t get any speed-up from the increased density of data. It’s magical.\n\nSecond assumption: our magical floppy drive from the 80’s is specifically a Commodore 1541, with no FastLoad. Fact, the 1541 with no FastLoad loads files at 403 bytes per second. But more importantly for this question, it saves files at 370 bytes per second.\n\nThird assumption: by gigabyte, we mean exactly 1,073,741,824 bytes.\n\nOK, that’s enough for our calculations. If the drive saves at 370 bytes per second, and has 1,073,741,824 bytes to save, then we just divide 1,073,741,824 / 370 to get the number of seconds required.\n\n1,073,741,824 ÷ 370 = 2,902,004.9 seconds = 48,366.7 minutes = 806.1 hours =\n\n33.6 days', 'result': {'fake': 0.0033, 'real': 0.9967}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995387, 'subscription': 0, 'content': 'OK, I’ll first state my assumptions, then show my calculations.\n\nFirst assumption: our magical floppy disk that can hold a GB doesn’t get any speed-up from the increased density of data. It’s magical.\n\nSecond assumption: our magical floppy drive from the 80’s is specifically a Commodore 1541, with no FastLoad. Fact, the 1541 with no FastLoad loads files at 403 bytes per second. But more importantly for this question, it saves files at 370 bytes per second.\n\nThird assumption: by gigabyte, we mean exactly 1,073,741,824 bytes.\n\nOK, that’s enough for our calculations. If the drive saves at 370 bytes per second, and has 1,073,741,824 bytes to save, then we just divide 1,073,741,824 / 370 to get the number of seconds required.\n\n1,073,741,824 ÷ 370 = 2,902,004.9 seconds = 48,366.7 minutes = 806.1 hours =\n\n33.6 days', 'aiModelVersion': '1'}",0.9967
Alan Kay,Updated 3y,Why was Alan Kay's Smalltalk in the Xerox Alto sufficiently efficient for a desktop GUI in 1979 but it isn't sufficient today?,"The two answers by Eliot Miranda and Mark Miller are very good, so there is little to add here. But I should say a few words about the general Parc approach to HW architectures (and more can be found in other answers I’ve given).

First, though, is to compare a one-bit-per-pixel architecture on a 16 bit word machine to a 24-bit-plus-alpha architecture on a 32 or 64 bit word machine. There’s a factor of 8 to 16 difference per memory cycle (3 to 4 doublings of Moore’s Law) just for this.

The problem with the 16-bit micros in the early 80s — such as the Motorola 68000 (used on the Mac) and the Intel 8086 (used for IBM PC compatibles) was not just cycle time but also the fundamental difference between being able to stick “hardware like functions” (such as a byte code interpreter, graphics painting, music, etc.) into a microcode engine that could run about 5–6 times faster than main memory vs. having to do this with a CPU that is using main-memory cycles for fetching its own code and data while also trying to do low-level functions.

The Alto was fabulously efficient because its approach to how its real CPU (the microcode engine) interacted with the available memory bandwidth (including the use of multiple program counters for zero overhead low-level task switching and avoiding interrupts).

Whereas the commercial CPU architectures had very poor notions of how to use memory and how to emulate low-level functionality. This made them extremely inefficient for any given clock cycle they could muster.

We put in quite a bit of energy during the last part of the 70s trying to explain software needs and how this should affect hardware design to both Intel and Motorola — and giving them the same kinds of demos that Steve and the Apple people saw — but to no avail. Neither company ever got interested and savvy in software.

In the 80s, this was even more difficult because the IBM PC architecture — for an deeply naive market — was quite successful (this is like the WWW/browser, which is a terrible architecture, but the market can’t see it).

When at Apple, old Parc hands urged the development of custom CPU chips and other custom hardware, but this was rejected for almost two decades (Steve mentioned this in the introduction of the iPhone).

A simple bottom line is to notice how difficult it was and how long it took for the industry to wake up to the need for GPUs, despite that screen painting was a major need for every personal computer and workstation. This kind of blindness is almost a defining characteristic of “computing” but I’ve never been able to understand quite why — especially given how successful Parc was because it was willing to do any and all hardware that advanced software required.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/1he7j0b5pdkzs2g6', 'title': ""Why was Alan Kay's Smalltalk in the Xerox Alto sufficiently efficient for a desktop GUI in 1979 but it isn't sufficient today?"", 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'The two answers by Eliot Miranda and Mark Miller are very good, so there is little to add here. But I should say a few words about the general Parc approach to HW architectures (and more can be found in other answers I’ve given).\n\nFirst, though, is to compare a one-bit-per-pixel architecture on a 16 bit word machine to a 24-bit-plus-alpha architecture on a 32 or 64 bit word machine. There’s a factor of 8 to 16 difference per memory cycle (3 to 4 doublings of Moore’s Law) just for this.\n\nThe problem with the 16-bit micros in the early 80s — such as the Motorola 68000 (used on the Mac) and the Intel 8086 (used for IBM PC compatibles) was not just cycle time but also the fundamental difference between being able to stick “hardware like functions” (such as a byte code interpreter, graphics painting, music, etc.) into a microcode engine that could run about 5–6 times faster than main memory vs. having to do this with a CPU that is using main-memory cycles for fetching its own code and data while also trying to do low-level functions.\n\nThe Alto was fabulously efficient because its approach to how its real CPU (the microcode engine) interacted with the available memory bandwidth (including the use of multiple program counters for zero overhead low-level task switching and avoiding interrupts).\n\nWhereas the commercial CPU architectures had very poor notions of how to use memory and how to emulate low-level functionality. This made them extremely inefficient for any given clock cycle they could muster.\n\nWe put in quite a bit of energy during the last part of the 70s trying to explain software needs and how this should affect hardware design to both Intel and Motorola — and giving them the same kinds of demos that Steve and the Apple people saw — but to no avail. Neither company ever got interested and savvy in software.\n\nIn the 80s, this was even more difficult because the IBM PC architecture — for an deeply naive market — was quite successful (this is like the WWW/browser, which is a terrible architecture, but the market can’t see it).\n\nWhen at Apple, old Parc hands urged the development of custom CPU chips and other custom hardware, but this was rejected for almost two decades (Steve mentioned this in the introduction of the iPhone).\n\nA simple bottom line is to notice how difficult it was and how long it took for the industry to wake up to the need for GPUs, despite that screen painting was a major need for every personal computer and workstation. This kind of blindness is almost a defining characteristic of “computing” but I’ve never been able to understand quite why — especially given how successful Parc was because it was willing to do any and all hardware that advanced software required.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995382, 'subscription': 0, 'content': 'The two answers by Eliot Miranda and Mark Miller are very good, so there is little to add here. But I should say a few words about the general Parc approach to HW architectures (and more can be found in other answers I’ve given).\n\nFirst, though, is to compare a one-bit-per-pixel architecture on a 16 bit word machine to a 24-bit-plus-alpha architecture on a 32 or 64 bit word machine. There’s a factor of 8 to 16 difference per memory cycle (3 to 4 doublings of Moore’s Law) just for this.\n\nThe problem with the 16-bit micros in the early 80s — such as the Motorola 68000 (used on the Mac) and the Intel 8086 (used for IBM PC compatibles) was not just cycle time but also the fundamental difference between being able to stick “hardware like functions” (such as a byte code interpreter, graphics painting, music, etc.) into a microcode engine that could run about 5–6 times faster than main memory vs. having to do this with a CPU that is using main-memory cycles for fetching its own code and data while also trying to do low-level functions.\n\nThe Alto was fabulously efficient because its approach to how its real CPU (the microcode engine) interacted with the available memory bandwidth (including the use of multiple program counters for zero overhead low-level task switching and avoiding interrupts).\n\nWhereas the commercial CPU architectures had very poor notions of how to use memory and how to emulate low-level functionality. This made them extremely inefficient for any given clock cycle they could muster.\n\nWe put in quite a bit of energy during the last part of the 70s trying to explain software needs and how this should affect hardware design to both Intel and Motorola — and giving them the same kinds of demos that Steve and the Apple people saw — but to no avail. Neither company ever got interested and savvy in software.\n\nIn the 80s, this was even more difficult because the IBM PC architecture — for an deeply naive market — was quite successful (this is like the WWW/browser, which is a terrible architecture, but the market can’t see it).\n\nWhen at Apple, old Parc hands urged the development of custom CPU chips and other custom hardware, but this was rejected for almost two decades (Steve mentioned this in the introduction of the iPhone).\n\nA simple bottom line is to notice how difficult it was and how long it took for the industry to wake up to the need for GPUs, despite that screen painting was a major need for every personal computer and workstation. This kind of blindness is almost a defining characteristic of “computing” but I’ve never been able to understand quite why — especially given how successful Parc was because it was willing to do any and all hardware that advanced software required.', 'aiModelVersion': '1'}",0.9998
Taciano Dreckmann Perez,Updated 1y,Can you buy an Apple II computer today? What about when you plug it in; will it word just like it did in the 1980's?,"A couple of years ago I bought an Apple IIe from a guy in Marktplaats (the Dutch equivalent to eBay). The Apple IIe had been my first computer back in the 80s and I craved to own it again.

I drove a couple hours to pick up the computer, two Disk ][ drives, and an original CRT monitor. The seller was a computer collector who had bought this computer from a school in the 1990s, when they were sold for almost nothing. It sat in storage for some thirty years, but the collector had it cleaned and tested before posting up for sale.

I took it home. When I powered it up, it worked.

To be precise, it worked for about minute or so, but then collapsed with a blow of smelly magic smoke.

After some due diligence (aka googling around), I determined the likely culprits to be the Rifa capacitors in the power supply. These components, sold under the brand Rifa, filter out ripples sent back to the AC mains, which can potentially interfere with other equipment. This is not so relevant for today’s appliances, but used to be back in the day. Anyway, the plastic casing around these capacitors tends to crack with age, letting moisture in, until one day the heat causes the water to expand and they blow up in smoke. That’s what happened to my unit.

Below you can see the blown up Rifa cap, the big one. Its smaller sibling sits to the right, waiting to do some mischief of its own.

As you can see, it has blown half of its plastic casing away, with a loud crack and dramatic smoke effects. These caps are a time bomb, and are very likely to go off one day just after you turn on the computer.

Fortunately I had the tools and skills to replace them with modern equivalents, which I ordered online. They cost only a few euros.

After reassembling everything back again, it has been working dutifully for two years already. If it ever stops working again, the likely culprits will be the electrolytic capacitors (the round, black components at the bottom of the picture above). They also have a lifetime numbered in decades, and there’s a few tens of them around the computer, mostly in the power supply. Almost all the other components are likely to last for centuries, given proper care and storage.

Keep them indoors, away from humidity and extreme temperatures. Cover them to prevent dust accumulating inside, especially the keyboard. Turn them on every month or so, and leave them on at least 20 minutes, and they may outlive you.

This Apple IIe is a warrior!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0mxng7a1qr3ib8d9', 'title': ""Can you buy an Apple II computer today? What about when you plug it in; will it word just like it did in the 1980's?"", 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'A couple of years ago I bought an Apple IIe from a guy in Marktplaats (the Dutch equivalent to eBay). The Apple IIe had been my first computer back in the 80s and I craved to own it again.\n\nI drove a couple hours to pick up the computer, two Disk ][ drives, and an original CRT monitor. The seller was a computer collector who had bought this computer from a school in the 1990s, when they were sold for almost nothing. It sat in storage for some thirty years, but the collector had it cleaned and tested before posting up for sale.\n\nI took it home. When I powered it up, it worked.\n\nTo be precise, it worked for about minute or so, but then collapsed with a blow of smelly magic smoke.\n\nAfter some due diligence (aka googling around), I determined the likely culprits to be the Rifa capacitors in the power supply. These components, sold under the brand Rifa, filter out ripples sent back to the AC mains, which can potentially interfere with other equipment. This is not so relevant for today’s appliances, but used to be back in the day. Anyway, the plastic casing around these capacitors tends to crack with age, letting moisture in, until one day the heat causes the water to expand and they blow up in smoke. That’s what happened to my unit.\n\nBelow you can see the blown up Rifa cap, the big one. Its smaller sibling sits to the right, waiting to do some mischief of its own.\n\nAs you can see, it has blown half of its plastic casing away, with a loud crack and dramatic smoke effects. These caps are a time bomb, and are very likely to go off one day just after you turn on the computer.\n\nFortunately I had the tools and skills to replace them with modern equivalents, which I ordered online. They cost only a few euros.\n\nAfter reassembling everything back again, it has been working dutifully for two years already. If it ever stops working again, the likely culprits will be the electrolytic capacitors (the round, black components at the bottom of the picture above). They also have a lifetime numbered in decades, and there’s a few tens of them around the computer, mostly in the power supply. Almost all the other components are likely to last for centuries, given proper care and storage.\n\nKeep them indoors, away from humidity and extreme temperatures. Cover them to prevent dust accumulating inside, especially the keyboard. Turn them on every month or so, and leave them on at least 20 minutes, and they may outlive you.\n\nThis Apple IIe is a warrior!', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995377, 'subscription': 0, 'content': 'A couple of years ago I bought an Apple IIe from a guy in Marktplaats (the Dutch equivalent to eBay). The Apple IIe had been my first computer back in the 80s and I craved to own it again.\n\nI drove a couple hours to pick up the computer, two Disk ][ drives, and an original CRT monitor. The seller was a computer collector who had bought this computer from a school in the 1990s, when they were sold for almost nothing. It sat in storage for some thirty years, but the collector had it cleaned and tested before posting up for sale.\n\nI took it home. When I powered it up, it worked.\n\nTo be precise, it worked for about minute or so, but then collapsed with a blow of smelly magic smoke.\n\nAfter some due diligence (aka googling around), I determined the likely culprits to be the Rifa capacitors in the power supply. These components, sold under the brand Rifa, filter out ripples sent back to the AC mains, which can potentially interfere with other equipment. This is not so relevant for today’s appliances, but used to be back in the day. Anyway, the plastic casing around these capacitors tends to crack with age, letting moisture in, until one day the heat causes the water to expand and they blow up in smoke. That’s what happened to my unit.\n\nBelow you can see the blown up Rifa cap, the big one. Its smaller sibling sits to the right, waiting to do some mischief of its own.\n\nAs you can see, it has blown half of its plastic casing away, with a loud crack and dramatic smoke effects. These caps are a time bomb, and are very likely to go off one day just after you turn on the computer.\n\nFortunately I had the tools and skills to replace them with modern equivalents, which I ordered online. They cost only a few euros.\n\nAfter reassembling everything back again, it has been working dutifully for two years already. If it ever stops working again, the likely culprits will be the electrolytic capacitors (the round, black components at the bottom of the picture above). They also have a lifetime numbered in decades, and there’s a few tens of them around the computer, mostly in the power supply. Almost all the other components are likely to last for centuries, given proper care and storage.\n\nKeep them indoors, away from humidity and extreme temperatures. Cover them to prevent dust accumulating inside, especially the keyboard. Turn them on every month or so, and leave them on at least 20 minutes, and they may outlive you.\n\nThis Apple IIe is a warrior!', 'aiModelVersion': '1'}",0.9998
Ian Lang,2y,Why were floppy disks called floppy disks when they are neither floppy nor a disk?,"Because they used to be both floppy and disc-shaped. This is an old IBM floppy disc:

8 inches, although here it’s measured in metric for people who can only count in tens. These wobbled like buggery and you handled them like eggs because the only thing stopping that magnetic disk on the inside from becoming a useless floppy frisbee was that thin card on the outside.

Here’s something of a later vintage, circa 1980:

5 1/4 inches. There’s still a wobbly substrate inside a thin card, though. Don’t put ’em in your bag.

This is a later version:

1.44 MB and 3 1/2″ and by now they’ve got a hard plastic cover but it’s still a spinny disc inside. The spinny was actually a flexible plastic coated in a magnetic material.

Here’s all three together:

Of course, nobody uses floppy disks any more. 1.44 MB of spinny plastic has become:

32 GB in something the size of a Rizla packet. This is what the floating gate transistor has led us to. Takes all the skill out of it.

I blame the EU. Insulate field effect-like.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/uwai2zmtlk5egqdb', 'title': 'Why were floppy disks called floppy disks when they are neither floppy nor a disk?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'Because they used to be both floppy and disc-shaped. This is an old IBM floppy disc:\n\n8 inches, although here it’s measured in metric for people who can only count in tens. These wobbled like buggery and you handled them like eggs because the only thing stopping that magnetic disk on the inside from becoming a useless floppy frisbee was that thin card on the outside.\n\nHere’s something of a later vintage, circa 1980:\n\n5 1/4 inches. There’s still a wobbly substrate inside a thin card, though. Don’t put ’em in your bag.\n\nThis is a later version:\n\n1.44 MB and 3 1/2″ and by now they’ve got a hard plastic cover but it’s still a spinny disc inside. The spinny was actually a flexible plastic coated in a magnetic material.\n\nHere’s all three together:\n\nOf course, nobody uses floppy disks any more. 1.44 MB of spinny plastic has become:\n\n32 GB in something the size of a Rizla packet. This is what the floating gate transistor has led us to. Takes all the skill out of it.\n\nI blame the EU. Insulate field effect-like.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995375, 'subscription': 0, 'content': 'Because they used to be both floppy and disc-shaped. This is an old IBM floppy disc:\n\n8 inches, although here it’s measured in metric for people who can only count in tens. These wobbled like buggery and you handled them like eggs because the only thing stopping that magnetic disk on the inside from becoming a useless floppy frisbee was that thin card on the outside.\n\nHere’s something of a later vintage, circa 1980:\n\n5 1/4 inches. There’s still a wobbly substrate inside a thin card, though. Don’t put ’em in your bag.\n\nThis is a later version:\n\n1.44 MB and 3 1/2″ and by now they’ve got a hard plastic cover but it’s still a spinny disc inside. The spinny was actually a flexible plastic coated in a magnetic material.\n\nHere’s all three together:\n\nOf course, nobody uses floppy disks any more. 1.44 MB of spinny plastic has become:\n\n32 GB in something the size of a Rizla packet. This is what the floating gate transistor has led us to. Takes all the skill out of it.\n\nI blame the EU. Insulate field effect-like.', 'aiModelVersion': '1'}",0.9997
Alan Clement,Updated 6y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","When we were teenagers in the UK in the 1980s, early home computers were just starting to appear in consumer electrical shops. Of course at the time, the salespeople selling these new fangled devices were more used to shifting white goods, so their technical knowledge was limited.

Many of the computers on offer had a BASIC language interpreter pre-installed. So for cheap kicks, we used to wander round the stores and on the display models, we would open up BASIC and type:

10 PRINT ""Buy this computer. HALF PRICE today only!!!"" 
20 GOTO 10 
RUN 

​This would fill the screen with endless copies of the line of text.

We'd then stand back and watch from a safe distance as the technically-challenged salespeople furiously and fruitlessly tried to break into the program to stop it.

Seems a bit puerile now, but at the time we thought we were hilarious.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/yiuodazvjk3nc8bq', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.999, 'ai': 0.001}, 'blocks': [{'text': 'When we were teenagers in the UK in the 1980s, early home computers were just starting to appear in consumer electrical shops. Of course at the time, the salespeople selling these new fangled devices were more used to shifting white goods, so their technical knowledge was limited.\n\nMany of the computers on offer had a BASIC language interpreter pre-installed. So for cheap kicks, we used to wander round the stores and on the display models, we would open up BASIC and type:\n\n10 PRINT ""Buy this computer. HALF PRICE today only!!!""\xa0\n20 GOTO 10\xa0\nRUN\xa0\n\n\u200bThis would fill the screen with endless copies of the line of text.\n\nWe\'d then stand back and watch from a safe distance as the technically-challenged salespeople furiously and fruitlessly tried to break into the program to stop it.\n\nSeems a bit puerile now, but at the time we thought we were hilarious.', 'result': {'fake': 0.0015, 'real': 0.9985}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995373, 'subscription': 0, 'content': 'When we were teenagers in the UK in the 1980s, early home computers were just starting to appear in consumer electrical shops. Of course at the time, the salespeople selling these new fangled devices were more used to shifting white goods, so their technical knowledge was limited.\n\nMany of the computers on offer had a BASIC language interpreter pre-installed. So for cheap kicks, we used to wander round the stores and on the display models, we would open up BASIC and type:\n\n10 PRINT ""Buy this computer. HALF PRICE today only!!!""\xa0\n20 GOTO 10\xa0\nRUN\xa0\n\n\u200bThis would fill the screen with endless copies of the line of text.\n\nWe\'d then stand back and watch from a safe distance as the technically-challenged salespeople furiously and fruitlessly tried to break into the program to stop it.\n\nSeems a bit puerile now, but at the time we thought we were hilarious.', 'aiModelVersion': '1'}",0.999
Ganesh Pandey,Updated 9y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","Play Racing Game from Microsoft Excel 2000

When i was in class 7-8 (2006). I used to play Racing Game from Microsoft Excel 2000 in my Computer Lab class with my friends.

Here's the step:

If you have Ms Excel 2000.


Open Ms-Excel 2000.
Create now worksheet.
Save as Web Page (From file menu).save anywhere you like. (extension .htm)
Now Open the saved .htm file. (I used to open with IExplorer browser back in 2006). I don't know if it works on chrome, firefox.
You will see the excel table in the browser
Scroll the column and row so that there will be WC in column and 2000 in a row. (Both in topmost left corner)
Select WC column with mouse. Press CTRL + ALT +DEL and at the same time click on the Square logo (Upper Left corner).

And you will see a Game window in Full Screen and now you can race the car. You can fire to other cars with space key.

Here is the screen-shot of the game window.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ryuz8m75vn3g9p4k', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': ""Play Racing Game from Microsoft Excel 2000\n\nWhen i was in class 7-8 (2006). I used to play Racing Game from Microsoft Excel 2000 in my Computer Lab class with my friends.\n\nHere's the step:\n\nIf you have Ms Excel 2000.\n\n\nOpen Ms-Excel 2000.\nCreate now worksheet.\nSave as Web Page (From file menu).save anywhere you like. (extension .htm)\nNow Open the saved .htm file. (I used to open with IExplorer browser back in 2006). I don't know if it works on chrome, firefox.\nYou will see the excel table in the browser\nScroll the column and row so that there will be WC in column and 2000 in a row. (Both in topmost left corner)\nSelect WC column with mouse. Press CTRL + ALT +DEL and at the same time click on the Square logo (Upper Left corner).\n\nAnd you will see a Game window in Full Screen and now you can race the car. You can fire to other cars with space key.\n\nHere is the screen-shot of the game window."", 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995371, 'subscription': 0, 'content': ""Play Racing Game from Microsoft Excel 2000\n\nWhen i was in class 7-8 (2006). I used to play Racing Game from Microsoft Excel 2000 in my Computer Lab class with my friends.\n\nHere's the step:\n\nIf you have Ms Excel 2000.\n\n\nOpen Ms-Excel 2000.\nCreate now worksheet.\nSave as Web Page (From file menu).save anywhere you like. (extension .htm)\nNow Open the saved .htm file. (I used to open with IExplorer browser back in 2006). I don't know if it works on chrome, firefox.\nYou will see the excel table in the browser\nScroll the column and row so that there will be WC in column and 2000 in a row. (Both in topmost left corner)\nSelect WC column with mouse. Press CTRL + ALT +DEL and at the same time click on the Square logo (Upper Left corner).\n\nAnd you will see a Game window in Full Screen and now you can race the car. You can fire to other cars with space key.\n\nHere is the screen-shot of the game window."", 'aiModelVersion': '1'}",0.9998
Tom Le,8y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","ASCII art used to be a unique skill, especially colored art, because it required knowledge of different ASCII values and some digital artistic skill.

While it is still an art form today and not many can just outright draw in ASCII art, there are programs that will take any image and convert to ASCII art, making it trivial.

Example of ASCII art:","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/9pb52uf38awjnetv', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.983, 'ai': 0.017}, 'blocks': [{'text': 'ASCII art used to be a unique skill, especially colored art, because it required knowledge of different ASCII values and some digital artistic skill.\n\nWhile it is still an art form today and not many can just outright draw in ASCII art, there are programs that will take any image and convert to ASCII art, making it trivial.\n\nExample of ASCII art:', 'result': {'fake': 0.017, 'real': 0.983}, 'status': 'success'}], 'credits_used': 1, 'credits': 1995370, 'subscription': 0, 'content': 'ASCII art used to be a unique skill, especially colored art, because it required knowledge of different ASCII values and some digital artistic skill.\n\nWhile it is still an art form today and not many can just outright draw in ASCII art, there are programs that will take any image and convert to ASCII art, making it trivial.\n\nExample of ASCII art:', 'aiModelVersion': '1'}",0.983
Stan Hanks,1y,"Which programming language has the most interesting ""backstory""?","If you run it alllll the way back, it’s C++

We’re going to start in 1953.

Grace Hopper (yeah, that Grace Hopper) was tasked by Rand with the job of making it easier for businessmen to use computers. She started off trying to teach them programming with a very mathematical approach and instantly got pushback “throw out those symbols, we don’t have time to learn those symbols”. This led to her deciding that the way forward was to use English language keywords.

Her lizard overlords at Rand were not amused, saying that would be too cumbersome and costly in terms of computer resources. Undaunted, Professor Hopper (y’all did know she was a professor of mathematics before that whole Admiral thing, right?) persisted and with her team designed and developed a prototype for a language called FLOW-MATIC by around 1955.

Most of the nascent field of computer science evolved and communicated via letters either privately circulated or published in various journals distributed monthly (how, exactly, do you think that “The Communications of the Association for Computing Machinery” got it’s name, mmm?). So her work - and the work of others (notably Laning and Zierler) were not un-noticed. So a bit further up the road, John Backus submitted a proposal to the Department of No at IBM to set out developing a very different kind of programming language (which was uncharacteristically was approved!), focused entirely on mathematical formulas, which were devilishly difficult to render in assembler without screwing up. This eventually led to the first specification of The IBM Mathematical Formula Translating System in late 1954.

And the race was on. Hopper was slogging through on how to make it possible for computers to be more friendly to business users, Backus was slogging through making computers more friendly for people with numbers to crunch, and that got other people thinking…. and wow, suddenly a lot of effort was going into thinking about programming languages

By about 1958, the ACM formed up a working group trying to bring peace to all and formulate a “universal” computer programming language, in cooperation with the German Gesellschaft für Angewandte Mathematik und Mechanik (""Society of Applied Mathematics and Mechanics""). So, in May/June 1958, a conclave was formed in Zurich at ETH to discuss forming the International Algorithmic Language or IAL.

The name? Kinda blew dead goats, so an alternative was picked — ALGOrithmic Language. That kind of stuck, and was pushed forward.

The astute will note that despite John Backus being on the team from the ACM, he continued to develop FORTRAN. Similarly, Dr. Hopper who likely wasn’t invited continued with COBOL, and now we have this third interloper — which will remain the focus of the story.

ALGOL was… mmm. ALGOL-58 had some serious deficiencies. Most of those were rectified in ALGOL-60. And ALGOL-60 was kind of the mothership from whence many other languages descended — and in the words of Tony Hoare, remained a significant improvement over many of it’s successors.

In the early 60s at Cambridge, and at University College London, there was much grumbling about ALGOL-60. Eventually, those researchers pooled their efforts into what became known as CPL - the Combined Programming Language (or Cambridge Plus London, if you were so inclined). Unlike ALGOL-60 which was stripped down to the barest of necessities and relied on elegant design to solve complex problems, CPL bulked up, adding “real solutions to real problem” found in all manner of computer usage — there were nods to FORTRAN, to COBOL, to SNOBOL… there were even hooks for industrial process control. It was, as the cool kids say, a thicc boi.

As such, it required hefty computing resources, and hefty financial support for those resources, and the more support it got, the more it grew (imagine a great algorithmic tick, attached directly to a vein of pure cash, expanding with every financial heartbeat).

In any event, by 1967, Martin Richards at Cambridge had had quite enough, and said “how about if we just strip this down, but even past ALGOL-60”… and gave us, well, a more basic language. Hey. That’s got a ring to it, a certain je ne sais quoi, right? How about…. Basic Combined Programming Language. Yeah. That’s the stuff.

And hey! You can write it in itself! And it’s super easy to port because while the concepts are portable but no claims are made about the programs. It’s all based on core machine architecture types! And it’s très spiffy for writing systems code and compilers and the like…

By 1969, there were versions for nearly two dozen completely diverse and different computer systems, and people were starting to use it to write whole operating systems. It was the most widely adopted such system for research purposes — noting the FORTRAN was the number one for anything with math and formulae, and COBOL had taken the business world by storm.

And one of those systems wound up being at Bell Labs, where Ken Thompson found that BCPL with it’s machine word orientation was ill suited to the coming wave of computer systems that were byte-addressable. Like the shiny new PDP-11 sitting in the corner of the lab that he and Dennis Ritchie were supposed to play with now that their access to the MULTICS project had been curtailed. So… he re-crafted BCPL to be… sort of BCPL-lite. Maybe a quarter of BCPL? So we can call it…. B.

B had some strong positive points — it was very like BCPL, but also brought back some of the ALGOL-60 features around data type handling. His desire was to create as he put it “BCPL semantics with a lot SMALGOL syntax”. And in that, he succeeded mightily. But it wasn’t quite right yet.

So starting around 1972, Dennis Ritchie undertook to fix the shortcomings. It was designed to be more general purpose than B - suitable for writing applications as well as systems code. And it was designed to take into account the widely divergent sizes and types of computers coming into being. And by about 1974, he was done and he had the thing that follows B — which of course was C.

C was the lingua franca of systems development and much of computer science research for a very, very long time. It’s a non-nonsense get-shit-done programming language, suitable for teaching as well as crafting very tense and performant code.

However, it was very firmly rooted in the ideas that had sprung from the late 50s, and the times they were a changing. There was this Alan Kay fellow over in California at a copier company who had this radically different idea about how programs might be constructed, and how to even think about computation, and by the late 1970s, that was catching fire in all sorts of interesting ways.

Even to the point that whacky Danish dude working on a PhD, Bjarne Stroustrup, decided that hey, you know, maybe nothing as radical as Simula… but what if we added classes and inheritance and strong typing and inlining to C?

In point of fact, that was the dynamic tension - he wanted Simula, but it was too slow; and BCPL was lightning fast but onerous to write complex programs (<cough> fucking lightweight <cough>) so he set out to create a tool that was performant, but also provided abstraction. His first cut, C with Classes, cleared the bar for his PhD research, and by 1982 he set out to really solve the problems of distributed computing as well, which lead to something new entirely. Maybe a little better than C? C, incremented? You know… C++

Initially, mind you, this was not a whole fully formed language. It was a preprocessor package for C. The first public release in 1985, Cfront 1.0, was pretty decent but you could still blow it up in a variety of entertaining ways — and figuring out where things had gone south meant you had to be expert in C as well as this new C++ thing.

By 1987, GCC 1.15.3 kinds/sorta supported C++, which was a step in the right direction, and in 1989 Cfront 2.0 dropped which was much better.

That kicked off the feeding frenzy that is an international standards gathering, and in 1990 the ANSI C++ committee was founded and then in 1991 the ISO C++ committee was founded… and the knives were drawn and backbiting commenced.

Which led eventually to C++98/03 then to C++11 then to C++14 and C++17 and C++20 and…

All of which started with “no, really, don’t make me learn new stuff”. Which led to a big language, that led to a small language that led to a big language that led to a small language that led to… Jesus, can you let me just write this in Java, m’kay?

Interesting? I think so.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/80rhnfmwab7skoly', 'title': 'Which programming language has the most interesting ""backstory""?', 'score': {'original': 0.9931, 'ai': 0.0069}, 'blocks': [{'text': 'If you run it alllll the way back, it’s C++\n\nWe’re going to start in 1953.\n\nGrace Hopper (yeah, that Grace Hopper) was tasked by Rand with the job of making it easier for businessmen to use computers. She started off trying to teach them programming with a very mathematical approach and instantly got pushback “throw out those symbols, we don’t have time to learn those symbols”. This led to her deciding that the way forward was to use English language keywords.\n\nHer lizard overlords at Rand were not amused, saying that would be too cumbersome and costly in terms of computer resources. Undaunted, Professor Hopper (y’all did know she was a professor of mathematics before that whole Admiral thing, right?) persisted and with her team designed and developed a prototype for a language called FLOW-MATIC by around 1955.\n\nMost of the nascent field of computer science evolved and communicated via letters either privately circulated or published in various journals distributed monthly (how, exactly, do you think that “The Communications of the Association for Computing Machinery” got it’s name, mmm?). So her work - and the work of others (notably Laning and Zierler) were not un-noticed. So a bit further up the road, John Backus submitted a proposal to the Department of No at IBM to set out developing a very different kind of programming language (which was uncharacteristically was approved!), focused entirely on mathematical formulas, which were devilishly difficult to render in assembler without screwing up. This eventually led to the first specification of The IBM Mathematical Formula Translating System in late 1954.\n\nAnd the race was on. Hopper was slogging through on how to make it possible for computers to be more friendly to business users, Backus was slogging through making computers more friendly for people with numbers to crunch, and that got other people thinking…. and wow, suddenly a lot of effort was going into thinking about programming languages\n\nBy about 1958, the ACM formed up a working group trying to bring peace to all and formulate a “universal” computer programming language, in cooperation with the German Gesellschaft für Angewandte Mathematik und Mechanik (""Society of Applied Mathematics and Mechanics""). So, in May/June 1958, a conclave was formed in Zurich at ETH to discuss forming the International Algorithmic Language or IAL.\n\nThe name? Kinda blew dead goats, so an alternative was picked — ALGOrithmic Language. That kind of stuck, and was pushed forward.\n\nThe astute will note that despite John Backus being on the team from the ACM, he continued to develop FORTRAN. Similarly, Dr. Hopper who likely wasn’t invited continued with COBOL, and now we have this third interloper — which will remain the focus of the story.\n\nALGOL was… mmm. ALGOL-58 had some serious deficiencies. Most of those were rectified in ALGOL-60. And ALGOL-60 was kind of the mothership from whence many other languages descended — and in the words of Tony Hoare, remained a significant improvement over many of it’s successors.\n\nIn the early 60s at Cambridge, and at University College London, there was much grumbling about ALGOL-60. Eventually, those researchers pooled their efforts into what became', 'result': {'fake': 0.0063, 'real': 0.9937}, 'status': 'success'}, {'text': 'known as CPL - the Combined Programming Language (or Cambridge Plus London, if you were so inclined). Unlike ALGOL-60 which was stripped down to the barest of necessities and relied on elegant design to solve complex problems, CPL bulked up, adding “real solutions to real problem” found in all manner of computer usage — there were nods to FORTRAN, to COBOL, to SNOBOL… there were even hooks for industrial process control. It was, as the cool kids say, a thicc boi.\n\nAs such, it required hefty computing resources, and hefty financial support for those resources, and the more support it got, the more it grew (imagine a great algorithmic tick, attached directly to a vein of pure cash, expanding with every financial heartbeat).\n\nIn any event, by 1967, Martin Richards at Cambridge had had quite enough, and said “how about if we just strip this down, but even past ALGOL-60”… and gave us, well, a more basic language. Hey. That’s got a ring to it, a certain je ne sais quoi, right? How about…. Basic Combined Programming Language. Yeah. That’s the stuff.\n\nAnd hey! You can write it in itself! And it’s super easy to port because while the concepts are portable but no claims are made about the programs. It’s all based on core machine architecture types! And it’s très spiffy for writing systems code and compilers and the like…\n\nBy 1969, there were versions for nearly two dozen completely diverse and different computer systems, and people were starting to use it to write whole operating systems. It was the most widely adopted such system for research purposes — noting the FORTRAN was the number one for anything with math and formulae, and COBOL had taken the business world by storm.\n\nAnd one of those systems wound up being at Bell Labs, where Ken Thompson found that BCPL with it’s machine word orientation was ill suited to the coming wave of computer systems that were byte-addressable. Like the shiny new PDP-11 sitting in the corner of the lab that he and Dennis Ritchie were supposed to play with now that their access to the MULTICS project had been curtailed. So… he re-crafted BCPL to be… sort of BCPL-lite. Maybe a quarter of BCPL? So we can call it…. B.\n\nB had some strong positive points — it was very like BCPL, but also brought back some of the ALGOL-60 features around data type handling. His desire was to create as he put it “BCPL semantics with a lot SMALGOL syntax”. And in that, he succeeded mightily. But it wasn’t quite right yet.\n\nSo starting around 1972, Dennis Ritchie undertook to fix the shortcomings. It was designed to be more general purpose than B - suitable for writing applications as well as systems code. And it was designed to take into account the widely divergent sizes and types of computers coming into being. And by about 1974, he was done and he had the thing that follows B — which of course was C.\n\nC was the lingua franca of systems development and much of computer science research for a', 'result': {'fake': 0.0805, 'real': 0.9195}, 'status': 'success'}, {'text': 'very, very long time. It’s a non-nonsense get-shit-done programming language, suitable for teaching as well as crafting very tense and performant code.\n\nHowever, it was very firmly rooted in the ideas that had sprung from the late 50s, and the times they were a changing. There was this Alan Kay fellow over in California at a copier company who had this radically different idea about how programs might be constructed, and how to even think about computation, and by the late 1970s, that was catching fire in all sorts of interesting ways.\n\nEven to the point that whacky Danish dude working on a PhD, Bjarne Stroustrup, decided that hey, you know, maybe nothing as radical as Simula… but what if we added classes and inheritance and strong typing and inlining to C?\n\nIn point of fact, that was the dynamic tension - he wanted Simula, but it was too slow; and BCPL was lightning fast but onerous to write complex programs (<cough> fucking lightweight <cough>) so he set out to create a tool that was performant, but also provided abstraction. His first cut, C with Classes, cleared the bar for his PhD research, and by 1982 he set out to really solve the problems of distributed computing as well, which lead to something new entirely. Maybe a little better than C? C, incremented? You know… C++\n\nInitially, mind you, this was not a whole fully formed language. It was a preprocessor package for C. The first public release in 1985, Cfront 1.0, was pretty decent but you could still blow it up in a variety of entertaining ways — and figuring out where things had gone south meant you had to be expert in C as well as this new C++ thing.\n\nBy 1987, GCC 1.15.3 kinds/sorta supported C++, which was a step in the right direction, and in 1989 Cfront 2.0 dropped which was much better.\n\nThat kicked off the feeding frenzy that is an international standards gathering, and in 1990 the ANSI C++ committee was founded and then in 1991 the ISO C++ committee was founded… and the knives were drawn and backbiting commenced.\n\nWhich led eventually to C++98/03 then to C++11 then to C++14 and C++17 and C++20 and…\n\nAll of which started with “no, really, don’t make me learn new stuff”. Which led to a big language, that led to a small language that led to a big language that led to a small language that led to… Jesus, can you let me just write this in Java, m’kay?\n\nInteresting? I think so.', 'result': {'fake': 0.2892, 'real': 0.7108}, 'status': 'success'}], 'credits_used': 15, 'credits': 1995355, 'subscription': 0, 'content': 'If you run it alllll the way back, it’s C++\n\nWe’re going to start in 1953.\n\nGrace Hopper (yeah, that Grace Hopper) was tasked by Rand with the job of making it easier for businessmen to use computers. She started off trying to teach them programming with a very mathematical approach and instantly got pushback “throw out those symbols, we don’t have time to learn those symbols”. This led to her deciding that the way forward was to use English language keywords.\n\nHer lizard overlords at Rand were not amused, saying that would be too cumbersome and costly in terms of computer resources. Undaunted, Professor Hopper (y’all did know she was a professor of mathematics before that whole Admiral thing, right?) persisted and with her team designed and developed a prototype for a language called FLOW-MATIC by around 1955.\n\nMost of the nascent field of computer science evolved and communicated via letters either privately circulated or published in various journals distributed monthly (how, exactly, do you think that “The Communications of the Association for Computing Machinery” got it’s name, mmm?). So her work - and the work of others (notably Laning and Zierler) were not un-noticed. So a bit further up the road, John Backus submitted a proposal to the Department of No at IBM to set out developing a very different kind of programming language (which was uncharacteristically was approved!), focused entirely on mathematical formulas, which were devilishly difficult to render in assembler without screwing up. This eventually led to the first specification of The IBM Mathematical Formula Translating System in late 1954.\n\nAnd the race was on. Hopper was slogging through on how to make it possible for computers to be more friendly to business users, Backus was slogging through making computers more friendly for people with numbers to crunch, and that got other people thinking…. and wow, suddenly a lot of effort was going into thinking about programming languages\n\nBy about 1958, the ACM formed up a working group trying to bring peace to all and formulate a “universal” computer programming language, in cooperation with the German Gesellschaft für Angewandte Mathematik und Mechanik (""Society of Applied Mathematics and Mechanics""). So, in May/June 1958, a conclave was formed in Zurich at ETH to discuss forming the International Algorithmic Language or IAL.\n\nThe name? Kinda blew dead goats, so an alternative was picked — ALGOrithmic Language. That kind of stuck, and was pushed forward.\n\nThe astute will note that despite John Backus being on the team from the ACM, he continued to develop FORTRAN. Similarly, Dr. Hopper who likely wasn’t invited continued with COBOL, and now we have this third interloper — which will remain the focus of the story.\n\nALGOL was… mmm. ALGOL-58 had some serious deficiencies. Most of those were rectified in ALGOL-60. And ALGOL-60 was kind of the mothership from whence many other languages descended — and in the words of Tony Hoare, remained a significant improvement over many of it’s successors.\n\nIn the early 60s at Cambridge, and at University College London, there was much grumbling about ALGOL-60. Eventually, those researchers pooled their efforts into what became known as CPL - the Combined Programming Language (or Cambridge Plus London, if you were so inclined). Unlike ALGOL-60 which was stripped down to the barest of necessities and relied on elegant design to solve complex problems, CPL bulked up, adding “real solutions to real problem” found in all manner of computer usage — there were nods to FORTRAN, to COBOL, to SNOBOL… there were even hooks for industrial process control. It was, as the cool kids say, a thicc boi.\n\nAs such, it required hefty computing resources, and hefty financial support for those resources, and the more support it got, the more it grew (imagine a great algorithmic tick, attached directly to a vein of pure cash, expanding with every financial heartbeat).\n\nIn any event, by 1967, Martin Richards at Cambridge had had quite enough, and said “how about if we just strip this down, but even past ALGOL-60”… and gave us, well, a more basic language. Hey. That’s got a ring to it, a certain je ne sais quoi, right? How about…. Basic Combined Programming Language. Yeah. That’s the stuff.\n\nAnd hey! You can write it in itself! And it’s super easy to port because while the concepts are portable but no claims are made about the programs. It’s all based on core machine architecture types! And it’s très spiffy for writing systems code and compilers and the like…\n\nBy 1969, there were versions for nearly two dozen completely diverse and different computer systems, and people were starting to use it to write whole operating systems. It was the most widely adopted such system for research purposes — noting the FORTRAN was the number one for anything with math and formulae, and COBOL had taken the business world by storm.\n\nAnd one of those systems wound up being at Bell Labs, where Ken Thompson found that BCPL with it’s machine word orientation was ill suited to the coming wave of computer systems that were byte-addressable. Like the shiny new PDP-11 sitting in the corner of the lab that he and Dennis Ritchie were supposed to play with now that their access to the MULTICS project had been curtailed. So… he re-crafted BCPL to be… sort of BCPL-lite. Maybe a quarter of BCPL? So we can call it…. B.\n\nB had some strong positive points — it was very like BCPL, but also brought back some of the ALGOL-60 features around data type handling. His desire was to create as he put it “BCPL semantics with a lot SMALGOL syntax”. And in that, he succeeded mightily. But it wasn’t quite right yet.\n\nSo starting around 1972, Dennis Ritchie undertook to fix the shortcomings. It was designed to be more general purpose than B - suitable for writing applications as well as systems code. And it was designed to take into account the widely divergent sizes and types of computers coming into being. And by about 1974, he was done and he had the thing that follows B — which of course was C.\n\nC was the lingua franca of systems development and much of computer science research for a very, very long time. It’s a non-nonsense get-shit-done programming language, suitable for teaching as well as crafting very tense and performant code.\n\nHowever, it was very firmly rooted in the ideas that had sprung from the late 50s, and the times they were a changing. There was this Alan Kay fellow over in California at a copier company who had this radically different idea about how programs might be constructed, and how to even think about computation, and by the late 1970s, that was catching fire in all sorts of interesting ways.\n\nEven to the point that whacky Danish dude working on a PhD, Bjarne Stroustrup, decided that hey, you know, maybe nothing as radical as Simula… but what if we added classes and inheritance and strong typing and inlining to C?\n\nIn point of fact, that was the dynamic tension - he wanted Simula, but it was too slow; and BCPL was lightning fast but onerous to write complex programs (<cough> fucking lightweight <cough>) so he set out to create a tool that was performant, but also provided abstraction. His first cut, C with Classes, cleared the bar for his PhD research, and by 1982 he set out to really solve the problems of distributed computing as well, which lead to something new entirely. Maybe a little better than C? C, incremented? You know… C++\n\nInitially, mind you, this was not a whole fully formed language. It was a preprocessor package for C. The first public release in 1985, Cfront 1.0, was pretty decent but you could still blow it up in a variety of entertaining ways — and figuring out where things had gone south meant you had to be expert in C as well as this new C++ thing.\n\nBy 1987, GCC 1.15.3 kinds/sorta supported C++, which was a step in the right direction, and in 1989 Cfront 2.0 dropped which was much better.\n\nThat kicked off the feeding frenzy that is an international standards gathering, and in 1990 the ANSI C++ committee was founded and then in 1991 the ISO C++ committee was founded… and the knives were drawn and backbiting commenced.\n\nWhich led eventually to C++98/03 then to C++11 then to C++14 and C++17 and C++20 and…\n\nAll of which started with “no, really, don’t make me learn new stuff”. Which led to a big language, that led to a small language that led to a big language that led to a small language that led to… Jesus, can you let me just write this in Java, m’kay?\n\nInteresting? I think so.', 'aiModelVersion': '1'}",0.9931
Brandon Ross,Updated 3y,Why were old games programmed in assembly when higher level languages existed?,"Yesterday, a programmer friend from college called me up. He was working on a vehicle controller for a watercraft. They had to calibrate this controller with some test data. They had only 8KB of memory. And all the “calibration data” was taking up a whopping 2 KB of memory. Their “young” programmer was worried they were going to run out of memory for the actual control program. And he wasn’t wrong.

Now, I’m considered an “old” guy. Old enough to remember how to actually cram a lot of code into a very small space. How to write that program instruction by instruction. So, I took their 2KB data and—15 minutes later—wrote a quick routine that just explicitly calculated the tabular data on the fly (yes, good reasons and bad reasons to do this; I'm aware). And it all fit into <200 bytes. So, that’s about 90% space savings. Which is pretty good.

Again, only 8 KB to work with. Most programmers today have no experience with programming with so little memory to work with. So it’s hard to imagine how little space that is. Need a visual? Here:

Here’s a classic image. It’s just over 1 KB in size. So, imagine all the memory space you have—program and data—is slightly smaller, byte-wise, than 8 of these images.

Doesn’t seem like very much, does it? Because it’s not much space at all.

Now, here is the Nintendo Entertainment System. One of the most successful video game platforms—probably the one that really solidified video games as a viable market in the United States:

It has 2 KB of “working” memory.

Meaning, you need to fit your data and any extra program code into less than this space:

They used rather ingenious bitmapping, sprite tables, and color pallete rotations to simulate a lot of visual display functionality. Here’s a dump from a sprite table from Nintendo’s Super Mario Brothers.

You can see that a lot of the graphic elements are really crammed together. And then, the functionality was tightly coded to reference elements in that table to simulate motion. Remember when an old NES game would crash and you’d get random “parts” of the game smattered everywhere? Yep. Some offset somewhere is mangled, and now every “object” being displayed is referencing some place in tile memory where the object isn’t.

Could you have done all this in high-level languages? Sure. Theoretically. But theory isn’t practice. At this stage of game development, bytes mattered. In short, there is no space to waste on these old machines. Every instruction was hand-coded and tweaked so it could fit on the machine.

Update: This got way more upvotes than I expected. Not sure why! Thanks for all your positive comments.

For everyone who’s been asking me more about how the NES works, here’s a great little teaser video. Which really captures the “neatness” of the NES.

If you’re a programmer or aspiring programmer, I strongly urge you to learn more about about your craft. Maybe this video will suggest a few “neat” things to you—especially on design (“Wow, a controller fits all its input in 1 byte?! How convenient!”).

And here’s a great video that shows some of the design thoughts on space optimization:","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/tz5c9qy1o4mvs6kp', 'title': 'Why were old games programmed in assembly when higher level languages existed?', 'score': {'original': 0.49145, 'ai': 0.50855}, 'blocks': [{'text': ""Yesterday, a programmer friend from college called me up. He was working on a vehicle controller for a watercraft. They had to calibrate this controller with some test data. They had only 8KB of memory. And all the “calibration data” was taking up a whopping 2 KB of memory. Their “young” programmer was worried they were going to run out of memory for the actual control program. And he wasn’t wrong.\n\nNow, I’m considered an “old” guy. Old enough to remember how to actually cram a lot of code into a very small space. How to write that program instruction by instruction. So, I took their 2KB data and—15 minutes later—wrote a quick routine that just explicitly calculated the tabular data on the fly (yes, good reasons and bad reasons to do this; I'm aware). And it all fit into <200 bytes. So, that’s about 90% space savings. Which is pretty good.\n\nAgain, only 8 KB to work with. Most programmers today have no experience with programming with so little memory to work with. So it’s hard to imagine how little space that is. Need a visual? Here:\n\nHere’s a classic image. It’s just over 1 KB in size. So, imagine all the memory space you have—program and data—is slightly smaller, byte-wise, than 8 of these images.\n\nDoesn’t seem like very much, does it? Because it’s not much space at all.\n\nNow, here is the Nintendo Entertainment System. One of the most successful video game platforms—probably the one that really solidified video games as a viable market in the United States:\n\nIt has 2 KB of “working” memory.\n\nMeaning, you need to fit your data and any extra program code into less than this space:\n\nThey used rather ingenious bitmapping, sprite tables, and color pallete rotations to simulate a lot of visual display functionality. Here’s a dump from a sprite table from Nintendo’s Super Mario Brothers.\n\nYou can see that a lot of the graphic elements are really crammed together. And then, the functionality was tightly coded to reference elements in that table to simulate motion. Remember when an old NES game would crash and you’d get random “parts” of the game smattered everywhere? Yep. Some offset somewhere is mangled, and now every “object” being displayed is referencing some place in tile memory where the object isn’t.\n\nCould you have done all this in high-level languages? Sure. Theoretically. But theory isn’t practice. At this stage of game development, bytes mattered. In short, there is no space to waste on these old machines. Every instruction was hand-coded and tweaked so it could fit on the machine.\n\nUpdate: This got way more upvotes than I expected. Not sure why! Thanks for all your positive comments.\n\nFor everyone who’s been asking me more about how the NES works, here’s a great little teaser video. Which really captures the “neatness” of the NES.\n\nIf you’re a programmer or aspiring programmer, I strongly urge you to learn more about about your craft. Maybe this video will suggest a few “neat” things to you—especially on design (“Wow, a controller fits all its input in 1 byte?! How convenient!”).\n\nAnd here’s a great"", 'result': {'fake': 0.0143, 'real': 0.9857}, 'status': 'success'}, {'text': 'video that shows some of the design thoughts on space optimization:', 'result': {'fake': 0.9745, 'real': 0.0255}, 'status': 'success'}], 'credits_used': 6, 'credits': 1995349, 'subscription': 0, 'content': ""Yesterday, a programmer friend from college called me up. He was working on a vehicle controller for a watercraft. They had to calibrate this controller with some test data. They had only 8KB of memory. And all the “calibration data” was taking up a whopping 2 KB of memory. Their “young” programmer was worried they were going to run out of memory for the actual control program. And he wasn’t wrong.\n\nNow, I’m considered an “old” guy. Old enough to remember how to actually cram a lot of code into a very small space. How to write that program instruction by instruction. So, I took their 2KB data and—15 minutes later—wrote a quick routine that just explicitly calculated the tabular data on the fly (yes, good reasons and bad reasons to do this; I'm aware). And it all fit into <200 bytes. So, that’s about 90% space savings. Which is pretty good.\n\nAgain, only 8 KB to work with. Most programmers today have no experience with programming with so little memory to work with. So it’s hard to imagine how little space that is. Need a visual? Here:\n\nHere’s a classic image. It’s just over 1 KB in size. So, imagine all the memory space you have—program and data—is slightly smaller, byte-wise, than 8 of these images.\n\nDoesn’t seem like very much, does it? Because it’s not much space at all.\n\nNow, here is the Nintendo Entertainment System. One of the most successful video game platforms—probably the one that really solidified video games as a viable market in the United States:\n\nIt has 2 KB of “working” memory.\n\nMeaning, you need to fit your data and any extra program code into less than this space:\n\nThey used rather ingenious bitmapping, sprite tables, and color pallete rotations to simulate a lot of visual display functionality. Here’s a dump from a sprite table from Nintendo’s Super Mario Brothers.\n\nYou can see that a lot of the graphic elements are really crammed together. And then, the functionality was tightly coded to reference elements in that table to simulate motion. Remember when an old NES game would crash and you’d get random “parts” of the game smattered everywhere? Yep. Some offset somewhere is mangled, and now every “object” being displayed is referencing some place in tile memory where the object isn’t.\n\nCould you have done all this in high-level languages? Sure. Theoretically. But theory isn’t practice. At this stage of game development, bytes mattered. In short, there is no space to waste on these old machines. Every instruction was hand-coded and tweaked so it could fit on the machine.\n\nUpdate: This got way more upvotes than I expected. Not sure why! Thanks for all your positive comments.\n\nFor everyone who’s been asking me more about how the NES works, here’s a great little teaser video. Which really captures the “neatness” of the NES.\n\nIf you’re a programmer or aspiring programmer, I strongly urge you to learn more about about your craft. Maybe this video will suggest a few “neat” things to you—especially on design (“Wow, a controller fits all its input in 1 byte?! How convenient!”).\n\nAnd here’s a great video that shows some of the design thoughts on space optimization:"", 'aiModelVersion': '1'}",0.49145
Lee Felsenstein,Updated 4y,The laptop was invented by Adam Osborne in 1981. Why is Adam Osborne not as well known as Steve Jobs and/or Steve Wozniak?,"I designed the Osborne-1, and was a founder of the company (part of my compensation for the design). It was intended not so much as a portable computer to be used “on the go” but as a self-contained CP/M computer which could hold its software disk library internally and make minimal demands on its owner for configuration and customization.

The weight was not 55 or 60 pounds - it was 23.5 pounds, and it created the class of computers that become known as “luggables”. Someone once commented that most were only carried from one side of the office to the other. We never considered it a “laptop”, though we were in discussions with multiple Japanese companies for years working toward a laptop.

I was aware of Alan Kay’s “Dynabook” conceptual design at the time (1980) and attempted to advocate making that our design goal, but adequate flat-panel displays were not then available (just look at the small display on the Epson HX-20) and the cost of memory ICs did not then permit a graphic display.

I describe the Osborne-1 as “the first commercially successful portable computer” - others had been sold before with handles on them and IBM called their 1975 5100 a “Portable computer” (with cartridge tape storage and a price tag in the several thousands of dollars) but it did not provide an interoperable operating system).

Osborne Computer Corporation announced their first product in March 1981 and entered chapter 11 bankruptcy in September of 1983. If the company had been managed properly it might still be in business today. Innovators are not the ones in control of that process - the managers, like Mike Markkula of Apple - deserve the credit for the survivability of the company. Osborne did not bring in sufficiently skilled management and did not foster an internal culture that could grow such managers.

About 150,000 Osborne-1 computers were shipped.

(19 hours after posting) I’m quite gratified by the response of people who want to thank me for helping their careers advance due to the availability of the Osborne-1.

(8 days after posting) Wow! The fundraising goal has been met, and exceeded! I have closed the GoFundMe account and will get the money transferred to the training school as I promised.

(July 24, 2019) Just an update - the young man in Gambia is now enrolled in technical school to learn networking and there is enough money to set aside for his enrollment later in the Cisco networking course. In addition, due to a large donation just at the end of the fundraising, there is enough money to provide him a significant stipend to replace income (from car-washing) lost during class days. I will be receiving reports from the school concerning his progress and attendance. Many, many thanks to those who contributed - I am your trustee in the disposition of these funds and will ensure that they are spent well.

I will be continue to move into designing “learning tools” for students 11 years and older (see <www.AndOrBit.com> for more details).

And some very heartfelt thanks to those who gave!

Lee Felsenstein","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6qjdtiu9xm3h7yrf', 'title': 'The laptop was invented by Adam Osborne in 1981. Why is Adam Osborne not as well known as Steve Jobs and/or Steve Wozniak?', 'score': {'original': 0.4997, 'ai': 0.5003}, 'blocks': [{'text': 'I designed the Osborne-1, and was a founder of the company (part of my compensation for the design). It was intended not so much as a portable computer to be used “on the go” but as a self-contained CP/M computer which could hold its software disk library internally and make minimal demands on its owner for configuration and customization.\n\nThe weight was not 55 or 60 pounds - it was 23.5 pounds, and it created the class of computers that become known as “luggables”. Someone once commented that most were only carried from one side of the office to the other. We never considered it a “laptop”, though we were in discussions with multiple Japanese companies for years working toward a laptop.\n\nI was aware of Alan Kay’s “Dynabook” conceptual design at the time (1980) and attempted to advocate making that our design goal, but adequate flat-panel displays were not then available (just look at the small display on the Epson HX-20) and the cost of memory ICs did not then permit a graphic display.\n\nI describe the Osborne-1 as “the first commercially successful portable computer” - others had been sold before with handles on them and IBM called their 1975 5100 a “Portable computer” (with cartridge tape storage and a price tag in the several thousands of dollars) but it did not provide an interoperable operating system).\n\nOsborne Computer Corporation announced their first product in March 1981 and entered chapter 11 bankruptcy in September of 1983. If the company had been managed properly it might still be in business today. Innovators are not the ones in control of that process - the managers, like Mike Markkula of Apple - deserve the credit for the survivability of the company. Osborne did not bring in sufficiently skilled management and did not foster an internal culture that could grow such managers.\n\nAbout 150,000 Osborne-1 computers were shipped.\n\n(19 hours after posting) I’m quite gratified by the response of people who want to thank me for helping their careers advance due to the availability of the Osborne-1.\n\n(8 days after posting) Wow! The fundraising goal has been met, and exceeded! I have closed the GoFundMe account and will get the money transferred to the training school as I promised.\n\n(July 24, 2019) Just an update - the young man in Gambia is now enrolled in technical school to learn networking and there is enough money to set aside for his enrollment later in the Cisco networking course. In addition, due to a large donation just at the end of the fundraising, there is enough money to provide him a significant stipend to replace income (from car-washing) lost during class days. I will be receiving reports from the school concerning his progress and attendance. Many, many thanks to those who contributed - I am your trustee in the disposition of these funds and will ensure that they are spent well.\n\nI will be continue to move into designing “learning tools” for students 11 years and older (see <www.AndOrBit.com> for more details).\n\nAnd some very heartfelt thanks to those who gave!\n\nLee Felsenstein', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 6, 'credits': 1995343, 'subscription': 0, 'content': 'I designed the Osborne-1, and was a founder of the company (part of my compensation for the design). It was intended not so much as a portable computer to be used “on the go” but as a self-contained CP/M computer which could hold its software disk library internally and make minimal demands on its owner for configuration and customization.\n\nThe weight was not 55 or 60 pounds - it was 23.5 pounds, and it created the class of computers that become known as “luggables”. Someone once commented that most were only carried from one side of the office to the other. We never considered it a “laptop”, though we were in discussions with multiple Japanese companies for years working toward a laptop.\n\nI was aware of Alan Kay’s “Dynabook” conceptual design at the time (1980) and attempted to advocate making that our design goal, but adequate flat-panel displays were not then available (just look at the small display on the Epson HX-20) and the cost of memory ICs did not then permit a graphic display.\n\nI describe the Osborne-1 as “the first commercially successful portable computer” - others had been sold before with handles on them and IBM called their 1975 5100 a “Portable computer” (with cartridge tape storage and a price tag in the several thousands of dollars) but it did not provide an interoperable operating system).\n\nOsborne Computer Corporation announced their first product in March 1981 and entered chapter 11 bankruptcy in September of 1983. If the company had been managed properly it might still be in business today. Innovators are not the ones in control of that process - the managers, like Mike Markkula of Apple - deserve the credit for the survivability of the company. Osborne did not bring in sufficiently skilled management and did not foster an internal culture that could grow such managers.\n\nAbout 150,000 Osborne-1 computers were shipped.\n\n(19 hours after posting) I’m quite gratified by the response of people who want to thank me for helping their careers advance due to the availability of the Osborne-1.\n\n(8 days after posting) Wow! The fundraising goal has been met, and exceeded! I have closed the GoFundMe account and will get the money transferred to the training school as I promised.\n\n(July 24, 2019) Just an update - the young man in Gambia is now enrolled in technical school to learn networking and there is enough money to set aside for his enrollment later in the Cisco networking course. In addition, due to a large donation just at the end of the fundraising, there is enough money to provide him a significant stipend to replace income (from car-washing) lost during class days. I will be receiving reports from the school concerning his progress and attendance. Many, many thanks to those who contributed - I am your trustee in the disposition of these funds and will ensure that they are spent well.\n\nI will be continue to move into designing “learning tools” for students 11 years and older (see <www.AndOrBit.com> for more details).\n\nAnd some very heartfelt thanks to those who gave!\n\nLee Felsenstein', 'aiModelVersion': '1'}",0.4997
William Beaty,Updated 1y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?",".

Hey Bill Gates, try topping THIS story! [1]

Back in the 70s, I accidentally figured out how to hack a DEC PDP-8/e minicomputer using only the very exotic B.A.S.I.C language (stored on papertape, no IBM punch-cards needed!) And then, accidentally transmitted my hack to a rival high-school, where I hear it created much consternation among faculty for an entire week!

1974, Southside HS. Elmira, NY, somehow our math teacher got hold of a PDP-8/e, and started a class to teach programming in DEC Basic. At first we had to enter the bootloader via the front panel switches, then run in the half-hour of 4K papertape through the teletype reader, into the computer’s six entire k-words of memory, doing this each time the system had been turned off. Finally someone figured out that USUALLY the Basic code could be re-started (from a secret start-addr,) rather than relying on the startup stuff at the end of the papertape (an important part of this story.)

The PDP-8/e had core memory. Several boads, 2K words each, iirc. If you knew how to gracefully crash the basic code, it usually could be re-started, even after power had been removed. (Plus, if you took the full bin of paper tape punch-holes off the side of the teletype, you could empty it out the window, and make it snow! )

Good times.

I discovered that our copy of Basic had some unknown string-array commands. LINPUT A$, inputting an entire line of text. These weren’t supposed to exist! The commands were found in the back of a manual for a more expensive Basic version. Mostly they didn’t work, but “LINPUT” was there. So was string-compare.

So, I wrote a tiny program to generate wise-ass responses to system commands, and which, when running, just printed READY(crlf), as the PDP normally did when halted. Heh, by fortunate chance, the LINPUT command didn’t print any question-mark while waiting for input like it was supposed to! So, nobody would realize that my program was running. A second lucky break.

Now, if you sat down at the teletype console, and typed in an entire Basic program, all your typing would be wasted (my program couldn’t store strings,) and then if you typed “LIST,” my program would say “Don’t bother me I’m busy.” Type “RUN” and it said “I can’t, I ain’t got no legs.” Plus a few others, one for each valid command.

Easily defeated by anybody, just type CTRL-C. And then swear a lot, depending on how many lines of code you’d just carefully typed into nowhere.

My program was only effective/hilarious for about a week, until everyone got into the habit of always hitting CTRL-C, to make sure that no joker who’d been using the system had left that f****g thing waiting for you.

Months later, by fortunate chance I happened to be using the computer on the day before summer vacation. I was told to perform the last halt-system procedure before summer vacation, when I suddenly had an idea. I said “Just a sec.” I input the short papertape for my program, ran it, THEN halted the PDP-8 and turned off the power …while a Basic program was still running. (I never tried this before. Didn’t know if it would work.)

[Summer vacation happens]

Months later, on the first day of school, the math teacher tracked me down, and said “What was that thing you did just before we turned off the PDP-8?”

Uh oh.

Actually he started laughing, and told the entire story.

Ours was the “football school,” while our rivals on the other side of the river were the “smart school” with all the science courses. Somehow they’d wangled the PDP-8 away from us during the summer. One of their teachers was going to present a teacher-only programming course, so they could all learn Basic language. Our own math teacher was NOT invited. By fortunate chance, the guy taking our computer had been carefully instructed in how to restart the PDP-8, so no bootloader and no thirty minutes of tapereader would be needed. He did this successfully.

The PDP-8 said READY.

Then he typed in his first “hello” program, and hit RUN.

It said …I CANT I AINT GOT NO LEGS.

Our math teacher swears that it took the other school an entire week to figure it out, and their summer course had to be delayed. The person using the computer had learned everything from manuals alone, and (by sheer bad luck) had somehow skipped the paragraph about breaking out of a running program. He didn’t know CTRL-C. Nobody else could solve the problem either, because apparently everyone else assumed he of course knew CTRL-C, and had already tried that, first thing …which he hadn’t.

So yes, if you have a PDP-8/e running DEC Basic, and a Basic prgram is currently running, then if you halt the system, remove power for weeks, then restart, not only is Basic still in memory, but any Basic programs are still in there, and still running!

Oldschool hacking. OLD-oldschool hacking, the master of the syncronicites. This “hack” required about eight unlikely amazing bits of luck and coincidences to first line up exactly. And, my “virus” was “transmitted” to our rival HS, by them coming in and physically taking our corememory-based PDP-8 computer away from us, so it couldn’t be used by Southside HS during the summer. Hacks via social engineering, performed by accident.

Alchemist synchronicities? A Toltec accomplishment? Me and Richard Feynman, we both have the Red Dwarf Luck Virus. (So why aren’t we millionaires?)

Other cool tricks, now somewhat lame…

WRITING IN ASSEMBLER. In 1979 I had Conway’s Life going fast, on Altair-era Polymorphics 88 graphics. Well not that fast, on 8080 at 1MHz cycle speed, where I think a glider-gun was updating about six times per second. Also a voice-recorder, a Morse typer, even a Morse code listener (microphone to text, our 8080 home computer could understand human speech in 1979, as long as the human sounded like DAH DI DAH DI. DAH DAH DI DAH)

Speaking of which, years later in 1982 I had another wonderful idea, but talked myself out of it: today called “computer viruses.”

Or more specifically, why shouldn’t I write the very first personal-computer virus, then keep it secret, …so I can sell it as computer-game security software, self-installing?

Infectious “Sony rootkits” of 1982.

I bought a used Apple from a guy at work. On the APPLE-][-plus, I realized that if you have a custom-modified master-disk, which when loaded, can sit waiting, and then modify any other master-disk placed in the drive, then soon all Apple disks now have your code. It spreads worldwide, kinda-sorta like a “disease,” but this disease …is for home computers! Perhaps give away free Apple games to various high schools, to “infect” entire distant cities!

And if your silent, unsuspected code is always watching for illegal copies of a certain video game, it can take action. It could even be *watching* for people to use their Apple-II home computers to make the illegal copies. Don’t interfere with this (that would expose its existence!) It could take vengeful action later, when the perps least suspect it. My “disease” can carry any sorts of horrific “symptoms” that I desire; all sorts of novel exotic punishments I was dreaming up. Back then in 1982, this was an idea to make millions, and the world would never be the same! (Heh.) Also, a simple way to spread unwanted software updates to every single Apple-II on the planet. (Today, if you could travel back in time to the early 1980s, which modern idea would make you a world-famous billionaire? Viral rootkits! Heh, maybe not. Instead try spreadsheets.)

I was just at the stage of doing 8080 dis-assembler on the Apple-][ OS when, I realized I didn’t want to become the famous person who invented the “home computer influenza.” Every computer-owner would be spitting on my grave (after they’d killed me slowly, hunted down by a crowd of Apple users equipped with tiny needlenose pilers. And soldering irons! They’ll kill me with cold, dirty soldering irons. With dull tips.)

Eventually that title went to a high-school kid in ?1986?, who did exactly the same thing with the OS on his Apple-II+. But he let the thing get loose and start spreading through his school, rather than keeping it quiet like I’d been planning to do, nor selling the whole “virus-spread rootkit” concept to all the major game-companies of 1983.

The “IBM PC” soon appeared, everyone started writing viruses for it, and the rest is history.

So, I got lucky and took subconscious warnings seriously, and really dodged a bullet there! More like, dodged thousands of bullets …and also the tips of those cold, unsharpened soldering irons.

[1] Are there any other “hacker” stories involving the PDP-8? I’ve yet to encounter one. I did hear that the Rochester NY planetarium back in the 1970s was run by a PDP-8. I wonder if they got up to any shenannigans with it? At least try some strobe-flashing audience mind-control, in your PLAN-NE -ARIUM.

See also:

How can I become a pro at electronics, so I can assemble any circuit I want?
What advice can you give to young electronics enthusiasts who don't have much money?

ALSO more answers (nine hundred. Mostly electricity) QUORA ANSWERS: W Beaty

.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/gth3cq6v8p5nu0se', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.725825, 'ai': 0.274175}, 'blocks': [{'text': '.\n\nHey Bill Gates, try topping THIS story! [1]\n\nBack in the 70s, I accidentally figured out how to hack a DEC PDP-8/e minicomputer using only the very exotic B.A.S.I.C language (stored on papertape, no IBM punch-cards needed!) And then, accidentally transmitted my hack to a rival high-school, where I hear it created much consternation among faculty for an entire week!\n\n1974, Southside HS. Elmira, NY, somehow our math teacher got hold of a PDP-8/e, and started a class to teach programming in DEC Basic. At first we had to enter the bootloader via the front panel switches, then run in the half-hour of 4K papertape through the teletype reader, into the computer’s six entire k-words of memory, doing this each time the system had been turned off. Finally someone figured out that USUALLY the Basic code could be re-started (from a secret start-addr,) rather than relying on the startup stuff at the end of the papertape (an important part of this story.)\n\nThe PDP-8/e had core memory. Several boads, 2K words each, iirc. If you knew how to gracefully crash the basic code, it usually could be re-started, even after power had been removed. (Plus, if you took the full bin of paper tape punch-holes off the side of the teletype, you could empty it out the window, and make it snow! )\n\nGood times.\n\nI discovered that our copy of Basic had some unknown string-array commands. LINPUT A$, inputting an entire line of text. These weren’t supposed to exist! The commands were found in the back of a manual for a more expensive Basic version. Mostly they didn’t work, but “LINPUT” was there. So was string-compare.\n\nSo, I wrote a tiny program to generate wise-ass responses to system commands, and which, when running, just printed READY(crlf), as the PDP normally did when halted. Heh, by fortunate chance, the LINPUT command didn’t print any question-mark while waiting for input like it was supposed to! So, nobody would realize that my program was running. A second lucky break.\n\nNow, if you sat down at the teletype console, and typed in an entire Basic program, all your typing would be wasted (my program couldn’t store strings,) and then if you typed “LIST,” my program would say “Don’t bother me I’m busy.” Type “RUN” and it said “I can’t, I ain’t got no legs.” Plus a few others, one for each valid command.\n\nEasily defeated by anybody, just type CTRL-C. And then swear a lot, depending on how many lines of code you’d just carefully typed into nowhere.\n\nMy program was only effective/hilarious for about a week, until everyone got into the habit of always hitting CTRL-C, to make sure that no joker who’d been using the system had left that f****g thing waiting for you.\n\nMonths later, by fortunate chance I happened to be using the computer on the day before summer vacation. I was told to perform the last halt-system procedure before summer vacation, when I suddenly had an idea. I said “Just a sec.” I input the short papertape for my program, ran it, THEN halted the PDP-8 and turned off the', 'result': {'fake': 0.0543, 'real': 0.9457}, 'status': 'success'}, {'text': 'power …while a Basic program was still running. (I never tried this before. Didn’t know if it would work.)\n\n[Summer vacation happens]\n\nMonths later, on the first day of school, the math teacher tracked me down, and said “What was that thing you did just before we turned off the PDP-8?”\n\nUh oh.\n\nActually he started laughing, and told the entire story.\n\nOurs was the “football school,” while our rivals on the other side of the river were the “smart school” with all the science courses. Somehow they’d wangled the PDP-8 away from us during the summer. One of their teachers was going to present a teacher-only programming course, so they could all learn Basic language. Our own math teacher was NOT invited. By fortunate chance, the guy taking our computer had been carefully instructed in how to restart the PDP-8, so no bootloader and no thirty minutes of tapereader would be needed. He did this successfully.\n\nThe PDP-8 said READY.\n\nThen he typed in his first “hello” program, and hit RUN.\n\nIt said …I CANT I AINT GOT NO LEGS.\n\nOur math teacher swears that it took the other school an entire week to figure it out, and their summer course had to be delayed. The person using the computer had learned everything from manuals alone, and (by sheer bad luck) had somehow skipped the paragraph about breaking out of a running program. He didn’t know CTRL-C. Nobody else could solve the problem either, because apparently everyone else assumed he of course knew CTRL-C, and had already tried that, first thing …which he hadn’t.\n\nSo yes, if you have a PDP-8/e running DEC Basic, and a Basic prgram is currently running, then if you halt the system, remove power for weeks, then restart, not only is Basic still in memory, but any Basic programs are still in there, and still running!\n\nOldschool hacking. OLD-oldschool hacking, the master of the syncronicites. This “hack” required about eight unlikely amazing bits of luck and coincidences to first line up exactly. And, my “virus” was “transmitted” to our rival HS, by them coming in and physically taking our corememory-based PDP-8 computer away from us, so it couldn’t be used by Southside HS during the summer. Hacks via social engineering, performed by accident.\n\nAlchemist synchronicities? A Toltec accomplishment? Me and Richard Feynman, we both have the Red Dwarf Luck Virus. (So why aren’t we millionaires?)\n\nOther cool tricks, now somewhat lame…\n\nWRITING IN ASSEMBLER. In 1979 I had Conway’s Life going fast, on Altair-era Polymorphics 88 graphics. Well not that fast, on 8080 at 1MHz cycle speed, where I think a glider-gun was updating about six times per second. Also a voice-recorder, a Morse typer, even a Morse code listener (microphone to text, our 8080 home computer could understand human speech in 1979, as long as the human sounded like DAH DI DAH DI. DAH DAH DI DAH)\n\nSpeaking of which, years later in 1982 I had another wonderful idea, but talked myself out of it: today called “computer viruses.”\n\nOr more specifically, why shouldn’t I write the very first personal-computer virus, then keep it secret, …so I can sell it as', 'result': {'fake': 0.0102, 'real': 0.9898}, 'status': 'success'}, {'text': ""computer-game security software, self-installing?\n\nInfectious “Sony rootkits” of 1982.\n\nI bought a used Apple from a guy at work. On the APPLE-][-plus, I realized that if you have a custom-modified master-disk, which when loaded, can sit waiting, and then modify any other master-disk placed in the drive, then soon all Apple disks now have your code. It spreads worldwide, kinda-sorta like a “disease,” but this disease …is for home computers! Perhaps give away free Apple games to various high schools, to “infect” entire distant cities!\n\nAnd if your silent, unsuspected code is always watching for illegal copies of a certain video game, it can take action. It could even be *watching* for people to use their Apple-II home computers to make the illegal copies. Don’t interfere with this (that would expose its existence!) It could take vengeful action later, when the perps least suspect it. My “disease” can carry any sorts of horrific “symptoms” that I desire; all sorts of novel exotic punishments I was dreaming up. Back then in 1982, this was an idea to make millions, and the world would never be the same! (Heh.) Also, a simple way to spread unwanted software updates to every single Apple-II on the planet. (Today, if you could travel back in time to the early 1980s, which modern idea would make you a world-famous billionaire? Viral rootkits! Heh, maybe not. Instead try spreadsheets.)\n\nI was just at the stage of doing 8080 dis-assembler on the Apple-][ OS when, I realized I didn’t want to become the famous person who invented the “home computer influenza.” Every computer-owner would be spitting on my grave (after they’d killed me slowly, hunted down by a crowd of Apple users equipped with tiny needlenose pilers. And soldering irons! They’ll kill me with cold, dirty soldering irons. With dull tips.)\n\nEventually that title went to a high-school kid in ?1986?, who did exactly the same thing with the OS on his Apple-II+. But he let the thing get loose and start spreading through his school, rather than keeping it quiet like I’d been planning to do, nor selling the whole “virus-spread rootkit” concept to all the major game-companies of 1983.\n\nThe “IBM PC” soon appeared, everyone started writing viruses for it, and the rest is history.\n\nSo, I got lucky and took subconscious warnings seriously, and really dodged a bullet there! More like, dodged thousands of bullets …and also the tips of those cold, unsharpened soldering irons.\n\n[1] Are there any other “hacker” stories involving the PDP-8? I’ve yet to encounter one. I did hear that the Rochester NY planetarium back in the 1970s was run by a PDP-8. I wonder if they got up to any shenannigans with it? At least try some strobe-flashing audience mind-control, in your PLAN-NE -ARIUM.\n\nSee also:\n\nHow can I become a pro at electronics, so I can assemble any circuit I want?\nWhat advice can you give to young electronics enthusiasts who don't have much money?\n\nALSO more answers (nine hundred. Mostly electricity) QUORA ANSWERS: W Beaty\n\n."", 'result': {'fake': 0.0136, 'real': 0.9864}, 'status': 'success'}], 'credits_used': 16, 'credits': 1995327, 'subscription': 0, 'content': "".\n\nHey Bill Gates, try topping THIS story! [1]\n\nBack in the 70s, I accidentally figured out how to hack a DEC PDP-8/e minicomputer using only the very exotic B.A.S.I.C language (stored on papertape, no IBM punch-cards needed!) And then, accidentally transmitted my hack to a rival high-school, where I hear it created much consternation among faculty for an entire week!\n\n1974, Southside HS. Elmira, NY, somehow our math teacher got hold of a PDP-8/e, and started a class to teach programming in DEC Basic. At first we had to enter the bootloader via the front panel switches, then run in the half-hour of 4K papertape through the teletype reader, into the computer’s six entire k-words of memory, doing this each time the system had been turned off. Finally someone figured out that USUALLY the Basic code could be re-started (from a secret start-addr,) rather than relying on the startup stuff at the end of the papertape (an important part of this story.)\n\nThe PDP-8/e had core memory. Several boads, 2K words each, iirc. If you knew how to gracefully crash the basic code, it usually could be re-started, even after power had been removed. (Plus, if you took the full bin of paper tape punch-holes off the side of the teletype, you could empty it out the window, and make it snow! )\n\nGood times.\n\nI discovered that our copy of Basic had some unknown string-array commands. LINPUT A$, inputting an entire line of text. These weren’t supposed to exist! The commands were found in the back of a manual for a more expensive Basic version. Mostly they didn’t work, but “LINPUT” was there. So was string-compare.\n\nSo, I wrote a tiny program to generate wise-ass responses to system commands, and which, when running, just printed READY(crlf), as the PDP normally did when halted. Heh, by fortunate chance, the LINPUT command didn’t print any question-mark while waiting for input like it was supposed to! So, nobody would realize that my program was running. A second lucky break.\n\nNow, if you sat down at the teletype console, and typed in an entire Basic program, all your typing would be wasted (my program couldn’t store strings,) and then if you typed “LIST,” my program would say “Don’t bother me I’m busy.” Type “RUN” and it said “I can’t, I ain’t got no legs.” Plus a few others, one for each valid command.\n\nEasily defeated by anybody, just type CTRL-C. And then swear a lot, depending on how many lines of code you’d just carefully typed into nowhere.\n\nMy program was only effective/hilarious for about a week, until everyone got into the habit of always hitting CTRL-C, to make sure that no joker who’d been using the system had left that f****g thing waiting for you.\n\nMonths later, by fortunate chance I happened to be using the computer on the day before summer vacation. I was told to perform the last halt-system procedure before summer vacation, when I suddenly had an idea. I said “Just a sec.” I input the short papertape for my program, ran it, THEN halted the PDP-8 and turned off the power …while a Basic program was still running. (I never tried this before. Didn’t know if it would work.)\n\n[Summer vacation happens]\n\nMonths later, on the first day of school, the math teacher tracked me down, and said “What was that thing you did just before we turned off the PDP-8?”\n\nUh oh.\n\nActually he started laughing, and told the entire story.\n\nOurs was the “football school,” while our rivals on the other side of the river were the “smart school” with all the science courses. Somehow they’d wangled the PDP-8 away from us during the summer. One of their teachers was going to present a teacher-only programming course, so they could all learn Basic language. Our own math teacher was NOT invited. By fortunate chance, the guy taking our computer had been carefully instructed in how to restart the PDP-8, so no bootloader and no thirty minutes of tapereader would be needed. He did this successfully.\n\nThe PDP-8 said READY.\n\nThen he typed in his first “hello” program, and hit RUN.\n\nIt said …I CANT I AINT GOT NO LEGS.\n\nOur math teacher swears that it took the other school an entire week to figure it out, and their summer course had to be delayed. The person using the computer had learned everything from manuals alone, and (by sheer bad luck) had somehow skipped the paragraph about breaking out of a running program. He didn’t know CTRL-C. Nobody else could solve the problem either, because apparently everyone else assumed he of course knew CTRL-C, and had already tried that, first thing …which he hadn’t.\n\nSo yes, if you have a PDP-8/e running DEC Basic, and a Basic prgram is currently running, then if you halt the system, remove power for weeks, then restart, not only is Basic still in memory, but any Basic programs are still in there, and still running!\n\nOldschool hacking. OLD-oldschool hacking, the master of the syncronicites. This “hack” required about eight unlikely amazing bits of luck and coincidences to first line up exactly. And, my “virus” was “transmitted” to our rival HS, by them coming in and physically taking our corememory-based PDP-8 computer away from us, so it couldn’t be used by Southside HS during the summer. Hacks via social engineering, performed by accident.\n\nAlchemist synchronicities? A Toltec accomplishment? Me and Richard Feynman, we both have the Red Dwarf Luck Virus. (So why aren’t we millionaires?)\n\nOther cool tricks, now somewhat lame…\n\nWRITING IN ASSEMBLER. In 1979 I had Conway’s Life going fast, on Altair-era Polymorphics 88 graphics. Well not that fast, on 8080 at 1MHz cycle speed, where I think a glider-gun was updating about six times per second. Also a voice-recorder, a Morse typer, even a Morse code listener (microphone to text, our 8080 home computer could understand human speech in 1979, as long as the human sounded like DAH DI DAH DI. DAH DAH DI DAH)\n\nSpeaking of which, years later in 1982 I had another wonderful idea, but talked myself out of it: today called “computer viruses.”\n\nOr more specifically, why shouldn’t I write the very first personal-computer virus, then keep it secret, …so I can sell it as computer-game security software, self-installing?\n\nInfectious “Sony rootkits” of 1982.\n\nI bought a used Apple from a guy at work. On the APPLE-][-plus, I realized that if you have a custom-modified master-disk, which when loaded, can sit waiting, and then modify any other master-disk placed in the drive, then soon all Apple disks now have your code. It spreads worldwide, kinda-sorta like a “disease,” but this disease …is for home computers! Perhaps give away free Apple games to various high schools, to “infect” entire distant cities!\n\nAnd if your silent, unsuspected code is always watching for illegal copies of a certain video game, it can take action. It could even be *watching* for people to use their Apple-II home computers to make the illegal copies. Don’t interfere with this (that would expose its existence!) It could take vengeful action later, when the perps least suspect it. My “disease” can carry any sorts of horrific “symptoms” that I desire; all sorts of novel exotic punishments I was dreaming up. Back then in 1982, this was an idea to make millions, and the world would never be the same! (Heh.) Also, a simple way to spread unwanted software updates to every single Apple-II on the planet. (Today, if you could travel back in time to the early 1980s, which modern idea would make you a world-famous billionaire? Viral rootkits! Heh, maybe not. Instead try spreadsheets.)\n\nI was just at the stage of doing 8080 dis-assembler on the Apple-][ OS when, I realized I didn’t want to become the famous person who invented the “home computer influenza.” Every computer-owner would be spitting on my grave (after they’d killed me slowly, hunted down by a crowd of Apple users equipped with tiny needlenose pilers. And soldering irons! They’ll kill me with cold, dirty soldering irons. With dull tips.)\n\nEventually that title went to a high-school kid in ?1986?, who did exactly the same thing with the OS on his Apple-II+. But he let the thing get loose and start spreading through his school, rather than keeping it quiet like I’d been planning to do, nor selling the whole “virus-spread rootkit” concept to all the major game-companies of 1983.\n\nThe “IBM PC” soon appeared, everyone started writing viruses for it, and the rest is history.\n\nSo, I got lucky and took subconscious warnings seriously, and really dodged a bullet there! More like, dodged thousands of bullets …and also the tips of those cold, unsharpened soldering irons.\n\n[1] Are there any other “hacker” stories involving the PDP-8? I’ve yet to encounter one. I did hear that the Rochester NY planetarium back in the 1970s was run by a PDP-8. I wonder if they got up to any shenannigans with it? At least try some strobe-flashing audience mind-control, in your PLAN-NE -ARIUM.\n\nSee also:\n\nHow can I become a pro at electronics, so I can assemble any circuit I want?\nWhat advice can you give to young electronics enthusiasts who don't have much money?\n\nALSO more answers (nine hundred. Mostly electricity) QUORA ANSWERS: W Beaty\n\n."", 'aiModelVersion': '1'}",0.725825
Michael B.,Updated 2y,What caused the millennium (Y2K) bug back in 1999/2000? What types of computers could have been affected by it?,"Technical debt.

In programming there’s a concept known as technical debt. It’s when you program something in such a way that you’re going to need to fix it in the future. Usually this occurs when programmers are trying to meet some sort of deadline and don’t have time to do thing the right way.

But in the case of the Y2K bug it was because of memory constraints. Back in the 60s and 70s computer memory was expensive. Programmers realized that they could save a substantial amount of memory by saving dates as 2 digits instead of 4. After all, most of the dates they used were nineteen something. And Moore’s Law meant they could go back and fix it once memory got cheaper.

Moore’s Law : Consumer computers double in power roughly every 18 months.

The problem is that they were thinking like programmers, not businessmen. When big businesses like banks switched over to computers, they didn’t really feel the need to shake things up again. The systems they had worked, were tested, secure, people knew how to use them. What did upgrading really give them?

Even today you can find lucrative jobs for COBOL developers. They don’t even teach COBOL in school anymore.

And this created a problem. Some of the most vital computer systems in the world, were positively ancient and had a ticking time bomb. Once the millennium rolled over, these your banks, your credit cards, your airports, would all think it was January 1st 1900. Fixing it at that point would take weeks and would basically render the systems unusable in the interim. Society could very well grind to a halt.

So the programmers rang the doom bell. They went to the media and worked them up into a panic, so their bosses would give them the time and resources to fix the bugs. Which for the most part they did.

The last bite of the bug
By BBC News Online internet reporter Mark Ward A year later than expected, the Millennium Bug is biting organisations around the world. This week, news has emerged about four incidents thought to have been caused by the computer glitch. A Swedish bank, 7-Eleven convenience stores, Norwegian trains and Jury members in Oregon have all been affected by date-related program problems. These computer anomalies may be the last gasp of the bug because they seem to be hitting only those organisations that did not realise that 2000 was a leap year. New Year hangover January 2000 was supposed to be the date that the Millennium Bug bit deepest, but it passed without the worldwide meltdown feared by some experts. The bug arose because some computer programmers used only two digits to record dates. When the year rolled over from 99 to 00, some machines interpreted this to mean that 1900 had only just dawned. What seems to have caused problems this January was the fact that many programmers did not realise that 2000 was a leap year. As a result, 2001 appeared to arrive a day early for some computers and they crashed in reaction. Convenience store chain 7-Eleven was hit by this problem. On 1 January, many of the cash registers in its 19,600 stores or affiliates around the world lost the ability to process credit cards. Cashed out Thinking that it was 1901, the tills rejected credit cards that looked like they had been issued 100 years into the future. A spokeswoman for 7-Eleven said it spent $8.8 (Ł6.2) million on its bug prevention program that left it largely free of problems until 2001 arrived. ""We did about 10,000 tests on it, and it was working fine until Monday,"" she said, adding that the problem was fixed by Wednesday. Another bug-related glitch is thought to have made the online, telebanking and cash machine systems operated by Swedish bank Nordbanken unreliable over the Christmas and New Year period. The systems crashed five times between 27 December and 3 January, rendering them unusable and leaving many of the bank's 3.5 million customers frustrated and short of cash. Turning back time Nordbanken has yet to say what caused the problems. The Swedish banking regulator is demanding an explanation for the glitches. Also hit by the bug were residents of Multnomah county in Oregon, US, who were summoned for jury duty. Almost 3,000 received letters asking them to turn up in 1901. An investigation revealed that the problem was caused by lazy court workers who were using only two digits to describe January 2001 dates. If they had used four, the court computer systems would have recognised the date correctly. Finally, earlier this week, the bug was blamed for disrupting the running of some Norwegian trains that refused to recognise the last day of 2000. The trains were made to run again by turning their internal clocks back a month.
http://news.bbc.co.uk/2/hi/science/nature/1101917.stm

Although a few cases still made it into the wild.

The irony of the situation is that because the Y2K bug was fixed most people don’t realize it was real. It could very well have severely damaged the global economy. It’s just that because people were made aware of it, they had time to fix it. And in IT, if you do your job right they wonder what they’re even paying you for.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pwtb0kdj9x87zs4n', 'title': 'What caused the millennium (Y2K) bug back in 1999/2000? What types of computers could have been affected by it?', 'score': {'original': 0.9333, 'ai': 0.0667}, 'blocks': [{'text': 'Technical debt.\n\nIn programming there’s a concept known as technical debt. It’s when you program something in such a way that you’re going to need to fix it in the future. Usually this occurs when programmers are trying to meet some sort of deadline and don’t have time to do thing the right way.\n\nBut in the case of the Y2K bug it was because of memory constraints. Back in the 60s and 70s computer memory was expensive. Programmers realized that they could save a substantial amount of memory by saving dates as 2 digits instead of 4. After all, most of the dates they used were nineteen something. And Moore’s Law meant they could go back and fix it once memory got cheaper.\n\nMoore’s Law : Consumer computers double in power roughly every 18 months.\n\nThe problem is that they were thinking like programmers, not businessmen. When big businesses like banks switched over to computers, they didn’t really feel the need to shake things up again. The systems they had worked, were tested, secure, people knew how to use them. What did upgrading really give them?\n\nEven today you can find lucrative jobs for COBOL developers. They don’t even teach COBOL in school anymore.\n\nAnd this created a problem. Some of the most vital computer systems in the world, were positively ancient and had a ticking time bomb. Once the millennium rolled over, these your banks, your credit cards, your airports, would all think it was January 1st 1900. Fixing it at that point would take weeks and would basically render the systems unusable in the interim. Society could very well grind to a halt.\n\nSo the programmers rang the doom bell. They went to the media and worked them up into a panic, so their bosses would give them the time and resources to fix the bugs. Which for the most part they did.\n\nThe last bite of the bug\nBy BBC News Online internet reporter Mark Ward A year later than expected, the Millennium Bug is biting organisations around the world. This week, news has emerged about four incidents thought to have been caused by the computer glitch. A Swedish bank, 7-Eleven convenience stores, Norwegian trains and Jury members in Oregon have all been affected by date-related program problems. These computer anomalies may be the last gasp of the bug because they seem to be hitting only those organisations that did not realise that 2000 was a leap year. New Year hangover January 2000 was supposed to be the date that the Millennium Bug bit deepest, but it passed without the worldwide meltdown feared by some experts. The bug arose because some computer programmers used only two digits to record dates. When the year rolled over from 99 to 00, some machines interpreted this to mean that 1900 had only just dawned. What seems to have caused problems this January was the fact that many programmers did not realise that 2000 was a leap year. As a result, 2001 appeared to arrive a day early for some computers and they crashed in reaction. Convenience store chain 7-Eleven was', 'result': {'fake': 0.0147, 'real': 0.9853}, 'status': 'success'}, {'text': 'hit by this problem. On 1 January, many of the cash registers in its 19,600 stores or affiliates around the world lost the ability to process credit cards. Cashed out Thinking that it was 1901, the tills rejected credit cards that looked like they had been issued 100 years into the future. A spokeswoman for 7-Eleven said it spent $8.8 (Ł6.2) million on its bug prevention program that left it largely free of problems until 2001 arrived. ""We did about 10,000 tests on it, and it was working fine until Monday,"" she said, adding that the problem was fixed by Wednesday. Another bug-related glitch is thought to have made the online, telebanking and cash machine systems operated by Swedish bank Nordbanken unreliable over the Christmas and New Year period. The systems crashed five times between 27 December and 3 January, rendering them unusable and leaving many of the bank\'s 3.5 million customers frustrated and short of cash. Turning back time Nordbanken has yet to say what caused the problems. The Swedish banking regulator is demanding an explanation for the glitches. Also hit by the bug were residents of Multnomah county in Oregon, US, who were summoned for jury duty. Almost 3,000 received letters asking them to turn up in 1901. An investigation revealed that the problem was caused by lazy court workers who were using only two digits to describe January 2001 dates. If they had used four, the court computer systems would have recognised the date correctly. Finally, earlier this week, the bug was blamed for disrupting the running of some Norwegian trains that refused to recognise the last day of 2000. The trains were made to run again by turning their internal clocks back a month.\nhttp://news.bbc.co.uk/2/hi/science/nature/1101917.stm\n\nAlthough a few cases still made it into the wild.\n\nThe irony of the situation is that because the Y2K bug was fixed most people don’t realize it was real. It could very well have severely damaged the global economy. It’s just that because people were made aware of it, they had time to fix it. And in IT, if you do your job right they wonder what they’re even paying you for.', 'result': {'fake': 0.6982, 'real': 0.3018}, 'status': 'success'}], 'credits_used': 9, 'credits': 1995318, 'subscription': 0, 'content': 'Technical debt.\n\nIn programming there’s a concept known as technical debt. It’s when you program something in such a way that you’re going to need to fix it in the future. Usually this occurs when programmers are trying to meet some sort of deadline and don’t have time to do thing the right way.\n\nBut in the case of the Y2K bug it was because of memory constraints. Back in the 60s and 70s computer memory was expensive. Programmers realized that they could save a substantial amount of memory by saving dates as 2 digits instead of 4. After all, most of the dates they used were nineteen something. And Moore’s Law meant they could go back and fix it once memory got cheaper.\n\nMoore’s Law : Consumer computers double in power roughly every 18 months.\n\nThe problem is that they were thinking like programmers, not businessmen. When big businesses like banks switched over to computers, they didn’t really feel the need to shake things up again. The systems they had worked, were tested, secure, people knew how to use them. What did upgrading really give them?\n\nEven today you can find lucrative jobs for COBOL developers. They don’t even teach COBOL in school anymore.\n\nAnd this created a problem. Some of the most vital computer systems in the world, were positively ancient and had a ticking time bomb. Once the millennium rolled over, these your banks, your credit cards, your airports, would all think it was January 1st 1900. Fixing it at that point would take weeks and would basically render the systems unusable in the interim. Society could very well grind to a halt.\n\nSo the programmers rang the doom bell. They went to the media and worked them up into a panic, so their bosses would give them the time and resources to fix the bugs. Which for the most part they did.\n\nThe last bite of the bug\nBy BBC News Online internet reporter Mark Ward A year later than expected, the Millennium Bug is biting organisations around the world. This week, news has emerged about four incidents thought to have been caused by the computer glitch. A Swedish bank, 7-Eleven convenience stores, Norwegian trains and Jury members in Oregon have all been affected by date-related program problems. These computer anomalies may be the last gasp of the bug because they seem to be hitting only those organisations that did not realise that 2000 was a leap year. New Year hangover January 2000 was supposed to be the date that the Millennium Bug bit deepest, but it passed without the worldwide meltdown feared by some experts. The bug arose because some computer programmers used only two digits to record dates. When the year rolled over from 99 to 00, some machines interpreted this to mean that 1900 had only just dawned. What seems to have caused problems this January was the fact that many programmers did not realise that 2000 was a leap year. As a result, 2001 appeared to arrive a day early for some computers and they crashed in reaction. Convenience store chain 7-Eleven was hit by this problem. On 1 January, many of the cash registers in its 19,600 stores or affiliates around the world lost the ability to process credit cards. Cashed out Thinking that it was 1901, the tills rejected credit cards that looked like they had been issued 100 years into the future. A spokeswoman for 7-Eleven said it spent $8.8 (Ł6.2) million on its bug prevention program that left it largely free of problems until 2001 arrived. ""We did about 10,000 tests on it, and it was working fine until Monday,"" she said, adding that the problem was fixed by Wednesday. Another bug-related glitch is thought to have made the online, telebanking and cash machine systems operated by Swedish bank Nordbanken unreliable over the Christmas and New Year period. The systems crashed five times between 27 December and 3 January, rendering them unusable and leaving many of the bank\'s 3.5 million customers frustrated and short of cash. Turning back time Nordbanken has yet to say what caused the problems. The Swedish banking regulator is demanding an explanation for the glitches. Also hit by the bug were residents of Multnomah county in Oregon, US, who were summoned for jury duty. Almost 3,000 received letters asking them to turn up in 1901. An investigation revealed that the problem was caused by lazy court workers who were using only two digits to describe January 2001 dates. If they had used four, the court computer systems would have recognised the date correctly. Finally, earlier this week, the bug was blamed for disrupting the running of some Norwegian trains that refused to recognise the last day of 2000. The trains were made to run again by turning their internal clocks back a month.\nhttp://news.bbc.co.uk/2/hi/science/nature/1101917.stm\n\nAlthough a few cases still made it into the wild.\n\nThe irony of the situation is that because the Y2K bug was fixed most people don’t realize it was real. It could very well have severely damaged the global economy. It’s just that because people were made aware of it, they had time to fix it. And in IT, if you do your job right they wonder what they’re even paying you for.', 'aiModelVersion': '1'}",0.9333
Kurt Guntheroth,Updated 5y,What was programming like back in the days when computers only had kilobytes of RAM?,"Not as different as you think. It still happened in offices with fluorescent lights and desks and cubicles. Geeky teams still met in conference rooms with yellow note pads and whiteboards. You cross-compiled your code on a minicomputer that had more RAM, and a real O/S, which if you were lucky was UNIX, and if you were unlucky was RSTS. You typed code on a keyboard and stared at a display all day.

Some stuff that was different? The compile/debug cycle was way longer when it took 15 minutes to blow a set of EPROMs, make a test run, and find the next error. You tended to dry-lab your code to a greater extent than today. If you were developing on the target system, assembler software loaded off of paper tape really, really sucked. Patching binaries in machine language was ugly.

The VT100 terminals we used produced only 24 lines by 80 columns of monochrome text, a bit less than half a typewritten page. It was like looking at your code through a keyhole. If you really wanted to see it, you printed it out on a line printer. There was no IDE. You had to remember where all the code was.

Projects that fit into a microcomputer were smaller, so teams were smaller, and there were fewer lines of code. You collaborated very closely with your team mates on a daily basis. The code itself was the same as what you’d write today if you were a C programmer. Once the ROM was full, you couldn’t add a feature unless you removed one, so projects came to a more certain end.

Oh, and it was OK to smoke cigarettes in the office. So if your desk was next to a smoker, you went home every evening with watery eyes, headache, and sinus congestion.

No diet coke! A cola product called Tab, made with saccharine, was a weak substitute. There were no such thing as energy drinks. Coffee was free, and tasted like it.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/mikgbfvqpw4j26ey', 'title': 'What was programming like back in the days when computers only had kilobytes of RAM?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Not as different as you think. It still happened in offices with fluorescent lights and desks and cubicles. Geeky teams still met in conference rooms with yellow note pads and whiteboards. You cross-compiled your code on a minicomputer that had more RAM, and a real O/S, which if you were lucky was UNIX, and if you were unlucky was RSTS. You typed code on a keyboard and stared at a display all day.\n\nSome stuff that was different? The compile/debug cycle was way longer when it took 15 minutes to blow a set of EPROMs, make a test run, and find the next error. You tended to dry-lab your code to a greater extent than today. If you were developing on the target system, assembler software loaded off of paper tape really, really sucked. Patching binaries in machine language was ugly.\n\nThe VT100 terminals we used produced only 24 lines by 80 columns of monochrome text, a bit less than half a typewritten page. It was like looking at your code through a keyhole. If you really wanted to see it, you printed it out on a line printer. There was no IDE. You had to remember where all the code was.\n\nProjects that fit into a microcomputer were smaller, so teams were smaller, and there were fewer lines of code. You collaborated very closely with your team mates on a daily basis. The code itself was the same as what you’d write today if you were a C programmer. Once the ROM was full, you couldn’t add a feature unless you removed one, so projects came to a more certain end.\n\nOh, and it was OK to smoke cigarettes in the office. So if your desk was next to a smoker, you went home every evening with watery eyes, headache, and sinus congestion.\n\nNo diet coke! A cola product called Tab, made with saccharine, was a weak substitute. There were no such thing as energy drinks. Coffee was free, and tasted like it.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 4, 'credits': 1995314, 'subscription': 0, 'content': 'Not as different as you think. It still happened in offices with fluorescent lights and desks and cubicles. Geeky teams still met in conference rooms with yellow note pads and whiteboards. You cross-compiled your code on a minicomputer that had more RAM, and a real O/S, which if you were lucky was UNIX, and if you were unlucky was RSTS. You typed code on a keyboard and stared at a display all day.\n\nSome stuff that was different? The compile/debug cycle was way longer when it took 15 minutes to blow a set of EPROMs, make a test run, and find the next error. You tended to dry-lab your code to a greater extent than today. If you were developing on the target system, assembler software loaded off of paper tape really, really sucked. Patching binaries in machine language was ugly.\n\nThe VT100 terminals we used produced only 24 lines by 80 columns of monochrome text, a bit less than half a typewritten page. It was like looking at your code through a keyhole. If you really wanted to see it, you printed it out on a line printer. There was no IDE. You had to remember where all the code was.\n\nProjects that fit into a microcomputer were smaller, so teams were smaller, and there were fewer lines of code. You collaborated very closely with your team mates on a daily basis. The code itself was the same as what you’d write today if you were a C programmer. Once the ROM was full, you couldn’t add a feature unless you removed one, so projects came to a more certain end.\n\nOh, and it was OK to smoke cigarettes in the office. So if your desk was next to a smoker, you went home every evening with watery eyes, headache, and sinus congestion.\n\nNo diet coke! A cola product called Tab, made with saccharine, was a weak substitute. There were no such thing as energy drinks. Coffee was free, and tasted like it.', 'aiModelVersion': '1'}",0.9998
Joshua Gross,Updated 1y,"If you need computers to code, and computers were made of coding, how was the first programming language created?","First, you don't need computers to ""code"" (please call it programming; the purpose is not to be cryptic). When Alan Turing invented a hypothetical machine, he used it to demonstrate a mathematical proof on what could not be computed. The computer existed on paper and in his head.

Second, you don't need a programming language to program. The first computers were programmed using physical circuits, so if you wanted to write what we would now call a subroutine, you'd build it as part of a table of such functions. It would be plugged into the computer at the right point.

Third, one of the first formally-specified programming languages, Lisp, wasn't created as a programming language. It was created by John McCarthy to allow mathematical expressions to be written in a form that could be computed. Afterward, Steve Russell pointed out that it was not only Turing-complete, but executable, and he implemented it.

Programming languages are NOT about communicating with computers. Assembly languages (which aren't really programming languages, merely instruction lists) are all you need. Programming languages (those that can be described in Backus-Naur Form) are for communicating with other people (including your future self). Computers do not give a flying fig about programming languages; in fact, we have to write complex programs to get from the programming code to a form that the computer can understand (compilers and interpreters).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/16ra8yifudlwoegs', 'title': 'If you need computers to code, and computers were made of coding, how was the first programming language created?', 'score': {'original': 0.997, 'ai': 0.003}, 'blocks': [{'text': 'First, you don\'t need computers to ""code"" (please call it programming; the purpose is not to be cryptic). When Alan Turing invented a hypothetical machine, he used it to demonstrate a mathematical proof on what could not be computed. The computer existed on paper and in his head.\n\nSecond, you don\'t need a programming language to program. The first computers were programmed using physical circuits, so if you wanted to write what we would now call a subroutine, you\'d build it as part of a table of such functions. It would be plugged into the computer at the right point.\n\nThird, one of the first formally-specified programming languages, Lisp, wasn\'t created as a programming language. It was created by John McCarthy to allow mathematical expressions to be written in a form that could be computed. Afterward, Steve Russell pointed out that it was not only Turing-complete, but executable, and he implemented it.\n\nProgramming languages are NOT about communicating with computers. Assembly languages (which aren\'t really programming languages, merely instruction lists) are all you need. Programming languages (those that can be described in Backus-Naur Form) are for communicating with other people (including your future self). Computers do not give a flying fig about programming languages; in fact, we have to write complex programs to get from the programming code to a form that the computer can understand (compilers and interpreters).', 'result': {'fake': 0.0132, 'real': 0.9868}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995311, 'subscription': 0, 'content': 'First, you don\'t need computers to ""code"" (please call it programming; the purpose is not to be cryptic). When Alan Turing invented a hypothetical machine, he used it to demonstrate a mathematical proof on what could not be computed. The computer existed on paper and in his head.\n\nSecond, you don\'t need a programming language to program. The first computers were programmed using physical circuits, so if you wanted to write what we would now call a subroutine, you\'d build it as part of a table of such functions. It would be plugged into the computer at the right point.\n\nThird, one of the first formally-specified programming languages, Lisp, wasn\'t created as a programming language. It was created by John McCarthy to allow mathematical expressions to be written in a form that could be computed. Afterward, Steve Russell pointed out that it was not only Turing-complete, but executable, and he implemented it.\n\nProgramming languages are NOT about communicating with computers. Assembly languages (which aren\'t really programming languages, merely instruction lists) are all you need. Programming languages (those that can be described in Backus-Naur Form) are for communicating with other people (including your future self). Computers do not give a flying fig about programming languages; in fact, we have to write complex programs to get from the programming code to a form that the computer can understand (compilers and interpreters).', 'aiModelVersion': '1'}",0.997
Stan Hanks,Updated 6y,How did Ken Thompson write UNIX in one month?,"You have to remember two things:

things were tremendously simpler then, and it was trivial to hold all the knowledge in your head about the architecture of the computer including all the instructions not just by mnemonic but by hex (or octal!) code as well as remember literally all of the OS code by heart
He really, really wanted to play Space Travel
 and by Bell Labs pulling out of Multics, he’d lost his access.

There’s nothing like serious motivation to make things like that happen!

As Thompson began porting the game to the new system, he decided not to base the code on any of the existing software for the computer, and instead write his own. As a result, he implemented his own base code libraries for programs to use, including arithmetic packages and graphics subsystems. These initial subsystems were coded in assembly language
 on the GECOS system and assembled, then the output physically put on punched tapes
 to be carried over and inserted into the PDP-7. Thompson then wrote an assembler
 for the PDP-7 to avoid this laborious process.

The game ran very slowly on the new machine, causing Thompson to branch out from there to design his own file system
 based on some ideas by Dennis Ritchie and Rudd Cassaway, rooted in their experience with the Multics file system, with which he then ran Space Travel

I’ve written OSes from scratch for ’80s-era hardware in under two months with cash being the only motivation. It’s completely NOT surprising that Ken wrote the first cut of UNIX in a month. That’s just how things worked.

Let’c compare and contrast here, so you have a point of reference.

The PDP-7
 that Ken started on (complete manual here -> http://bitsavers.trailing-edge.com/pdf/dec/pdp7/F-75P_PDP7prelimUM_Dec64.pdf 
) had 31 instructions. Thirty-one. There were four addressing modes, and a very limited memory pool, and no memory management unit as we’d understand one today.

A modern x64 processor (incomplete docs, circa 2014 here -> x86 Instruction Set Reference
) has 664 instructions, full MMU capability, and about a dozen addressing modes. Plus it requires you to understand how to talk to the northbridge and southbridge to do anything useful at all.

I don’t even remember all that crap, and count heavily on on-line reference materials any time I need to touch it. But I could code PDP-11 routines in my head and hand load them on the toggle panel of the computer. I could probably still do that, if I take a few moments to think about it, and if I had a PDP-11 handy.

When we say “things were much simpler then”, we’re not kidding, or exaggerating.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/o6j075zgrm1snqld', 'title': 'How did Ken Thompson write UNIX in one month?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'You have to remember two things:\n\nthings were tremendously simpler then, and it was trivial to hold all the knowledge in your head about the architecture of the computer including all the instructions not just by mnemonic but by hex (or octal!) code as well as remember literally all of the OS code by heart\nHe really, really wanted to play Space Travel\n and by Bell Labs pulling out of Multics, he’d lost his access.\n\nThere’s nothing like serious motivation to make things like that happen!\n\nAs Thompson began porting the game to the new system, he decided not to base the code on any of the existing software for the computer, and instead write his own. As a result, he implemented his own base code libraries for programs to use, including arithmetic packages and graphics subsystems. These initial subsystems were coded in assembly language\n on the GECOS system and assembled, then the output physically put on punched tapes\n to be carried over and inserted into the PDP-7. Thompson then wrote an assembler\n for the PDP-7 to avoid this laborious process.\n\nThe game ran very slowly on the new machine, causing Thompson to branch out from there to design his own file system\n based on some ideas by Dennis Ritchie and Rudd Cassaway, rooted in their experience with the Multics file system, with which he then ran Space Travel\n\nI’ve written OSes from scratch for ’80s-era hardware in under two months with cash being the only motivation. It’s completely NOT surprising that Ken wrote the first cut of UNIX in a month. That’s just how things worked.\n\nLet’c compare and contrast here, so you have a point of reference.\n\nThe PDP-7\n that Ken started on (complete manual here -> http://bitsavers.trailing-edge.com/pdf/dec/pdp7/F-75P_PDP7prelimUM_Dec64.pdf \n) had 31 instructions. Thirty-one. There were four addressing modes, and a very limited memory pool, and no memory management unit as we’d understand one today.\n\nA modern x64 processor (incomplete docs, circa 2014 here -> x86 Instruction Set Reference\n) has 664 instructions, full MMU capability, and about a dozen addressing modes. Plus it requires you to understand how to talk to the northbridge and southbridge to do anything useful at all.\n\nI don’t even remember all that crap, and count heavily on on-line reference materials any time I need to touch it. But I could code PDP-11 routines in my head and hand load them on the toggle panel of the computer. I could probably still do that, if I take a few moments to think about it, and if I had a PDP-11 handy.\n\nWhen we say “things were much simpler then”, we’re not kidding, or exaggerating.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995306, 'subscription': 0, 'content': 'You have to remember two things:\n\nthings were tremendously simpler then, and it was trivial to hold all the knowledge in your head about the architecture of the computer including all the instructions not just by mnemonic but by hex (or octal!) code as well as remember literally all of the OS code by heart\nHe really, really wanted to play Space Travel\n and by Bell Labs pulling out of Multics, he’d lost his access.\n\nThere’s nothing like serious motivation to make things like that happen!\n\nAs Thompson began porting the game to the new system, he decided not to base the code on any of the existing software for the computer, and instead write his own. As a result, he implemented his own base code libraries for programs to use, including arithmetic packages and graphics subsystems. These initial subsystems were coded in assembly language\n on the GECOS system and assembled, then the output physically put on punched tapes\n to be carried over and inserted into the PDP-7. Thompson then wrote an assembler\n for the PDP-7 to avoid this laborious process.\n\nThe game ran very slowly on the new machine, causing Thompson to branch out from there to design his own file system\n based on some ideas by Dennis Ritchie and Rudd Cassaway, rooted in their experience with the Multics file system, with which he then ran Space Travel\n\nI’ve written OSes from scratch for ’80s-era hardware in under two months with cash being the only motivation. It’s completely NOT surprising that Ken wrote the first cut of UNIX in a month. That’s just how things worked.\n\nLet’c compare and contrast here, so you have a point of reference.\n\nThe PDP-7\n that Ken started on (complete manual here -> http://bitsavers.trailing-edge.com/pdf/dec/pdp7/F-75P_PDP7prelimUM_Dec64.pdf \n) had 31 instructions. Thirty-one. There were four addressing modes, and a very limited memory pool, and no memory management unit as we’d understand one today.\n\nA modern x64 processor (incomplete docs, circa 2014 here -> x86 Instruction Set Reference\n) has 664 instructions, full MMU capability, and about a dozen addressing modes. Plus it requires you to understand how to talk to the northbridge and southbridge to do anything useful at all.\n\nI don’t even remember all that crap, and count heavily on on-line reference materials any time I need to touch it. But I could code PDP-11 routines in my head and hand load them on the toggle panel of the computer. I could probably still do that, if I take a few moments to think about it, and if I had a PDP-11 handy.\n\nWhen we say “things were much simpler then”, we’re not kidding, or exaggerating.', 'aiModelVersion': '1'}",0.9998
Aditya Nair,Updated 7y,Who is the person who invented USB?,"The devlopement of USB began in 1994 .

Compaq,IBM,DEC ,Intel,Microsoft,NEC and Nortel joined hands together for the devlopement of USB

This guy named Ajay Bhatt co-invented USB in 1995

He was the lead of the team which was formed by these seven companies.
One day he was sick of printer plugs as they were not easy to handle and decided to invent something which would be easy to use
Hence he and his team came up with the invention of USB while working at Intel .

The first specification for the USB version 1.0 was introduced in 1996
Hope this helps :)
P.S.-thanks for such amazing response","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/3xvflg9yr2hjo7q8', 'title': 'Who is the person who invented USB?', 'score': {'original': 0.9976, 'ai': 0.0024}, 'blocks': [{'text': 'The devlopement of USB began in 1994 .\n\nCompaq,IBM,DEC ,Intel,Microsoft,NEC and Nortel joined hands together for the devlopement of USB\n\nThis guy named Ajay Bhatt co-invented USB in 1995\n\nHe was the lead of the team which was formed by these seven companies.\nOne day he was sick of printer plugs as they were not easy to handle and decided to invent something which would be easy to use\nHence he and his team came up with the invention of USB while working at Intel .\n\nThe first specification for the USB version 1.0 was introduced in 1996\nHope this helps :)\nP.S.-thanks for such amazing response', 'result': {'fake': 0.0024, 'real': 0.9976}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995304, 'subscription': 0, 'content': 'The devlopement of USB began in 1994 .\n\nCompaq,IBM,DEC ,Intel,Microsoft,NEC and Nortel joined hands together for the devlopement of USB\n\nThis guy named Ajay Bhatt co-invented USB in 1995\n\nHe was the lead of the team which was formed by these seven companies.\nOne day he was sick of printer plugs as they were not easy to handle and decided to invent something which would be easy to use\nHence he and his team came up with the invention of USB while working at Intel .\n\nThe first specification for the USB version 1.0 was introduced in 1996\nHope this helps :)\nP.S.-thanks for such amazing response', 'aiModelVersion': '1'}",0.9976
Pat Farrell,1y,The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?,"No, the premise of your question is flat out wrong. Y2K was a feature, not a bug, but the media didn’t understand it.

Most of the programs that had a Y2K problem were written in the 1960s or 1970s. At that time, memory was insanely expensive. Programmers worked hard to save a single byte, no one would waste 2 bytes on each of hundreds of thousands of records.

Data point for reference: In April 1980, I flew with our company VPs up to DEC (Digital Equipment Corporation) to buy disk drives. We were flown in the DEC corporate helicopter out to the HQ, and met with all the top level executives of DEC. We bought ten disk drives. They cost $33,000 each, held 200mb, were the size of a washing machine, and required three phase power. So each dollar of purchase price bought only 6060 bytes of storage.

It would have been professional malpractice in 1970 to waste two bytes on every date field on every record. Y2K did not sneak up on people, the problem was obvious by the mid-1980s. It took huge amounts of money and labor to handle the transition that was only possible because both memory and storage was orders of magnitude cheaper towards the end of the century.

There were no major Y2K disasters. There was one PC company that had a problem, because they sold their software at commercial business stores, and this was pre-Internet, so they had no way to know who had bought their package so they could get an update.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ri69yczhm4jvnpb7', 'title': 'The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?', 'score': {'original': 0.9984, 'ai': 0.0016}, 'blocks': [{'text': 'No, the premise of your question is flat out wrong. Y2K was a feature, not a bug, but the media didn’t understand it.\n\nMost of the programs that had a Y2K problem were written in the 1960s or 1970s. At that time, memory was insanely expensive. Programmers worked hard to save a single byte, no one would waste 2 bytes on each of hundreds of thousands of records.\n\nData point for reference: In April 1980, I flew with our company VPs up to DEC (Digital Equipment Corporation) to buy disk drives. We were flown in the DEC corporate helicopter out to the HQ, and met with all the top level executives of DEC. We bought ten disk drives. They cost $33,000 each, held 200mb, were the size of a washing machine, and required three phase power. So each dollar of purchase price bought only 6060 bytes of storage.\n\nIt would have been professional malpractice in 1970 to waste two bytes on every date field on every record. Y2K did not sneak up on people, the problem was obvious by the mid-1980s. It took huge amounts of money and labor to handle the transition that was only possible because both memory and storage was orders of magnitude cheaper towards the end of the century.\n\nThere were no major Y2K disasters. There was one PC company that had a problem, because they sold their software at commercial business stores, and this was pre-Internet, so they had no way to know who had bought their package so they could get an update.', 'result': {'fake': 0.0016, 'real': 0.9984}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995301, 'subscription': 0, 'content': 'No, the premise of your question is flat out wrong. Y2K was a feature, not a bug, but the media didn’t understand it.\n\nMost of the programs that had a Y2K problem were written in the 1960s or 1970s. At that time, memory was insanely expensive. Programmers worked hard to save a single byte, no one would waste 2 bytes on each of hundreds of thousands of records.\n\nData point for reference: In April 1980, I flew with our company VPs up to DEC (Digital Equipment Corporation) to buy disk drives. We were flown in the DEC corporate helicopter out to the HQ, and met with all the top level executives of DEC. We bought ten disk drives. They cost $33,000 each, held 200mb, were the size of a washing machine, and required three phase power. So each dollar of purchase price bought only 6060 bytes of storage.\n\nIt would have been professional malpractice in 1970 to waste two bytes on every date field on every record. Y2K did not sneak up on people, the problem was obvious by the mid-1980s. It took huge amounts of money and labor to handle the transition that was only possible because both memory and storage was orders of magnitude cheaper towards the end of the century.\n\nThere were no major Y2K disasters. There was one PC company that had a problem, because they sold their software at commercial business stores, and this was pre-Internet, so they had no way to know who had bought their package so they could get an update.', 'aiModelVersion': '1'}",0.9984
Bob Frankston,Updated 7y,What is it like to have been programming for the past fifty years? What has stayed the same? What are the biggest changes? What do you miss? What's a vast improvement?,"Update: For more thoughts on how the concept programming has evolved you can read my July 2016 column http://rmf.vc/IEEEAboutSoftware.

I write this as I'm sitting here trying to learn new programming languages and tools. That’s something that hasn’t changed in fifty years – it’s still a lot of fun for me and there is still so much to learn.

In answering this question I need to be clear that I think of programming very broadly. There is the skill of taking a task and coding it so that a computer can follow the instructions. But I see programming as something much broader – the ability to harness the expressive power of software and redefine the world. That sounds grandiose but, well, take a look around you at today’s world constantly being redefined by software.

On a personal level it’s been exciting because by learning to program in 1963 I grew with the field and had this new infinitely malleable thing called software – that gave me an interesting perspective on the world – one that I’m still struggling to explain to those who use the technology as one uses a typewriter to type rather than to write and express oneself.

I’ve also been online from home since 1966 and that has given me a view of computing as something direct and personal. My first job was on Wall St helping to give users access to financial data. Later at MIT I was on the Multics project which was the first large scale “Information Utility” and the direct predecessor of much of today’s cloud computing. Unix was created by members of that project who took advantage of the new minicomputers.

Over the last 50 or so years hardware has improved by billions of times in each dimension. Memory went from about $1/bit in 1970 to today’s $1/gigabit (or far less). Same for CPU speeds, disk performance capacity, etc. We went from a few simple computations per second in the early 1950s to trillions of floating point operations per second. My 1960s' teletype was 10 characters per second. A megabyte web page would have taken more than a day. In the 1970s 16 kilobytes were a lot for a personal computer and today 16 gigabyte is becoming common.

Yet during that time software has changed, but if you go back you’ll see that many of the big ideas aren’t at all new. Sure it’s exciting that Java finally got lambdas and closures, but John McCarthy’s Lisp had those ideas in the 1950s. John also proposed shared computing in 1958. In 1967 I took a class in the history of programming languages from Saul Rosen using his thick text book Programming Systems and Languages. (Sorry, I can’t give you the ISBN number, because we didn’t have them back then).

Programming practice has certainly changed. Even into the 1970s it was common, even normal, to program in assembly language. Instead of writing “A=B+C” you’d write

Load B into register R1
Add C to R1
Store the result in A

And that program itself was typically represented as holes in pieces of cardboard (punched cards).

Today I rarely get into that level of detail. Seeing how we’ve built on those foundations gives me both an understanding and appreciation not just of computing today but the possibilities for other paths we can and, perhaps, should take.

This is also true of the Internet which I see as an engineering marvel, but also a work-in-progress. For me it’s not so much a network as part of a large shift from computing being seen as something done by specialists in MIS (Management Information Systems) departments to tools everyone uses supported (in corporations) by IT (Information Technology) departments. The job of MIS was to provide reports and answers. The job of IT is to support people creating their own solutions.

Today we have IT (Information Technology) departments which support the technology and provide tools essential to the business so that the various people in the company can take advantage of computing to find their own solutions.

In a sense I’ve spent the last fifty years not programming as much as giving others the ability to share in the power and concepts of computing by building tools, be they electronic spreadsheets or giving people the ability to participate in connected computing.

Over the next fifty years we’re going to face the challenge of sharing programming in the guise of a new literacy. I consider the past fifty years as practice. The sad part is how quickly the time has passed with so many interesting things left to learn and do.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/psnlotuig9xr85kz', 'title': ""What is it like to have been programming for the past fifty years? What has stayed the same? What are the biggest changes? What do you miss? What's a vast improvement?"", 'score': {'original': 0.5695, 'ai': 0.4305}, 'blocks': [{'text': ""Update: For more thoughts on how the concept programming has evolved you can read my July 2016 column http://rmf.vc/IEEEAboutSoftware.\n\nI write this as I'm sitting here trying to learn new programming languages and tools. That’s something that hasn’t changed in fifty years – it’s still a lot of fun for me and there is still so much to learn.\n\nIn answering this question I need to be clear that I think of programming very broadly. There is the skill of taking a task and coding it so that a computer can follow the instructions. But I see programming as something much broader – the ability to harness the expressive power of software and redefine the world. That sounds grandiose but, well, take a look around you at today’s world constantly being redefined by software.\n\nOn a personal level it’s been exciting because by learning to program in 1963 I grew with the field and had this new infinitely malleable thing called software – that gave me an interesting perspective on the world – one that I’m still struggling to explain to those who use the technology as one uses a typewriter to type rather than to write and express oneself.\n\nI’ve also been online from home since 1966 and that has given me a view of computing as something direct and personal. My first job was on Wall St helping to give users access to financial data. Later at MIT I was on the Multics project which was the first large scale “Information Utility” and the direct predecessor of much of today’s cloud computing. Unix was created by members of that project who took advantage of the new minicomputers.\n\nOver the last 50 or so years hardware has improved by billions of times in each dimension. Memory went from about $1/bit in 1970 to today’s $1/gigabit (or far less). Same for CPU speeds, disk performance capacity, etc. We went from a few simple computations per second in the early 1950s to trillions of floating point operations per second. My 1960s' teletype was 10 characters per second. A megabyte web page would have taken more than a day. In the 1970s 16 kilobytes were a lot for a personal computer and today 16 gigabyte is becoming common.\n\nYet during that time software has changed, but if you go back you’ll see that many of the big ideas aren’t at all new. Sure it’s exciting that Java finally got lambdas and closures, but John McCarthy’s Lisp had those ideas in the 1950s. John also proposed shared computing in 1958. In 1967 I took a class in the history of programming languages from Saul Rosen using his thick text book Programming Systems and Languages. (Sorry, I can’t give you the ISBN number, because we didn’t have them back then).\n\nProgramming practice has certainly changed. Even into the 1970s it was common, even normal, to program in assembly language. Instead of writing “A=B+C” you’d write\n\nLoad B into register R1\nAdd C to R1\nStore the result in A\n\nAnd that program itself was typically represented as holes in pieces of cardboard (punched cards).\n\nToday I rarely get"", 'result': {'fake': 0.0123, 'real': 0.9877}, 'status': 'success'}, {'text': 'into that level of detail. Seeing how we’ve built on those foundations gives me both an understanding and appreciation not just of computing today but the possibilities for other paths we can and, perhaps, should take.\n\nThis is also true of the Internet which I see as an engineering marvel, but also a work-in-progress. For me it’s not so much a network as part of a large shift from computing being seen as something done by specialists in MIS (Management Information Systems) departments to tools everyone uses supported (in corporations) by IT (Information Technology) departments. The job of MIS was to provide reports and answers. The job of IT is to support people creating their own solutions.\n\nToday we have IT (Information Technology) departments which support the technology and provide tools essential to the business so that the various people in the company can take advantage of computing to find their own solutions.\n\nIn a sense I’ve spent the last fifty years not programming as much as giving others the ability to share in the power and concepts of computing by building tools, be they electronic spreadsheets or giving people the ability to participate in connected computing.\n\nOver the next fifty years we’re going to face the challenge of sharing programming in the guise of a new literacy. I consider the past fifty years as practice. The sad part is how quickly the time has passed with so many interesting things left to learn and do.', 'result': {'fake': 0.8144, 'real': 0.1856}, 'status': 'success'}], 'credits_used': 8, 'credits': 1995293, 'subscription': 0, 'content': ""Update: For more thoughts on how the concept programming has evolved you can read my July 2016 column http://rmf.vc/IEEEAboutSoftware.\n\nI write this as I'm sitting here trying to learn new programming languages and tools. That’s something that hasn’t changed in fifty years – it’s still a lot of fun for me and there is still so much to learn.\n\nIn answering this question I need to be clear that I think of programming very broadly. There is the skill of taking a task and coding it so that a computer can follow the instructions. But I see programming as something much broader – the ability to harness the expressive power of software and redefine the world. That sounds grandiose but, well, take a look around you at today’s world constantly being redefined by software.\n\nOn a personal level it’s been exciting because by learning to program in 1963 I grew with the field and had this new infinitely malleable thing called software – that gave me an interesting perspective on the world – one that I’m still struggling to explain to those who use the technology as one uses a typewriter to type rather than to write and express oneself.\n\nI’ve also been online from home since 1966 and that has given me a view of computing as something direct and personal. My first job was on Wall St helping to give users access to financial data. Later at MIT I was on the Multics project which was the first large scale “Information Utility” and the direct predecessor of much of today’s cloud computing. Unix was created by members of that project who took advantage of the new minicomputers.\n\nOver the last 50 or so years hardware has improved by billions of times in each dimension. Memory went from about $1/bit in 1970 to today’s $1/gigabit (or far less). Same for CPU speeds, disk performance capacity, etc. We went from a few simple computations per second in the early 1950s to trillions of floating point operations per second. My 1960s' teletype was 10 characters per second. A megabyte web page would have taken more than a day. In the 1970s 16 kilobytes were a lot for a personal computer and today 16 gigabyte is becoming common.\n\nYet during that time software has changed, but if you go back you’ll see that many of the big ideas aren’t at all new. Sure it’s exciting that Java finally got lambdas and closures, but John McCarthy’s Lisp had those ideas in the 1950s. John also proposed shared computing in 1958. In 1967 I took a class in the history of programming languages from Saul Rosen using his thick text book Programming Systems and Languages. (Sorry, I can’t give you the ISBN number, because we didn’t have them back then).\n\nProgramming practice has certainly changed. Even into the 1970s it was common, even normal, to program in assembly language. Instead of writing “A=B+C” you’d write\n\nLoad B into register R1\nAdd C to R1\nStore the result in A\n\nAnd that program itself was typically represented as holes in pieces of cardboard (punched cards).\n\nToday I rarely get into that level of detail. Seeing how we’ve built on those foundations gives me both an understanding and appreciation not just of computing today but the possibilities for other paths we can and, perhaps, should take.\n\nThis is also true of the Internet which I see as an engineering marvel, but also a work-in-progress. For me it’s not so much a network as part of a large shift from computing being seen as something done by specialists in MIS (Management Information Systems) departments to tools everyone uses supported (in corporations) by IT (Information Technology) departments. The job of MIS was to provide reports and answers. The job of IT is to support people creating their own solutions.\n\nToday we have IT (Information Technology) departments which support the technology and provide tools essential to the business so that the various people in the company can take advantage of computing to find their own solutions.\n\nIn a sense I’ve spent the last fifty years not programming as much as giving others the ability to share in the power and concepts of computing by building tools, be they electronic spreadsheets or giving people the ability to participate in connected computing.\n\nOver the next fifty years we’re going to face the challenge of sharing programming in the guise of a new literacy. I consider the past fifty years as practice. The sad part is how quickly the time has passed with so many interesting things left to learn and do."", 'aiModelVersion': '1'}",0.5695
Steve Baker,4y,Why were old games programmed in assembly when higher level languages existed?,"Games that went into systems like the Atari 2600 really had to be crammed into tiny amounts of memory - use every dirty trick in the book (including some that weren’t even IN the book) - and use every microsecond of CPU time.

The compilers of that era didn’t compile at all efficiently and you would be lucky to get half as much done (either space-wise or time-wise) if you didn’t use assembly code.

Those early games had to fit in 2 Kbytes. A modern Xbox has 8 Gbytes…four MILLION times more than the Atari 2600.

On the plus side - if you’re only writing about a thousand machine code instructions - you can keep all of the code in your head - a single programmer can easily write an entire game in a month. With so little code - you really can lean on it from an optimisation standpoint.

Also, once the game “went gold” - it was utterly fixed in ROM forever - no bug fixes, no patches, no updates. So you didn’t even need to write particularly legible code. You’d be the only programmer who’d ever read it - and once you were done with it - there would be zero future maintenance.

As the cartridges got larger and the CPU’s more powerful, high level languages did allow a programmer to get more work done in a given amount of time - which was a big “win”…so gradually, there was less and less assembly language code - until, over the last decade or so - there is essentially zero assembly code in most games.

I kinda miss having to code that way - obsessing over 1,000 lines of code is much easier than fighting a million lines of the stuff! But aside from a VERY few embedded computer applications - that era is over. It’s no longer possible to have endless waves of aliens attacking you - with their number and speed increasing with each level. People expect a fully fleshed out world full of people. buildings and cars - all realistically drawn in three dimensions.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/jm7owvfiydq5ahut', 'title': 'Why were old games programmed in assembly when higher level languages existed?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Games that went into systems like the Atari 2600 really had to be crammed into tiny amounts of memory - use every dirty trick in the book (including some that weren’t even IN the book) - and use every microsecond of CPU time.\n\nThe compilers of that era didn’t compile at all efficiently and you would be lucky to get half as much done (either space-wise or time-wise) if you didn’t use assembly code.\n\nThose early games had to fit in 2 Kbytes. A modern Xbox has 8 Gbytes…four MILLION times more than the Atari 2600.\n\nOn the plus side - if you’re only writing about a thousand machine code instructions - you can keep all of the code in your head - a single programmer can easily write an entire game in a month. With so little code - you really can lean on it from an optimisation standpoint.\n\nAlso, once the game “went gold” - it was utterly fixed in ROM forever - no bug fixes, no patches, no updates. So you didn’t even need to write particularly legible code. You’d be the only programmer who’d ever read it - and once you were done with it - there would be zero future maintenance.\n\nAs the cartridges got larger and the CPU’s more powerful, high level languages did allow a programmer to get more work done in a given amount of time - which was a big “win”…so gradually, there was less and less assembly language code - until, over the last decade or so - there is essentially zero assembly code in most games.\n\nI kinda miss having to code that way - obsessing over 1,000 lines of code is much easier than fighting a million lines of the stuff! But aside from a VERY few embedded computer applications - that era is over. It’s no longer possible to have endless waves of aliens attacking you - with their number and speed increasing with each level. People expect a fully fleshed out world full of people. buildings and cars - all realistically drawn in three dimensions.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 4, 'credits': 1995289, 'subscription': 0, 'content': 'Games that went into systems like the Atari 2600 really had to be crammed into tiny amounts of memory - use every dirty trick in the book (including some that weren’t even IN the book) - and use every microsecond of CPU time.\n\nThe compilers of that era didn’t compile at all efficiently and you would be lucky to get half as much done (either space-wise or time-wise) if you didn’t use assembly code.\n\nThose early games had to fit in 2 Kbytes. A modern Xbox has 8 Gbytes…four MILLION times more than the Atari 2600.\n\nOn the plus side - if you’re only writing about a thousand machine code instructions - you can keep all of the code in your head - a single programmer can easily write an entire game in a month. With so little code - you really can lean on it from an optimisation standpoint.\n\nAlso, once the game “went gold” - it was utterly fixed in ROM forever - no bug fixes, no patches, no updates. So you didn’t even need to write particularly legible code. You’d be the only programmer who’d ever read it - and once you were done with it - there would be zero future maintenance.\n\nAs the cartridges got larger and the CPU’s more powerful, high level languages did allow a programmer to get more work done in a given amount of time - which was a big “win”…so gradually, there was less and less assembly language code - until, over the last decade or so - there is essentially zero assembly code in most games.\n\nI kinda miss having to code that way - obsessing over 1,000 lines of code is much easier than fighting a million lines of the stuff! But aside from a VERY few embedded computer applications - that era is over. It’s no longer possible to have endless waves of aliens attacking you - with their number and speed increasing with each level. People expect a fully fleshed out world full of people. buildings and cars - all realistically drawn in three dimensions.', 'aiModelVersion': '1'}",0.9998
Jerry Rufener,Updated 1y,What are some famous computer bugs that can't or shouldn't be fixed?,"Many years ago I worked for a minicomputer manufacturer - DEC. We made a device that sat on the bus of a IBM mainframe and looked like what I believe IBM called a “channel controller”. In reality it was a PDP-11 minicomputer.

Now DEC numbered the bits in their data from right to left - the low order bit was 0, the next higher was 1 and so forth up to the 16th bit which was, of course, bit 15.

IBM mainframes numbered theirs from left to right.

The hardware guy that built the interface, which was a low volume special, did not realize this and connected them in numerical order The guy that wrote the diagnostics passed data in one direction and then passed it back. He then just looked to see of the data was the same. Having been reversed twice it, of course, was.

The guy that wrote the driver on the DEC side noticed the problem and called it to everyone’s attention. The design had been committed to PC boards, so he was told to write a “fix” in the driver. He just reversed the bits - quick fix.

Several years passed. We sold more than expected and some of the components were slated for obsolescence. We did a redesign. The guy that did the redesign knew that IBM and DEC bit ordering was different so he did it correctly. Unfortunately, the driver and the diagnostics no longer worked - because the design was correct! A fair amount of customer written software would not work either.

An option had to be put into the driver to either reverse the bits (or not) thereby reintroducing the original error..","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ouw521n8pjcza0tl', 'title': ""What are some famous computer bugs that can't or shouldn't be fixed?"", 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Many years ago I worked for a minicomputer manufacturer - DEC. We made a device that sat on the bus of a IBM mainframe and looked like what I believe IBM called a “channel controller”. In reality it was a PDP-11 minicomputer.\n\nNow DEC numbered the bits in their data from right to left - the low order bit was 0, the next higher was 1 and so forth up to the 16th bit which was, of course, bit 15.\n\nIBM mainframes numbered theirs from left to right.\n\nThe hardware guy that built the interface, which was a low volume special, did not realize this and connected them in numerical order The guy that wrote the diagnostics passed data in one direction and then passed it back. He then just looked to see of the data was the same. Having been reversed twice it, of course, was.\n\nThe guy that wrote the driver on the DEC side noticed the problem and called it to everyone’s attention. The design had been committed to PC boards, so he was told to write a “fix” in the driver. He just reversed the bits - quick fix.\n\nSeveral years passed. We sold more than expected and some of the components were slated for obsolescence. We did a redesign. The guy that did the redesign knew that IBM and DEC bit ordering was different so he did it correctly. Unfortunately, the driver and the diagnostics no longer worked - because the design was correct! A fair amount of customer written software would not work either.\n\nAn option had to be put into the driver to either reverse the bits (or not) thereby reintroducing the original error..', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995286, 'subscription': 0, 'content': 'Many years ago I worked for a minicomputer manufacturer - DEC. We made a device that sat on the bus of a IBM mainframe and looked like what I believe IBM called a “channel controller”. In reality it was a PDP-11 minicomputer.\n\nNow DEC numbered the bits in their data from right to left - the low order bit was 0, the next higher was 1 and so forth up to the 16th bit which was, of course, bit 15.\n\nIBM mainframes numbered theirs from left to right.\n\nThe hardware guy that built the interface, which was a low volume special, did not realize this and connected them in numerical order The guy that wrote the diagnostics passed data in one direction and then passed it back. He then just looked to see of the data was the same. Having been reversed twice it, of course, was.\n\nThe guy that wrote the driver on the DEC side noticed the problem and called it to everyone’s attention. The design had been committed to PC boards, so he was told to write a “fix” in the driver. He just reversed the bits - quick fix.\n\nSeveral years passed. We sold more than expected and some of the components were slated for obsolescence. We did a redesign. The guy that did the redesign knew that IBM and DEC bit ordering was different so he did it correctly. Unfortunately, the driver and the diagnostics no longer worked - because the design was correct! A fair amount of customer written software would not work either.\n\nAn option had to be put into the driver to either reverse the bits (or not) thereby reintroducing the original error..', 'aiModelVersion': '1'}",0.9998
Trausti Thor Johannsson,Feb 7,Why did Borland fail?,"Microsoft killed it isn’t untrue, isn’t a lie. But it was more like a traffic cop killing a horse that had been run over and was barely alive. Now, Microsoft is to blame for a lot though and they were guilty as sin.

Incompetent management, pure and simple a lot of MBA, master bulls#it artists. The company was overrun by incompetent people. And their incompetence was amazing. Like their management style was similar as if they made a rocket to go to the moon, it would need to go through earth first.

Borland was an amazing company that did amazing products. They did one thing better than any other company of their size, their products were cheap. Turbo Pascal was like $100, with books, boxes and everything you needed and much more.

Then managers took over. They considered everything Borland was doing as shit, they increased prices to thousands of dollars and you basically had the same product, Delphi for a few hundred and a pro version in the thousands of dollars. Sure some objects came with the box or not but you could still get them and use them.

They changed the name of the company. They started to sell business objects which no one understood or could use. They completely buried the products that made money.

Then they changed the name again and I do believe a few more times. They treated their employees badly. They purchased companies that were going out of business fast like Ashton Tate. They basically kind of forced their best people to go to Microsoft for work, which Microsoft gladly did and waited outside their buildings with a fleet of limousines, waiting for people.

Now, a lot of the bad management is written on Philippe Kahn. He is partly to blame and was made to step down. But the people who took over were just crazy incompetent.

So sad.

It is pretty well documented in the book In Search of Stupidity.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0lp71zh9td5i4rwg', 'title': 'Why did Borland fail?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': 'Microsoft killed it isn’t untrue, isn’t a lie. But it was more like a traffic cop killing a horse that had been run over and was barely alive. Now, Microsoft is to blame for a lot though and they were guilty as sin.\n\nIncompetent management, pure and simple a lot of MBA, master bulls#it artists. The company was overrun by incompetent people. And their incompetence was amazing. Like their management style was similar as if they made a rocket to go to the moon, it would need to go through earth first.\n\nBorland was an amazing company that did amazing products. They did one thing better than any other company of their size, their products were cheap. Turbo Pascal was like $100, with books, boxes and everything you needed and much more.\n\nThen managers took over. They considered everything Borland was doing as shit, they increased prices to thousands of dollars and you basically had the same product, Delphi for a few hundred and a pro version in the thousands of dollars. Sure some objects came with the box or not but you could still get them and use them.\n\nThey changed the name of the company. They started to sell business objects which no one understood or could use. They completely buried the products that made money.\n\nThen they changed the name again and I do believe a few more times. They treated their employees badly. They purchased companies that were going out of business fast like Ashton Tate. They basically kind of forced their best people to go to Microsoft for work, which Microsoft gladly did and waited outside their buildings with a fleet of limousines, waiting for people.\n\nNow, a lot of the bad management is written on Philippe Kahn. He is partly to blame and was made to step down. But the people who took over were just crazy incompetent.\n\nSo sad.\n\nIt is pretty well documented in the book In Search of Stupidity.', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 4, 'credits': 1995282, 'subscription': 0, 'content': 'Microsoft killed it isn’t untrue, isn’t a lie. But it was more like a traffic cop killing a horse that had been run over and was barely alive. Now, Microsoft is to blame for a lot though and they were guilty as sin.\n\nIncompetent management, pure and simple a lot of MBA, master bulls#it artists. The company was overrun by incompetent people. And their incompetence was amazing. Like their management style was similar as if they made a rocket to go to the moon, it would need to go through earth first.\n\nBorland was an amazing company that did amazing products. They did one thing better than any other company of their size, their products were cheap. Turbo Pascal was like $100, with books, boxes and everything you needed and much more.\n\nThen managers took over. They considered everything Borland was doing as shit, they increased prices to thousands of dollars and you basically had the same product, Delphi for a few hundred and a pro version in the thousands of dollars. Sure some objects came with the box or not but you could still get them and use them.\n\nThey changed the name of the company. They started to sell business objects which no one understood or could use. They completely buried the products that made money.\n\nThen they changed the name again and I do believe a few more times. They treated their employees badly. They purchased companies that were going out of business fast like Ashton Tate. They basically kind of forced their best people to go to Microsoft for work, which Microsoft gladly did and waited outside their buildings with a fleet of limousines, waiting for people.\n\nNow, a lot of the bad management is written on Philippe Kahn. He is partly to blame and was made to step down. But the people who took over were just crazy incompetent.\n\nSo sad.\n\nIt is pretty well documented in the book In Search of Stupidity.', 'aiModelVersion': '1'}",0.9995
Alan Kay,1y,How did Ada start programming when computers were not invented?,"You don’t need a computer to write a program, or to execute it. For example, an early algorithm that was done by hand for many years was the greatest common divider (one of the earliest is attributed to Euclid).

One of Ada’s programs was how one would compute Bernoulli numbers on Babbage’s proposed Analytical Engine. The Engine was never built, but she probably executed at least parts of it by hand (and also left a few bugs). Take a look at

What Did Ada Lovelace's Program Actually Do?
In 1843, Ada Lovelace published the first nontrivial program. How did it work?
https://twobithistory.org/2018/08/18/ada-lovelace-note-g.html

to get more of a feel for this program.

A side note here is that in the early days of computing, most programmers did most of their debugging at their desks, via hand execution (I certainly did when I started in the early 60s).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/5qgaic03empbhj2f', 'title': 'How did Ada start programming when computers were not invented?', 'score': {'original': 0.9982, 'ai': 0.0018}, 'blocks': [{'text': ""You don’t need a computer to write a program, or to execute it. For example, an early algorithm that was done by hand for many years was the greatest common divider (one of the earliest is attributed to Euclid).\n\nOne of Ada’s programs was how one would compute Bernoulli numbers on Babbage’s proposed Analytical Engine. The Engine was never built, but she probably executed at least parts of it by hand (and also left a few bugs). Take a look at\n\nWhat Did Ada Lovelace's Program Actually Do?\nIn 1843, Ada Lovelace published the first nontrivial program. How did it work?\nhttps://twobithistory.org/2018/08/18/ada-lovelace-note-g.html\n\nto get more of a feel for this program.\n\nA side note here is that in the early days of computing, most programmers did most of their debugging at their desks, via hand execution (I certainly did when I started in the early 60s)."", 'result': {'fake': 0.0018, 'real': 0.9982}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995280, 'subscription': 0, 'content': ""You don’t need a computer to write a program, or to execute it. For example, an early algorithm that was done by hand for many years was the greatest common divider (one of the earliest is attributed to Euclid).\n\nOne of Ada’s programs was how one would compute Bernoulli numbers on Babbage’s proposed Analytical Engine. The Engine was never built, but she probably executed at least parts of it by hand (and also left a few bugs). Take a look at\n\nWhat Did Ada Lovelace's Program Actually Do?\nIn 1843, Ada Lovelace published the first nontrivial program. How did it work?\nhttps://twobithistory.org/2018/08/18/ada-lovelace-note-g.html\n\nto get more of a feel for this program.\n\nA side note here is that in the early days of computing, most programmers did most of their debugging at their desks, via hand execution (I certainly did when I started in the early 60s)."", 'aiModelVersion': '1'}",0.9982
Joshua Gross,1y,What was programming like back in the days when computers only had kilobytes of RAM?,"When you're dealing with resource-constrained systems, you start to realize how many computing resources we waste.

In grad school, I decided to use a genetic algorithm (GA) to solve mazes using a robotic arm.

It was an attempt to shoehorn a GA into a robotics project, because I had wanted to build a meaningful GA. It was an odd choice.

The robot was built from a first-gen Lego Mindstorm. It used a “8-bit Renesas H8/300 microcontroller as its CPU”
, and for the purposes of what I was doing, I had access to an array of 32 16-bit integers for all of my GA data.

I needed a hell of a lot more discrete values than that, because each entity was a series of moves with a direction and distance. I decided that each move could be comprised of three bits: two for direction (it was a circularly-moving arm, so the four directions were clockwise, counter-clockwise, in (toward the center) and out (away from the center), and one for duration (long or short move). That meant that I could store 5 moves per int and easily achieve 160 different moves, enough for several generations of history. I could have calculated offsets and treated the whole thing as one bit 512-bit structure and had an additional 10 moves, but it wasn’t worth the effort.

There’s only one problem. This is not a good use of the most heavily-constrained resource these days, which is programmer time.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/zelrvfqj89x0aon4', 'title': 'What was programming like back in the days when computers only had kilobytes of RAM?', 'score': {'original': 0.9988, 'ai': 0.0012}, 'blocks': [{'text': ""When you're dealing with resource-constrained systems, you start to realize how many computing resources we waste.\n\nIn grad school, I decided to use a genetic algorithm (GA) to solve mazes using a robotic arm.\n\nIt was an attempt to shoehorn a GA into a robotics project, because I had wanted to build a meaningful GA. It was an odd choice.\n\nThe robot was built from a first-gen Lego Mindstorm. It used a “8-bit Renesas H8/300 microcontroller as its CPU”\n, and for the purposes of what I was doing, I had access to an array of 32 16-bit integers for all of my GA data.\n\nI needed a hell of a lot more discrete values than that, because each entity was a series of moves with a direction and distance. I decided that each move could be comprised of three bits: two for direction (it was a circularly-moving arm, so the four directions were clockwise, counter-clockwise, in (toward the center) and out (away from the center), and one for duration (long or short move). That meant that I could store 5 moves per int and easily achieve 160 different moves, enough for several generations of history. I could have calculated offsets and treated the whole thing as one bit 512-bit structure and had an additional 10 moves, but it wasn’t worth the effort.\n\nThere’s only one problem. This is not a good use of the most heavily-constrained resource these days, which is programmer time."", 'result': {'fake': 0.0012, 'real': 0.9988}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995277, 'subscription': 0, 'content': ""When you're dealing with resource-constrained systems, you start to realize how many computing resources we waste.\n\nIn grad school, I decided to use a genetic algorithm (GA) to solve mazes using a robotic arm.\n\nIt was an attempt to shoehorn a GA into a robotics project, because I had wanted to build a meaningful GA. It was an odd choice.\n\nThe robot was built from a first-gen Lego Mindstorm. It used a “8-bit Renesas H8/300 microcontroller as its CPU”\n, and for the purposes of what I was doing, I had access to an array of 32 16-bit integers for all of my GA data.\n\nI needed a hell of a lot more discrete values than that, because each entity was a series of moves with a direction and distance. I decided that each move could be comprised of three bits: two for direction (it was a circularly-moving arm, so the four directions were clockwise, counter-clockwise, in (toward the center) and out (away from the center), and one for duration (long or short move). That meant that I could store 5 moves per int and easily achieve 160 different moves, enough for several generations of history. I could have calculated offsets and treated the whole thing as one bit 512-bit structure and had an additional 10 moves, but it wasn’t worth the effort.\n\nThere’s only one problem. This is not a good use of the most heavily-constrained resource these days, which is programmer time."", 'aiModelVersion': '1'}",0.9988
Joshua Gross,Updated 1y,Why is Ada Lovelace known as the mother of computer if it was Al Khawarizmi who invented the algorithm?,"Ada Lovelace is not known as the ""mother of computer"", and Al Khwarizmi did not invent the algorithm.

Ada Lovelace is sometimes called the first computer programmer because she wrote some ""programs"" for Charles Babbage's Analytical Engine, which were published as an appendix to a paper she translated, with assistance from Babbage.

Algorithms existed millennia before Al Khwarizmi, as did algebra. He is often called the ""father of algebra"" for being the first to completely systematize and formalize algebra. His name is the source of the word algorithm because his book on algebra incidentally formalized the process of algorithmic solutions.

Similarly, Alan Turing didn't invent computers. He solved a mathematics problem in such a way that the major boundaries of computer science were set, and he did so with a conceptual mechanical device, putting the mathematics on even footing with engineering. Of course, that's a mouthful, so we call him the ""father of computer science"" as shorthand.

This is the problem with things like the Guinness Book of World Records and labels like ""the inventor of"". In many cases, the person we imagine to be the original inventor is remembered for having made something more practical or available or usable.

All three of these people made contributions. Al Khwarizmi is one of the most important scholars in history. These labels wouldn't change that.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6yifdrnut50m7vl4', 'title': 'Why is Ada Lovelace known as the mother of computer if it was Al Khawarizmi who invented the algorithm?', 'score': {'original': 0.8981, 'ai': 0.1019}, 'blocks': [{'text': 'Ada Lovelace is not known as the ""mother of computer"", and Al Khwarizmi did not invent the algorithm.\n\nAda Lovelace is sometimes called the first computer programmer because she wrote some ""programs"" for Charles Babbage\'s Analytical Engine, which were published as an appendix to a paper she translated, with assistance from Babbage.\n\nAlgorithms existed millennia before Al Khwarizmi, as did algebra. He is often called the ""father of algebra"" for being the first to completely systematize and formalize algebra. His name is the source of the word algorithm because his book on algebra incidentally formalized the process of algorithmic solutions.\n\nSimilarly, Alan Turing didn\'t invent computers. He solved a mathematics problem in such a way that the major boundaries of computer science were set, and he did so with a conceptual mechanical device, putting the mathematics on even footing with engineering. Of course, that\'s a mouthful, so we call him the ""father of computer science"" as shorthand.\n\nThis is the problem with things like the Guinness Book of World Records and labels like ""the inventor of"". In many cases, the person we imagine to be the original inventor is remembered for having made something more practical or available or usable.\n\nAll three of these people made contributions. Al Khwarizmi is one of the most important scholars in history. These labels wouldn\'t change that.', 'result': {'fake': 0.7978, 'real': 0.2022}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995274, 'subscription': 0, 'content': 'Ada Lovelace is not known as the ""mother of computer"", and Al Khwarizmi did not invent the algorithm.\n\nAda Lovelace is sometimes called the first computer programmer because she wrote some ""programs"" for Charles Babbage\'s Analytical Engine, which were published as an appendix to a paper she translated, with assistance from Babbage.\n\nAlgorithms existed millennia before Al Khwarizmi, as did algebra. He is often called the ""father of algebra"" for being the first to completely systematize and formalize algebra. His name is the source of the word algorithm because his book on algebra incidentally formalized the process of algorithmic solutions.\n\nSimilarly, Alan Turing didn\'t invent computers. He solved a mathematics problem in such a way that the major boundaries of computer science were set, and he did so with a conceptual mechanical device, putting the mathematics on even footing with engineering. Of course, that\'s a mouthful, so we call him the ""father of computer science"" as shorthand.\n\nThis is the problem with things like the Guinness Book of World Records and labels like ""the inventor of"". In many cases, the person we imagine to be the original inventor is remembered for having made something more practical or available or usable.\n\nAll three of these people made contributions. Al Khwarizmi is one of the most important scholars in history. These labels wouldn\'t change that.', 'aiModelVersion': '1'}",0.8981
Dave Haynie,1y,What was the operating system like in computers such as the ZX Spectrum or Commodore 64?,"Let’s start with something I know a little about. Not every early computer did the same thing, but even in early machines, there was usually some kind of BIOS or operating system, in the sense of CP/M’s BDOS or the MS-DOS function table. The Exidy Sorcerer My first personal computer was the relatively unknown Exidy Sorcerer in 1979.The Sorcerer was based on a Z-80 processor running at a paltry 2.5MHz. It was one of the first personal computers that could display 64x30 characters (512x240 pixels) on a TV screen, though more in theory than in practice. You could get a decent image with a high quality UHF RF modulator, but I wound up hacking a composite input into my 13″ B&W television in order to get a great image. The Sorcerer had a built-in 4K ROM that contained a basic machine language monitor with keyboard interface and what you could arguably call a BIOS. There were system calls that could manage I/O to RS-232 or tape, things like that. It relied on the CP/M addition to do anything with floppy discs, so in some ways it was less sophisticated out of the box than the Commodore PET/CBM systems of the day, all of which supported floppys as they came. The Sorcerer was also one of the first machines to put BASIC on a cartridge. My system came with 8K basic on a cartridge, though the cartridges could support up to 16K. They used modified plastic from 8-Track tape cartridges for the ROM carts, so they were quite roomy! The only cartridges I know of making it to market was a dedicated word processor and an assembler. The Commodore 64 My Sorcerer was stolen from a house I was renovating for my Mom’s company back in 1983. Since they were impossible to find, I replaced mine with a Commodore 64. A few months later, I was hired to design computers for Commodore! While the C64 was already out, I was the #2 hardware engineering on the Commodore 128, working under the great Bil Herd. I was more or less in charge of overseeing the last version of the Commodore 64, the “E” board as we called it, in the mid 1980s. Most of the engineering had been done by our Japanese office, and I had my hands full with Amiga things by then.You can actually see it here! These are the Commodore 64 ROMs, which contain, respectively, BASIC (8K), Kernal (8K), and character set (4K). The Kernal is the operating system, such as it is. The story goes that they had originally just called it a “kernel” but but my good buddy Bob Russell misspelled it “kernal” in his notes on the VIC-20 as it was being created, and that made its way to the documentation. The Commodore 64 is, as you might expect, fairly primitive compared to modern computer operating systems. It’s more like CP/M BDOS or MS-DOS, in that there are a small number of functions, a total of 39, that are accessed via a jump table (MS-DOS 1.0 had about 45 functions, CP/M 2.0 had about 40 BDOS functions).Commodore BASIC V2.0 was built on-top of the Kernal, and the machine booted you directly into the BASIC interpreter prompt. So there was not much in the way of an operating system defined user interface, much less a GUI or anything like that. The C64’s serial bus was basically a serialized version of the IEEE488 bus used on the Commodore PET computers. The Kernal could do quite a bit in a little code because every serial bus peripheral was an intelligent peripheral. Your printer, your floppy drive, etc. had its own 6502 or similar processor, its own RAM, ROM, etc. However, in 1986 Berkeley Softworks introduced the GEOS operating system for the Commodore 64. This also ran on top of the C64 Kernal, but offered a real graphic user interface, a whole new of GEOS system calls (which they also called a kernal), GUI shell, and a basic set of GUI-based apps. GEOS 64 was made some version of freeware a few decades ago. The Sinclair ZX Spectrum I don’t know the Spectrum that well, but I did look up some information.The Spectrum seems to have a single 16K ROM that contains character set, BASIC, and a set of system functions pretty similar to the idea of the C64 Kernal. It was a direct but fairly huge expansion of the system design used in the ZX81. Not bad for a much lower cost computer than the C64, both introduced in 1982. Read MoreExidy Sorcerer computerExidy Sorcerer Released: 1978 Model: DP1000-1, DP1000-2 Price: US$895 with 8K RAM US$1150 with 16K RAM US$1395 with 32K RAM CPU: Zilog Z-80 @ 2MHz RAM: 8K, 16K, 32K Display: composite video (B&W) 512 x 240, 64 x 30 text Ports: composite video, cassette serial, parallel, cart, bus Storage external cassette optional floppy drive OS: 'Monitor' in ROM BASIC, CP/M in cart or floppyhttp://oldcomputers.net/sorcerer.html Exidy Sorcerer MicrocomputerExidy, a leading manufacturer of home and arcade video games, introduced its Sorcerer home computer in 1978. The Sorcer used a Z-80 processor that ran at 2.106 MHz and came with 8 KB of RAM, which could be expanded to 32 KB. It ran the CP/M operating system.https://americanhistory.si.edu/collections/search/object/nmah_334653 Commodore 64 - The Best Selling Computer In Historyby Ian Matthews of Commodore.ca May 19, 2003  – Revised Feb 1, 2020 Sections on this page: Commodore 64 History Commodore 64 In Browser Emulator Commodore 64 Chronology Commodore 64 Manuals, Magazine Articles, Announcements, Videos and Advertisements Commodore 64 Prototype Slide Show Commodore 64 Picture Gallery Commodore 64 Video Review From 2018 Commodore 64: Machine of Destiny The  64 began its design life in January of 1981 when MOS Technology engineers decided they needed a new chip project. MOS’ Albert Charpentier had been responsible for several of the highly successful VIC-20 chips. “We were fresh out of ideas for whatever chips the rest of the world might want us to do. So we decided to produce a state-of-the-art video and sound chips for the worlds next great video game”. By November of 1981, the chips were completed but Commodore’s president Jack Tramiel decided against using them in the faltering arcade game market .  Instead he tasked the engineers with developing a 64 kilobyte home computer for show at the Winter Consumer Electronics Show (CES) the second week  of January 1982; just 6 weeks away. Two days after Jacks request, the basic design was completed and by the end of December 1981 the hardware for five VIC-30 (the C64’s development name) prototypes were assembled. In the remaining two weeks, the VIC-20 operating system with lowly Commodore Basic 2.0 was stretched onto the C64.  With an estimated retail price of just $595 ($1250 dollars in 2018), it was the buzz of the show.  It did not hurt that there were no other new powerful computers shown at CES by Commodores competitors that year.  The Commodore 64 was alive: it was immediately ordered into production which hit full stride by August 1982. In addition to being vastly more powerful than anything on the market at the time, it was drastically cheaper than its competitors like the Apple II, IBM PC , or Radio Shack TRS-80 . (Click the advert on the right.) COMPUTERS FOR EVERYBODY, PRICED FOR NOBODY – C64 for under $600 – c64 compared to Radio Shack TRS 80, IBM PC JR, and Apple IIc – Compute! August 1983 The Commodore 64 is arguably the easiest to use programmable computer that has ever been made.  Like the PET and VIC-20 before it, the 64 booted to a friendly screen with the Commodore Basic Operating System ready and waiting for instruction.  If writing your own programs was too daunting and loading software from cassettes or floppies was ‘just too much’ for you, you could just jam a cartridge in the back of the unit and like magic your machine was doing whatever you wanted it to. Creating the best selling machine in history is no small feat.  Commodore did not ‘knock the ball out of the park’, they ‘knocked the park into the next city’.  The pushed the industry to a level of scale that was previously thought impossible. Like its VIC-20 predecessor, the 64 was the first computer that millions of today’s programmers, designers, engineers and enthusiasts had ever used.  It has https://www.commodore.ca/commodore-products/commodore-64-the-best-selling-computer-in-history/ GEOS for the Commodore 64GEOS for the Commodore 64 (last updated 2023-11-04) GEOS desktop (click to enlarge) the ""GEOS lobster"" (click to enlarge) fonts in geoWrite (click to enlarge) GEOS (Graphic Environment Operating System) is a disk-based GUI
operating system for the Commodore 64, released by Berkeley Softworks
in 1986. It includes features like pull-down menus, icons, dialog
boxes, and proportional fonts. It also has extensive drag and drop
capability (e.g. drag a data file to the printer icon to print it, to
a disk icon to copy it, or to the trashcan to delete it). Some of its
features are still considered modern, like the ability to double-click
a data file and have it load in its parent application regardless of
name (i.e. no file extensions or magic numbers). Later enhancements
made it possible to use RAM expanders, CPU accelerators, hard drives,
and more. Berkeley's GEOS 2.0 brochure gives a good impression of what GEOS is like and how it was
marketed. And here's their winter
1988 product
catalog . On these pages you can find everything you need to use GEOS on a
Commodore 64: the operating system and applications (all cracked),
programming tools and tips, and a few miscellaneous goodies (including
a comprehensive analysis of GEOS fonts that includes a PDF catalog with samples of
over 1000 of them). This isn't the entire contents of my collection,
so if there's something you're looking for, email me
(cenbe at protonmail dot com) and
I'll see if I have it. But please note: the main purpose of these
pages is not to provide a comprehensive archive of GEOS
software. And I've never owned a 128, so if that's what you've
got, you're on your own! A word about system resources: GEOS pushes the Commodore 64
to its limits as far as hardware and memory usage are concerned;
little bits of code and data are constantly being swapped in from
disk. You can run it from a single 1541 drive, but you'll be sorry
(remember the days of ""Bad or missing COMMAND.COM""?). A 1541 and a
1581 or CMD FD are better, and a CMD HD is a joy to use (and well
supported). An even faster solution is to get a RAM expansion unit
(REU or geoRAM), and set it up as a RAMdisk using the CONFIGURE
program. You can work from the RAMdisk and copy your data back to the
""real"" disk every so often during your session; the speed is really
amazing even without an accelerator like the TurboMaster or
SuperCPU. If you are using more than one type of drive without a
RAM expander, you will need to have a copy of CONFIGURE on every disk
that contains the deskTop (see GEOS 2.0
manual , page 27). This is because the disk drivers are normally
loaded from CONFIGURE, but if you are using a RAM expander, they are
cached there. This is one of the reasons why a RAM expander is
strongly recommended to work with GEOS. (If you don't have a real '64
with an REU, the Ultimate 64 implements one of up to 16M. There is also a modern clone of the
geoRAM made by GGLABS ; a
one-megabyte version called GRAM/1D can be found
on eBay .) back thttps://www.lyonlabs.org/commodore/onrequest/geos/index.html Commodore 128 - The Most Versatile 8-Bit Computer Ever Madeby Ian Matthews of Commodore.ca   July 11 2003 – Revised Dec 19 2022 On this page: Commodore 128 History: Chips, Peripherals, 128D, People, Wrap up Commodore 128 Reference Materials: Manuals, Videos & Magazine Articles Commodore 128 Chronology Commodore 128 Photo Gallery Commodore 128 Review Video – with Special Guest Bil Herd – 2018 Commodore 128 History: In the summer of 1984 Commodore decided that they needed a replacement for the amazingly successful C64 .  More accurately, they decided that the TED / 116 / Plus/4 / 264 Series was a failure as a replacement for the C64. This machine would be Commodores last 8-Bit computer; after this they would produce only 16/32 Bit Amiga’s and IBM PC clones . Customer reaction to Commodores failure to provide native CP/M support in the C64 and (much worse) their failure to provide C64 compatibility in the Plus/4 / 264 Series taught Commodore engineers some hard lessons.  Commodore’s founder and visionary, Jack Tramiel , had quit months earlier and the new management team wanted to just forget the Plus/4 / 264 Series fiasco. The engineers knew they needed a new product and that product had better be compatible with the best selling computer in the world, the C64. Bil Herd got the top job as 128 lead Engineer because of his vocal criticism of the new management teams lack of vision: “No one dreamed that C64 compatibility was possible so, no one thought along those lines. I had decided to make the next machine compatible with something instead of yet another incompatible CBM machine. (I won’t go into the “yes Virginia there is Compatibility” memo that I wrote that had the lawyers many years later still chuckling, suffice it to say I made some fairly brash statements regarding my opinion of product strategy).  Consequently, I was allowed/forced to put my money where my mouth was and I took over the C128 project.” The first C128 concept machines (pre-prototype) made no attempt at C64 compatibility as per Commodore managements instructions.  Bil recalls: “I looked at the existing schematics once and then started with a new design based on C64’ness. The manager of the chip group approached me and said they had a color version of the 6845 if I was interested and it would definitely be done in time having been worked on already for a year and a half… And so the story begins.” Commodore SuperPET Brochure Cover – 5 Languages, two Processors – $2795 ($8600 in 2023 dollars) Commodore needed its next computer to be a serious upgrade from the C64 if it was to successfully battle its arch nemesis; it needed to keep Jack Tramiel’s, Atari , from besting them with features in the their rumored new “ST” line . In 1982 , Commodore released the worlds first multi-processor personal computer, called SuperPET , but it was $2800 ($8600 in 2023 dollars) and targeted at the education / scientific markets.  The Commodore 128 was to be the worlds first mass market multi-processor computer.  It would also have two video subsystems, one https://www.commodore.ca/commodore-products/commodore-128-the-most-versatile-8-bit-computer-ever-made/ ZX Spectrum: 40 Years of the Punk Rock PCThe ZX Spectrum is 40 years old! Here's how the strange little 80s computer changed the worldhttps://www.techadvisor.com/article/746263/zx-spectrum-at-40-why-its-the-most-important-computer-in-history.html ZX Spectrum at 40: a look backAs the ZX Spectrum turns 40, we look back at the monumental influence it had on the games industry in the UK.https://www.nme.com/features/gaming-features/zx-spectrum-at-40-a-look-back-3162913C64 MoreC64 JavaScript EmulatorOverview 2020 Bugfixes, WebAssembly Deployment, Raspberry GBM Support, Snapshot Folder Selection, Android 10 Support January 2020 Again some improvements for all platforms. The C64 JavaScript Emulator will be provided as WebAssembly from now on. July 2019 The new Raspberry 4 is available and here my new C64 Emulator Version. June 2019 Have a look in the new C64 SID Music Corner . September 2018 Complete C64 SID sound rework for all platforms. March 2018 HTTPS/SSL changeover for website and Android App. December 2017 New multiplayer network mode for the C64 Emulator on Windows, Raspberry and Android. Play together on different devices. November 2017 Couldn't resist: I would like to present my new C64 Emulator Android App . October 2017 A new version of the JavaScript Emulator is available. Enhanced mobile support and file drop. June-August 2017 Web page rework: Rating, Game Help, Highscore, Documentation, D64 Directory . Oct 28, 2016 The first version of the C64 Emulator for the Raspberry PI . Jan 4, 2016 A new year and some new improvements . Oct 18, 2015 Full Floppy 1541 write support and included disk editor Apr 5, 2015 One of my biggest goals are reached: The accurate emulation of IK+ Jan 25 / Mar 19, 2015 The C64 Emulator is now direct executable in the browser . Mar 2 / Aug 29 / Oct 20, 2014 C64 Emulator enhancements and bug fixes. Version changes . September 22, 2013 Some C64 Emulator bug fixes and new features. See here for version changes.https://c64emulator.111mb.de/index.php?site=pp_javascript&lang=en&group=c64 Commodore 64 online emulator on Virtual ConsolesThe C64 online emulator is a fully functional emulator supporting all the well accepted file formats. Just drag & drop your files and play with retro games.https://virtualconsoles.com/online-emulators/c64/ All-in-One C64 Emulator, Games, Demoscene and SupportC64 Forever 10: 
Create & Play Download , Free Express Edition Buy Now , From $9.95 When the C64 was launched by Commodore in 1982 it immediately 
set the standard for 8-bit home computers. Its low 
cost, superior graphics, high quality sound and a 
massive 64 KB of RAM positioned it as the winner in 
the home computer wars, knocking out competitors 
from the likes of Atari, Texas Instruments, 
Sinclair, Apple and IBM. Selling over 30 million units and introducing a 
whole generation to computers and programming, the 
C64 shook up the video games industry and sparked 
cultural phenomena such as computer music and the 
demoscene. In recent years the C64 has enjoyed a 
spectacular revival manifesting itself once again as 
a retrocomputing platform. To allow you to experience and relive the wonders 
of this unique computer, Cloanto, developers of 
Commodore/Amiga software since the 1980s, has 
introduced C64 Forever, the official CBM 8-bit 
preservation, emulation and support package. C64 
Forever embodies an intuitive player interface, 
backed by a built-in database containing more than 
5,000 C64 game entries. Support for the innovative 
RP9 format allows for advanced title authoring, 
preservation and 
easy cross-platform playback.https://www.c64forever.com/","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/tukqs3y86j0f7i19', 'title': 'What was the operating system like in computers such as the ZX Spectrum or Commodore 64?', 'score': {'original': 0.8289, 'ai': 0.1711}, 'blocks': [{'text': 'Let’s start with something I know a little about. Not every early computer did the same thing, but even in early machines, there was usually some kind of BIOS or operating system, in the sense of CP/M’s BDOS or the MS-DOS function table. The Exidy Sorcerer My first personal computer was the relatively unknown Exidy Sorcerer in 1979.The Sorcerer was based on a Z-80 processor running at a paltry 2.5MHz. It was one of the first personal computers that could display 64x30 characters (512x240 pixels) on a TV screen, though more in theory than in practice. You could get a decent image with a high quality UHF RF modulator, but I wound up hacking a composite input into my 13″ B&W television in order to get a great image. The Sorcerer had a built-in 4K ROM that contained a basic machine language monitor with keyboard interface and what you could arguably call a BIOS. There were system calls that could manage I/O to RS-232 or tape, things like that. It relied on the CP/M addition to do anything with floppy discs, so in some ways it was less sophisticated out of the box than the Commodore PET/CBM systems of the day, all of which supported floppys as they came. The Sorcerer was also one of the first machines to put BASIC on a cartridge. My system came with 8K basic on a cartridge, though the cartridges could support up to 16K. They used modified plastic from 8-Track tape cartridges for the ROM carts, so they were quite roomy! The only cartridges I know of making it to market was a dedicated word processor and an assembler. The Commodore 64 My Sorcerer was stolen from a house I was renovating for my Mom’s company back in 1983. Since they were impossible to find, I replaced mine with a Commodore 64. A few months later, I was hired to design computers for Commodore! While the C64 was already out, I was the #2 hardware engineering on the Commodore 128, working under the great Bil Herd. I was more or less in charge of overseeing the last version of the Commodore 64, the “E” board as we called it, in the mid 1980s. Most of the engineering had been done by our Japanese office, and I had my hands full with Amiga things by then.You can actually see it here! These are the Commodore 64 ROMs, which contain, respectively, BASIC (8K), Kernal (8K), and character set (4K). The Kernal is the operating system, such as it is. The story goes that they had originally just called it a “kernel” but but my good buddy Bob Russell misspelled it “kernal” in his notes on the VIC-20 as it was being created, and that made its way to the documentation. The Commodore 64 is, as you might expect, fairly primitive compared to modern computer operating systems. It’s more like CP/M BDOS or MS-DOS, in that there are a small number of functions, a total of 39, that are accessed via a jump table (MS-DOS 1.0 had about', 'result': {'fake': 0.839, 'real': 0.161}, 'status': 'success'}, {'text': ""45 functions, CP/M 2.0 had about 40 BDOS functions).Commodore BASIC V2.0 was built on-top of the Kernal, and the machine booted you directly into the BASIC interpreter prompt. So there was not much in the way of an operating system defined user interface, much less a GUI or anything like that. The C64’s serial bus was basically a serialized version of the IEEE488 bus used on the Commodore PET computers. The Kernal could do quite a bit in a little code because every serial bus peripheral was an intelligent peripheral. Your printer, your floppy drive, etc. had its own 6502 or similar processor, its own RAM, ROM, etc. However, in 1986 Berkeley Softworks introduced the GEOS operating system for the Commodore 64. This also ran on top of the C64 Kernal, but offered a real graphic user interface, a whole new of GEOS system calls (which they also called a kernal), GUI shell, and a basic set of GUI-based apps. GEOS 64 was made some version of freeware a few decades ago. The Sinclair ZX Spectrum I don’t know the Spectrum that well, but I did look up some information.The Spectrum seems to have a single 16K ROM that contains character set, BASIC, and a set of system functions pretty similar to the idea of the C64 Kernal. It was a direct but fairly huge expansion of the system design used in the ZX81. Not bad for a much lower cost computer than the C64, both introduced in 1982. Read MoreExidy Sorcerer computerExidy Sorcerer Released: 1978 Model: DP1000-1, DP1000-2 Price: US$895 with 8K RAM US$1150 with 16K RAM US$1395 with 32K RAM CPU: Zilog Z-80 @ 2MHz RAM: 8K, 16K, 32K Display: composite video (B&W) 512 x 240, 64 x 30 text Ports: composite video, cassette serial, parallel, cart, bus Storage external cassette optional floppy drive OS: 'Monitor' in ROM BASIC, CP/M in cart or floppyhttp://oldcomputers.net/sorcerer.html Exidy Sorcerer MicrocomputerExidy, a leading manufacturer of home and arcade video games, introduced its Sorcerer home computer in 1978. The Sorcer used a Z-80 processor that ran at 2.106 MHz and came with 8 KB of RAM, which could be expanded to 32 KB. It ran the CP/M operating system.https://americanhistory.si.edu/collections/search/object/nmah_334653 Commodore 64 - The Best Selling Computer In Historyby Ian Matthews of Commodore.ca May 19, 2003\xa0 – Revised Feb 1, 2020 Sections on this page: Commodore 64 History Commodore 64 In Browser Emulator Commodore 64 Chronology Commodore 64 Manuals,\xa0Magazine Articles, Announcements, Videos and Advertisements Commodore 64 Prototype Slide Show Commodore 64 Picture\xa0Gallery Commodore 64 Video Review From 2018 Commodore 64: Machine of Destiny The\xa0 64 began its design life in January of 1981 when MOS Technology engineers decided they needed a new chip project. MOS’ Albert Charpentier had been responsible for several of the highly successful VIC-20 chips. “We were fresh out of ideas for whatever chips the rest of the world might want us to do. So we decided to produce a state-of-the-art video and sound chips for the worlds next great video game”. By November of 1981, the chips were completed but Commodore’s president\xa0Jack"", 'result': {'fake': 0.1129, 'real': 0.8871}, 'status': 'success'}, {'text': 'Tramiel decided against using them in the faltering arcade game market .\xa0 Instead he tasked the engineers with developing a 64 kilobyte home computer for show at the Winter Consumer Electronics Show (CES) the second week\xa0 of January 1982; just 6 weeks away. Two days after Jacks request, the basic design was completed and by the end of December 1981 the hardware for five VIC-30 (the C64’s development name) prototypes were assembled. In the remaining two weeks, the VIC-20 operating system with lowly Commodore Basic 2.0 was stretched onto the C64.\xa0 With an estimated retail price of just $595 ($1250 dollars in 2018), it was the buzz of the show.\xa0 It did not hurt that there were no other new powerful computers shown at CES by Commodores competitors that year.\xa0 The Commodore 64 was alive: it was immediately ordered into production which hit full stride by August 1982. In addition to being vastly more powerful than anything on the market at the time, it was drastically cheaper than its competitors like the Apple II, IBM PC , or Radio Shack TRS-80 . (Click the advert on the right.) COMPUTERS FOR EVERYBODY, PRICED FOR NOBODY – C64 for under $600 – c64 compared to Radio Shack TRS 80, IBM PC JR, and Apple IIc – Compute! August 1983 The Commodore 64 is arguably the easiest to use programmable computer that has ever been made.\xa0 Like the PET and VIC-20 before it, the 64 booted to a friendly screen with the Commodore Basic Operating System ready and waiting for instruction.\xa0 If writing your own programs was too daunting and loading software from cassettes or floppies was ‘just too much’ for you, you could just jam a cartridge in the back of the unit and like magic your machine was doing whatever you wanted it to. Creating the best selling machine in history is no small feat.\xa0 Commodore did not ‘knock the ball out of the park’, they ‘knocked the park into the next city’.\xa0 The pushed the industry to a level of scale that was previously thought impossible. Like its VIC-20 predecessor, the 64 was the first computer that millions of today’s programmers, designers, engineers and enthusiasts had ever used.\xa0 It has https://www.commodore.ca/commodore-products/commodore-64-the-best-selling-computer-in-history/ GEOS for the Commodore 64GEOS for the Commodore 64 (last updated 2023-11-04) GEOS desktop (click to enlarge) the ""GEOS lobster"" (click to enlarge) fonts in geoWrite (click to enlarge) GEOS (Graphic Environment Operating System) is a disk-based GUI\noperating system for the Commodore 64, released by Berkeley Softworks\nin 1986. It includes features like pull-down menus, icons, dialog\nboxes, and proportional fonts. It also has extensive drag and drop\ncapability (e.g. drag a data file to the printer icon to print it, to\na disk icon to copy it, or to the trashcan to delete it). Some of its\nfeatures are still considered modern, like the ability to double-click\na data file and have it load in its parent application regardless of\nname (i.e. no file extensions or magic numbers). Later enhancements\nmade it possible to use RAM expanders, CPU accelerators, hard drives,\nand more. Berkeley\'s GEOS 2.0 brochure gives a', 'result': {'fake': 0.0196, 'real': 0.9804}, 'status': 'success'}, {'text': 'good impression of what GEOS is like and how it was\nmarketed. And here\'s their winter\n1988 product\ncatalog . On these pages you can find everything you need to use GEOS on a\nCommodore 64: the operating system and applications (all cracked),\nprogramming tools and tips, and a few miscellaneous goodies (including\na comprehensive analysis of GEOS fonts that includes a PDF catalog with samples of\nover 1000 of them). This isn\'t the entire contents of my collection,\nso if there\'s something you\'re looking for, email me\n(cenbe at protonmail dot com) and\nI\'ll see if I have it. But please note: the main purpose of these\npages is not to provide a comprehensive archive of GEOS\nsoftware. And I\'ve never owned a 128, so if that\'s what you\'ve\ngot, you\'re on your own! A word about system resources: GEOS pushes the Commodore 64\nto its limits as far as hardware and memory usage are concerned;\nlittle bits of code and data are constantly being swapped in from\ndisk. You can run it from a single 1541 drive, but you\'ll be sorry\n(remember the days of ""Bad or missing COMMAND.COM""?). A 1541 and a\n1581 or CMD FD are better, and a CMD HD is a joy to use (and well\nsupported). An even faster solution is to get a RAM expansion unit\n(REU or geoRAM), and set it up as a RAMdisk using the CONFIGURE\nprogram. You can work from the RAMdisk and copy your data back to the\n""real"" disk every so often during your session; the speed is really\namazing even without an accelerator like the TurboMaster or\nSuperCPU. If you are using more than one type of drive without a\nRAM expander, you will need to have a copy of CONFIGURE on every disk\nthat contains the deskTop (see GEOS 2.0\nmanual , page 27). This is because the disk drivers are normally\nloaded from CONFIGURE, but if you are using a RAM expander, they are\ncached there. This is one of the reasons why a RAM expander is\nstrongly recommended to work with GEOS. (If you don\'t have a real \'64\nwith an REU, the Ultimate 64 implements one of up to 16M. There is also a modern clone of the\ngeoRAM made by GGLABS ; a\none-megabyte version called GRAM/1D can be found\non eBay .) back thttps://www.lyonlabs.org/commodore/onrequest/geos/index.html Commodore 128 - The Most Versatile 8-Bit Computer Ever Madeby Ian Matthews of Commodore.ca \xa0 July 11 2003 – Revised Dec 19 2022 On this page: Commodore 128 History: Chips, Peripherals, 128D, People, Wrap up Commodore 128 Reference Materials: Manuals, Videos & Magazine Articles Commodore 128 Chronology Commodore 128 Photo Gallery Commodore 128 Review Video – with Special Guest Bil Herd – 2018 Commodore 128 History: In the summer of 1984 Commodore decided that they needed a replacement for the amazingly successful C64 .\xa0 More accurately, they decided that the TED / 116 / Plus/4 / 264 Series was a failure as a replacement for the C64. This machine would be Commodores last 8-Bit computer; after this they would produce only 16/32 Bit Amiga’s and IBM PC clones . Customer reaction to Commodores failure to provide native CP/M support in the C64 and\xa0(much worse) their failure to provide C64 compatibility in', 'result': {'fake': 0.0028, 'real': 0.9972}, 'status': 'success'}, {'text': ""the Plus/4 / 264 Series taught\xa0Commodore engineers\xa0some hard lessons.\xa0 Commodore’s founder and visionary, Jack Tramiel , had quit months earlier and the new management team wanted to just forget the Plus/4 / 264 Series fiasco. The engineers knew they needed a new product and that product had better be compatible with the best selling computer in the world, the C64. Bil Herd got the top job as 128 lead Engineer because of his vocal criticism of the new management teams lack of vision: “No one dreamed that C64 compatibility was possible so, no one thought along those lines. I had decided to make the next machine compatible with something instead of yet another incompatible CBM machine. (I won’t go into the “yes Virginia there is Compatibility” memo that I wrote that had the lawyers many years later still chuckling, suffice it to say I made some fairly brash statements regarding my opinion of product strategy).\xa0 Consequently, I was allowed/forced to put my money where my mouth was and I took over the C128 project.” The first C128 concept machines (pre-prototype) made no attempt at C64 compatibility as per Commodore managements instructions.\xa0 Bil recalls: “I looked at the existing schematics once and then started with a new design based on C64’ness. The manager of the chip group approached me and said they had a color version of the 6845 if I was interested and it would definitely be done in time having been worked on already for a year and a half… And so the story begins.” Commodore SuperPET Brochure Cover – 5 Languages, two Processors – $2795 ($8600 in 2023 dollars) Commodore needed its next computer to be a serious upgrade from the C64 if it was to successfully battle its arch nemesis; it needed to keep Jack Tramiel’s,\xa0Atari ,\xa0from besting them with features in the their rumored new “ST” line . In 1982 , Commodore released the worlds\xa0first multi-processor personal computer, called SuperPET , but it was $2800 ($8600 in 2023 dollars) and targeted at the education / scientific markets.\xa0 The Commodore 128 was to be the worlds first mass market multi-processor computer.\xa0 It would also have two video subsystems, one https://www.commodore.ca/commodore-products/commodore-128-the-most-versatile-8-bit-computer-ever-made/ ZX Spectrum: 40 Years of the Punk Rock PCThe ZX Spectrum is 40 years old! Here's how the strange little 80s computer changed the worldhttps://www.techadvisor.com/article/746263/zx-spectrum-at-40-why-its-the-most-important-computer-in-history.html ZX Spectrum at 40: a look backAs the ZX Spectrum turns 40, we look back at the monumental influence it had on the games industry in the UK.https://www.nme.com/features/gaming-features/zx-spectrum-at-40-a-look-back-3162913C64 MoreC64 JavaScript EmulatorOverview 2020 Bugfixes, WebAssembly Deployment, Raspberry GBM Support, Snapshot Folder Selection, Android 10 Support January 2020 Again some improvements for all platforms. The C64 JavaScript Emulator will be provided as WebAssembly from now on. July 2019 The new Raspberry 4 is available and here my new C64 Emulator Version. June 2019 Have a look in the new C64 SID Music Corner . September 2018 Complete C64 SID sound rework for all platforms. March 2018 HTTPS/SSL changeover for website and Android App. December 2017 New multiplayer network mode for the C64 Emulator on Windows, Raspberry"", 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}, {'text': ""and Android. Play together on different devices. November 2017 Couldn't resist: I would like to present my new C64 Emulator Android App . October 2017 A new version of the JavaScript Emulator is available. Enhanced mobile support and file drop. June-August 2017 Web page rework: Rating, Game Help, Highscore, Documentation, D64 Directory . Oct 28, 2016 The first version of the C64 Emulator for the Raspberry PI . Jan 4, 2016 A new year and some new improvements . Oct 18, 2015 Full Floppy 1541 write support and included disk editor Apr 5, 2015 One of my biggest goals are reached: The accurate emulation of IK+ Jan 25 / Mar 19, 2015 The C64 Emulator is now direct executable in the browser . Mar 2 / Aug 29 / Oct 20, 2014 C64 Emulator enhancements and bug fixes. Version changes . September 22, 2013 Some C64 Emulator bug fixes and new features. See here for version changes.https://c64emulator.111mb.de/index.php?site=pp_javascript&lang=en&group=c64 Commodore 64 online emulator on Virtual ConsolesThe C64 online emulator is a fully functional emulator supporting all the well accepted file formats. Just drag & drop your files and play with retro games.https://virtualconsoles.com/online-emulators/c64/ All-in-One C64 Emulator, Games, Demoscene and SupportC64 Forever 10: \r\nCreate & Play Download , Free Express Edition Buy Now , From $9.95 When the C64 was launched by Commodore in 1982 it immediately \r\nset the standard for 8-bit home computers. Its low \r\ncost, superior graphics, high quality sound and a \r\nmassive 64 KB of RAM positioned it as the winner in \r\nthe home computer wars, knocking out competitors \r\nfrom the likes of Atari, Texas Instruments, \r\nSinclair, Apple and IBM. Selling over 30 million units and introducing a \r\nwhole generation to computers and programming, the \r\nC64 shook up the video games industry and sparked \r\ncultural phenomena such as computer music and the \r\ndemoscene. In recent years the C64 has enjoyed a \r\nspectacular revival manifesting itself once again as \r\na retrocomputing platform. To allow you to experience and relive the wonders \r\nof this unique computer, Cloanto, developers of \r\nCommodore/Amiga software since the 1980s, has \r\nintroduced C64 Forever, the official CBM 8-bit \r\npreservation, emulation and support package. C64 \r\nForever embodies an intuitive player interface, \r\nbacked by a built-in database containing more than \r\n5,000 C64 game entries. Support for the innovative \r\nRP9 format allows for advanced title authoring, \r\npreservation and \r\neasy cross-platform playback.https://www.c64forever.com/"", 'result': {'fake': 0.0095, 'real': 0.9905}, 'status': 'success'}], 'credits_used': 30, 'credits': 1995244, 'subscription': 0, 'content': 'Let’s start with something I know a little about. Not every early computer did the same thing, but even in early machines, there was usually some kind of BIOS or operating system, in the sense of CP/M’s BDOS or the MS-DOS function table. The Exidy Sorcerer My first personal computer was the relatively unknown Exidy Sorcerer in 1979.The Sorcerer was based on a Z-80 processor running at a paltry 2.5MHz. It was one of the first personal computers that could display 64x30 characters (512x240 pixels) on a TV screen, though more in theory than in practice. You could get a decent image with a high quality UHF RF modulator, but I wound up hacking a composite input into my 13″ B&W television in order to get a great image. The Sorcerer had a built-in 4K ROM that contained a basic machine language monitor with keyboard interface and what you could arguably call a BIOS. There were system calls that could manage I/O to RS-232 or tape, things like that. It relied on the CP/M addition to do anything with floppy discs, so in some ways it was less sophisticated out of the box than the Commodore PET/CBM systems of the day, all of which supported floppys as they came. The Sorcerer was also one of the first machines to put BASIC on a cartridge. My system came with 8K basic on a cartridge, though the cartridges could support up to 16K. They used modified plastic from 8-Track tape cartridges for the ROM carts, so they were quite roomy! The only cartridges I know of making it to market was a dedicated word processor and an assembler. The Commodore 64 My Sorcerer was stolen from a house I was renovating for my Mom’s company back in 1983. Since they were impossible to find, I replaced mine with a Commodore 64. A few months later, I was hired to design computers for Commodore! While the C64 was already out, I was the #2 hardware engineering on the Commodore 128, working under the great Bil Herd. I was more or less in charge of overseeing the last version of the Commodore 64, the “E” board as we called it, in the mid 1980s. Most of the engineering had been done by our Japanese office, and I had my hands full with Amiga things by then.You can actually see it here! These are the Commodore 64 ROMs, which contain, respectively, BASIC (8K), Kernal (8K), and character set (4K). The Kernal is the operating system, such as it is. The story goes that they had originally just called it a “kernel” but but my good buddy Bob Russell misspelled it “kernal” in his notes on the VIC-20 as it was being created, and that made its way to the documentation. The Commodore 64 is, as you might expect, fairly primitive compared to modern computer operating systems. It’s more like CP/M BDOS or MS-DOS, in that there are a small number of functions, a total of 39, that are accessed via a jump table (MS-DOS 1.0 had about 45 functions, CP/M 2.0 had about 40 BDOS functions).Commodore BASIC V2.0 was built on-top of the Kernal, and the machine booted you directly into the BASIC interpreter prompt. So there was not much in the way of an operating system defined user interface, much less a GUI or anything like that. The C64’s serial bus was basically a serialized version of the IEEE488 bus used on the Commodore PET computers. The Kernal could do quite a bit in a little code because every serial bus peripheral was an intelligent peripheral. Your printer, your floppy drive, etc. had its own 6502 or similar processor, its own RAM, ROM, etc. However, in 1986 Berkeley Softworks introduced the GEOS operating system for the Commodore 64. This also ran on top of the C64 Kernal, but offered a real graphic user interface, a whole new of GEOS system calls (which they also called a kernal), GUI shell, and a basic set of GUI-based apps. GEOS 64 was made some version of freeware a few decades ago. The Sinclair ZX Spectrum I don’t know the Spectrum that well, but I did look up some information.The Spectrum seems to have a single 16K ROM that contains character set, BASIC, and a set of system functions pretty similar to the idea of the C64 Kernal. It was a direct but fairly huge expansion of the system design used in the ZX81. Not bad for a much lower cost computer than the C64, both introduced in 1982. Read MoreExidy Sorcerer computerExidy Sorcerer Released: 1978 Model: DP1000-1, DP1000-2 Price: US$895 with 8K RAM US$1150 with 16K RAM US$1395 with 32K RAM CPU: Zilog Z-80 @ 2MHz RAM: 8K, 16K, 32K Display: composite video (B&W) 512 x 240, 64 x 30 text Ports: composite video, cassette serial, parallel, cart, bus Storage external cassette optional floppy drive OS: \'Monitor\' in ROM BASIC, CP/M in cart or floppyhttp://oldcomputers.net/sorcerer.html Exidy Sorcerer MicrocomputerExidy, a leading manufacturer of home and arcade video games, introduced its Sorcerer home computer in 1978. The Sorcer used a Z-80 processor that ran at 2.106 MHz and came with 8 KB of RAM, which could be expanded to 32 KB. It ran the CP/M operating system.https://americanhistory.si.edu/collections/search/object/nmah_334653 Commodore 64 - The Best Selling Computer In Historyby Ian Matthews of Commodore.ca May 19, 2003\xa0 – Revised Feb 1, 2020 Sections on this page: Commodore 64 History Commodore 64 In Browser Emulator Commodore 64 Chronology Commodore 64 Manuals,\xa0Magazine Articles, Announcements, Videos and Advertisements Commodore 64 Prototype Slide Show Commodore 64 Picture\xa0Gallery Commodore 64 Video Review From 2018 Commodore 64: Machine of Destiny The\xa0 64 began its design life in January of 1981 when MOS Technology engineers decided they needed a new chip project. MOS’ Albert Charpentier had been responsible for several of the highly successful VIC-20 chips. “We were fresh out of ideas for whatever chips the rest of the world might want us to do. So we decided to produce a state-of-the-art video and sound chips for the worlds next great video game”. By November of 1981, the chips were completed but Commodore’s president\xa0Jack Tramiel decided against using them in the faltering arcade game market .\xa0 Instead he tasked the engineers with developing a 64 kilobyte home computer for show at the Winter Consumer Electronics Show (CES) the second week\xa0 of January 1982; just 6 weeks away. Two days after Jacks request, the basic design was completed and by the end of December 1981 the hardware for five VIC-30 (the C64’s development name) prototypes were assembled. In the remaining two weeks, the VIC-20 operating system with lowly Commodore Basic 2.0 was stretched onto the C64.\xa0 With an estimated retail price of just $595 ($1250 dollars in 2018), it was the buzz of the show.\xa0 It did not hurt that there were no other new powerful computers shown at CES by Commodores competitors that year.\xa0 The Commodore 64 was alive: it was immediately ordered into production which hit full stride by August 1982. In addition to being vastly more powerful than anything on the market at the time, it was drastically cheaper than its competitors like the Apple II, IBM PC , or Radio Shack TRS-80 . (Click the advert on the right.) COMPUTERS FOR EVERYBODY, PRICED FOR NOBODY – C64 for under $600 – c64 compared to Radio Shack TRS 80, IBM PC JR, and Apple IIc – Compute! August 1983 The Commodore 64 is arguably the easiest to use programmable computer that has ever been made.\xa0 Like the PET and VIC-20 before it, the 64 booted to a friendly screen with the Commodore Basic Operating System ready and waiting for instruction.\xa0 If writing your own programs was too daunting and loading software from cassettes or floppies was ‘just too much’ for you, you could just jam a cartridge in the back of the unit and like magic your machine was doing whatever you wanted it to. Creating the best selling machine in history is no small feat.\xa0 Commodore did not ‘knock the ball out of the park’, they ‘knocked the park into the next city’.\xa0 The pushed the industry to a level of scale that was previously thought impossible. Like its VIC-20 predecessor, the 64 was the first computer that millions of today’s programmers, designers, engineers and enthusiasts had ever used.\xa0 It has https://www.commodore.ca/commodore-products/commodore-64-the-best-selling-computer-in-history/ GEOS for the Commodore 64GEOS for the Commodore 64 (last updated 2023-11-04) GEOS desktop (click to enlarge) the ""GEOS lobster"" (click to enlarge) fonts in geoWrite (click to enlarge) GEOS (Graphic Environment Operating System) is a disk-based GUI\noperating system for the Commodore 64, released by Berkeley Softworks\nin 1986. It includes features like pull-down menus, icons, dialog\nboxes, and proportional fonts. It also has extensive drag and drop\ncapability (e.g. drag a data file to the printer icon to print it, to\na disk icon to copy it, or to the trashcan to delete it). Some of its\nfeatures are still considered modern, like the ability to double-click\na data file and have it load in its parent application regardless of\nname (i.e. no file extensions or magic numbers). Later enhancements\nmade it possible to use RAM expanders, CPU accelerators, hard drives,\nand more. Berkeley\'s GEOS 2.0 brochure gives a good impression of what GEOS is like and how it was\nmarketed. And here\'s their winter\n1988 product\ncatalog . On these pages you can find everything you need to use GEOS on a\nCommodore 64: the operating system and applications (all cracked),\nprogramming tools and tips, and a few miscellaneous goodies (including\na comprehensive analysis of GEOS fonts that includes a PDF catalog with samples of\nover 1000 of them). This isn\'t the entire contents of my collection,\nso if there\'s something you\'re looking for, email me\n(cenbe at protonmail dot com) and\nI\'ll see if I have it. But please note: the main purpose of these\npages is not to provide a comprehensive archive of GEOS\nsoftware. And I\'ve never owned a 128, so if that\'s what you\'ve\ngot, you\'re on your own! A word about system resources: GEOS pushes the Commodore 64\nto its limits as far as hardware and memory usage are concerned;\nlittle bits of code and data are constantly being swapped in from\ndisk. You can run it from a single 1541 drive, but you\'ll be sorry\n(remember the days of ""Bad or missing COMMAND.COM""?). A 1541 and a\n1581 or CMD FD are better, and a CMD HD is a joy to use (and well\nsupported). An even faster solution is to get a RAM expansion unit\n(REU or geoRAM), and set it up as a RAMdisk using the CONFIGURE\nprogram. You can work from the RAMdisk and copy your data back to the\n""real"" disk every so often during your session; the speed is really\namazing even without an accelerator like the TurboMaster or\nSuperCPU. If you are using more than one type of drive without a\nRAM expander, you will need to have a copy of CONFIGURE on every disk\nthat contains the deskTop (see GEOS 2.0\nmanual , page 27). This is because the disk drivers are normally\nloaded from CONFIGURE, but if you are using a RAM expander, they are\ncached there. This is one of the reasons why a RAM expander is\nstrongly recommended to work with GEOS. (If you don\'t have a real \'64\nwith an REU, the Ultimate 64 implements one of up to 16M. There is also a modern clone of the\ngeoRAM made by GGLABS ; a\none-megabyte version called GRAM/1D can be found\non eBay .) back thttps://www.lyonlabs.org/commodore/onrequest/geos/index.html Commodore 128 - The Most Versatile 8-Bit Computer Ever Madeby Ian Matthews of Commodore.ca \xa0 July 11 2003 – Revised Dec 19 2022 On this page: Commodore 128 History: Chips, Peripherals, 128D, People, Wrap up Commodore 128 Reference Materials: Manuals, Videos & Magazine Articles Commodore 128 Chronology Commodore 128 Photo Gallery Commodore 128 Review Video – with Special Guest Bil Herd – 2018 Commodore 128 History: In the summer of 1984 Commodore decided that they needed a replacement for the amazingly successful C64 .\xa0 More accurately, they decided that the TED / 116 / Plus/4 / 264 Series was a failure as a replacement for the C64. This machine would be Commodores last 8-Bit computer; after this they would produce only 16/32 Bit Amiga’s and IBM PC clones . Customer reaction to Commodores failure to provide native CP/M support in the C64 and\xa0(much worse) their failure to provide C64 compatibility in the Plus/4 / 264 Series taught\xa0Commodore engineers\xa0some hard lessons.\xa0 Commodore’s founder and visionary, Jack Tramiel , had quit months earlier and the new management team wanted to just forget the Plus/4 / 264 Series fiasco. The engineers knew they needed a new product and that product had better be compatible with the best selling computer in the world, the C64. Bil Herd got the top job as 128 lead Engineer because of his vocal criticism of the new management teams lack of vision: “No one dreamed that C64 compatibility was possible so, no one thought along those lines. I had decided to make the next machine compatible with something instead of yet another incompatible CBM machine. (I won’t go into the “yes Virginia there is Compatibility” memo that I wrote that had the lawyers many years later still chuckling, suffice it to say I made some fairly brash statements regarding my opinion of product strategy).\xa0 Consequently, I was allowed/forced to put my money where my mouth was and I took over the C128 project.” The first C128 concept machines (pre-prototype) made no attempt at C64 compatibility as per Commodore managements instructions.\xa0 Bil recalls: “I looked at the existing schematics once and then started with a new design based on C64’ness. The manager of the chip group approached me and said they had a color version of the 6845 if I was interested and it would definitely be done in time having been worked on already for a year and a half… And so the story begins.” Commodore SuperPET Brochure Cover – 5 Languages, two Processors – $2795 ($8600 in 2023 dollars) Commodore needed its next computer to be a serious upgrade from the C64 if it was to successfully battle its arch nemesis; it needed to keep Jack Tramiel’s,\xa0Atari ,\xa0from besting them with features in the their rumored new “ST” line . In 1982 , Commodore released the worlds\xa0first multi-processor personal computer, called SuperPET , but it was $2800 ($8600 in 2023 dollars) and targeted at the education / scientific markets.\xa0 The Commodore 128 was to be the worlds first mass market multi-processor computer.\xa0 It would also have two video subsystems, one https://www.commodore.ca/commodore-products/commodore-128-the-most-versatile-8-bit-computer-ever-made/ ZX Spectrum: 40 Years of the Punk Rock PCThe ZX Spectrum is 40 years old! Here\'s how the strange little 80s computer changed the worldhttps://www.techadvisor.com/article/746263/zx-spectrum-at-40-why-its-the-most-important-computer-in-history.html ZX Spectrum at 40: a look backAs the ZX Spectrum turns 40, we look back at the monumental influence it had on the games industry in the UK.https://www.nme.com/features/gaming-features/zx-spectrum-at-40-a-look-back-3162913C64 MoreC64 JavaScript EmulatorOverview 2020 Bugfixes, WebAssembly Deployment, Raspberry GBM Support, Snapshot Folder Selection, Android 10 Support January 2020 Again some improvements for all platforms. The C64 JavaScript Emulator will be provided as WebAssembly from now on. July 2019 The new Raspberry 4 is available and here my new C64 Emulator Version. June 2019 Have a look in the new C64 SID Music Corner . September 2018 Complete C64 SID sound rework for all platforms. March 2018 HTTPS/SSL changeover for website and Android App. December 2017 New multiplayer network mode for the C64 Emulator on Windows, Raspberry and Android. Play together on different devices. November 2017 Couldn\'t resist: I would like to present my new C64 Emulator Android App . October 2017 A new version of the JavaScript Emulator is available. Enhanced mobile support and file drop. June-August 2017 Web page rework: Rating, Game Help, Highscore, Documentation, D64 Directory . Oct 28, 2016 The first version of the C64 Emulator for the Raspberry PI . Jan 4, 2016 A new year and some new improvements . Oct 18, 2015 Full Floppy 1541 write support and included disk editor Apr 5, 2015 One of my biggest goals are reached: The accurate emulation of IK+ Jan 25 / Mar 19, 2015 The C64 Emulator is now direct executable in the browser . Mar 2 / Aug 29 / Oct 20, 2014 C64 Emulator enhancements and bug fixes. Version changes . September 22, 2013 Some C64 Emulator bug fixes and new features. See here for version changes.https://c64emulator.111mb.de/index.php?site=pp_javascript&lang=en&group=c64 Commodore 64 online emulator on Virtual ConsolesThe C64 online emulator is a fully functional emulator supporting all the well accepted file formats. Just drag & drop your files and play with retro games.https://virtualconsoles.com/online-emulators/c64/ All-in-One C64 Emulator, Games, Demoscene and SupportC64 Forever 10: \r\nCreate & Play Download , Free Express Edition Buy Now , From $9.95 When the C64 was launched by Commodore in 1982 it immediately \r\nset the standard for 8-bit home computers. Its low \r\ncost, superior graphics, high quality sound and a \r\nmassive 64 KB of RAM positioned it as the winner in \r\nthe home computer wars, knocking out competitors \r\nfrom the likes of Atari, Texas Instruments, \r\nSinclair, Apple and IBM. Selling over 30 million units and introducing a \r\nwhole generation to computers and programming, the \r\nC64 shook up the video games industry and sparked \r\ncultural phenomena such as computer music and the \r\ndemoscene. In recent years the C64 has enjoyed a \r\nspectacular revival manifesting itself once again as \r\na retrocomputing platform. To allow you to experience and relive the wonders \r\nof this unique computer, Cloanto, developers of \r\nCommodore/Amiga software since the 1980s, has \r\nintroduced C64 Forever, the official CBM 8-bit \r\npreservation, emulation and support package. C64 \r\nForever embodies an intuitive player interface, \r\nbacked by a built-in database containing more than \r\n5,000 C64 game entries. Support for the innovative \r\nRP9 format allows for advanced title authoring, \r\npreservation and \r\neasy cross-platform playback.https://www.c64forever.com/', 'aiModelVersion': '1'}",0.8289
Jeff Erickson,Updated 1y,"Did Donald Knuth, back in the 1970s, say something along the lines of ""Everything in computer science has been discovered now, now we just need to document it?""","That would be remarkably out of character for Knuth, even in the 1970s.

In 1973, Knuth ran the community poll to decide what to call the class of problems that Steve Cook and Dick Karp proved are as hard as any decision problem where yes answers can be verified in polynomial time. Knuth first advocated for the term “Herculean”, but then after a conversation with Karp, switched to “formidable”.

A terminological proposal | ACM SIGACT News
While preparing a book on combinatorial algorithms, I felt a strong need for a new technical term, a word which is essentially a one-sided version of polynomial complete. A great many problems of practical interest have the property that they are at ...
https://dl.acm.org/doi/10.1145/1811129.1811130

This is, of course, the domain of the notoriously unsolved P-versus-NP problem.

I can’t be completely sure that Knuth’s offer of a live turkey is still viable, but I think it’s far more likely than your quotation being accurate.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/kp10q6ivzyoh8wgs', 'title': 'Did Donald Knuth, back in the 1970s, say something along the lines of ""Everything in computer science has been discovered now, now we just need to document it?""', 'score': {'original': 0.9821, 'ai': 0.0179}, 'blocks': [{'text': 'That would be remarkably out of character for Knuth, even in the 1970s.\n\nIn 1973, Knuth ran the community poll to decide what to call the class of problems that Steve Cook and Dick Karp proved are as hard as any decision problem where yes answers can be verified in polynomial time. Knuth first advocated for the term “Herculean”, but then after a conversation with Karp, switched to “formidable”.\n\nA terminological proposal | ACM SIGACT News\nWhile preparing a book on combinatorial algorithms, I felt a strong need for a new technical term, a word which is essentially a one-sided version of polynomial complete. A great many problems of practical interest have the property that they are at ...\nhttps://dl.acm.org/doi/10.1145/1811129.1811130\n\nThis is, of course, the domain of the notoriously unsolved P-versus-NP problem.\n\nI can’t be completely sure that Knuth’s offer of a live turkey is still viable, but I think it’s far more likely than your quotation being accurate.', 'result': {'fake': 0.0179, 'real': 0.9821}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995242, 'subscription': 0, 'content': 'That would be remarkably out of character for Knuth, even in the 1970s.\n\nIn 1973, Knuth ran the community poll to decide what to call the class of problems that Steve Cook and Dick Karp proved are as hard as any decision problem where yes answers can be verified in polynomial time. Knuth first advocated for the term “Herculean”, but then after a conversation with Karp, switched to “formidable”.\n\nA terminological proposal | ACM SIGACT News\nWhile preparing a book on combinatorial algorithms, I felt a strong need for a new technical term, a word which is essentially a one-sided version of polynomial complete. A great many problems of practical interest have the property that they are at ...\nhttps://dl.acm.org/doi/10.1145/1811129.1811130\n\nThis is, of course, the domain of the notoriously unsolved P-versus-NP problem.\n\nI can’t be completely sure that Knuth’s offer of a live turkey is still viable, but I think it’s far more likely than your quotation being accurate.', 'aiModelVersion': '1'}",0.9821
Colin Zhou,4y,What were the most laughably large first iterations of technology?,"In 1983, Motorola stunned the world when it introduced a revolutionary product called the DynaTAC 8000X mobile phone.

Previously, mobile phones had to be used through a dedicated operator or were strictly installed in cars. But the DynaTAC showed that a single person was all that was needed to operate such a device.

This massive brick looking device was the beginning of the mobile revolution. But it wasn’t your average consumer device.

A full charge took 10 hours for a 30 minute talk session and it cost the equivalent of 10,000 dollars today.

But on October...

Access this answer and support the author as a Quora+ subscriber
Access all answers reserved by 
Colin Zhou
 for Quora+ subscribers
Access exclusive answers from thousands more participating creators in Quora+
Browse ad‑free and support creators
Start free trial
Learn more","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/79rmzd126nfbo43i', 'title': 'What were the most laughably large first iterations of technology?', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'In 1983, Motorola stunned the world when it introduced a revolutionary product called the DynaTAC 8000X mobile phone.\n\nPreviously, mobile phones had to be used through a dedicated operator or were strictly installed in cars. But the DynaTAC showed that a single person was all that was needed to operate such a device.\n\nThis massive brick looking device was the beginning of the mobile revolution. But it wasn’t your average consumer device.\n\nA full charge took 10 hours for a 30 minute talk session and it cost the equivalent of 10,000 dollars today.\n\nBut on October...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nColin Zhou\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995240, 'subscription': 0, 'content': 'In 1983, Motorola stunned the world when it introduced a revolutionary product called the DynaTAC 8000X mobile phone.\n\nPreviously, mobile phones had to be used through a dedicated operator or were strictly installed in cars. But the DynaTAC showed that a single person was all that was needed to operate such a device.\n\nThis massive brick looking device was the beginning of the mobile revolution. But it wasn’t your average consumer device.\n\nA full charge took 10 hours for a 30 minute talk session and it cost the equivalent of 10,000 dollars today.\n\nBut on October...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nColin Zhou\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'aiModelVersion': '1'}",0.9996
Alan Kay,Updated 5y,What was Alan Kay's experience like working at Apple?,"There were many Apples in the period I was there (from about May 1984 to about Sept 1996). This was not just the turnover in Chairmen and CEOs (from Mike Markula, to Steve Jobs to John Sculley to Mike Spindler to Gil Amelio, etc.) but also having the BoD throw out Steve in 1985 made a big difference.

My opinion of the BoD was very low. They consistently passed up deep opportunities to grasp and make the future of personal computing and networking. For brevity, I will leave these out here.

Also, very especially in Apple, just who was COO really made a difference (main job is to make things happen that are supposed to happen and vice versa).

And who was head of product and marketing (e.g. the personality of Jean-Louis Gassee had quite an influence both positive and negative).

And Apple Software when it was directly part of Apple and then spun to the side as Claris.

Some of the high points revolved around Steve for the year we overlapped, and then John Sculley. The latter was willing to fund for multiple years a big education project I’d thought up (the “Vivarium”), and was the big backer of Hypercard when Apple Marketing didn’t want to have anything to do with it.

When John was forced out, there was no one left to really back “what’s next?”. For example, Hypercard was (a) a great start (b) needed a brand new version (c) needed to be made the basis of an Apple WWW browser (think about how wonderful this would have been!) The old Parc hands (who had also worked to help invent the Internet) put a lot of effort into trying to convince management to have Apple software be the main source of high quality wide spectrum authoring for the web — but to no avail.

The Newton was a good idea to do, but Apple Marketing forbade important features, fearing that it would then hurt the Mac.

Etc. Etc.

So there was quite a downward spiral happening after Steve and Sculley. I and my group left in Sept 1996 as Gil Amelio came on board (as far as Gary Starkweather and I could tell, he was both fearful (literally) and didn’t have a clue about what Apple was and could be about.

An important point — I think — is that when Steve came back he was able to start a new direction for the company — kind of “consumer conveniences” — but he was no longer interested in the ideals of personal computing and “Wheels for the Mind”. This was a shame (but people who see no higher than money loved it).

Despite all this, he and I remained friends.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6sbe71kzr08yvd5m', 'title': ""What was Alan Kay's experience like working at Apple?"", 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'There were many Apples in the period I was there (from about May 1984 to about Sept 1996). This was not just the turnover in Chairmen and CEOs (from Mike Markula, to Steve Jobs to John Sculley to Mike Spindler to Gil Amelio, etc.) but also having the BoD throw out Steve in 1985 made a big difference.\n\nMy opinion of the BoD was very low. They consistently passed up deep opportunities to grasp and make the future of personal computing and networking. For brevity, I will leave these out here.\n\nAlso, very especially in Apple, just who was COO really made a difference (main job is to make things happen that are supposed to happen and vice versa).\n\nAnd who was head of product and marketing (e.g. the personality of Jean-Louis Gassee had quite an influence both positive and negative).\n\nAnd Apple Software when it was directly part of Apple and then spun to the side as Claris.\n\nSome of the high points revolved around Steve for the year we overlapped, and then John Sculley. The latter was willing to fund for multiple years a big education project I’d thought up (the “Vivarium”), and was the big backer of Hypercard when Apple Marketing didn’t want to have anything to do with it.\n\nWhen John was forced out, there was no one left to really back “what’s next?”. For example, Hypercard was (a) a great start (b) needed a brand new version (c) needed to be made the basis of an Apple WWW browser (think about how wonderful this would have been!) The old Parc hands (who had also worked to help invent the Internet) put a lot of effort into trying to convince management to have Apple software be the main source of high quality wide spectrum authoring for the web — but to no avail.\n\nThe Newton was a good idea to do, but Apple Marketing forbade important features, fearing that it would then hurt the Mac.\n\nEtc. Etc.\n\nSo there was quite a downward spiral happening after Steve and Sculley. I and my group left in Sept 1996 as Gil Amelio came on board (as far as Gary Starkweather and I could tell, he was both fearful (literally) and didn’t have a clue about what Apple was and could be about.\n\nAn important point — I think — is that when Steve came back he was able to start a new direction for the company — kind of “consumer conveniences” — but he was no longer interested in the ideals of personal computing and “Wheels for the Mind”. This was a shame (but people who see no higher than money loved it).\n\nDespite all this, he and I remained friends.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1995235, 'subscription': 0, 'content': 'There were many Apples in the period I was there (from about May 1984 to about Sept 1996). This was not just the turnover in Chairmen and CEOs (from Mike Markula, to Steve Jobs to John Sculley to Mike Spindler to Gil Amelio, etc.) but also having the BoD throw out Steve in 1985 made a big difference.\n\nMy opinion of the BoD was very low. They consistently passed up deep opportunities to grasp and make the future of personal computing and networking. For brevity, I will leave these out here.\n\nAlso, very especially in Apple, just who was COO really made a difference (main job is to make things happen that are supposed to happen and vice versa).\n\nAnd who was head of product and marketing (e.g. the personality of Jean-Louis Gassee had quite an influence both positive and negative).\n\nAnd Apple Software when it was directly part of Apple and then spun to the side as Claris.\n\nSome of the high points revolved around Steve for the year we overlapped, and then John Sculley. The latter was willing to fund for multiple years a big education project I’d thought up (the “Vivarium”), and was the big backer of Hypercard when Apple Marketing didn’t want to have anything to do with it.\n\nWhen John was forced out, there was no one left to really back “what’s next?”. For example, Hypercard was (a) a great start (b) needed a brand new version (c) needed to be made the basis of an Apple WWW browser (think about how wonderful this would have been!) The old Parc hands (who had also worked to help invent the Internet) put a lot of effort into trying to convince management to have Apple software be the main source of high quality wide spectrum authoring for the web — but to no avail.\n\nThe Newton was a good idea to do, but Apple Marketing forbade important features, fearing that it would then hurt the Mac.\n\nEtc. Etc.\n\nSo there was quite a downward spiral happening after Steve and Sculley. I and my group left in Sept 1996 as Gil Amelio came on board (as far as Gary Starkweather and I could tell, he was both fearful (literally) and didn’t have a clue about what Apple was and could be about.\n\nAn important point — I think — is that when Steve came back he was able to start a new direction for the company — kind of “consumer conveniences” — but he was no longer interested in the ideals of personal computing and “Wheels for the Mind”. This was a shame (but people who see no higher than money loved it).\n\nDespite all this, he and I remained friends.', 'aiModelVersion': '1'}",0.9998
Tony Li,9y,What was programming like back in the days when computers only had kilobytes of RAM?,"Much more careful. Wasting a buffer was unthinkable. Memory leaks were instant death. GUIs were unthinkable and we were really happy when we got just about anything to work.

The C compiler required that you change floppy disks.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/36kzodev1hfq0tl4', 'title': 'What was programming like back in the days when computers only had kilobytes of RAM?', 'score': {'original': 0.9136, 'ai': 0.0864}, 'blocks': [{'text': 'Much more careful. Wasting a buffer was unthinkable. Memory leaks were instant death. GUIs were unthinkable and we were really happy when we got just about anything to work.\n\nThe C compiler required that you change floppy disks.', 'result': {'fake': 0.0864, 'real': 0.9136}, 'status': 'success'}], 'credits_used': 1, 'credits': 1995234, 'subscription': 0, 'content': 'Much more careful. Wasting a buffer was unthinkable. Memory leaks were instant death. GUIs were unthinkable and we were really happy when we got just about anything to work.\n\nThe C compiler required that you change floppy disks.', 'aiModelVersion': '1'}",0.9136
Mark Decker,Updated 1y,What were the most laughably large first iterations of technology?,"This device, the Xerox DocuTech was arguably the first fully integrated scanner/printer and was the predecessor to all the MFP devices we all know and love today.

The DocuTech was introduced in the early 1990s and was a beast, capable of printing at what was then a blazing-fast 135 monochrome pages per minute. It had a scanner that while looking like a standard copier was actually a high-quality 600dpi scanning device with a document feeder. Your scanned files could then be viewed on a high resolution touchscreen and manipulated at will. Features included cut & paste, masking, adjust images, add, remove, reorder pages, and many other operations.

DocuTech 135 Production Publisher

The system was also one of the first of it’s kind with connectivity to ethernet networks, complete with a Postscript print driver allowing the user to create high quality output directly from document creation applications, something we take for granted today. The marketing of the Docutech emphasized the notion of “On Demand Printing”, since you could scan and store an unlimited amount of documents to be printed later. With traditional offset printing you had to print shelves and shelves of manuals and brochures to be distributed as needed, often becoming outdated and trashed. The Docutech offered the ability to print documents as they were needed, eliminating costly storage and disposal issues.

The system also boasted inline stapling and tape binding options for what was to be called a “printshop in a box”. The DT135 and it’s successors were wildly popular and kept me employed at Xerox for a long while.

Docutech touchscreen

These days we have devices such as this Epson, which can scan, print, even fax if desired. The resulting files can be manipulated in Adobe Creative Suite, saved and then sent to the printer for final output.

To be fair, I wouldn’t laugh at the DT135 because it was a marvel of modern engineering that took full advantage of many of the Xerox PARC innovations, and it was a beast. Yet it does illustrate how much of what they invented found its way into today’s tabletop devices.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/wmskoyg2p8175x0e', 'title': 'What were the most laughably large first iterations of technology?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'This device, the Xerox DocuTech was arguably the first fully integrated scanner/printer and was the predecessor to all the MFP devices we all know and love today.\n\nThe DocuTech was introduced in the early 1990s and was a beast, capable of printing at what was then a blazing-fast 135 monochrome pages per minute. It had a scanner that while looking like a standard copier was actually a high-quality 600dpi scanning device with a document feeder. Your scanned files could then be viewed on a high resolution touchscreen and manipulated at will. Features included cut & paste, masking, adjust images, add, remove, reorder pages, and many other operations.\n\nDocuTech 135 Production Publisher\n\nThe system was also one of the first of it’s kind with connectivity to ethernet networks, complete with a Postscript print driver allowing the user to create high quality output directly from document creation applications, something we take for granted today. The marketing of the Docutech emphasized the notion of “On Demand Printing”, since you could scan and store an unlimited amount of documents to be printed later. With traditional offset printing you had to print shelves and shelves of manuals and brochures to be distributed as needed, often becoming outdated and trashed. The Docutech offered the ability to print documents as they were needed, eliminating costly storage and disposal issues.\n\nThe system also boasted inline stapling and tape binding options for what was to be called a “printshop in a box”. The DT135 and it’s successors were wildly popular and kept me employed at Xerox for a long while.\n\nDocutech touchscreen\n\nThese days we have devices such as this Epson, which can scan, print, even fax if desired. The resulting files can be manipulated in Adobe Creative Suite, saved and then sent to the printer for final output.\n\nTo be fair, I wouldn’t laugh at the DT135 because it was a marvel of modern engineering that took full advantage of many of the Xerox PARC innovations, and it was a beast. Yet it does illustrate how much of what they invented found its way into today’s tabletop devices.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 4, 'credits': 1995230, 'subscription': 0, 'content': 'This device, the Xerox DocuTech was arguably the first fully integrated scanner/printer and was the predecessor to all the MFP devices we all know and love today.\n\nThe DocuTech was introduced in the early 1990s and was a beast, capable of printing at what was then a blazing-fast 135 monochrome pages per minute. It had a scanner that while looking like a standard copier was actually a high-quality 600dpi scanning device with a document feeder. Your scanned files could then be viewed on a high resolution touchscreen and manipulated at will. Features included cut & paste, masking, adjust images, add, remove, reorder pages, and many other operations.\n\nDocuTech 135 Production Publisher\n\nThe system was also one of the first of it’s kind with connectivity to ethernet networks, complete with a Postscript print driver allowing the user to create high quality output directly from document creation applications, something we take for granted today. The marketing of the Docutech emphasized the notion of “On Demand Printing”, since you could scan and store an unlimited amount of documents to be printed later. With traditional offset printing you had to print shelves and shelves of manuals and brochures to be distributed as needed, often becoming outdated and trashed. The Docutech offered the ability to print documents as they were needed, eliminating costly storage and disposal issues.\n\nThe system also boasted inline stapling and tape binding options for what was to be called a “printshop in a box”. The DT135 and it’s successors were wildly popular and kept me employed at Xerox for a long while.\n\nDocutech touchscreen\n\nThese days we have devices such as this Epson, which can scan, print, even fax if desired. The resulting files can be manipulated in Adobe Creative Suite, saved and then sent to the printer for final output.\n\nTo be fair, I wouldn’t laugh at the DT135 because it was a marvel of modern engineering that took full advantage of many of the Xerox PARC innovations, and it was a beast. Yet it does illustrate how much of what they invented found its way into today’s tabletop devices.', 'aiModelVersion': '1'}",0.9997
Mike West,4y,Who is the most influential person in the history of Machine Learning?,"They all stood on the shoulders of Turing.

In 1936, while studying for his Ph.D. at Princeton University, the English mathematician Alan Turing
 published a paper, “On Computable Numbers, with an application to the Entscheidungsproblem,” which became the foundation of computer science.

In it Turing presented a theoretical machine that could solve any problem
 that could be described by simple instructions encoded on a paper tape. One Turing Machine could calculate square roots, whilst another might solve Sudoku puzzles. Turing demonstrated you could construct a single Universal Machine that could simulate any Turing Machine. One machine solving any problem, performing any task for which a program could be written—sound familiar? He’d invented the computer.

Turing’s legacy is not complete. In 1950 he published a paper called “Computing machinery and intelligence.” He had an idea that computers would become so powerful that they would think
. He envisaged a time when artificial intelligence (AI) would be a reality. But, how would you know if a machine was intelligent?

He devised the Turing Test: A judge sitting at a computer terminal types questions to two entities, one a person and the other a computer. The judge decides which entity is human and which the computer. If the judge is wrong the computer has passed the Turing Test and is intelligent.

Turing is easily the most influential person in this space.

Would you have Einstein without Newton? Maybe… but probably not.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/3280ncblasp9gdrw', 'title': 'Who is the most influential person in the history of Machine Learning?', 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': 'They all stood on the shoulders of Turing.\n\nIn 1936, while studying for his Ph.D. at Princeton University, the English mathematician Alan Turing\n published a paper, “On Computable Numbers, with an application to the Entscheidungsproblem,” which became the foundation of computer science.\n\nIn it Turing presented a theoretical machine that could solve any problem\n that could be described by simple instructions encoded on a paper tape. One Turing Machine could calculate square roots, whilst another might solve Sudoku puzzles. Turing demonstrated you could construct a single Universal Machine that could simulate any Turing Machine. One machine solving any problem, performing any task for which a program could be written—sound familiar? He’d invented the computer.\n\nTuring’s legacy is not complete. In 1950 he published a paper called “Computing machinery and intelligence.” He had an idea that computers would become so powerful that they would think\n. He envisaged a time when artificial intelligence (AI) would be a reality. But, how would you know if a machine was intelligent?\n\nHe devised the Turing Test: A judge sitting at a computer terminal types questions to two entities, one a person and the other a computer. The judge decides which entity is human and which the computer. If the judge is wrong the computer has passed the Turing Test and is intelligent.\n\nTuring is easily the most influential person in this space.\n\nWould you have Einstein without Newton? Maybe… but probably not.', 'result': {'fake': 0.0007, 'real': 0.9993}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995227, 'subscription': 0, 'content': 'They all stood on the shoulders of Turing.\n\nIn 1936, while studying for his Ph.D. at Princeton University, the English mathematician Alan Turing\n published a paper, “On Computable Numbers, with an application to the Entscheidungsproblem,” which became the foundation of computer science.\n\nIn it Turing presented a theoretical machine that could solve any problem\n that could be described by simple instructions encoded on a paper tape. One Turing Machine could calculate square roots, whilst another might solve Sudoku puzzles. Turing demonstrated you could construct a single Universal Machine that could simulate any Turing Machine. One machine solving any problem, performing any task for which a program could be written—sound familiar? He’d invented the computer.\n\nTuring’s legacy is not complete. In 1950 he published a paper called “Computing machinery and intelligence.” He had an idea that computers would become so powerful that they would think\n. He envisaged a time when artificial intelligence (AI) would be a reality. But, how would you know if a machine was intelligent?\n\nHe devised the Turing Test: A judge sitting at a computer terminal types questions to two entities, one a person and the other a computer. The judge decides which entity is human and which the computer. If the judge is wrong the computer has passed the Turing Test and is intelligent.\n\nTuring is easily the most influential person in this space.\n\nWould you have Einstein without Newton? Maybe… but probably not.', 'aiModelVersion': '1'}",0.9993
Dave Haynie,Updated 1y,Why did the USB port become so popular?,"It was the combination of the right solution being around when the need arose.

The Old Peripheral Mess

Prior to USB, we had a variety of specialized ports on a PC. There was a parallel port for printers, a serial port for modems and other communications, a custom port for the keyboard, and a catch-all expansion bus that allowed you to drop in cards for anything else.

The problem was there was only one or two each of these ports. And because parallel and serial ports existed on PCs, they got used for other attachments. As PCs got more popular and more powerful, the number of things folks did with them increased, and so did the hardware add-ons to make those things work better.

Early Efforts to Address This Problem

There had been a few attempts at something more general. In 1980, Commodore released the VIC-20 with the first version of the Commodore Serial Bus. This allowed daisy-chaining of drives and printers from a single port on the computer. The Commodore serial bus connected to a VIC-20 (and later a C64 or C128) via a single reliable 6-pin DIN connector. It was in essence a serialized version of the IEEE488 bus used for peripheral daisy-chaining in the Commodore PET series. The bus logically supported up to 31 devices, but in practice, it was more like 8 devices. Due to a chip flaw in the original VIC-20, speeds are limited to a theoretical maximum of about 160kb/s… in practice, more like 3.2kb/s - 19.2kb/s. The C128 improvements get you to 35–64kb/s, depending on what’s happening. No other companies used the Commodore Serial Bus in their computers.

In 1986, Steve Wozniak introduced the Apple Desktop Bus in the Apple IIGS, which provided a similar idea for keyboards, mice, and other low-speed peripherals. The single data wire of ADB and software polling by the host processor severely limited the performance of ADB as well. Theoretical maximum speed was 128kb/s, but in practice, it delivered about 10kb/s performance. Macintoshes adopted ADB from the Mac II on, and Steve Jobs brought it with him to NeXT.

Yet another attempt at a general purpose peripheral bus was made by Digital Equipment Corporation (DEC) and Philips around 1992, dubbed Access.bus (or sometimes A.b). This used the existing Philips Inter-IC (I²C) bus. I²C is a two-wire bus usually used on circuit boards, allowing a large number of low-speed devices to communicate on just two wires. There are higher I²C modes, but A.b supported 100kb/s and 10kb/s transfer modes. It allows 128 devices to be connected, and used a phone-jack connector, which also included +5Vdc and GND. Also like USB, it supported hot-plugging.

And DEC/Philips had put together an industry support organization (ABIG) around Access.bus, with 29 members, including Microsoft. That was in 1993. A.b introduced peripheral classes, the idea that storage, human interface devices (HID), etc. would have a standard protocol that’s part of the spec, and thus, easy to support by any computer without the need for custom device drivers.

The Dawn of The Universal Serial Bus

So just a year later, Compaq, DEC, IBM, Intel, Microsoft, NEC, and Nortel got together to solve this same problem… and USB was born. DEC brought the experience of Access.bus, but it was agreed that they needed a faster connection. USB would launch with 1.5Mb/s and 12Mb/s modes.

USB has some advantages. It was a differential serial bus, meaning two wires (plus power) to do anything you needed, with very robust signalling (long cables). Rather than daisy chaining, USB would be a hub-and-spoke architecture, but allow add-on hubs, for practical expansion that’s essentially unlimited (A.b supported hubs as well). Like A.b, it would support hot plugging, but with a more reliable, consumer friendly connector (the phone cables break). That made it cheap in terms of simple and rugged connectors, in terms of pin count on support chips, in terms of cabling. The standard cable supplied power, a +5Vdc supply just like A.b, there was an isochronous transfer mode to allow audio devices. It was very well thought out… built on what came before.

Once USB started being populated in PCs and got good software support, it became obvious to make makes of small hardware add-ons that USB was the way to go. That, of course, drove the demand for more USB ports.

In the mid 1990s, Apple had produced a more advanced serial bus, which they dubbed Firewire. This was kind of a mess. It was very good for some things, like video, because unlike USB, Firewire was multi-master (any Firewire device could just start sending data). So a Firewire port could function as a digital video output from a camcorder without a computer-like tape drive that could react to flow control signals. And it was very fast, up to 400Mb/s, versus USB's 12Mb/s. There were suggestions that Firewire would replace all the higher speed stuff, leaving USB just for keyboards, mice, and other low-speed things. Much of the industry was willing to consider that.

The problem was, Apple was greedy and not very good at establishing standards. They wanted manufacturers to license Firewire at $1.00/port, and they even trademarked the name “Firewire”, though the interconnect itself has been made the IEEE 1394 standard. So every manufacturer had a different name for it, the antithesis of consumer friendliness.

One of the reasons Intel has been so successful at creating standards is their motivation: they want to sell chips. And if they deliver better PC functionality, you’ll toss out that old PC and buy a new one. It’s probably full of Intel chips. So Intel, alone or with partners, has been able to introduce all sorts of standards that carried no licensing or patent fees, like USB, PCI, PCI Express, etc. The USB group responded to the Firewire standard (and, as well, some shortcomings in USB) with the USB 2.0 spec in 2000. Because it was free to use, PC manufacturers generally built in a bunch of USB ports, maybe one Firewire port if you had a higher-end PC or laptop. And so USB 2.0, at 480Mb/s, extended this already popular port to be much more useful for hard drives and, eventually, solid-state cameras, which more replaced tape. So the one thing demanding USB’s only real PC-based competition essentially jumped over to using USB… because it was already “universal”.

Smartphones also helped cement the deal. Early smartphones had various ways of hooking to your PC, usually via a serial port, and it was ugly. Over time, they all moved to using USB. It helped that USB introduced a battery-charging standard in 2000, which would allow for charging 3x faster than a pure data port on a PC. And while older phones had proprietary USB cabling in some cases, in 2007, the Micro USB Type-B connector was introduced. That was designed to be thinner and to be more rugged (on the phone end) than the previous mini Type-B connector. The European Union mandated the micro-USB connector as the data/power connection for phones, in order to help stop filling landfills with useless, proprietary power dongles.

And so went the rest of the world. Digital cameras really caught on in the late 90s/early 2000s, and since this was one more thing to add onto your already busy computer, USB was pretty much the universal choice for cameras. There’s also a USB port type that identifies as a camera, so before long, every operating system knew what a USB camera was.

In short, USB was designed as the Universal Serial Bus. And it was the right approach to that at just the right time in history, with few other viable options to compete against it. There’s really not much point in making something different these days for 99% of all hardware hard-connected interface needs.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2exf3q5obyzrlh1a', 'title': 'Why did the USB port become so popular?', 'score': {'original': 0.52813333333333, 'ai': 0.47186666666667}, 'blocks': [{'text': 'It was the combination of the right solution being around when the need arose.\n\nThe Old Peripheral Mess\n\nPrior to USB, we had a variety of specialized ports on a PC. There was a parallel port for printers, a serial port for modems and other communications, a custom port for the keyboard, and a catch-all expansion bus that allowed you to drop in cards for anything else.\n\nThe problem was there was only one or two each of these ports. And because parallel and serial ports existed on PCs, they got used for other attachments. As PCs got more popular and more powerful, the number of things folks did with them increased, and so did the hardware add-ons to make those things work better.\n\nEarly Efforts to Address This Problem\n\nThere had been a few attempts at something more general. In 1980, Commodore released the VIC-20 with the first version of the Commodore Serial Bus. This allowed daisy-chaining of drives and printers from a single port on the computer. The Commodore serial bus connected to a VIC-20 (and later a C64 or C128) via a single reliable 6-pin DIN connector. It was in essence a serialized version of the IEEE488 bus used for peripheral daisy-chaining in the Commodore PET series. The bus logically supported up to 31 devices, but in practice, it was more like 8 devices. Due to a chip flaw in the original VIC-20, speeds are limited to a theoretical maximum of about 160kb/s… in practice, more like 3.2kb/s - 19.2kb/s. The C128 improvements get you to 35–64kb/s, depending on what’s happening. No other companies used the Commodore Serial Bus in their computers.\n\nIn 1986, Steve Wozniak introduced the Apple Desktop Bus in the Apple IIGS, which provided a similar idea for keyboards, mice, and other low-speed peripherals. The single data wire of ADB and software polling by the host processor severely limited the performance of ADB as well. Theoretical maximum speed was 128kb/s, but in practice, it delivered about 10kb/s performance. Macintoshes adopted ADB from the Mac II on, and Steve Jobs brought it with him to NeXT.\n\nYet another attempt at a general purpose peripheral bus was made by Digital Equipment Corporation (DEC) and Philips around 1992, dubbed Access.bus (or sometimes A.b). This used the existing Philips Inter-IC (I²C) bus. I²C is a two-wire bus usually used on circuit boards, allowing a large number of low-speed devices to communicate on just two wires. There are higher I²C modes, but A.b supported 100kb/s and 10kb/s transfer modes. It allows 128 devices to be connected, and used a phone-jack connector, which also included +5Vdc and GND. Also like USB, it supported hot-plugging.\n\nAnd DEC/Philips had put together an industry support organization (ABIG) around Access.bus, with 29 members, including Microsoft. That was in 1993. A.b introduced peripheral classes, the idea that storage, human interface devices (HID), etc. would have a standard protocol that’s part of the spec, and thus, easy to support by any computer without the need for custom device drivers.\n\nThe Dawn of The Universal Serial Bus\n\nSo just a year later, Compaq, DEC, IBM, Intel, Microsoft, NEC, and', 'result': {'fake': 0.0105, 'real': 0.9895}, 'status': 'success'}, {'text': ""Nortel got together to solve this same problem… and USB was born. DEC brought the experience of Access.bus, but it was agreed that they needed a faster connection. USB would launch with 1.5Mb/s and 12Mb/s modes.\n\nUSB has some advantages. It was a differential serial bus, meaning two wires (plus power) to do anything you needed, with very robust signalling (long cables). Rather than daisy chaining, USB would be a hub-and-spoke architecture, but allow add-on hubs, for practical expansion that’s essentially unlimited (A.b supported hubs as well). Like A.b, it would support hot plugging, but with a more reliable, consumer friendly connector (the phone cables break). That made it cheap in terms of simple and rugged connectors, in terms of pin count on support chips, in terms of cabling. The standard cable supplied power, a +5Vdc supply just like A.b, there was an isochronous transfer mode to allow audio devices. It was very well thought out… built on what came before.\n\nOnce USB started being populated in PCs and got good software support, it became obvious to make makes of small hardware add-ons that USB was the way to go. That, of course, drove the demand for more USB ports.\n\nIn the mid 1990s, Apple had produced a more advanced serial bus, which they dubbed Firewire. This was kind of a mess. It was very good for some things, like video, because unlike USB, Firewire was multi-master (any Firewire device could just start sending data). So a Firewire port could function as a digital video output from a camcorder without a computer-like tape drive that could react to flow control signals. And it was very fast, up to 400Mb/s, versus USB's 12Mb/s. There were suggestions that Firewire would replace all the higher speed stuff, leaving USB just for keyboards, mice, and other low-speed things. Much of the industry was willing to consider that.\n\nThe problem was, Apple was greedy and not very good at establishing standards. They wanted manufacturers to license Firewire at $1.00/port, and they even trademarked the name “Firewire”, though the interconnect itself has been made the IEEE 1394 standard. So every manufacturer had a different name for it, the antithesis of consumer friendliness.\n\nOne of the reasons Intel has been so successful at creating standards is their motivation: they want to sell chips. And if they deliver better PC functionality, you’ll toss out that old PC and buy a new one. It’s probably full of Intel chips. So Intel, alone or with partners, has been able to introduce all sorts of standards that carried no licensing or patent fees, like USB, PCI, PCI Express, etc. The USB group responded to the Firewire standard (and, as well, some shortcomings in USB) with the USB 2.0 spec in 2000. Because it was free to use, PC manufacturers generally built in a bunch of USB ports, maybe one Firewire port if you had a higher-end PC or laptop. And so USB 2.0, at 480Mb/s, extended this already popular port to be much more useful for hard drives and, eventually, solid-state cameras, which more replaced tape. So the"", 'result': {'fake': 0.6636, 'real': 0.3364}, 'status': 'success'}, {'text': 'one thing demanding USB’s only real PC-based competition essentially jumped over to using USB… because it was already “universal”.\n\nSmartphones also helped cement the deal. Early smartphones had various ways of hooking to your PC, usually via a serial port, and it was ugly. Over time, they all moved to using USB. It helped that USB introduced a battery-charging standard in 2000, which would allow for charging 3x faster than a pure data port on a PC. And while older phones had proprietary USB cabling in some cases, in 2007, the Micro USB Type-B connector was introduced. That was designed to be thinner and to be more rugged (on the phone end) than the previous mini Type-B connector. The European Union mandated the micro-USB connector as the data/power connection for phones, in order to help stop filling landfills with useless, proprietary power dongles.\n\nAnd so went the rest of the world. Digital cameras really caught on in the late 90s/early 2000s, and since this was one more thing to add onto your already busy computer, USB was pretty much the universal choice for cameras. There’s also a USB port type that identifies as a camera, so before long, every operating system knew what a USB camera was.\n\nIn short, USB was designed as the Universal Serial Bus. And it was the right approach to that at just the right time in history, with few other viable options to compete against it. There’s really not much point in making something different these days for 99% of all hardware hard-connected interface needs.', 'result': {'fake': 0.948, 'real': 0.052}, 'status': 'success'}], 'credits_used': 14, 'credits': 1995213, 'subscription': 0, 'content': ""It was the combination of the right solution being around when the need arose.\n\nThe Old Peripheral Mess\n\nPrior to USB, we had a variety of specialized ports on a PC. There was a parallel port for printers, a serial port for modems and other communications, a custom port for the keyboard, and a catch-all expansion bus that allowed you to drop in cards for anything else.\n\nThe problem was there was only one or two each of these ports. And because parallel and serial ports existed on PCs, they got used for other attachments. As PCs got more popular and more powerful, the number of things folks did with them increased, and so did the hardware add-ons to make those things work better.\n\nEarly Efforts to Address This Problem\n\nThere had been a few attempts at something more general. In 1980, Commodore released the VIC-20 with the first version of the Commodore Serial Bus. This allowed daisy-chaining of drives and printers from a single port on the computer. The Commodore serial bus connected to a VIC-20 (and later a C64 or C128) via a single reliable 6-pin DIN connector. It was in essence a serialized version of the IEEE488 bus used for peripheral daisy-chaining in the Commodore PET series. The bus logically supported up to 31 devices, but in practice, it was more like 8 devices. Due to a chip flaw in the original VIC-20, speeds are limited to a theoretical maximum of about 160kb/s… in practice, more like 3.2kb/s - 19.2kb/s. The C128 improvements get you to 35–64kb/s, depending on what’s happening. No other companies used the Commodore Serial Bus in their computers.\n\nIn 1986, Steve Wozniak introduced the Apple Desktop Bus in the Apple IIGS, which provided a similar idea for keyboards, mice, and other low-speed peripherals. The single data wire of ADB and software polling by the host processor severely limited the performance of ADB as well. Theoretical maximum speed was 128kb/s, but in practice, it delivered about 10kb/s performance. Macintoshes adopted ADB from the Mac II on, and Steve Jobs brought it with him to NeXT.\n\nYet another attempt at a general purpose peripheral bus was made by Digital Equipment Corporation (DEC) and Philips around 1992, dubbed Access.bus (or sometimes A.b). This used the existing Philips Inter-IC (I²C) bus. I²C is a two-wire bus usually used on circuit boards, allowing a large number of low-speed devices to communicate on just two wires. There are higher I²C modes, but A.b supported 100kb/s and 10kb/s transfer modes. It allows 128 devices to be connected, and used a phone-jack connector, which also included +5Vdc and GND. Also like USB, it supported hot-plugging.\n\nAnd DEC/Philips had put together an industry support organization (ABIG) around Access.bus, with 29 members, including Microsoft. That was in 1993. A.b introduced peripheral classes, the idea that storage, human interface devices (HID), etc. would have a standard protocol that’s part of the spec, and thus, easy to support by any computer without the need for custom device drivers.\n\nThe Dawn of The Universal Serial Bus\n\nSo just a year later, Compaq, DEC, IBM, Intel, Microsoft, NEC, and Nortel got together to solve this same problem… and USB was born. DEC brought the experience of Access.bus, but it was agreed that they needed a faster connection. USB would launch with 1.5Mb/s and 12Mb/s modes.\n\nUSB has some advantages. It was a differential serial bus, meaning two wires (plus power) to do anything you needed, with very robust signalling (long cables). Rather than daisy chaining, USB would be a hub-and-spoke architecture, but allow add-on hubs, for practical expansion that’s essentially unlimited (A.b supported hubs as well). Like A.b, it would support hot plugging, but with a more reliable, consumer friendly connector (the phone cables break). That made it cheap in terms of simple and rugged connectors, in terms of pin count on support chips, in terms of cabling. The standard cable supplied power, a +5Vdc supply just like A.b, there was an isochronous transfer mode to allow audio devices. It was very well thought out… built on what came before.\n\nOnce USB started being populated in PCs and got good software support, it became obvious to make makes of small hardware add-ons that USB was the way to go. That, of course, drove the demand for more USB ports.\n\nIn the mid 1990s, Apple had produced a more advanced serial bus, which they dubbed Firewire. This was kind of a mess. It was very good for some things, like video, because unlike USB, Firewire was multi-master (any Firewire device could just start sending data). So a Firewire port could function as a digital video output from a camcorder without a computer-like tape drive that could react to flow control signals. And it was very fast, up to 400Mb/s, versus USB's 12Mb/s. There were suggestions that Firewire would replace all the higher speed stuff, leaving USB just for keyboards, mice, and other low-speed things. Much of the industry was willing to consider that.\n\nThe problem was, Apple was greedy and not very good at establishing standards. They wanted manufacturers to license Firewire at $1.00/port, and they even trademarked the name “Firewire”, though the interconnect itself has been made the IEEE 1394 standard. So every manufacturer had a different name for it, the antithesis of consumer friendliness.\n\nOne of the reasons Intel has been so successful at creating standards is their motivation: they want to sell chips. And if they deliver better PC functionality, you’ll toss out that old PC and buy a new one. It’s probably full of Intel chips. So Intel, alone or with partners, has been able to introduce all sorts of standards that carried no licensing or patent fees, like USB, PCI, PCI Express, etc. The USB group responded to the Firewire standard (and, as well, some shortcomings in USB) with the USB 2.0 spec in 2000. Because it was free to use, PC manufacturers generally built in a bunch of USB ports, maybe one Firewire port if you had a higher-end PC or laptop. And so USB 2.0, at 480Mb/s, extended this already popular port to be much more useful for hard drives and, eventually, solid-state cameras, which more replaced tape. So the one thing demanding USB’s only real PC-based competition essentially jumped over to using USB… because it was already “universal”.\n\nSmartphones also helped cement the deal. Early smartphones had various ways of hooking to your PC, usually via a serial port, and it was ugly. Over time, they all moved to using USB. It helped that USB introduced a battery-charging standard in 2000, which would allow for charging 3x faster than a pure data port on a PC. And while older phones had proprietary USB cabling in some cases, in 2007, the Micro USB Type-B connector was introduced. That was designed to be thinner and to be more rugged (on the phone end) than the previous mini Type-B connector. The European Union mandated the micro-USB connector as the data/power connection for phones, in order to help stop filling landfills with useless, proprietary power dongles.\n\nAnd so went the rest of the world. Digital cameras really caught on in the late 90s/early 2000s, and since this was one more thing to add onto your already busy computer, USB was pretty much the universal choice for cameras. There’s also a USB port type that identifies as a camera, so before long, every operating system knew what a USB camera was.\n\nIn short, USB was designed as the Universal Serial Bus. And it was the right approach to that at just the right time in history, with few other viable options to compete against it. There’s really not much point in making something different these days for 99% of all hardware hard-connected interface needs."", 'aiModelVersion': '1'}",0.52813333333333
Ben Podgursky,4y,Why are there so few Linux experts? Even a simple problem seems impossible to solve. Forums are full of trial and error solutions and not solid knowledge.,"Try to debug your plumbing on StackExchange next time:

“My toilet doesn’t flush”

“Well, ok. Have you tried plunging it?”

“Plunging didn’t help I’m stuck now what”

“Well, how old is your sewer? Do you maybe have trees with aggressive roots?”

“Ugh I just want a quick fix, this is complicated”

I think it’s actually more remarkable how many Java/Ruby/Python solutions can be answered on StackOverflow with mindless stack-trace-matching rather than deep debugging and comprehensive explanations.

Linux is a system, not a unified platform like the JDK. Your issues are rarely cookie-cutter problems that a hundred people have solved before. Your problem is more likely some weird combination of half-installed aptitude packages, a weird .bashrc, and your ssh-agent not caching files correctly.

I’ve found that there really are experts scattered on the Linux forums / SO posts, but very few people want a deep explanation from experts — they want a quick fix.

So you end up with a few expert opinions, getting ignored, and a lot of people offering their personal anecdote quick fix, getting upvoted to the moon. Kinda like politics.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pbk4n2qj0s5toced', 'title': 'Why are there so few Linux experts? Even a simple problem seems impossible to solve. Forums are full of trial and error solutions and not solid knowledge.', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'Try to debug your plumbing on StackExchange next time:\n\n“My toilet doesn’t flush”\n\n“Well, ok. Have you tried plunging it?”\n\n“Plunging didn’t help I’m stuck now what”\n\n“Well, how old is your sewer? Do you maybe have trees with aggressive roots?”\n\n“Ugh I just want a quick fix, this is complicated”\n\nI think it’s actually more remarkable how many Java/Ruby/Python solutions can be answered on StackOverflow with mindless stack-trace-matching rather than deep debugging and comprehensive explanations.\n\nLinux is a system, not a unified platform like the JDK. Your issues are rarely cookie-cutter problems that a hundred people have solved before. Your problem is more likely some weird combination of half-installed aptitude packages, a weird .bashrc, and your ssh-agent not caching files correctly.\n\nI’ve found that there really are experts scattered on the Linux forums / SO posts, but very few people want a deep explanation from experts — they want a quick fix.\n\nSo you end up with a few expert opinions, getting ignored, and a lot of people offering their personal anecdote quick fix, getting upvoted to the moon. Kinda like politics.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995211, 'subscription': 0, 'content': 'Try to debug your plumbing on StackExchange next time:\n\n“My toilet doesn’t flush”\n\n“Well, ok. Have you tried plunging it?”\n\n“Plunging didn’t help I’m stuck now what”\n\n“Well, how old is your sewer? Do you maybe have trees with aggressive roots?”\n\n“Ugh I just want a quick fix, this is complicated”\n\nI think it’s actually more remarkable how many Java/Ruby/Python solutions can be answered on StackOverflow with mindless stack-trace-matching rather than deep debugging and comprehensive explanations.\n\nLinux is a system, not a unified platform like the JDK. Your issues are rarely cookie-cutter problems that a hundred people have solved before. Your problem is more likely some weird combination of half-installed aptitude packages, a weird .bashrc, and your ssh-agent not caching files correctly.\n\nI’ve found that there really are experts scattered on the Linux forums / SO posts, but very few people want a deep explanation from experts — they want a quick fix.\n\nSo you end up with a few expert opinions, getting ignored, and a lot of people offering their personal anecdote quick fix, getting upvoted to the moon. Kinda like politics.', 'aiModelVersion': '1'}",0.9996
Douglas Cogan,Updated 2y,How was the first home computer in the 1980s different from the computers that are in our homes now?,"Here is a picture of my first computer, TRS-80 Model I (picture from Wikipedia)

The “keyboard” was the entire computer. It had 4K of RAM (of which 1K was for the screen). (For reference, the text in this answer is over 3700 letters, so this computer could not actually even store the amount of text in this answer - not counting the pictures - in memory). It had a monochrome monitor (which was basically a TV) and could fit 16 lines of text at 64 characters per line. It had a “graphics” mode of 128x48 pixels (for the whole screen.) Some Windows or Mac icons are taller than that at 64x64 pixels. It was not properly shielded, so when I ran it, it caused interference on the “real” TV in the same room.

As you can see, it had no hard drive storage, or even floppy drive storage. To load in programs, you can to use the cassette tape recorder (which was just a basic tape player with cords to plug into the computer.) You had to get the volume level just right and fast forward through the tape to find the part that had what you wanted to load. (You used a tape counter to measure it.)

It came with a book of games that you had to type into it in BASIC. Someone gave me a book of games for the TRS-80 but unfortunately it was for Level 2 BASIC and I only had Level 1. So, in order to play the games, I had to understand the difference between the two languages and basically port the games from level 2 to level 1. Some games were too complex for Level 1, but for those I could, I simplified them and typed them in and saved them on tape. I also wrote a lot of games myself so I could play them. It took about 20 minutes to save one of these programs to the cassette tape (then another 20 minutes to verify it by rewinding the tape and asking the computer to read it back in and confirm it saved right.) The processor speed was about 900 KHz. An IBM PC of the 1980’s was about 4MHz. A modern computer runs at least 3 GHz, well over 1000 times as fast (just in raw CPU usage. The memory speed is equally enhanced.)

The keyboard only supported uppercase text. Eventually I got it upgraded to 16K of RAM and Level 2 BASIC and Upper/Lower case. Eventually I also got the floppy disk add-on (which also required an expansion interface.) The floppy drive alone cost $500 in 1981 and the expansion interface cost $300. The disks were 5 1/4 inch, (and cost $5 each in a package of 10) single sided disks and stored, if I recall, 90K per disk. I remember that CDs came out around then and a CD stored 44000 samples per second, so this floppy could store 2 seconds of music (not that you could play it all back in 2 seconds.)

Here’s a super fancy version with 2 (!) floppy drives: (note that the monitor is now on top of the expansion interface.)

One thing the computer did better than today is that it would start up in about 1 second and be ready to type into before the screen brightened. It had no operating system to read from disk - it just loaded its starting instructions from ROM.

I was in high school then and no one knew how to program these things. I taught myself and got jobs writing software. There was a mattress factory right by my house and I wrote them a database for a couple hundred dollars. Beat mowing lawns as a summer job! I taught myself how to program on that thing and it has served me well. I really learned how the computer worked. I still have this computer but haven’t turned it on lately. But I did run my Model 4 that I used in college not too long ago and it worked great.

This image is of a game I wrote for it and this is as high-res as it got. It had enough memory for things like checkers. I remember the chess game it had barely fit into memory and couldn’t store all the rules. There are some moves you could make and it wouldn’t stop you because it didn’t have the logic to check every valid move.

Where my Mom worked they had the business computer which was the TRS-80 Model II. This had an optional hard drive:

This thing was about 2 1/2 feet deep, about 2 feet wide, and 4 inches tall. As you see, it has a key to start it (you can push the red button to write protect it). It stored a whopping 8 MB of storage (but was very slow, and used the SCSI interface). When it was running it sounded like an airplane taking off and if you bumped it while it was writing it would head crash and you’d lose data. When they were done with it, they gave it to me and I still have it. [Edit: A prior version of this answer listed an incorrect price. Thanks, Phillip for the correction!]

Keep in mind that an average new car in 1980 cost $7000 and gas was 90 cents a gallon.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/1iyu9zmlrpt6ceoh', 'title': 'How was the first home computer in the 1980s different from the computers that are in our homes now?', 'score': {'original': 0.99485, 'ai': 0.00515}, 'blocks': [{'text': 'Here is a picture of my first computer, TRS-80 Model I (picture from Wikipedia)\n\nThe “keyboard” was the entire computer. It had 4K of RAM (of which 1K was for the screen). (For reference, the text in this answer is over 3700 letters, so this computer could not actually even store the amount of text in this answer - not counting the pictures - in memory). It had a monochrome monitor (which was basically a TV) and could fit 16 lines of text at 64 characters per line. It had a “graphics” mode of 128x48 pixels (for the whole screen.) Some Windows or Mac icons are taller than that at 64x64 pixels. It was not properly shielded, so when I ran it, it caused interference on the “real” TV in the same room.\n\nAs you can see, it had no hard drive storage, or even floppy drive storage. To load in programs, you can to use the cassette tape recorder (which was just a basic tape player with cords to plug into the computer.) You had to get the volume level just right and fast forward through the tape to find the part that had what you wanted to load. (You used a tape counter to measure it.)\n\nIt came with a book of games that you had to type into it in BASIC. Someone gave me a book of games for the TRS-80 but unfortunately it was for Level 2 BASIC and I only had Level 1. So, in order to play the games, I had to understand the difference between the two languages and basically port the games from level 2 to level 1. Some games were too complex for Level 1, but for those I could, I simplified them and typed them in and saved them on tape. I also wrote a lot of games myself so I could play them. It took about 20 minutes to save one of these programs to the cassette tape (then another 20 minutes to verify it by rewinding the tape and asking the computer to read it back in and confirm it saved right.) The processor speed was about 900 KHz. An IBM PC of the 1980’s was about 4MHz. A modern computer runs at least 3 GHz, well over 1000 times as fast (just in raw CPU usage. The memory speed is equally enhanced.)\n\nThe keyboard only supported uppercase text. Eventually I got it upgraded to 16K of RAM and Level 2 BASIC and Upper/Lower case. Eventually I also got the floppy disk add-on (which also required an expansion interface.) The floppy drive alone cost $500 in 1981 and the expansion interface cost $300. The disks were 5 1/4 inch, (and cost $5 each in a package of 10) single sided disks and stored, if I recall, 90K per disk. I remember that CDs came out around then and a CD stored 44000 samples per second, so this floppy could store 2 seconds of music (not that you could play it all back in 2 seconds.)\n\nHere’s a super fancy version with 2 (!) floppy drives:', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}, {'text': '(note that the monitor is now on top of the expansion interface.)\n\nOne thing the computer did better than today is that it would start up in about 1 second and be ready to type into before the screen brightened. It had no operating system to read from disk - it just loaded its starting instructions from ROM.\n\nI was in high school then and no one knew how to program these things. I taught myself and got jobs writing software. There was a mattress factory right by my house and I wrote them a database for a couple hundred dollars. Beat mowing lawns as a summer job! I taught myself how to program on that thing and it has served me well. I really learned how the computer worked. I still have this computer but haven’t turned it on lately. But I did run my Model 4 that I used in college not too long ago and it worked great.\n\nThis image is of a game I wrote for it and this is as high-res as it got. It had enough memory for things like checkers. I remember the chess game it had barely fit into memory and couldn’t store all the rules. There are some moves you could make and it wouldn’t stop you because it didn’t have the logic to check every valid move.\n\nWhere my Mom worked they had the business computer which was the TRS-80 Model II. This had an optional hard drive:\n\nThis thing was about 2 1/2 feet deep, about 2 feet wide, and 4 inches tall. As you see, it has a key to start it (you can push the red button to write protect it). It stored a whopping 8 MB of storage (but was very slow, and used the SCSI interface). When it was running it sounded like an airplane taking off and if you bumped it while it was writing it would head crash and you’d lose data. When they were done with it, they gave it to me and I still have it. [Edit: A prior version of this answer listed an incorrect price. Thanks, Phillip for the correction!]\n\nKeep in mind that an average new car in 1980 cost $7000 and gas was 90 cents a gallon.', 'result': {'fake': 0.0008, 'real': 0.9992}, 'status': 'success'}], 'credits_used': 9, 'credits': 1995202, 'subscription': 0, 'content': 'Here is a picture of my first computer, TRS-80 Model I (picture from Wikipedia)\n\nThe “keyboard” was the entire computer. It had 4K of RAM (of which 1K was for the screen). (For reference, the text in this answer is over 3700 letters, so this computer could not actually even store the amount of text in this answer - not counting the pictures - in memory). It had a monochrome monitor (which was basically a TV) and could fit 16 lines of text at 64 characters per line. It had a “graphics” mode of 128x48 pixels (for the whole screen.) Some Windows or Mac icons are taller than that at 64x64 pixels. It was not properly shielded, so when I ran it, it caused interference on the “real” TV in the same room.\n\nAs you can see, it had no hard drive storage, or even floppy drive storage. To load in programs, you can to use the cassette tape recorder (which was just a basic tape player with cords to plug into the computer.) You had to get the volume level just right and fast forward through the tape to find the part that had what you wanted to load. (You used a tape counter to measure it.)\n\nIt came with a book of games that you had to type into it in BASIC. Someone gave me a book of games for the TRS-80 but unfortunately it was for Level 2 BASIC and I only had Level 1. So, in order to play the games, I had to understand the difference between the two languages and basically port the games from level 2 to level 1. Some games were too complex for Level 1, but for those I could, I simplified them and typed them in and saved them on tape. I also wrote a lot of games myself so I could play them. It took about 20 minutes to save one of these programs to the cassette tape (then another 20 minutes to verify it by rewinding the tape and asking the computer to read it back in and confirm it saved right.) The processor speed was about 900 KHz. An IBM PC of the 1980’s was about 4MHz. A modern computer runs at least 3 GHz, well over 1000 times as fast (just in raw CPU usage. The memory speed is equally enhanced.)\n\nThe keyboard only supported uppercase text. Eventually I got it upgraded to 16K of RAM and Level 2 BASIC and Upper/Lower case. Eventually I also got the floppy disk add-on (which also required an expansion interface.) The floppy drive alone cost $500 in 1981 and the expansion interface cost $300. The disks were 5 1/4 inch, (and cost $5 each in a package of 10) single sided disks and stored, if I recall, 90K per disk. I remember that CDs came out around then and a CD stored 44000 samples per second, so this floppy could store 2 seconds of music (not that you could play it all back in 2 seconds.)\n\nHere’s a super fancy version with 2 (!) floppy drives: (note that the monitor is now on top of the expansion interface.)\n\nOne thing the computer did better than today is that it would start up in about 1 second and be ready to type into before the screen brightened. It had no operating system to read from disk - it just loaded its starting instructions from ROM.\n\nI was in high school then and no one knew how to program these things. I taught myself and got jobs writing software. There was a mattress factory right by my house and I wrote them a database for a couple hundred dollars. Beat mowing lawns as a summer job! I taught myself how to program on that thing and it has served me well. I really learned how the computer worked. I still have this computer but haven’t turned it on lately. But I did run my Model 4 that I used in college not too long ago and it worked great.\n\nThis image is of a game I wrote for it and this is as high-res as it got. It had enough memory for things like checkers. I remember the chess game it had barely fit into memory and couldn’t store all the rules. There are some moves you could make and it wouldn’t stop you because it didn’t have the logic to check every valid move.\n\nWhere my Mom worked they had the business computer which was the TRS-80 Model II. This had an optional hard drive:\n\nThis thing was about 2 1/2 feet deep, about 2 feet wide, and 4 inches tall. As you see, it has a key to start it (you can push the red button to write protect it). It stored a whopping 8 MB of storage (but was very slow, and used the SCSI interface). When it was running it sounded like an airplane taking off and if you bumped it while it was writing it would head crash and you’d lose data. When they were done with it, they gave it to me and I still have it. [Edit: A prior version of this answer listed an incorrect price. Thanks, Phillip for the correction!]\n\nKeep in mind that an average new car in 1980 cost $7000 and gas was 90 cents a gallon.', 'aiModelVersion': '1'}",0.99485
Andrew McGregor,6y,What was programming like back in the days when computers only had kilobytes of RAM?,"Well, if you want to find out, do this:

Buy an Arduino and a 7-segment display and keypad module.
Load up a Linux machine with AVR development tools, VIM, and no X11 so you have to use the console. For best effect, use a Raspberry Pi and a 4GB SD card for your development machine (bear in mind, back in the day, 4GB was an enormous hard drive, and the Pi has gobs of memory and is scorchingly fast… but anything smaller now is going to make this exercise harder than it should be).
Print all the documentation you think you will need, put it in binders, and never use a browser or Google (or any other search engine) for the rest of the exercise. You may use email, that has been around since the 1970s, and if you really have to download something that’s probably OK too… but if you download anything, you’re done for the day.
Turn the Arduino into a pocket calculator.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/hi0tlcdpybz4rng7', 'title': 'What was programming like back in the days when computers only had kilobytes of RAM?', 'score': {'original': 0.9958, 'ai': 0.0042}, 'blocks': [{'text': 'Well, if you want to find out, do this:\n\nBuy an Arduino and a 7-segment display and keypad module.\nLoad up a Linux machine with AVR development tools, VIM, and no X11 so you have to use the console. For best effect, use a Raspberry Pi and a 4GB SD card for your development machine (bear in mind, back in the day, 4GB was an enormous hard drive, and the Pi has gobs of memory and is scorchingly fast… but anything smaller now is going to make this exercise harder than it should be).\nPrint all the documentation you think you will need, put it in binders, and never use a browser or Google (or any other search engine) for the rest of the exercise. You may use email, that has been around since the 1970s, and if you really have to download something that’s probably OK too… but if you download anything, you’re done for the day.\nTurn the Arduino into a pocket calculator.', 'result': {'fake': 0.0042, 'real': 0.9958}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995200, 'subscription': 0, 'content': 'Well, if you want to find out, do this:\n\nBuy an Arduino and a 7-segment display and keypad module.\nLoad up a Linux machine with AVR development tools, VIM, and no X11 so you have to use the console. For best effect, use a Raspberry Pi and a 4GB SD card for your development machine (bear in mind, back in the day, 4GB was an enormous hard drive, and the Pi has gobs of memory and is scorchingly fast… but anything smaller now is going to make this exercise harder than it should be).\nPrint all the documentation you think you will need, put it in binders, and never use a browser or Google (or any other search engine) for the rest of the exercise. You may use email, that has been around since the 1970s, and if you really have to download something that’s probably OK too… but if you download anything, you’re done for the day.\nTurn the Arduino into a pocket calculator.', 'aiModelVersion': '1'}",0.9958
Jamie Lawson,Updated 1y,What was the last breakthrough in computer programming?,"Monads and monoids.

Parallel programming has been a thing for 50 years. But the problem is that it was always too hard, and then too hard to verify and so it lived in niche applications. But a decade ago, processors stopped getting faster because we reached some physical limits. Processing power continued to increase, but by adding more cores rather than faster clock speeds. This meant that in order to utilize those processors, we needed to finally get serious about parallel programming.

The big “new” idea in parallel programming is to express the computation rather than the process. Monads and monoids are very very useful devices for expressing those computations. For instance, let’s say that I want to sum the first 1 million prime numbers. I could describe a procedure for doing this with a loop starting from the first prime, then going to the next prime, etc. Or I could describe the computation with a monad (or monoid). Let’s use Wolfram syntax:

Plus @@ Table[Prime[i], {i, 1000000}] 

Plus is the sum-monoid in Wolfram. This doesn’t mean to find the first prime and then the second and then the third, and so on and then add the second to the first and the third to that and so on. This just says to take the sum of the first 1000000 primes. Maybe that’s a little sloppy. It says to find the first million primes and then find their total, so the programmer still has something to say about the steps, but that constrain the computation very much. The Wolfram kernel is free to find the primes in any order that it pleases, and then sum the primes in any order that it pleases. If there are 10 cores available, it can break the tasks into 10 chunks and dispatch each chunk to a different core. It specifies the computation, not the process. The processor is then at liberty to find the best procedure to realize the computation. This is huge because it relieves the programmer of much of the burden of parallelization. If I move the code to a different system, I don’t have to make any changes to make it exploit the resources of that system, because I’m still just describing the computation. If the Wolfram kernel gets souped up and improved in the next revision and makes better choices, I don’t have to make any changes to the code. I’m still describing the same computation, it’s just that it’s more efficient now.

Perhaps most important of all, it’s easier to prove that the code is correct. When you express your computations in monads (and monoids) rather than in procedures, verification tends to be a matter of demonstrating that you didn’t make any typing errors. You don’t have to verify the steps of a procedure. This is important, and will become more important in the future. To demonstrate this consider a definition of the factorial function like this:

Fact[i_Integer/;NonNegative[i]] := Times @@ Range[i] 

Easy to verify. Basically it says that if i is a positive integer, Fact[i] is the product of the integers from 1 to i (and if i is 0 Fact[i] is the multiplicative identity). That’s the formal definition of a factorial. Thumbs up, we typed it in right. Now consider an imperative procedural definition:

Fact[0] = 1; 
Fact[i_Integer/;i>=1] := Module[{prod=1},  
   For[j=1, j<=i, j++, prod=prod j] 
] 

Okay, we can still verify it, but it’s a lot harder, because what we are interested in is whether the coded function is equivalent to the formal definition of the function, and the coded function has little similarity to the formal definition of the function. There’s a loop variable, and an accumulator variable and they’re changing all the time. Do they do the right thing at the right time? And this is a very very simple example. As our computation gets more complex, the monad/monoid approach is still about making sure we typed the formal definition in correctly, and the procedural definition is still about loop variables and accumulators and do they do the right things at the right times and do the steps of the procedure actually accomplish what the formal definition specifies. It gets a lot harder.

Another way to think of this is that we recognized the central importance of associativity and finally provided a general syntax for expressing things that are associative. Monoids are syntax for expressing associative things within a type and monads are syntax for expressing associative things across types. And once we have that syntax for associative things, we can use it to build even larger computations (packages of associative things within associative things within associative things) that can be offered to processors in the aggregate, and the processor can exploit the associativity to unroll that computation. That is what MapReduce is: syntax for packaging very large computations (so long as they are associative). MapReduce is just one kind of monad.

Hence, monads and monoids are the latest big breakthrough in programming.

AFTERWORD:

There has been a lot of discussion on this thread about “yeah, but then you have to verify that the prime number generator works”, or “but then you have to verify that the runtime system works”. This is just wrong and since it has come up over and over, it’s important to address those concerns directly. None of this should have to be said…but apparently it has to be said anyway.

The Black Box Abstraction is key to all of science, engineering, computing, and in fact to civilization itself. A result once established can be used without knowing how the result was achieved. Once a black box has been verified, I can rely on that black box to satisfy its specification. In mathematics, we can rely on an accepted theorem without proving the theorem all over again. It’s already accepted. That’s basically what this is about.

Perhaps the most apt example comes in proofs of NP-Completeness. Pretty much all proofs of NP-Completeness are performed by polynomial reduction. I show that some problem known to be NP-Complete can be reduced (in polynomial time) to my problem. And so if my problem can be solved in polynomial time, then so can the other one. That’s basically the whole proof. So let’s say that the Traveling Salesman Problem had not been shown to be NP-Complete. I can show that the Hamilton Cycle Problem (which has been shown to be NP-Complete) can be transformed into a Traveling Salesman Problem (in polynomial time). And with an ounce of housekeeping, I can declare the Traveling Salesman Problem to be NP-Complete. I don’t have to re-prove that the Hamilton Cycle Problem is NP-Complete. I can depend on it because that fact has already been accepted. And to take this a step further, I don’t have to re-prove that Hamiltonian Path Problem that Hamilton Cycle is reduced from is NP-Complete, and I don’t have to re-prove that the 3-SAT problem which Hamiltonian Cycle is reduced from is NP-Complete and I don’t have to re-prove that the Circuit Satisfiability Problem that 3-SAT is reduced from is NP-Complete. All of those results have been accepted. I don’t have to redo them. And the chain of accepted results is an accepted result. I rely on a foundation of accepted truth.

Each of those problems is a black box. And each of those black boxes has been verified to meet its specification. I have to verify that the way I used those black boxes is meets my specification (i.e. that the Traveling Salesman Problem is NP-Complete) but the way that I use those black boxes is the only thing that’s new; the only thing that hasn’t been verified to be true.

The same is true in programming. If functions are well specified, and verified to meet their specifications, I can use those functions as black boxes without additional verification. In the famous Abelson and Sussman Structure and Interpretation of Computer Programs lectures: Jerry Sussman says:

Not only are black boxes a useful tool for isolating components of a system, they also provide the basis for connecting things together. A key issue is providing methods for connecting together basic units into components that themselves can be treated as basic units. Thus, we will spend a lot of time talking about conventional interfaces -- standard ways of interconnecting simpler pieces. This is much like hooking up parts of a stereo system. One has standard interfaces by which components can be intertwined, and this can be done without worrying about the internal aspects of the components. Similarly in programming, we will describe conventions for interfacing simpler components to create new elements that can further be connected together [emphasis added].

In most programming situations, we lack the level of verification present in formal proofs. That’s coming, but we aren’t there yet. But there are ways to give great confidence that a software component or platform meets its specification. For instance, any implementation of the Java Standard Edition platform must pass (literally) millions of compatibility tests. The runtime alone on a large cluster takes days. Once all tests are passed, that version of the JSE is “certified” to be compatible, and users need not bother themselves to verify that their JSE is actually Java. It’s certified to be compliant to the Java specification, and we can use it with confidence and without further verification on our part. If we don’t trust the tests, we shouldn’t program on the Java Platform. If we do trust the tests, we can use a certified platform with confidence. What is more, if we don’t trust the tests and wish to verify the Java Platform independently, it would take thousands of man-years to develop the same level of confidence as the JavaTest Harness imbues. So we trust it and accept it as a black box, or we don’t trust and don’t use.

Much the same is true for a platform like Mathematica (which supports the extraordinarily powerful Wolfram language). The examples above are in Wolfram, and in a couple of places exploit some of the terseness and power of Wolfram. The Wolfram test suites are massive, and no Mathematica version is released without passing all of the tests. If you trust it, you can use it as a black box without doing further verification, and if you don’t trust it, don’t use it! But developing your own tests of the Mathematica kernel and standard libraries and developing your own verification plan will not be productive unless you have thousands of man-years available to devote to it. It is an industry standard with millions of users. Painstaking care is done to make it right, and you either trust it or you don’t.

As for the specific question in the thread about proving that Mathematica’s Prime[] function works, there are plenty of computations in the peer-reviewed literature that rely on the Prime[] function, and the function is well-documented and verified by many tests and thorough analysis. Another way of thinking about this is that our computations act on whatever the Prime[] function in Mathematica returns. This is generally agreed to be the primes in the community of mathematicians, but if you are suspicious of that, just think of these computations as “whatever the Prime[] function returns be it the actual prime numbers or not”.

The question of trusting the Mathematica kernel (or the JVM or the Scala runtime or the Haskell runtime) goes back to our original statement. These platforms are extremely well vetted and the compatibility test suites are vast and the communities that verify these platforms and the communities of users that use these platforms are all huge. But if you don’t trust one of these platforms, you shouldn’t use it. Pick a platform that you trust and use it. And if you don’t trust any computational platform, you probably shouldn’t be programming.

Oddly, there do not seem to be such suspicions about, say, the C compiler and its standard libraries. Some of the critics here are arguing that the Mathematica kernel needs to be verified by each new user for each new application. But these critics do not share that suspicion for malloc() function or the sin() function in C and are comfortable using them without extensive independent verification. There seems to be an implicit belief within these programmers that purely imperative programming is somehow more trustable. There is no reason to believe this.

There are various competing models of computation, that have all been verified to be theoretically equivalent. One of the first high-level programming languages (Fortran) was based on the Turing Machine. That’s an imperative model of computation. The other very early high-level language (Lisp) was based on Lambda Calculus. The Lambda Calculus is equally powerful, but doesn’t use an imperative model. It doesn’t say “do this instruction, then do that instruction, then the next instruction”. In the Lambda Calculus/Lisp model of computation, you define a computation and it is evaluated. The actual sequence of instructions that are performed is irrelevant. Like Prof. Sussman said in the quote above, we don’t worry about the internals of how that happens. That’s not how we define a program in these more declarative computational norms. There’s no magic. It’s just a different way of defining computation. Our program is correct if it uses the specifications of the components it depends on correctly.

What is more, it is vastly easier to verify a program in a Lambda Calculus-based computational model than it is in an a purely imperative model. And that’s not just me saying that. The Lambda Calculus model is based on pure functions and we know much much more about pure functions than we do about imperative structures. The factorial example we gave above is good. The functional, Lambda Calculus-based definition is easy to verify, and the imperative version is a bit of a tussle to verify. Even beyond this, most imperative languages don’t stay strict to the given instructions. The optimizer will move redundant assignments out of loops, strip unused functions, and a whole lot more. And that’s good because the optimizer is fixing our mistakes. But the order of instructions that you write in code doesn’t match the instructions performed by the machine. It’s an “equivalent” set of instructions: a set of instructions that promises to give the same results as the instructions you specified.

Even more important is the essence of the Black Box Abstraction model. You don’t have to look inside someone else’s black box. That principle is the bedrock of systems engineering. We decompose the system into independent components, and we build a specification for each component. I have my component and my swimlane. I build my component to meet its specification and I verify that it does so. There are other components in other swimlanes, and some of these depend on my component. Those swimlanes need to trust my verification. Likewise, my component depends on other components. And I build my component without crossing over into those swimlanes. I know what the specifications of those components are and I build my components to exercise those specifications, but I don’t have to do a code review and re-validation of those components; I don’t look inside their black box. It’s their black box; their swimlane. If we’re all in the same building, I may meet with the product managers of the other components, and I may have questions about their verification process, the answers to which will hopefully give me trust in those components. But I do not re-verify those components because that would vastly increase the complexity and cost of system development. If I don’t trust a component, I don’t use it. I may be able to talk to the product manager of that component and explain what he or she needs to do to gain my trust, but I don’t cross into their swimlane; I don’t re-verify their component. If their component is a COTS product like the Java Standard Edition, it has its own verification and certification process, which, as mentioned above is probably vast, and I either trust it or I don’t. If I don’t trust it, I don’t use it. But I don’t have the hubris to believe that I can identify faults that were overlooked by thousands and thousands of reviewers who know much more about how the product works than I do, and whose job it is to verify that the product meets its specification.

This strategy of doing my best job in my swimlane and letting others do their best job in their swimlane works. The vast majority of faults exhibited in the wild are not attributable to the major development platforms. The faults are almost always in the enduser program that failed to meet its specifications (whether those specifications were explicit or not): buffer overflow errors that weren’t trapped, message replay errors that were improperly programmed, or, in the case of my former employer (not my swimlane) failing to convert English units to metric units (yes, we lost a Mars lander that way; not a project I was involved in). In the famous case of the THERAC-25, the hardware and operating system performed to their specification, and the application program had a concurrency error that created a deadlock condition that cost lives. All of these are programming errors close to the enduser, and far from the computational platforms those programs operate on. All of these cases could have been eliminated if the application programmers had done a better job of verifying the code in their swimlane. None of these problems would have been prevented by the application programmers crossing into other swimlanes and trying to re-verify the platforms that had already been verified, certified, and accepted as industry standards by the community.

There was a time, early in the life of the Java Platform where there was a list of about a dozen vulnerabilities. And the list scared people, as it should have. But none of these was exploited in the wild (to the best of our knowledge). And the faults were identified not by endusers, but by people in the community whose role it was to verify the Java Platform: mostly professors engaged to look at the APIs and find their faults. In other words, the system worked.

The lesson in all of this is to program with humility. Stay in your swimlane and spend your energy verifying that your work is correct. And don’t focus so much on the instruction level. Computation is not about specifying sequences of instructions. There are some computational models that work that way. But not all computational models work that way. Some computational models work by building computations and passing them to a (trusted) black box for evaluation. These models are at least equally valid, and provide the advantage that it is easier to verify that the computations are correct, and easier to distribute these computations to multiple processors.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0k8pwxj21yeo5svm', 'title': 'What was the last breakthrough in computer programming?', 'score': {'original': 0.61624285714286, 'ai': 0.38375714285714}, 'blocks': [{'text': 'Monads and monoids.\n\nParallel programming has been a thing for 50 years. But the problem is that it was always too hard, and then too hard to verify and so it lived in niche applications. But a decade ago, processors stopped getting faster because we reached some physical limits. Processing power continued to increase, but by adding more cores rather than faster clock speeds. This meant that in order to utilize those processors, we needed to finally get serious about parallel programming.\n\nThe big “new” idea in parallel programming is to express the computation rather than the process. Monads and monoids are very very useful devices for expressing those computations. For instance, let’s say that I want to sum the first 1 million prime numbers. I could describe a procedure for doing this with a loop starting from the first prime, then going to the next prime, etc. Or I could describe the computation with a monad (or monoid). Let’s use Wolfram syntax:\n\nPlus @@ Table[Prime[i], {i, 1000000}]\xa0\n\nPlus is the sum-monoid in Wolfram. This doesn’t mean to find the first prime and then the second and then the third, and so on and then add the second to the first and the third to that and so on. This just says to take the sum of the first 1000000 primes. Maybe that’s a little sloppy. It says to find the first million primes and then find their total, so the programmer still has something to say about the steps, but that constrain the computation very much. The Wolfram kernel is free to find the primes in any order that it pleases, and then sum the primes in any order that it pleases. If there are 10 cores available, it can break the tasks into 10 chunks and dispatch each chunk to a different core. It specifies the computation, not the process. The processor is then at liberty to find the best procedure to realize the computation. This is huge because it relieves the programmer of much of the burden of parallelization. If I move the code to a different system, I don’t have to make any changes to make it exploit the resources of that system, because I’m still just describing the computation. If the Wolfram kernel gets souped up and improved in the next revision and makes better choices, I don’t have to make any changes to the code. I’m still describing the same computation, it’s just that it’s more efficient now.\n\nPerhaps most important of all, it’s easier to prove that the code is correct. When you express your computations in monads (and monoids) rather than in procedures, verification tends to be a matter of demonstrating that you didn’t make any typing errors. You don’t have to verify the steps of a procedure. This is important, and will become more important in the future. To demonstrate this consider a definition of the factorial function like this:\n\nFact[i_Integer/;NonNegative[i]] := Times @@ Range[i]\xa0\n\nEasy to verify. Basically it says that if i is a positive integer, Fact[i] is the product of the integers from 1 to i (and', 'result': {'fake': 0.0118, 'real': 0.9882}, 'status': 'success'}, {'text': 'if i is 0 Fact[i] is the multiplicative identity). That’s the formal definition of a factorial. Thumbs up, we typed it in right. Now consider an imperative procedural definition:\n\nFact[0] = 1;\xa0\nFact[i_Integer/;i>=1] := Module[{prod=1}, \xa0\n   For[j=1, j<=i, j++, prod=prod j]\xa0\n]\xa0\n\nOkay, we can still verify it, but it’s a lot harder, because what we are interested in is whether the coded function is equivalent to the formal definition of the function, and the coded function has little similarity to the formal definition of the function. There’s a loop variable, and an accumulator variable and they’re changing all the time. Do they do the right thing at the right time? And this is a very very simple example. As our computation gets more complex, the monad/monoid approach is still about making sure we typed the formal definition in correctly, and the procedural definition is still about loop variables and accumulators and do they do the right things at the right times and do the steps of the procedure actually accomplish what the formal definition specifies. It gets a lot harder.\n\nAnother way to think of this is that we recognized the central importance of associativity and finally provided a general syntax for expressing things that are associative. Monoids are syntax for expressing associative things within a type and monads are syntax for expressing associative things across types. And once we have that syntax for associative things, we can use it to build even larger computations (packages of associative things within associative things within associative things) that can be offered to processors in the aggregate, and the processor can exploit the associativity to unroll that computation. That is what MapReduce is: syntax for packaging very large computations (so long as they are associative). MapReduce is just one kind of monad.\n\nHence, monads and monoids are the latest big breakthrough in programming.\n\nAFTERWORD:\n\nThere has been a lot of discussion on this thread about “yeah, but then you have to verify that the prime number generator works”, or “but then you have to verify that the runtime system works”. This is just wrong and since it has come up over and over, it’s important to address those concerns directly. None of this should have to be said…but apparently it has to be said anyway.\n\nThe Black Box Abstraction is key to all of science, engineering, computing, and in fact to civilization itself. A result once established can be used without knowing how the result was achieved. Once a black box has been verified, I can rely on that black box to satisfy its specification. In mathematics, we can rely on an accepted theorem without proving the theorem all over again. It’s already accepted. That’s basically what this is about.\n\nPerhaps the most apt example comes in proofs of NP-Completeness. Pretty much all proofs of NP-Completeness are performed by polynomial reduction. I show that some problem known to be NP-Complete can be reduced (in polynomial time) to my problem. And so if my problem can be solved in polynomial time, then so can the other one. That’s basically the whole proof. So', 'result': {'fake': 0.0411, 'real': 0.9589}, 'status': 'success'}, {'text': 'let’s say that the Traveling Salesman Problem had not been shown to be NP-Complete. I can show that the Hamilton Cycle Problem (which has been shown to be NP-Complete) can be transformed into a Traveling Salesman Problem (in polynomial time). And with an ounce of housekeeping, I can declare the Traveling Salesman Problem to be NP-Complete. I don’t have to re-prove that the Hamilton Cycle Problem is NP-Complete. I can depend on it because that fact has already been accepted. And to take this a step further, I don’t have to re-prove that Hamiltonian Path Problem that Hamilton Cycle is reduced from is NP-Complete, and I don’t have to re-prove that the 3-SAT problem which Hamiltonian Cycle is reduced from is NP-Complete and I don’t have to re-prove that the Circuit Satisfiability Problem that 3-SAT is reduced from is NP-Complete. All of those results have been accepted. I don’t have to redo them. And the chain of accepted results is an accepted result. I rely on a foundation of accepted truth.\n\nEach of those problems is a black box. And each of those black boxes has been verified to meet its specification. I have to verify that the way I used those black boxes is meets my specification (i.e. that the Traveling Salesman Problem is NP-Complete) but the way that I use those black boxes is the only thing that’s new; the only thing that hasn’t been verified to be true.\n\nThe same is true in programming. If functions are well specified, and verified to meet their specifications, I can use those functions as black boxes without additional verification. In the famous Abelson and Sussman Structure and Interpretation of Computer Programs lectures: Jerry Sussman says:\n\nNot only are black boxes a useful tool for isolating components of a system, they also provide the basis for connecting things together. A key issue is providing methods for connecting together basic units into components that themselves can be treated as basic units. Thus, we will spend a lot of time talking about conventional interfaces -- standard ways of interconnecting simpler pieces. This is much like hooking up parts of a stereo system. One has standard interfaces by which components can be intertwined, and this can be done without worrying about the internal aspects of the components. Similarly in programming, we will describe conventions for interfacing simpler components to create new elements that can further be connected together [emphasis added].\n\nIn most programming situations, we lack the level of verification present in formal proofs. That’s coming, but we aren’t there yet. But there are ways to give great confidence that a software component or platform meets its specification. For instance, any implementation of the Java Standard Edition platform must pass (literally) millions of compatibility tests. The runtime alone on a large cluster takes days. Once all tests are passed, that version of the JSE is “certified” to be compatible, and users need not bother themselves to verify that their JSE is actually Java. It’s certified to be compliant to the Java specification, and we can use it with confidence and', 'result': {'fake': 0.001, 'real': 0.999}, 'status': 'success'}, {'text': 'without further verification on our part. If we don’t trust the tests, we shouldn’t program on the Java Platform. If we do trust the tests, we can use a certified platform with confidence. What is more, if we don’t trust the tests and wish to verify the Java Platform independently, it would take thousands of man-years to develop the same level of confidence as the JavaTest Harness imbues. So we trust it and accept it as a black box, or we don’t trust and don’t use.\n\nMuch the same is true for a platform like Mathematica (which supports the extraordinarily powerful Wolfram language). The examples above are in Wolfram, and in a couple of places exploit some of the terseness and power of Wolfram. The Wolfram test suites are massive, and no Mathematica version is released without passing all of the tests. If you trust it, you can use it as a black box without doing further verification, and if you don’t trust it, don’t use it! But developing your own tests of the Mathematica kernel and standard libraries and developing your own verification plan will not be productive unless you have thousands of man-years available to devote to it. It is an industry standard with millions of users. Painstaking care is done to make it right, and you either trust it or you don’t.\n\nAs for the specific question in the thread about proving that Mathematica’s Prime[] function works, there are plenty of computations in the peer-reviewed literature that rely on the Prime[] function, and the function is well-documented and verified by many tests and thorough analysis. Another way of thinking about this is that our computations act on whatever the Prime[] function in Mathematica returns. This is generally agreed to be the primes in the community of mathematicians, but if you are suspicious of that, just think of these computations as “whatever the Prime[] function returns be it the actual prime numbers or not”.\n\nThe question of trusting the Mathematica kernel (or the JVM or the Scala runtime or the Haskell runtime) goes back to our original statement. These platforms are extremely well vetted and the compatibility test suites are vast and the communities that verify these platforms and the communities of users that use these platforms are all huge. But if you don’t trust one of these platforms, you shouldn’t use it. Pick a platform that you trust and use it. And if you don’t trust any computational platform, you probably shouldn’t be programming.\n\nOddly, there do not seem to be such suspicions about, say, the C compiler and its standard libraries. Some of the critics here are arguing that the Mathematica kernel needs to be verified by each new user for each new application. But these critics do not share that suspicion for malloc() function or the sin() function in C and are comfortable using them without extensive independent verification. There seems to be an implicit belief within these programmers that purely imperative programming is somehow more trustable. There is no reason to believe this.\n\nThere are various competing models of computation, that', 'result': {'fake': 0.6157, 'real': 0.3843}, 'status': 'success'}, {'text': 'have all been verified to be theoretically equivalent. One of the first high-level programming languages (Fortran) was based on the Turing Machine. That’s an imperative model of computation. The other very early high-level language (Lisp) was based on Lambda Calculus. The Lambda Calculus is equally powerful, but doesn’t use an imperative model. It doesn’t say “do this instruction, then do that instruction, then the next instruction”. In the Lambda Calculus/Lisp model of computation, you define a computation and it is evaluated. The actual sequence of instructions that are performed is irrelevant. Like Prof. Sussman said in the quote above, we don’t worry about the internals of how that happens. That’s not how we define a program in these more declarative computational norms. There’s no magic. It’s just a different way of defining computation. Our program is correct if it uses the specifications of the components it depends on correctly.\n\nWhat is more, it is vastly easier to verify a program in a Lambda Calculus-based computational model than it is in an a purely imperative model. And that’s not just me saying that. The Lambda Calculus model is based on pure functions and we know much much more about pure functions than we do about imperative structures. The factorial example we gave above is good. The functional, Lambda Calculus-based definition is easy to verify, and the imperative version is a bit of a tussle to verify. Even beyond this, most imperative languages don’t stay strict to the given instructions. The optimizer will move redundant assignments out of loops, strip unused functions, and a whole lot more. And that’s good because the optimizer is fixing our mistakes. But the order of instructions that you write in code doesn’t match the instructions performed by the machine. It’s an “equivalent” set of instructions: a set of instructions that promises to give the same results as the instructions you specified.\n\nEven more important is the essence of the Black Box Abstraction model. You don’t have to look inside someone else’s black box. That principle is the bedrock of systems engineering. We decompose the system into independent components, and we build a specification for each component. I have my component and my swimlane. I build my component to meet its specification and I verify that it does so. There are other components in other swimlanes, and some of these depend on my component. Those swimlanes need to trust my verification. Likewise, my component depends on other components. And I build my component without crossing over into those swimlanes. I know what the specifications of those components are and I build my components to exercise those specifications, but I don’t have to do a code review and re-validation of those components; I don’t look inside their black box. It’s their black box; their swimlane. If we’re all in the same building, I may meet with the product managers of the other components, and I may have questions about their verification process, the answers to which will hopefully give me trust in those components. But I do not re-verify those components because', 'result': {'fake': 0.3033, 'real': 0.6967}, 'status': 'success'}, {'text': 'that would vastly increase the complexity and cost of system development. If I don’t trust a component, I don’t use it. I may be able to talk to the product manager of that component and explain what he or she needs to do to gain my trust, but I don’t cross into their swimlane; I don’t re-verify their component. If their component is a COTS product like the Java Standard Edition, it has its own verification and certification process, which, as mentioned above is probably vast, and I either trust it or I don’t. If I don’t trust it, I don’t use it. But I don’t have the hubris to believe that I can identify faults that were overlooked by thousands and thousands of reviewers who know much more about how the product works than I do, and whose job it is to verify that the product meets its specification.\n\nThis strategy of doing my best job in my swimlane and letting others do their best job in their swimlane works. The vast majority of faults exhibited in the wild are not attributable to the major development platforms. The faults are almost always in the enduser program that failed to meet its specifications (whether those specifications were explicit or not): buffer overflow errors that weren’t trapped, message replay errors that were improperly programmed, or, in the case of my former employer (not my swimlane) failing to convert English units to metric units (yes, we lost a Mars lander that way; not a project I was involved in). In the famous case of the THERAC-25, the hardware and operating system performed to their specification, and the application program had a concurrency error that created a deadlock condition that cost lives. All of these are programming errors close to the enduser, and far from the computational platforms those programs operate on. All of these cases could have been eliminated if the application programmers had done a better job of verifying the code in their swimlane. None of these problems would have been prevented by the application programmers crossing into other swimlanes and trying to re-verify the platforms that had already been verified, certified, and accepted as industry standards by the community.\n\nThere was a time, early in the life of the Java Platform where there was a list of about a dozen vulnerabilities. And the list scared people, as it should have. But none of these was exploited in the wild (to the best of our knowledge). And the faults were identified not by endusers, but by people in the community whose role it was to verify the Java Platform: mostly professors engaged to look at the APIs and find their faults. In other words, the system worked.\n\nThe lesson in all of this is to program with humility. Stay in your swimlane and spend your energy verifying that your work is correct. And don’t focus so much on the instruction level. Computation is not about specifying sequences of instructions. There are some computational models that work that way. But not all computational models work that way.', 'result': {'fake': 0.985, 'real': 0.015}, 'status': 'success'}, {'text': 'Some computational models work by building computations and passing them to a (trusted) black box for evaluation. These models are at least equally valid, and provide the advantage that it is easier to verify that the computations are correct, and easier to distribute these computations to multiple processors.', 'result': {'fake': 0.2618, 'real': 0.7382}, 'status': 'success'}], 'credits_used': 33, 'credits': 1995167, 'subscription': 0, 'content': 'Monads and monoids.\n\nParallel programming has been a thing for 50 years. But the problem is that it was always too hard, and then too hard to verify and so it lived in niche applications. But a decade ago, processors stopped getting faster because we reached some physical limits. Processing power continued to increase, but by adding more cores rather than faster clock speeds. This meant that in order to utilize those processors, we needed to finally get serious about parallel programming.\n\nThe big “new” idea in parallel programming is to express the computation rather than the process. Monads and monoids are very very useful devices for expressing those computations. For instance, let’s say that I want to sum the first 1 million prime numbers. I could describe a procedure for doing this with a loop starting from the first prime, then going to the next prime, etc. Or I could describe the computation with a monad (or monoid). Let’s use Wolfram syntax:\n\nPlus @@ Table[Prime[i], {i, 1000000}]\xa0\n\nPlus is the sum-monoid in Wolfram. This doesn’t mean to find the first prime and then the second and then the third, and so on and then add the second to the first and the third to that and so on. This just says to take the sum of the first 1000000 primes. Maybe that’s a little sloppy. It says to find the first million primes and then find their total, so the programmer still has something to say about the steps, but that constrain the computation very much. The Wolfram kernel is free to find the primes in any order that it pleases, and then sum the primes in any order that it pleases. If there are 10 cores available, it can break the tasks into 10 chunks and dispatch each chunk to a different core. It specifies the computation, not the process. The processor is then at liberty to find the best procedure to realize the computation. This is huge because it relieves the programmer of much of the burden of parallelization. If I move the code to a different system, I don’t have to make any changes to make it exploit the resources of that system, because I’m still just describing the computation. If the Wolfram kernel gets souped up and improved in the next revision and makes better choices, I don’t have to make any changes to the code. I’m still describing the same computation, it’s just that it’s more efficient now.\n\nPerhaps most important of all, it’s easier to prove that the code is correct. When you express your computations in monads (and monoids) rather than in procedures, verification tends to be a matter of demonstrating that you didn’t make any typing errors. You don’t have to verify the steps of a procedure. This is important, and will become more important in the future. To demonstrate this consider a definition of the factorial function like this:\n\nFact[i_Integer/;NonNegative[i]] := Times @@ Range[i]\xa0\n\nEasy to verify. Basically it says that if i is a positive integer, Fact[i] is the product of the integers from 1 to i (and if i is 0 Fact[i] is the multiplicative identity). That’s the formal definition of a factorial. Thumbs up, we typed it in right. Now consider an imperative procedural definition:\n\nFact[0] = 1;\xa0\nFact[i_Integer/;i>=1] := Module[{prod=1}, \xa0\n   For[j=1, j<=i, j++, prod=prod j]\xa0\n]\xa0\n\nOkay, we can still verify it, but it’s a lot harder, because what we are interested in is whether the coded function is equivalent to the formal definition of the function, and the coded function has little similarity to the formal definition of the function. There’s a loop variable, and an accumulator variable and they’re changing all the time. Do they do the right thing at the right time? And this is a very very simple example. As our computation gets more complex, the monad/monoid approach is still about making sure we typed the formal definition in correctly, and the procedural definition is still about loop variables and accumulators and do they do the right things at the right times and do the steps of the procedure actually accomplish what the formal definition specifies. It gets a lot harder.\n\nAnother way to think of this is that we recognized the central importance of associativity and finally provided a general syntax for expressing things that are associative. Monoids are syntax for expressing associative things within a type and monads are syntax for expressing associative things across types. And once we have that syntax for associative things, we can use it to build even larger computations (packages of associative things within associative things within associative things) that can be offered to processors in the aggregate, and the processor can exploit the associativity to unroll that computation. That is what MapReduce is: syntax for packaging very large computations (so long as they are associative). MapReduce is just one kind of monad.\n\nHence, monads and monoids are the latest big breakthrough in programming.\n\nAFTERWORD:\n\nThere has been a lot of discussion on this thread about “yeah, but then you have to verify that the prime number generator works”, or “but then you have to verify that the runtime system works”. This is just wrong and since it has come up over and over, it’s important to address those concerns directly. None of this should have to be said…but apparently it has to be said anyway.\n\nThe Black Box Abstraction is key to all of science, engineering, computing, and in fact to civilization itself. A result once established can be used without knowing how the result was achieved. Once a black box has been verified, I can rely on that black box to satisfy its specification. In mathematics, we can rely on an accepted theorem without proving the theorem all over again. It’s already accepted. That’s basically what this is about.\n\nPerhaps the most apt example comes in proofs of NP-Completeness. Pretty much all proofs of NP-Completeness are performed by polynomial reduction. I show that some problem known to be NP-Complete can be reduced (in polynomial time) to my problem. And so if my problem can be solved in polynomial time, then so can the other one. That’s basically the whole proof. So let’s say that the Traveling Salesman Problem had not been shown to be NP-Complete. I can show that the Hamilton Cycle Problem (which has been shown to be NP-Complete) can be transformed into a Traveling Salesman Problem (in polynomial time). And with an ounce of housekeeping, I can declare the Traveling Salesman Problem to be NP-Complete. I don’t have to re-prove that the Hamilton Cycle Problem is NP-Complete. I can depend on it because that fact has already been accepted. And to take this a step further, I don’t have to re-prove that Hamiltonian Path Problem that Hamilton Cycle is reduced from is NP-Complete, and I don’t have to re-prove that the 3-SAT problem which Hamiltonian Cycle is reduced from is NP-Complete and I don’t have to re-prove that the Circuit Satisfiability Problem that 3-SAT is reduced from is NP-Complete. All of those results have been accepted. I don’t have to redo them. And the chain of accepted results is an accepted result. I rely on a foundation of accepted truth.\n\nEach of those problems is a black box. And each of those black boxes has been verified to meet its specification. I have to verify that the way I used those black boxes is meets my specification (i.e. that the Traveling Salesman Problem is NP-Complete) but the way that I use those black boxes is the only thing that’s new; the only thing that hasn’t been verified to be true.\n\nThe same is true in programming. If functions are well specified, and verified to meet their specifications, I can use those functions as black boxes without additional verification. In the famous Abelson and Sussman Structure and Interpretation of Computer Programs lectures: Jerry Sussman says:\n\nNot only are black boxes a useful tool for isolating components of a system, they also provide the basis for connecting things together. A key issue is providing methods for connecting together basic units into components that themselves can be treated as basic units. Thus, we will spend a lot of time talking about conventional interfaces -- standard ways of interconnecting simpler pieces. This is much like hooking up parts of a stereo system. One has standard interfaces by which components can be intertwined, and this can be done without worrying about the internal aspects of the components. Similarly in programming, we will describe conventions for interfacing simpler components to create new elements that can further be connected together [emphasis added].\n\nIn most programming situations, we lack the level of verification present in formal proofs. That’s coming, but we aren’t there yet. But there are ways to give great confidence that a software component or platform meets its specification. For instance, any implementation of the Java Standard Edition platform must pass (literally) millions of compatibility tests. The runtime alone on a large cluster takes days. Once all tests are passed, that version of the JSE is “certified” to be compatible, and users need not bother themselves to verify that their JSE is actually Java. It’s certified to be compliant to the Java specification, and we can use it with confidence and without further verification on our part. If we don’t trust the tests, we shouldn’t program on the Java Platform. If we do trust the tests, we can use a certified platform with confidence. What is more, if we don’t trust the tests and wish to verify the Java Platform independently, it would take thousands of man-years to develop the same level of confidence as the JavaTest Harness imbues. So we trust it and accept it as a black box, or we don’t trust and don’t use.\n\nMuch the same is true for a platform like Mathematica (which supports the extraordinarily powerful Wolfram language). The examples above are in Wolfram, and in a couple of places exploit some of the terseness and power of Wolfram. The Wolfram test suites are massive, and no Mathematica version is released without passing all of the tests. If you trust it, you can use it as a black box without doing further verification, and if you don’t trust it, don’t use it! But developing your own tests of the Mathematica kernel and standard libraries and developing your own verification plan will not be productive unless you have thousands of man-years available to devote to it. It is an industry standard with millions of users. Painstaking care is done to make it right, and you either trust it or you don’t.\n\nAs for the specific question in the thread about proving that Mathematica’s Prime[] function works, there are plenty of computations in the peer-reviewed literature that rely on the Prime[] function, and the function is well-documented and verified by many tests and thorough analysis. Another way of thinking about this is that our computations act on whatever the Prime[] function in Mathematica returns. This is generally agreed to be the primes in the community of mathematicians, but if you are suspicious of that, just think of these computations as “whatever the Prime[] function returns be it the actual prime numbers or not”.\n\nThe question of trusting the Mathematica kernel (or the JVM or the Scala runtime or the Haskell runtime) goes back to our original statement. These platforms are extremely well vetted and the compatibility test suites are vast and the communities that verify these platforms and the communities of users that use these platforms are all huge. But if you don’t trust one of these platforms, you shouldn’t use it. Pick a platform that you trust and use it. And if you don’t trust any computational platform, you probably shouldn’t be programming.\n\nOddly, there do not seem to be such suspicions about, say, the C compiler and its standard libraries. Some of the critics here are arguing that the Mathematica kernel needs to be verified by each new user for each new application. But these critics do not share that suspicion for malloc() function or the sin() function in C and are comfortable using them without extensive independent verification. There seems to be an implicit belief within these programmers that purely imperative programming is somehow more trustable. There is no reason to believe this.\n\nThere are various competing models of computation, that have all been verified to be theoretically equivalent. One of the first high-level programming languages (Fortran) was based on the Turing Machine. That’s an imperative model of computation. The other very early high-level language (Lisp) was based on Lambda Calculus. The Lambda Calculus is equally powerful, but doesn’t use an imperative model. It doesn’t say “do this instruction, then do that instruction, then the next instruction”. In the Lambda Calculus/Lisp model of computation, you define a computation and it is evaluated. The actual sequence of instructions that are performed is irrelevant. Like Prof. Sussman said in the quote above, we don’t worry about the internals of how that happens. That’s not how we define a program in these more declarative computational norms. There’s no magic. It’s just a different way of defining computation. Our program is correct if it uses the specifications of the components it depends on correctly.\n\nWhat is more, it is vastly easier to verify a program in a Lambda Calculus-based computational model than it is in an a purely imperative model. And that’s not just me saying that. The Lambda Calculus model is based on pure functions and we know much much more about pure functions than we do about imperative structures. The factorial example we gave above is good. The functional, Lambda Calculus-based definition is easy to verify, and the imperative version is a bit of a tussle to verify. Even beyond this, most imperative languages don’t stay strict to the given instructions. The optimizer will move redundant assignments out of loops, strip unused functions, and a whole lot more. And that’s good because the optimizer is fixing our mistakes. But the order of instructions that you write in code doesn’t match the instructions performed by the machine. It’s an “equivalent” set of instructions: a set of instructions that promises to give the same results as the instructions you specified.\n\nEven more important is the essence of the Black Box Abstraction model. You don’t have to look inside someone else’s black box. That principle is the bedrock of systems engineering. We decompose the system into independent components, and we build a specification for each component. I have my component and my swimlane. I build my component to meet its specification and I verify that it does so. There are other components in other swimlanes, and some of these depend on my component. Those swimlanes need to trust my verification. Likewise, my component depends on other components. And I build my component without crossing over into those swimlanes. I know what the specifications of those components are and I build my components to exercise those specifications, but I don’t have to do a code review and re-validation of those components; I don’t look inside their black box. It’s their black box; their swimlane. If we’re all in the same building, I may meet with the product managers of the other components, and I may have questions about their verification process, the answers to which will hopefully give me trust in those components. But I do not re-verify those components because that would vastly increase the complexity and cost of system development. If I don’t trust a component, I don’t use it. I may be able to talk to the product manager of that component and explain what he or she needs to do to gain my trust, but I don’t cross into their swimlane; I don’t re-verify their component. If their component is a COTS product like the Java Standard Edition, it has its own verification and certification process, which, as mentioned above is probably vast, and I either trust it or I don’t. If I don’t trust it, I don’t use it. But I don’t have the hubris to believe that I can identify faults that were overlooked by thousands and thousands of reviewers who know much more about how the product works than I do, and whose job it is to verify that the product meets its specification.\n\nThis strategy of doing my best job in my swimlane and letting others do their best job in their swimlane works. The vast majority of faults exhibited in the wild are not attributable to the major development platforms. The faults are almost always in the enduser program that failed to meet its specifications (whether those specifications were explicit or not): buffer overflow errors that weren’t trapped, message replay errors that were improperly programmed, or, in the case of my former employer (not my swimlane) failing to convert English units to metric units (yes, we lost a Mars lander that way; not a project I was involved in). In the famous case of the THERAC-25, the hardware and operating system performed to their specification, and the application program had a concurrency error that created a deadlock condition that cost lives. All of these are programming errors close to the enduser, and far from the computational platforms those programs operate on. All of these cases could have been eliminated if the application programmers had done a better job of verifying the code in their swimlane. None of these problems would have been prevented by the application programmers crossing into other swimlanes and trying to re-verify the platforms that had already been verified, certified, and accepted as industry standards by the community.\n\nThere was a time, early in the life of the Java Platform where there was a list of about a dozen vulnerabilities. And the list scared people, as it should have. But none of these was exploited in the wild (to the best of our knowledge). And the faults were identified not by endusers, but by people in the community whose role it was to verify the Java Platform: mostly professors engaged to look at the APIs and find their faults. In other words, the system worked.\n\nThe lesson in all of this is to program with humility. Stay in your swimlane and spend your energy verifying that your work is correct. And don’t focus so much on the instruction level. Computation is not about specifying sequences of instructions. There are some computational models that work that way. But not all computational models work that way. Some computational models work by building computations and passing them to a (trusted) black box for evaluation. These models are at least equally valid, and provide the advantage that it is easier to verify that the computations are correct, and easier to distribute these computations to multiple processors.', 'aiModelVersion': '1'}",0.61624285714286
Ivanildo Miranda,1y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","In the old days computer memory was a luxury.

Very expensive and programmers must squize each bit of it to make room for the applications and data.

So one trick was use one byte to store eight binary variables. In a single byte you could store such information as :

bit 1 - Sex (M/F)

bit 2 - Employed (Y/N)

bit 3 - House owner (Y/N)

and so on.

Nowadays no one cares about memory space and such variables use many bytes.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/jgaqcmew8d0t4ubl', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.9914, 'ai': 0.0086}, 'blocks': [{'text': 'In the old days computer memory was a luxury.\n\nVery expensive and programmers must squize each bit of it to make room for the applications and data.\n\nSo one trick was use one byte to store eight binary variables. In a single byte you could store such information as :\n\nbit 1 - Sex (M/F)\n\nbit 2 - Employed (Y/N)\n\nbit 3 - House owner (Y/N)\n\nand so on.\n\nNowadays no one cares about memory space and such variables use many bytes.', 'result': {'fake': 0.0086, 'real': 0.9914}, 'status': 'success'}], 'credits_used': 1, 'credits': 1995166, 'subscription': 0, 'content': 'In the old days computer memory was a luxury.\n\nVery expensive and programmers must squize each bit of it to make room for the applications and data.\n\nSo one trick was use one byte to store eight binary variables. In a single byte you could store such information as :\n\nbit 1 - Sex (M/F)\n\nbit 2 - Employed (Y/N)\n\nbit 3 - House owner (Y/N)\n\nand so on.\n\nNowadays no one cares about memory space and such variables use many bytes.', 'aiModelVersion': '1'}",0.9914
Steve Baker,5y,Was Unix a breakthrough in the computer science world?,"Yes - definitely.

It embodies many really important concepts (some taken from Multics).

All files are just streams of bytes.
All devices behave much like files.
All directories are also files (this is no longer strictly true).
Each program does one small job VERY well.
The “shell” provides powerful mechanisms like pipes and redirects that allow you to combine multiple commands into a single step.
The shell is just another program…nothing special.
Users on the system are protected from each other.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6hfs7b4j5iaygo2q', 'title': 'Was Unix a breakthrough in the computer science world?', 'score': {'original': 0.994, 'ai': 0.006}, 'blocks': [{'text': 'Yes - definitely.\n\nIt embodies many really important concepts (some taken from Multics).\n\nAll files are just streams of bytes.\nAll devices behave much like files.\nAll directories are also files (this is no longer strictly true).\nEach program does one small job VERY well.\nThe “shell” provides powerful mechanisms like pipes and redirects that allow you to combine multiple commands into a single step.\nThe shell is just another program…nothing special.\nUsers on the system are protected from each other.', 'result': {'fake': 0.006, 'real': 0.994}, 'status': 'success'}], 'credits_used': 1, 'credits': 1995165, 'subscription': 0, 'content': 'Yes - definitely.\n\nIt embodies many really important concepts (some taken from Multics).\n\nAll files are just streams of bytes.\nAll devices behave much like files.\nAll directories are also files (this is no longer strictly true).\nEach program does one small job VERY well.\nThe “shell” provides powerful mechanisms like pipes and redirects that allow you to combine multiple commands into a single step.\nThe shell is just another program…nothing special.\nUsers on the system are protected from each other.', 'aiModelVersion': '1'}",0.994
Karl Brace,5y,"Why is it called ""logging in""?","In the days when sailing ships crossed oceans but there was no GPS, and longitude was hard to calculate by looking at the sky, the best way to estimate your longitude was to keep careful track of your speed, especially the east-west component of your speed. You did this by measuring your speed frequently. How did they measure speed? They had a rope with a log tied to the end, and knots tied at regular intervals. They tossed the log off of the stern and counted how many knots went out over a short period of time - That was the speed of the ship in knots, by the way. Each time they tossed the log overboard they of course wrote down the speed (and their heading)… In the “log book”, of course!

Other significant events that needed to be recorded would often also be recorded in the log book, I imagine because that book was always handy, and why would the ship want to keep multiple record books? Eventually people were “logging” lots of other information that had nothing to do with how many knots went by when the log was tossed overboard. For example, when cargo was taken on board, or people arrived and left, those events were logged. People eventually began to use the term “log book” in non-shipping contexts where information is recorded over time. And this eventually included using the term “log on” and “log off” for signing onto and off of a computer!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/y7pwv40oa5x6f9cn', 'title': 'Why is it called ""logging in""?', 'score': {'original': 0.9994, 'ai': 0.0006}, 'blocks': [{'text': 'In the days when sailing ships crossed oceans but there was no GPS, and longitude was hard to calculate by looking at the sky, the best way to estimate your longitude was to keep careful track of your speed, especially the east-west component of your speed. You did this by measuring your speed frequently. How did they measure speed? They had a rope with a log tied to the end, and knots tied at regular intervals. They tossed the log off of the stern and counted how many knots went out over a short period of time - That was the speed of the ship in knots, by the way. Each time they tossed the log overboard they of course wrote down the speed (and their heading)… In the “log book”, of course!\n\nOther significant events that needed to be recorded would often also be recorded in the log book, I imagine because that book was always handy, and why would the ship want to keep multiple record books? Eventually people were “logging” lots of other information that had nothing to do with how many knots went by when the log was tossed overboard. For example, when cargo was taken on board, or people arrived and left, those events were logged. People eventually began to use the term “log book” in non-shipping contexts where information is recorded over time. And this eventually included using the term “log on” and “log off” for signing onto and off of a computer!', 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995162, 'subscription': 0, 'content': 'In the days when sailing ships crossed oceans but there was no GPS, and longitude was hard to calculate by looking at the sky, the best way to estimate your longitude was to keep careful track of your speed, especially the east-west component of your speed. You did this by measuring your speed frequently. How did they measure speed? They had a rope with a log tied to the end, and knots tied at regular intervals. They tossed the log off of the stern and counted how many knots went out over a short period of time - That was the speed of the ship in knots, by the way. Each time they tossed the log overboard they of course wrote down the speed (and their heading)… In the “log book”, of course!\n\nOther significant events that needed to be recorded would often also be recorded in the log book, I imagine because that book was always handy, and why would the ship want to keep multiple record books? Eventually people were “logging” lots of other information that had nothing to do with how many knots went by when the log was tossed overboard. For example, when cargo was taken on board, or people arrived and left, those events were logged. People eventually began to use the term “log book” in non-shipping contexts where information is recorded over time. And this eventually included using the term “log on” and “log off” for signing onto and off of a computer!', 'aiModelVersion': '1'}",0.9994
Christopher Reiss,10y,Who are the top ten game programmers from the start of video-game history to the modern day? Why?,"It probably starts with this guy - Bill Budge : (then - 1983) :  Now - (lookin' good, Bill!) He was a programmer rock star for writing two very popular games for the Apple II (eventually ported), Raster Blaster and Pinball Construction Set. Raster Blaster (1981) emulated a pinball machine - very, very well on some pretty anemic hardware.  His next hit - Pinball Construction Set (1983), enabled you to design your own pinball game and play it :   If these games look crude, let me remind you of the hardware specs of the Apple II :  CPU speed : 1 Megahertz.Memory      :  0.046   MB  (46KB.    Maxed out.)Video resolution : 280x192, 4 colors.Graphics coprocessor : Hell no. Bill Budge was famous among programmers in his day for his insane ability to write tight, fast assembly code.    He set the standard for anybody trying to write games : code right on the chip, optimize, optimize, optimize.   It's gotta pop and sing. EDIT : Please see Joe Wezorek's comment - Budge introduced a graphical interface prior to the launch of Mac's GUI ...","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/4g1pz2ivkyfw9jne', 'title': 'Who are the top ten game programmers from the start of video-game history to the modern day? Why?', 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': ""It probably starts with this guy - Bill Budge : (then - 1983) :  Now - (lookin' good, Bill!) He was a programmer rock star for writing two very popular games for the Apple II (eventually ported), Raster Blaster and Pinball Construction Set. Raster Blaster (1981) emulated a pinball machine - very, very well on some pretty anemic hardware.  His next hit - Pinball Construction Set (1983), enabled you to design your own pinball game and play it :   If these games look crude, let me remind you of the hardware specs of the Apple II :  CPU speed : 1 Megahertz.Memory      :  0.046   MB  (46KB.    Maxed out.)Video resolution : 280x192, 4 colors.Graphics coprocessor : Hell no. Bill Budge was famous among programmers in his day for his insane ability to write tight, fast assembly code.    He set the standard for anybody trying to write games : code right on the chip, optimize, optimize, optimize.   It's gotta pop and sing. EDIT : Please see Joe Wezorek's comment - Budge introduced a graphical interface prior to the launch of Mac's GUI ..."", 'result': {'fake': 0.0007, 'real': 0.9993}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995160, 'subscription': 0, 'content': ""It probably starts with this guy - Bill Budge : (then - 1983) :  Now - (lookin' good, Bill!) He was a programmer rock star for writing two very popular games for the Apple II (eventually ported), Raster Blaster and Pinball Construction Set. Raster Blaster (1981) emulated a pinball machine - very, very well on some pretty anemic hardware.  His next hit - Pinball Construction Set (1983), enabled you to design your own pinball game and play it :   If these games look crude, let me remind you of the hardware specs of the Apple II :  CPU speed : 1 Megahertz.Memory      :  0.046   MB  (46KB.    Maxed out.)Video resolution : 280x192, 4 colors.Graphics coprocessor : Hell no. Bill Budge was famous among programmers in his day for his insane ability to write tight, fast assembly code.    He set the standard for anybody trying to write games : code right on the chip, optimize, optimize, optimize.   It's gotta pop and sing. EDIT : Please see Joe Wezorek's comment - Budge introduced a graphical interface prior to the launch of Mac's GUI ..."", 'aiModelVersion': '1'}",0.9993
Archie D’Cruz,6y,"What's the story behind the ""clear"" key on Apple's keyboard? The clear button only clears the selected text. Why does it work this way?","Ah, that button. It seems like a puzzler, doesn’t it?

What you might not have noticed is that the Clear button is only available on Apple’s Extended keyboards (not the standard ones), and is always positioned in the numeric area to the right.

The story, however, will become immediately (ahem) clear when I ask you to take a look at this stone-age device:

Yep, the Clear button works exactly as it would on a standard desktop calculator. If you know any accountants who use Macs, you will notice that when it comes to inputting numbers, they will almost exclusively stick to the numeric part of the keypad rather than the numbers row above the alphabets.

Having that button (which also triggers the AC or All Clear on the Mac’s Calculator app) on the right allows their fingers to fly when working with numbers.

So now we’re clear.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/tepc3ruxqah4bg6z', 'title': 'What\'s the story behind the ""clear"" key on Apple\'s keyboard? The clear button only clears the selected text. Why does it work this way?', 'score': {'original': 0.9983, 'ai': 0.0017}, 'blocks': [{'text': 'Ah, that button. It seems like a puzzler, doesn’t it?\n\nWhat you might not have noticed is that the Clear button is only available on Apple’s Extended keyboards (not the standard ones), and is always positioned in the numeric area to the right.\n\nThe story, however, will become immediately (ahem) clear when I ask you to take a look at this stone-age device:\n\nYep, the Clear button works exactly as it would on a standard desktop calculator. If you know any accountants who use Macs, you will notice that when it comes to inputting numbers, they will almost exclusively stick to the numeric part of the keypad rather than the numbers row above the alphabets.\n\nHaving that button (which also triggers the AC or All Clear on the Mac’s Calculator app) on the right allows their fingers to fly when working with numbers.\n\nSo now we’re clear.', 'result': {'fake': 0.0017, 'real': 0.9983}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995158, 'subscription': 0, 'content': 'Ah, that button. It seems like a puzzler, doesn’t it?\n\nWhat you might not have noticed is that the Clear button is only available on Apple’s Extended keyboards (not the standard ones), and is always positioned in the numeric area to the right.\n\nThe story, however, will become immediately (ahem) clear when I ask you to take a look at this stone-age device:\n\nYep, the Clear button works exactly as it would on a standard desktop calculator. If you know any accountants who use Macs, you will notice that when it comes to inputting numbers, they will almost exclusively stick to the numeric part of the keypad rather than the numbers row above the alphabets.\n\nHaving that button (which also triggers the AC or All Clear on the Mac’s Calculator app) on the right allows their fingers to fly when working with numbers.\n\nSo now we’re clear.', 'aiModelVersion': '1'}",0.9983
Alan Kay,Updated 5y,To what extent was it possible to build a digital computer during ancient Rome?,"This is a very interesting question — right up there with one that I used to put to a class on computer design: “What computer would Captain Nemo have designed for the Nautilus? — Sketch out the design for this!”

The current question can be taken in a number of ways: including the meaning “With the technologies available during that time could a complete genius — beyond Leonardo — have figured out a device that we would call a digital computer?”

This way of posing the question also allows for “Greek thought” to be employed, since the Romans had many Greeks — free and slaves — who did some of the deeper thinking for that practical minded people. This is a good combination because the Greeks didn’t care much for Engineering, and the Romans did — many of the most interesting mechanical inventions of the Greeks were toys, especially by the Alexandrian Greeks.

Let’s leave aside that one of the problems of slavery is that it removes one large motivation for inventing mechanisms to do much general work, including calculations. And let’s leave aside the question of “build-up” of precursor ideas, which even geniuses need and use — the question is about “can something be built?” at a given time in history.

Some of the other answers have correctly noted that both the Greeks and the Romans calculated using a kind of abacus
 (the word “calculus” is the term for the stones moved around on their counting tables). It’s important to note in passing that abacii *do* have “zero” implicit in how they work.

Reproduction of a Roman “pocket” abacus that would fit in a modern day shirt pocket. The “calculi” here are not stones but beads in sliding slots.

As a computer guy I was naturally quite interested in Babbage (and Ada), and was initially very impressed with the inventions that he had to do as precursors (like careful mechanical drawing, the turret lathe, etc.) and the inspiration of the Jacquard Loom. It was fun to get to crank the reproduction of the Difference Engine at the London Science Museum (super high precision construction, but a lot of a backlash and effort).

A few years later I had occasion to really look at the earlier Jacquard Loom, and was completely knocked out by its astoundingly superior approach to mechanical design; even the huge mechanical action pipe organs of the time were not as wonderfully done!

I realized that Babbage had taken a very poor engineering approach — he was a mathematician, and tried to do things “exactly” — and really should have looked at the actual mechanisms of the Jacquard Loom much more closely.

Only the top 1/3 is the Jacquard mechanism. The whole loom is mostly wood with a little mechanism (including a lot of logic made from “coat hangers”).

Jacquard went completely in the opposite direction: everything is just flopping around except and only except when he needed precision (a lot of the precision he got was via “guide holes” that would constrain the floppy things but let them flop on either side (one way to look at this is that the guides were to “reduce errors” periodically in the chains of causes and effects).

The result is that there is virtually no propagation and multiplication of error (in Babbage’s scheme you get enormous propagation of error). One of the many fun things about Jacquard’s approach is that the mechanism does not require inventing a new kind of loom: the Jacquard apparatus sits on top of an existing loom and simply pulls the threads according to the program on the punched card chain.

So one person operating a Jacquard Loom could affect thousands of threads via the punched card programs with human power alone.

What’s interesting here, is that there is nothing tricky in the Jacquard Loom mechanically — large parts of the logic can be built from coat hangers! — and it could have been made using just the technologies available on either side of 0 BCE. Here is a short precis of some of the technologies available
.

The Wikipedia article on the Jacquard Loom
 is a start, but lacks enough detail. I’m still looking for the best description of the Jacquard Loom mechanism online (please help!). What’s important here are the actual details of how Jacquard saw to make selection not require careful precision except only where actually needed. For now try this pdf starting at page 5
. And try this YouTube video to see some of the different parts at work driven by one person.

There are several books with useful details: Essinger, James (2004). Jacquard's web. Oxford University Press, Oxford, and especially: Bradbury, Fred (1912). Jacquard Mechanisms and Harness Mounting. John Heywood Ltd., Technical Book Depot, Halifax, Yorks. The best book I found was: (1888) The Jacquard Machine Analyzed and Explained, by E. A. Posselt. Thanks to the wonderful Brewster Kahle and his Archive.org
, this book can be found and downloaded via Google

I think it would be quite possible to use modern thinking to design a programmable computer based strongly on the Jacquard mechanisms, and then to build it using only techniques, tools, and materials available ca 0 BCE.

I will devote the next week to trying to find an understandable online account of how the Jacquard Loom actually works (it is almost always the case that museums — even “science and technology” museums — just exhibit artifacts, and do not provide explanations or demonstrations of *how* and *why* they work).

Addition I

I had forgotten that I’d written about Babbage for Quora, and mentioned there more detail about the Jacquard Loom. What’s the best book about Charles Babbage in terms of technical detail?

I found a pretty good YouTube video from the V & A Museum that includes a 3D animated explanation of the Jacquard invention.

It seems to not have a sound track, but I think the whole visual explanation is pretty clear. You might want to run the animation at 1/2 speed to really be able to follow the cause and effect relationships.

Addition II

Marcel Levy asked a question that is worth putting in the main body here: “So you’re saying it was not the practical side that was lacking but the theoretical one ?”

I think: Yes. It’s worth looking at some of the mechanical inventions of the Alexandrian Greeks, including: the pipe organ played with keys with wind stabilized by water pressure, etc. Also Greek theater mechanics, etc. And of course, the Antikythera astronomical calculator.

Heron of Alexandria was the most interesting inventor of mechanisms ca 0 BCE, besides the pipe organ, there was reported an odometer that counted digitally, and most interestingly a programmable cart (a recreation here on YouTube).

What else did Heron do along the lines of these “toys to amuse and amaze”? He had the idea of “programming”, and probably devised other forms of it for some of the automata he came up with.

Looms had existed for thousands of years, and elaborate ones for hundreds.

The Greeks (and the Romans following) realized that the cultural “official” way to write numbers was “inconvenient”: we can see their real thinking by looking at their calculating machines (the various abacii).

We could definitely ask questions about “what did they really want to compute that was outside the scope of their abacii?” Babbage was driven initially by inaccuracies in mathematical tables (“I wish to God these calculations had been executed by steam!”) Turing was intially driven by Goedel’s results. Physical computing was driven by needs of warfare from WWII onward.

They most definitely had the brains and much of the mathematical and physical outlook. It looks to me that they lacked the feeling of need more than anything else that would drive them to use what they knew to see how to make a machine to compute for them.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/4n8jqty1v36lpbzh', 'title': 'To what extent was it possible to build a digital computer during ancient Rome?', 'score': {'original': 0.64506666666667, 'ai': 0.35493333333333}, 'blocks': [{'text': 'This is a very interesting question — right up there with one that I used to put to a class on computer design: “What computer would Captain Nemo have designed for the Nautilus? — Sketch out the design for this!”\n\nThe current question can be taken in a number of ways: including the meaning “With the technologies available during that time could a complete genius — beyond Leonardo — have figured out a device that we would call a digital computer?”\n\nThis way of posing the question also allows for “Greek thought” to be employed, since the Romans had many Greeks — free and slaves — who did some of the deeper thinking for that practical minded people. This is a good combination because the Greeks didn’t care much for Engineering, and the Romans did — many of the most interesting mechanical inventions of the Greeks were toys, especially by the Alexandrian Greeks.\n\nLet’s leave aside that one of the problems of slavery is that it removes one large motivation for inventing mechanisms to do much general work, including calculations. And let’s leave aside the question of “build-up” of precursor ideas, which even geniuses need and use — the question is about “can something be built?” at a given time in history.\n\nSome of the other answers have correctly noted that both the Greeks and the Romans calculated using a kind of abacus\n (the word “calculus” is the term for the stones moved around on their counting tables). It’s important to note in passing that abacii *do* have “zero” implicit in how they work.\n\nReproduction of a Roman “pocket” abacus that would fit in a modern day shirt pocket. The “calculi” here are not stones but beads in sliding slots.\n\nAs a computer guy I was naturally quite interested in Babbage (and Ada), and was initially very impressed with the inventions that he had to do as precursors (like careful mechanical drawing, the turret lathe, etc.) and the inspiration of the Jacquard Loom. It was fun to get to crank the reproduction of the Difference Engine at the London Science Museum (super high precision construction, but a lot of a backlash and effort).\n\nA few years later I had occasion to really look at the earlier Jacquard Loom, and was completely knocked out by its astoundingly superior approach to mechanical design; even the huge mechanical action pipe organs of the time were not as wonderfully done!\n\nI realized that Babbage had taken a very poor engineering approach — he was a mathematician, and tried to do things “exactly” — and really should have looked at the actual mechanisms of the Jacquard Loom much more closely.\n\nOnly the top 1/3 is the Jacquard mechanism. The whole loom is mostly wood with a little mechanism (including a lot of logic made from “coat hangers”).\n\nJacquard went completely in the opposite direction: everything is just flopping around except and only except when he needed precision (a lot of the precision he got was via “guide holes” that would constrain the floppy things but let them flop on either side (one way to look at this', 'result': {'fake': 0.2875, 'real': 0.7125}, 'status': 'success'}, {'text': ""is that the guides were to “reduce errors” periodically in the chains of causes and effects).\n\nThe result is that there is virtually no propagation and multiplication of error (in Babbage’s scheme you get enormous propagation of error). One of the many fun things about Jacquard’s approach is that the mechanism does not require inventing a new kind of loom: the Jacquard apparatus sits on top of an existing loom and simply pulls the threads according to the program on the punched card chain.\n\nSo one person operating a Jacquard Loom could affect thousands of threads via the punched card programs with human power alone.\n\nWhat’s interesting here, is that there is nothing tricky in the Jacquard Loom mechanically — large parts of the logic can be built from coat hangers! — and it could have been made using just the technologies available on either side of 0 BCE. Here is a short precis of some of the technologies available\n.\n\nThe Wikipedia article on the Jacquard Loom\n is a start, but lacks enough detail. I’m still looking for the best description of the Jacquard Loom mechanism online (please help!). What’s important here are the actual details of how Jacquard saw to make selection not require careful precision except only where actually needed. For now try this pdf starting at page 5\n. And try this YouTube video to see some of the different parts at work driven by one person.\n\nThere are several books with useful details: Essinger, James (2004). Jacquard's web. Oxford University Press, Oxford, and especially: Bradbury, Fred (1912). Jacquard Mechanisms and Harness Mounting. John Heywood Ltd., Technical Book Depot, Halifax, Yorks. The best book I found was: (1888) The Jacquard Machine Analyzed and Explained, by E. A. Posselt. Thanks to the wonderful Brewster Kahle and his Archive.org\n, this book can be found and downloaded via Google\n\nI think it would be quite possible to use modern thinking to design a programmable computer based strongly on the Jacquard mechanisms, and then to build it using only techniques, tools, and materials available ca 0 BCE.\n\nI will devote the next week to trying to find an understandable online account of how the Jacquard Loom actually works (it is almost always the case that museums — even “science and technology” museums — just exhibit artifacts, and do not provide explanations or demonstrations of *how* and *why* they work).\n\nAddition I\n\nI had forgotten that I’d written about Babbage for Quora, and mentioned there more detail about the Jacquard Loom. What’s the best book about Charles Babbage in terms of technical detail?\n\nI found a pretty good YouTube video from the V & A Museum that includes a 3D animated explanation of the Jacquard invention.\n\nIt seems to not have a sound track, but I think the whole visual explanation is pretty clear. You might want to run the animation at 1/2 speed to really be able to follow the cause and effect relationships.\n\nAddition II\n\nMarcel Levy asked a question that is worth putting in the main body here: “So you’re saying it was not the practical side that was lacking but the theoretical one ?”\n\nI"", 'result': {'fake': 0.1027, 'real': 0.8973}, 'status': 'success'}, {'text': 'think: Yes. It’s worth looking at some of the mechanical inventions of the Alexandrian Greeks, including: the pipe organ played with keys with wind stabilized by water pressure, etc. Also Greek theater mechanics, etc. And of course, the Antikythera astronomical calculator.\n\nHeron of Alexandria was the most interesting inventor of mechanisms ca 0 BCE, besides the pipe organ, there was reported an odometer that counted digitally, and most interestingly a programmable cart (a recreation here on YouTube).\n\nWhat else did Heron do along the lines of these “toys to amuse and amaze”? He had the idea of “programming”, and probably devised other forms of it for some of the automata he came up with.\n\nLooms had existed for thousands of years, and elaborate ones for hundreds.\n\nThe Greeks (and the Romans following) realized that the cultural “official” way to write numbers was “inconvenient”: we can see their real thinking by looking at their calculating machines (the various abacii).\n\nWe could definitely ask questions about “what did they really want to compute that was outside the scope of their abacii?” Babbage was driven initially by inaccuracies in mathematical tables (“I wish to God these calculations had been executed by steam!”) Turing was intially driven by Goedel’s results. Physical computing was driven by needs of warfare from WWII onward.\n\nThey most definitely had the brains and much of the mathematical and physical outlook. It looks to me that they lacked the feeling of need more than anything else that would drive them to use what they knew to see how to make a machine to compute for them.', 'result': {'fake': 0.0024, 'real': 0.9976}, 'status': 'success'}], 'credits_used': 14, 'credits': 1995144, 'subscription': 0, 'content': ""This is a very interesting question — right up there with one that I used to put to a class on computer design: “What computer would Captain Nemo have designed for the Nautilus? — Sketch out the design for this!”\n\nThe current question can be taken in a number of ways: including the meaning “With the technologies available during that time could a complete genius — beyond Leonardo — have figured out a device that we would call a digital computer?”\n\nThis way of posing the question also allows for “Greek thought” to be employed, since the Romans had many Greeks — free and slaves — who did some of the deeper thinking for that practical minded people. This is a good combination because the Greeks didn’t care much for Engineering, and the Romans did — many of the most interesting mechanical inventions of the Greeks were toys, especially by the Alexandrian Greeks.\n\nLet’s leave aside that one of the problems of slavery is that it removes one large motivation for inventing mechanisms to do much general work, including calculations. And let’s leave aside the question of “build-up” of precursor ideas, which even geniuses need and use — the question is about “can something be built?” at a given time in history.\n\nSome of the other answers have correctly noted that both the Greeks and the Romans calculated using a kind of abacus\n (the word “calculus” is the term for the stones moved around on their counting tables). It’s important to note in passing that abacii *do* have “zero” implicit in how they work.\n\nReproduction of a Roman “pocket” abacus that would fit in a modern day shirt pocket. The “calculi” here are not stones but beads in sliding slots.\n\nAs a computer guy I was naturally quite interested in Babbage (and Ada), and was initially very impressed with the inventions that he had to do as precursors (like careful mechanical drawing, the turret lathe, etc.) and the inspiration of the Jacquard Loom. It was fun to get to crank the reproduction of the Difference Engine at the London Science Museum (super high precision construction, but a lot of a backlash and effort).\n\nA few years later I had occasion to really look at the earlier Jacquard Loom, and was completely knocked out by its astoundingly superior approach to mechanical design; even the huge mechanical action pipe organs of the time were not as wonderfully done!\n\nI realized that Babbage had taken a very poor engineering approach — he was a mathematician, and tried to do things “exactly” — and really should have looked at the actual mechanisms of the Jacquard Loom much more closely.\n\nOnly the top 1/3 is the Jacquard mechanism. The whole loom is mostly wood with a little mechanism (including a lot of logic made from “coat hangers”).\n\nJacquard went completely in the opposite direction: everything is just flopping around except and only except when he needed precision (a lot of the precision he got was via “guide holes” that would constrain the floppy things but let them flop on either side (one way to look at this is that the guides were to “reduce errors” periodically in the chains of causes and effects).\n\nThe result is that there is virtually no propagation and multiplication of error (in Babbage’s scheme you get enormous propagation of error). One of the many fun things about Jacquard’s approach is that the mechanism does not require inventing a new kind of loom: the Jacquard apparatus sits on top of an existing loom and simply pulls the threads according to the program on the punched card chain.\n\nSo one person operating a Jacquard Loom could affect thousands of threads via the punched card programs with human power alone.\n\nWhat’s interesting here, is that there is nothing tricky in the Jacquard Loom mechanically — large parts of the logic can be built from coat hangers! — and it could have been made using just the technologies available on either side of 0 BCE. Here is a short precis of some of the technologies available\n.\n\nThe Wikipedia article on the Jacquard Loom\n is a start, but lacks enough detail. I’m still looking for the best description of the Jacquard Loom mechanism online (please help!). What’s important here are the actual details of how Jacquard saw to make selection not require careful precision except only where actually needed. For now try this pdf starting at page 5\n. And try this YouTube video to see some of the different parts at work driven by one person.\n\nThere are several books with useful details: Essinger, James (2004). Jacquard's web. Oxford University Press, Oxford, and especially: Bradbury, Fred (1912). Jacquard Mechanisms and Harness Mounting. John Heywood Ltd., Technical Book Depot, Halifax, Yorks. The best book I found was: (1888) The Jacquard Machine Analyzed and Explained, by E. A. Posselt. Thanks to the wonderful Brewster Kahle and his Archive.org\n, this book can be found and downloaded via Google\n\nI think it would be quite possible to use modern thinking to design a programmable computer based strongly on the Jacquard mechanisms, and then to build it using only techniques, tools, and materials available ca 0 BCE.\n\nI will devote the next week to trying to find an understandable online account of how the Jacquard Loom actually works (it is almost always the case that museums — even “science and technology” museums — just exhibit artifacts, and do not provide explanations or demonstrations of *how* and *why* they work).\n\nAddition I\n\nI had forgotten that I’d written about Babbage for Quora, and mentioned there more detail about the Jacquard Loom. What’s the best book about Charles Babbage in terms of technical detail?\n\nI found a pretty good YouTube video from the V & A Museum that includes a 3D animated explanation of the Jacquard invention.\n\nIt seems to not have a sound track, but I think the whole visual explanation is pretty clear. You might want to run the animation at 1/2 speed to really be able to follow the cause and effect relationships.\n\nAddition II\n\nMarcel Levy asked a question that is worth putting in the main body here: “So you’re saying it was not the practical side that was lacking but the theoretical one ?”\n\nI think: Yes. It’s worth looking at some of the mechanical inventions of the Alexandrian Greeks, including: the pipe organ played with keys with wind stabilized by water pressure, etc. Also Greek theater mechanics, etc. And of course, the Antikythera astronomical calculator.\n\nHeron of Alexandria was the most interesting inventor of mechanisms ca 0 BCE, besides the pipe organ, there was reported an odometer that counted digitally, and most interestingly a programmable cart (a recreation here on YouTube).\n\nWhat else did Heron do along the lines of these “toys to amuse and amaze”? He had the idea of “programming”, and probably devised other forms of it for some of the automata he came up with.\n\nLooms had existed for thousands of years, and elaborate ones for hundreds.\n\nThe Greeks (and the Romans following) realized that the cultural “official” way to write numbers was “inconvenient”: we can see their real thinking by looking at their calculating machines (the various abacii).\n\nWe could definitely ask questions about “what did they really want to compute that was outside the scope of their abacii?” Babbage was driven initially by inaccuracies in mathematical tables (“I wish to God these calculations had been executed by steam!”) Turing was intially driven by Goedel’s results. Physical computing was driven by needs of warfare from WWII onward.\n\nThey most definitely had the brains and much of the mathematical and physical outlook. It looks to me that they lacked the feeling of need more than anything else that would drive them to use what they knew to see how to make a machine to compute for them."", 'aiModelVersion': '1'}",0.64506666666667
Bruce F. Webster,1y,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","Back 40 years ago, personal computers used to emit a lot of radio frequency (RF) electro-magnetic waves. If you put an AM radio near one, you could tune the dial and actually hear those waves, usually as a varying high pitch noise. Here’s a modern example of RF interference:

So, computer geeks being what they are, a number figured out that with the right combination of instructions and delays, they could control the pitch and duration of those sounds and play simple tunes over a nearby AM radio.

I’m pretty sure the advent of personal computers caused the FCC to greatly step up its enforcement of regulations in RF emissions. 🤣👍","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/cs3rgt6u4od7pk1a', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': 'Back 40 years ago, personal computers used to emit a lot of radio frequency (RF) electro-magnetic waves. If you put an AM radio near one, you could tune the dial and actually hear those waves, usually as a varying high pitch noise. Here’s a modern example of RF interference:\n\nSo, computer geeks being what they are, a number figured out that with the right combination of instructions and delays, they could control the pitch and duration of those sounds and play simple tunes over a nearby AM radio.\n\nI’m pretty sure the advent of personal computers caused the FCC to greatly step up its enforcement of regulations in RF emissions. 🤣👍', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995142, 'subscription': 0, 'content': 'Back 40 years ago, personal computers used to emit a lot of radio frequency (RF) electro-magnetic waves. If you put an AM radio near one, you could tune the dial and actually hear those waves, usually as a varying high pitch noise. Here’s a modern example of RF interference:\n\nSo, computer geeks being what they are, a number figured out that with the right combination of instructions and delays, they could control the pitch and duration of those sounds and play simple tunes over a nearby AM radio.\n\nI’m pretty sure the advent of personal computers caused the FCC to greatly step up its enforcement of regulations in RF emissions. 🤣👍', 'aiModelVersion': '1'}",0.9995
David Mitchell,9y,What was programming like back in the days when computers only had kilobytes of RAM?,"Very different to today. For a start, programming languages were very different; there were no ""scripting languages"" like Python, Ruby or Perl, so coding even simple solutions was quite time intensive. No JVM, so porting your code to another architecture was usually complex. Speaking of architectures, pre-1983 when the IBM PC first launched, there were no architectural standards as such; you spent serious $$$ to buy a machine in the knowledge that you had to find peripherals and software for that specific machine. The closest thing to a standard was the Apple II, but even then getting ""standard"" peripherals to work was often problematic.

I started out programming in BASIC on a 16k Exidy Sorcerer, and (I think) jumped straight to assembly programming next as that was the only other option available. Thinking back, it was astonishing how far you could get using BASIC alone; PEEK and POKE let you set or get data out of specific locations in memory, so it was feasible (though very painful) to do just about anything the computer was capable of doing using just BASIC. My switch to assembly was out of a desire for extra speed, not functionality.

Using languages like Fortran and BASIC, there was no concept of local variables - it was considered best practice to reuse your variables where possible to reduce the total memory footprint. Add in liberal use of GOTOs, which were just about mandatory due to the lack of looping structures, and zero debugging tools, and tracking down bugs in your code was a complex task. Luckily multitasking was way off in the future.

Switching to Pascal and C a few years later was a revelation - it had while loops and functions! Suddenly we had to learn how to break code up into distinct units of functionality, which was a pretty big deal given the best programmers of the day were considered those who could squeeze their code into the smallest amount of memory. Suddenly the concept of code reuse emerged, where you could write a function in C and then reuse it in another completely different program - good programmers started carrying around libraries of their old code, and even occasionally sharing it, once this magic reuse thing became available.

As Stan Hanks noted, we started to try to work as much as possible in registers rather than memory, because memory was SLOW. We used to examine the machine code that compilers produced using a disassembler, then hand optimise either our code or the raw machine code to try to squeeze out an extra few bytes.

I had friends who memorised the entire instruction set of the CPUs they worked with - not because it was a cool geek trick, but because it reduced time looking this stuff up over and over during the day. CPUs themselves were wildly different - by the early 1980s, Intel and Motorola were 2 of the biggest CPU vendors of the day, but their CPU's instruction sets were vastly different and it was very difficult for a programmer to switch from one to the other.

As an old guy, I think that programming back then was much more of a ""craft"", or even a ""guild"", where you just tried to learn every clever trick you could from the experts you had access to and put them to use wherever you could. Learning something new took a LONG time in the pre-Internet era, so you tried to squeeze as much as you possibly could out of the knowledge you already had rather than continually extending your skills in small chunks as is normal these days. These days, programming is much more about your expertise with particular frameworks or libraries, playing nicely with others, and coding is much more about glueing together pieces of existing proven functionality that very few people deeply understand. What percentage of programmers truly understand the inner workings of the JVM, or even something much smaller such as Spring? - 30 years ago, you had to know comparable stuff in detail, or know someone else with the knowledge, to get anything to work. We're working at a much higher level of abstraction than was possible 30 years ago - the entire concept of ""design patterns"" was crazy-haired professor stuff, not a basic requirement as it is now.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pa97wxt6lvneobcz', 'title': 'What was programming like back in the days when computers only had kilobytes of RAM?', 'score': {'original': 0.24825, 'ai': 0.75175}, 'blocks': [{'text': 'Very different to today. For a start, programming languages were very different; there were no ""scripting languages"" like Python, Ruby or Perl, so coding even simple solutions was quite time intensive. No JVM, so porting your code to another architecture was usually complex. Speaking of architectures, pre-1983 when the IBM PC first launched, there were no architectural standards as such; you spent serious $$$ to buy a machine in the knowledge that you had to find peripherals and software for that specific machine. The closest thing to a standard was the Apple II, but even then getting ""standard"" peripherals to work was often problematic.\n\nI started out programming in BASIC on a 16k Exidy Sorcerer, and (I think) jumped straight to assembly programming next as that was the only other option available. Thinking back, it was astonishing how far you could get using BASIC alone; PEEK and POKE let you set or get data out of specific locations in memory, so it was feasible (though very painful) to do just about anything the computer was capable of doing using just BASIC. My switch to assembly was out of a desire for extra speed, not functionality.\n\nUsing languages like Fortran and BASIC, there was no concept of local variables - it was considered best practice to reuse your variables where possible to reduce the total memory footprint. Add in liberal use of GOTOs, which were just about mandatory due to the lack of looping structures, and zero debugging tools, and tracking down bugs in your code was a complex task. Luckily multitasking was way off in the future.\n\nSwitching to Pascal and C a few years later was a revelation - it had while loops and functions! Suddenly we had to learn how to break code up into distinct units of functionality, which was a pretty big deal given the best programmers of the day were considered those who could squeeze their code into the smallest amount of memory. Suddenly the concept of code reuse emerged, where you could write a function in C and then reuse it in another completely different program - good programmers started carrying around libraries of their old code, and even occasionally sharing it, once this magic reuse thing became available.\n\nAs Stan Hanks noted, we started to try to work as much as possible in registers rather than memory, because memory was SLOW. We used to examine the machine code that compilers produced using a disassembler, then hand optimise either our code or the raw machine code to try to squeeze out an extra few bytes.\n\nI had friends who memorised the entire instruction set of the CPUs they worked with - not because it was a cool geek trick, but because it reduced time looking this stuff up over and over during the day. CPUs themselves were wildly different - by the early 1980s, Intel and Motorola were 2 of the biggest CPU vendors of the day, but their CPU\'s instruction sets were vastly different and it was very difficult for a programmer to switch from one to the other.\n\nAs an', 'result': {'fake': 0.419, 'real': 0.581}, 'status': 'success'}, {'text': 'old guy, I think that programming back then was much more of a ""craft"", or even a ""guild"", where you just tried to learn every clever trick you could from the experts you had access to and put them to use wherever you could. Learning something new took a LONG time in the pre-Internet era, so you tried to squeeze as much as you possibly could out of the knowledge you already had rather than continually extending your skills in small chunks as is normal these days. These days, programming is much more about your expertise with particular frameworks or libraries, playing nicely with others, and coding is much more about glueing together pieces of existing proven functionality that very few people deeply understand. What percentage of programmers truly understand the inner workings of the JVM, or even something much smaller such as Spring? - 30 years ago, you had to know comparable stuff in detail, or know someone else with the knowledge, to get anything to work. We\'re working at a much higher level of abstraction than was possible 30 years ago - the entire concept of ""design patterns"" was crazy-haired professor stuff, not a basic requirement as it is now.', 'result': {'fake': 0.7156, 'real': 0.2844}, 'status': 'success'}], 'credits_used': 8, 'credits': 1995134, 'subscription': 0, 'content': 'Very different to today. For a start, programming languages were very different; there were no ""scripting languages"" like Python, Ruby or Perl, so coding even simple solutions was quite time intensive. No JVM, so porting your code to another architecture was usually complex. Speaking of architectures, pre-1983 when the IBM PC first launched, there were no architectural standards as such; you spent serious $$$ to buy a machine in the knowledge that you had to find peripherals and software for that specific machine. The closest thing to a standard was the Apple II, but even then getting ""standard"" peripherals to work was often problematic.\n\nI started out programming in BASIC on a 16k Exidy Sorcerer, and (I think) jumped straight to assembly programming next as that was the only other option available. Thinking back, it was astonishing how far you could get using BASIC alone; PEEK and POKE let you set or get data out of specific locations in memory, so it was feasible (though very painful) to do just about anything the computer was capable of doing using just BASIC. My switch to assembly was out of a desire for extra speed, not functionality.\n\nUsing languages like Fortran and BASIC, there was no concept of local variables - it was considered best practice to reuse your variables where possible to reduce the total memory footprint. Add in liberal use of GOTOs, which were just about mandatory due to the lack of looping structures, and zero debugging tools, and tracking down bugs in your code was a complex task. Luckily multitasking was way off in the future.\n\nSwitching to Pascal and C a few years later was a revelation - it had while loops and functions! Suddenly we had to learn how to break code up into distinct units of functionality, which was a pretty big deal given the best programmers of the day were considered those who could squeeze their code into the smallest amount of memory. Suddenly the concept of code reuse emerged, where you could write a function in C and then reuse it in another completely different program - good programmers started carrying around libraries of their old code, and even occasionally sharing it, once this magic reuse thing became available.\n\nAs Stan Hanks noted, we started to try to work as much as possible in registers rather than memory, because memory was SLOW. We used to examine the machine code that compilers produced using a disassembler, then hand optimise either our code or the raw machine code to try to squeeze out an extra few bytes.\n\nI had friends who memorised the entire instruction set of the CPUs they worked with - not because it was a cool geek trick, but because it reduced time looking this stuff up over and over during the day. CPUs themselves were wildly different - by the early 1980s, Intel and Motorola were 2 of the biggest CPU vendors of the day, but their CPU\'s instruction sets were vastly different and it was very difficult for a programmer to switch from one to the other.\n\nAs an old guy, I think that programming back then was much more of a ""craft"", or even a ""guild"", where you just tried to learn every clever trick you could from the experts you had access to and put them to use wherever you could. Learning something new took a LONG time in the pre-Internet era, so you tried to squeeze as much as you possibly could out of the knowledge you already had rather than continually extending your skills in small chunks as is normal these days. These days, programming is much more about your expertise with particular frameworks or libraries, playing nicely with others, and coding is much more about glueing together pieces of existing proven functionality that very few people deeply understand. What percentage of programmers truly understand the inner workings of the JVM, or even something much smaller such as Spring? - 30 years ago, you had to know comparable stuff in detail, or know someone else with the knowledge, to get anything to work. We\'re working at a much higher level of abstraction than was possible 30 years ago - the entire concept of ""design patterns"" was crazy-haired professor stuff, not a basic requirement as it is now.', 'aiModelVersion': '1'}",0.24825
Ian Lang,3y,"How did Peter Norton or Bill Gates write programming code in the 60s, without access to the Internet's coding forums? Were they geniuses?","I did it in the 80s in the same way it had been done in the 60s and 70s.

We used these:

Or others like them, anyway. Books. They were made of paper and you didn’t need electricity to work them; well, an electric light came in handy if you were doing it at night, I suppose.

We also used these:

which were people who knew which page in which book (and often which paragraph) explained the bit you were stuck on.

Then we just twatted about bashing out code that didn’t work and thought about why it didn’t work and mended it. Breaking something else in the process. That just went on until we actually ran out of stuff to break and lo, some actual working stuff emerged!

It wasn’t genius, it was just application, practice, and perseverance. Some would just sit there for hours. Some would do Friday teatime to Monday morning, dropping to sleep on their keyboards and then waking up and doing some more. They were the serious ones.

Coding forums. Pah!

:P","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6qnz3kvu18bjgfp2', 'title': ""How did Peter Norton or Bill Gates write programming code in the 60s, without access to the Internet's coding forums? Were they geniuses?"", 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'I did it in the 80s in the same way it had been done in the 60s and 70s.\n\nWe used these:\n\nOr others like them, anyway. Books. They were made of paper and you didn’t need electricity to work them; well, an electric light came in handy if you were doing it at night, I suppose.\n\nWe also used these:\n\nwhich were people who knew which page in which book (and often which paragraph) explained the bit you were stuck on.\n\nThen we just twatted about bashing out code that didn’t work and thought about why it didn’t work and mended it. Breaking something else in the process. That just went on until we actually ran out of stuff to break and lo, some actual working stuff emerged!\n\nIt wasn’t genius, it was just application, practice, and perseverance. Some would just sit there for hours. Some would do Friday teatime to Monday morning, dropping to sleep on their keyboards and then waking up and doing some more. They were the serious ones.\n\nCoding forums. Pah!\n\n:P', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995132, 'subscription': 0, 'content': 'I did it in the 80s in the same way it had been done in the 60s and 70s.\n\nWe used these:\n\nOr others like them, anyway. Books. They were made of paper and you didn’t need electricity to work them; well, an electric light came in handy if you were doing it at night, I suppose.\n\nWe also used these:\n\nwhich were people who knew which page in which book (and often which paragraph) explained the bit you were stuck on.\n\nThen we just twatted about bashing out code that didn’t work and thought about why it didn’t work and mended it. Breaking something else in the process. That just went on until we actually ran out of stuff to break and lo, some actual working stuff emerged!\n\nIt wasn’t genius, it was just application, practice, and perseverance. Some would just sit there for hours. Some would do Friday teatime to Monday morning, dropping to sleep on their keyboards and then waking up and doing some more. They were the serious ones.\n\nCoding forums. Pah!\n\n:P', 'aiModelVersion': '1'}",0.9997
Greg Kemnitz,4y,What were the most laughably large first iterations of technology?,"Here’s the first usable hard drive from 1956 - all of 5 megabytes: IBM 350 RAMAC

It was portable, as you can see. The aircraft was custom-built to deliver these bad boys….

You couldn’t buy it, but you could lease it for $3200/month in 1956 dollars. $3200 could buy a luxury car; the average new car cost about $1700 in 1956. IBM ultimately built and shipped over 1000 of them…

It did very well in the business world, and sparked the mass storage revolution that continues to this day…

The above Seagate 3.5 inch datacenter-grade drive costs $387.91 in 2020 dollars on NewEgg, and holds several million times more storage than the IBM 350…","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2i8al1rnptqbm6sj', 'title': 'What were the most laughably large first iterations of technology?', 'score': {'original': 0.999, 'ai': 0.001}, 'blocks': [{'text': 'Here’s the first usable hard drive from 1956 - all of 5 megabytes: IBM 350 RAMAC\n\nIt was portable, as you can see. The aircraft was custom-built to deliver these bad boys….\n\nYou couldn’t buy it, but you could lease it for $3200/month in 1956 dollars. $3200 could buy a luxury car; the average new car cost about $1700 in 1956. IBM ultimately built and shipped over 1000 of them…\n\nIt did very well in the business world, and sparked the mass storage revolution that continues to this day…\n\nThe above Seagate 3.5 inch datacenter-grade drive costs $387.91 in 2020 dollars on NewEgg, and holds several million times more storage than the IBM 350…', 'result': {'fake': 0.001, 'real': 0.999}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995130, 'subscription': 0, 'content': 'Here’s the first usable hard drive from 1956 - all of 5 megabytes: IBM 350 RAMAC\n\nIt was portable, as you can see. The aircraft was custom-built to deliver these bad boys….\n\nYou couldn’t buy it, but you could lease it for $3200/month in 1956 dollars. $3200 could buy a luxury car; the average new car cost about $1700 in 1956. IBM ultimately built and shipped over 1000 of them…\n\nIt did very well in the business world, and sparked the mass storage revolution that continues to this day…\n\nThe above Seagate 3.5 inch datacenter-grade drive costs $387.91 in 2020 dollars on NewEgg, and holds several million times more storage than the IBM 350…', 'aiModelVersion': '1'}",0.999
Greg Kemnitz,4y,Why did Silicon Graphics (SGI) go bankrupt? What could they have done better in the early 2000's?,"There’s an ancient cycle in computer technology:

A Hard Problem appears.
Hard Problem is business-interesting.
Existing standard hardware sucks at handling Hard Problem.
Dedicated hardware appears to solve Hard Problem.
Company making dedicated hardware makes megabux solving Hard Problem. For a while.
Meanwhile, a combination of Moore's law
, custom chips, better software, etc improves performance on Hard Problem on general-purpose hardware.
Company making dedicated hardware tries to keep up. It does for awhile, but eventually fails. And dies.

Silicon Graphics, Symbolics
, and many other companies that made special-purpose hardware to solve some Hard Problem are examples of the casualties of this rather remorseless cycle.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/psbh7mcnx8ao3du4', 'title': ""Why did Silicon Graphics (SGI) go bankrupt? What could they have done better in the early 2000's?"", 'score': {'original': 0.9984, 'ai': 0.0016}, 'blocks': [{'text': ""There’s an ancient cycle in computer technology:\n\nA Hard Problem appears.\nHard Problem is business-interesting.\nExisting standard hardware sucks at handling Hard Problem.\nDedicated hardware appears to solve Hard Problem.\nCompany making dedicated hardware makes megabux solving Hard Problem. For a while.\nMeanwhile, a combination of Moore's law\n, custom chips, better software, etc improves performance on Hard Problem on general-purpose hardware.\nCompany making dedicated hardware tries to keep up. It does for awhile, but eventually fails. And dies.\n\nSilicon Graphics, Symbolics\n, and many other companies that made special-purpose hardware to solve some Hard Problem are examples of the casualties of this rather remorseless cycle."", 'result': {'fake': 0.0016, 'real': 0.9984}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995128, 'subscription': 0, 'content': ""There’s an ancient cycle in computer technology:\n\nA Hard Problem appears.\nHard Problem is business-interesting.\nExisting standard hardware sucks at handling Hard Problem.\nDedicated hardware appears to solve Hard Problem.\nCompany making dedicated hardware makes megabux solving Hard Problem. For a while.\nMeanwhile, a combination of Moore's law\n, custom chips, better software, etc improves performance on Hard Problem on general-purpose hardware.\nCompany making dedicated hardware tries to keep up. It does for awhile, but eventually fails. And dies.\n\nSilicon Graphics, Symbolics\n, and many other companies that made special-purpose hardware to solve some Hard Problem are examples of the casualties of this rather remorseless cycle."", 'aiModelVersion': '1'}",0.9984
Ralf Quint,Updated 1y,"Who invented the programming language Pascal, when and why? What are the strengths of the programming language?","Pascal was “invented” by Niklaus Wirth, a professor of computer science at the Swiss university ETH Zürich, first released in 1970.

Its first versions were based on the programming languages ALGOL and Euler, and was intended to be a programming language to help to teach structured programming.

In the ‘70s, several versions of Pascal were developed by others, including the very successful version called UCSD Pascal - Wikipedia
, developed by an extension of the University of California at San Diego in Irvine. This introduced quite a few extensions to original Pascal, making it very suitable for application programming, as well as a way to make it “cross-platform” by developing a 16 bit “pseudo code” core (all “home computers” were at that time still 8 bit, and commonly incompatible to each other), where compiled application code could be transferred between systems as long as a p-Code interpreter for that host system, that understands and executes that p-Code. Pretty much what the folks from Java did +20 years later with their “Java Virtual Machine” and tried to sell as the greatest invention since sliced bread.

UCSD Pascal also introduced the concept of units, which allowed to develop and debug larger applications in smaller, independent pieces, which in a more perverted way has been all the rave in recent years by folks promoting “unit testing”.

UCSD Pascal was also the first commercially successful Pascal compiler, adapted and rebadged by Apple as Apple Pascal for their Apple II and Apple III series of computers. I personally also used a version of UCSD Pascal sold by HP on HP 9816 workstations and Sage micro computers in the early ‘80s.

In the early 80s, a lot of other compilers started to emerge, all further evolving and extending the original Wirth language definition. Along with the starting wide spread availability of first CP/M based micro computers and the advent of the IBM PC (and the flood of clones), the most successful version (probably of any compiler/language implementation, ever) was Turbo Pascal - Wikipedia
 (first released by Danish author Anders Hejlsberg under the names CompasPascal and PolyPascal) by Philip Kahn and his company Borland.

Turbo Pascal adopted at first the crude menu based “development environment” from UCSD Pascal (at that time, there were no (affordable) graphical desktop systems around, in fact a lot of CP/M systems even just used a serial terminal), together with a very compact compiler, generating native code (first for the Z80, then the Intel 8086 family of CPUs), which due to the nature of the Pascal language allowed for an extremely fast one pass compiler, being magnitudes faster than any other compilers at that time, while at the same time costing less than 10% than any other competitors. Strangely enough, Hejlsberg did not copy the unit concept from UCSD Pascal at first, but that changed after initial success with version 4.0, which included the unit system with the addition of a smart linker as well was (one of) the first compiler to include a usable IDE which allowed for editing, compiling and debugging all in one place.

Other quite popular Pascal compilers at that time was Pascal MT+ from Digital Research (the developer of CP/M) as well as Metaware Professional Pascal, mainly for Z80 and Intel x86 systems (Metaware was an extremely well optimizing 80386 compiler).

At that time (early ‘80s) Apple introduced first the Apple Lisa and then the Apple Macintosh, for which Think Pascal and Metrowerks CodeWarrior became the most popular Pascal compilers for their Motorola 68k and PowerPC based systems, beside Apple’s own Macintosh Programmer’s Workshop (MPW,Macintosh Programmer's Workshop - Wikipedia
). In fact most of the early Mac OS was written in Pascal, so were a lot of 3rd party applications for it (like Adobe Photoshop Adobe Photoshop Source Code
).

Apple also helped to evolve Pascal even further (due to the nature of their now GUI based OS) into what became known as Object Pascal - Wikipedia
. That was also started to be adopted by Borland first with Turbo Pascal version 5.5, that was then expanded into Borland Pascal 7.0 with Objects, which then further developed into the Delphi RAD development systems based on Object Pascal, which is sold as a commercial product to this day, allowing for development on x86/x64 based Windows, as well as cross development for macOS, iOS and Android, as well as to some degree for Linux based servers.

In the late ’90s an Open Source compiler named Free Pascal - Wikipedia
 (actually named FPK-Florian’s Pascal Kompiler, by the German author, Florian Klämpfl) emerged. At first, this was a DOS based compiler producing 32bit 80386 code (at a time when Borland, and most other compiler vendors only supported up to the 80286 CPU) but then developed in Delphi compatible Object Pascal compiler. This was then quickly supplemented by an IDE called Lazarus (IDE) - Wikipedia
. Both allow these days not only development on Windows and x86/x64 based systems but more than a dozen different CPUs and operating system and GUI environments (Free Pascal - Wikipedia
).

And just for the record, in the late 80s, due to the fact of a lot of different Pascal compilers being around, usually not compatible with their own take of extensions and enhancements of Wirth’s original compiler, an attempt was made to introduce an ISO standard for the language, which became the abomination known as ISO Pascal/IEC/ISO 7185 and further “enhanced” as ISO/IEC 7185:1990 Pascal and ISO/IEC 10206. These standards however have a strong disconnect with real life, ignored how Pascal developed in the free world and are practically unused outside of some academic circles.

The main advantages of Pascal since its inception, which lend itself very well for large scale application development, is that it is being a well structured, readable, strongly typed programming language. That, together with the fact that pretty much all Pascal implementation have range checks and IO checks enabled by default, allows for a very safe software development, not allowing a lot of goofs and fubars that are commonly plaguing other programming languages, though its rather strict natures doesn’t sit well with a lot of folks that are just hacking away without a concept…","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/oxpzl2qm8vuwjni0', 'title': 'Who invented the programming language Pascal, when and why? What are the strengths of the programming language?', 'score': {'original': 0.63403333333333, 'ai': 0.36596666666667}, 'blocks': [{'text': 'Pascal was “invented” by Niklaus Wirth, a professor of computer science at the Swiss university ETH Zürich, first released in 1970.\n\nIts first versions were based on the programming languages ALGOL and Euler, and was intended to be a programming language to help to teach structured programming.\n\nIn the ‘70s, several versions of Pascal were developed by others, including the very successful version called UCSD Pascal - Wikipedia\n, developed by an extension of the University of California at San Diego in Irvine. This introduced quite a few extensions to original Pascal, making it very suitable for application programming, as well as a way to make it “cross-platform” by developing a 16 bit “pseudo code” core (all “home computers” were at that time still 8 bit, and commonly incompatible to each other), where compiled application code could be transferred between systems as long as a p-Code interpreter for that host system, that understands and executes that p-Code. Pretty much what the folks from Java did +20 years later with their “Java Virtual Machine” and tried to sell as the greatest invention since sliced bread.\n\nUCSD Pascal also introduced the concept of units, which allowed to develop and debug larger applications in smaller, independent pieces, which in a more perverted way has been all the rave in recent years by folks promoting “unit testing”.\n\nUCSD Pascal was also the first commercially successful Pascal compiler, adapted and rebadged by Apple as Apple Pascal for their Apple II and Apple III series of computers. I personally also used a version of UCSD Pascal sold by HP on HP 9816 workstations and Sage micro computers in the early ‘80s.\n\nIn the early 80s, a lot of other compilers started to emerge, all further evolving and extending the original Wirth language definition. Along with the starting wide spread availability of first CP/M based micro computers and the advent of the IBM PC (and the flood of clones), the most successful version (probably of any compiler/language implementation, ever) was Turbo Pascal - Wikipedia\n (first released by Danish author Anders Hejlsberg under the names CompasPascal and PolyPascal) by Philip Kahn and his company Borland.\n\nTurbo Pascal adopted at first the crude menu based “development environment” from UCSD Pascal (at that time, there were no (affordable) graphical desktop systems around, in fact a lot of CP/M systems even just used a serial terminal), together with a very compact compiler, generating native code (first for the Z80, then the Intel 8086 family of CPUs), which due to the nature of the Pascal language allowed for an extremely fast one pass compiler, being magnitudes faster than any other compilers at that time, while at the same time costing less than 10% than any other competitors. Strangely enough, Hejlsberg did not copy the unit concept from UCSD Pascal at first, but that changed after initial success with version 4.0, which included the unit system with the addition of a smart linker as well was (one of) the first compiler to include a usable IDE which allowed for editing, compiling and debugging all in one place.\n\nOther quite popular Pascal compilers at', 'result': {'fake': 0.005, 'real': 0.995}, 'status': 'success'}, {'text': ""that time was Pascal MT+ from Digital Research (the developer of CP/M) as well as Metaware Professional Pascal, mainly for Z80 and Intel x86 systems (Metaware was an extremely well optimizing 80386 compiler).\n\nAt that time (early ‘80s) Apple introduced first the Apple Lisa and then the Apple Macintosh, for which Think Pascal and Metrowerks CodeWarrior became the most popular Pascal compilers for their Motorola 68k and PowerPC based systems, beside Apple’s own Macintosh Programmer’s Workshop (MPW,Macintosh Programmer's Workshop - Wikipedia\n). In fact most of the early Mac OS was written in Pascal, so were a lot of 3rd party applications for it (like Adobe Photoshop Adobe Photoshop Source Code\n).\n\nApple also helped to evolve Pascal even further (due to the nature of their now GUI based OS) into what became known as Object Pascal - Wikipedia\n. That was also started to be adopted by Borland first with Turbo Pascal version 5.5, that was then expanded into Borland Pascal 7.0 with Objects, which then further developed into the Delphi RAD development systems based on Object Pascal, which is sold as a commercial product to this day, allowing for development on x86/x64 based Windows, as well as cross development for macOS, iOS and Android, as well as to some degree for Linux based servers.\n\nIn the late ’90s an Open Source compiler named Free Pascal - Wikipedia\n (actually named FPK-Florian’s Pascal Kompiler, by the German author, Florian Klämpfl) emerged. At first, this was a DOS based compiler producing 32bit 80386 code (at a time when Borland, and most other compiler vendors only supported up to the 80286 CPU) but then developed in Delphi compatible Object Pascal compiler. This was then quickly supplemented by an IDE called Lazarus (IDE) - Wikipedia\n. Both allow these days not only development on Windows and x86/x64 based systems but more than a dozen different CPUs and operating system and GUI environments (Free Pascal - Wikipedia\n).\n\nAnd just for the record, in the late 80s, due to the fact of a lot of different Pascal compilers being around, usually not compatible with their own take of extensions and enhancements of Wirth’s original compiler, an attempt was made to introduce an ISO standard for the language, which became the abomination known as ISO Pascal/IEC/ISO 7185 and further “enhanced” as ISO/IEC 7185:1990 Pascal and ISO/IEC 10206. These standards however have a strong disconnect with real life, ignored how Pascal developed in the free world and are practically unused outside of some academic circles.\n\nThe main advantages of Pascal since its inception, which lend itself very well for large scale application development, is that it is being a well structured, readable, strongly typed programming language. That, together with the fact that pretty much all Pascal implementation have range checks and IO checks enabled by default, allows for a very safe software development, not allowing a lot of goofs and fubars that are commonly plaguing other programming languages, though its rather strict natures doesn’t sit well with a lot of folks that are just hacking away without a concept…"", 'result': {'fake': 0.005, 'real': 0.995}, 'status': 'success'}], 'credits_used': 11, 'credits': 1995117, 'subscription': 0, 'content': ""Pascal was “invented” by Niklaus Wirth, a professor of computer science at the Swiss university ETH Zürich, first released in 1970.\n\nIts first versions were based on the programming languages ALGOL and Euler, and was intended to be a programming language to help to teach structured programming.\n\nIn the ‘70s, several versions of Pascal were developed by others, including the very successful version called UCSD Pascal - Wikipedia\n, developed by an extension of the University of California at San Diego in Irvine. This introduced quite a few extensions to original Pascal, making it very suitable for application programming, as well as a way to make it “cross-platform” by developing a 16 bit “pseudo code” core (all “home computers” were at that time still 8 bit, and commonly incompatible to each other), where compiled application code could be transferred between systems as long as a p-Code interpreter for that host system, that understands and executes that p-Code. Pretty much what the folks from Java did +20 years later with their “Java Virtual Machine” and tried to sell as the greatest invention since sliced bread.\n\nUCSD Pascal also introduced the concept of units, which allowed to develop and debug larger applications in smaller, independent pieces, which in a more perverted way has been all the rave in recent years by folks promoting “unit testing”.\n\nUCSD Pascal was also the first commercially successful Pascal compiler, adapted and rebadged by Apple as Apple Pascal for their Apple II and Apple III series of computers. I personally also used a version of UCSD Pascal sold by HP on HP 9816 workstations and Sage micro computers in the early ‘80s.\n\nIn the early 80s, a lot of other compilers started to emerge, all further evolving and extending the original Wirth language definition. Along with the starting wide spread availability of first CP/M based micro computers and the advent of the IBM PC (and the flood of clones), the most successful version (probably of any compiler/language implementation, ever) was Turbo Pascal - Wikipedia\n (first released by Danish author Anders Hejlsberg under the names CompasPascal and PolyPascal) by Philip Kahn and his company Borland.\n\nTurbo Pascal adopted at first the crude menu based “development environment” from UCSD Pascal (at that time, there were no (affordable) graphical desktop systems around, in fact a lot of CP/M systems even just used a serial terminal), together with a very compact compiler, generating native code (first for the Z80, then the Intel 8086 family of CPUs), which due to the nature of the Pascal language allowed for an extremely fast one pass compiler, being magnitudes faster than any other compilers at that time, while at the same time costing less than 10% than any other competitors. Strangely enough, Hejlsberg did not copy the unit concept from UCSD Pascal at first, but that changed after initial success with version 4.0, which included the unit system with the addition of a smart linker as well was (one of) the first compiler to include a usable IDE which allowed for editing, compiling and debugging all in one place.\n\nOther quite popular Pascal compilers at that time was Pascal MT+ from Digital Research (the developer of CP/M) as well as Metaware Professional Pascal, mainly for Z80 and Intel x86 systems (Metaware was an extremely well optimizing 80386 compiler).\n\nAt that time (early ‘80s) Apple introduced first the Apple Lisa and then the Apple Macintosh, for which Think Pascal and Metrowerks CodeWarrior became the most popular Pascal compilers for their Motorola 68k and PowerPC based systems, beside Apple’s own Macintosh Programmer’s Workshop (MPW,Macintosh Programmer's Workshop - Wikipedia\n). In fact most of the early Mac OS was written in Pascal, so were a lot of 3rd party applications for it (like Adobe Photoshop Adobe Photoshop Source Code\n).\n\nApple also helped to evolve Pascal even further (due to the nature of their now GUI based OS) into what became known as Object Pascal - Wikipedia\n. That was also started to be adopted by Borland first with Turbo Pascal version 5.5, that was then expanded into Borland Pascal 7.0 with Objects, which then further developed into the Delphi RAD development systems based on Object Pascal, which is sold as a commercial product to this day, allowing for development on x86/x64 based Windows, as well as cross development for macOS, iOS and Android, as well as to some degree for Linux based servers.\n\nIn the late ’90s an Open Source compiler named Free Pascal - Wikipedia\n (actually named FPK-Florian’s Pascal Kompiler, by the German author, Florian Klämpfl) emerged. At first, this was a DOS based compiler producing 32bit 80386 code (at a time when Borland, and most other compiler vendors only supported up to the 80286 CPU) but then developed in Delphi compatible Object Pascal compiler. This was then quickly supplemented by an IDE called Lazarus (IDE) - Wikipedia\n. Both allow these days not only development on Windows and x86/x64 based systems but more than a dozen different CPUs and operating system and GUI environments (Free Pascal - Wikipedia\n).\n\nAnd just for the record, in the late 80s, due to the fact of a lot of different Pascal compilers being around, usually not compatible with their own take of extensions and enhancements of Wirth’s original compiler, an attempt was made to introduce an ISO standard for the language, which became the abomination known as ISO Pascal/IEC/ISO 7185 and further “enhanced” as ISO/IEC 7185:1990 Pascal and ISO/IEC 10206. These standards however have a strong disconnect with real life, ignored how Pascal developed in the free world and are practically unused outside of some academic circles.\n\nThe main advantages of Pascal since its inception, which lend itself very well for large scale application development, is that it is being a well structured, readable, strongly typed programming language. That, together with the fact that pretty much all Pascal implementation have range checks and IO checks enabled by default, allows for a very safe software development, not allowing a lot of goofs and fubars that are commonly plaguing other programming languages, though its rather strict natures doesn’t sit well with a lot of folks that are just hacking away without a concept…"", 'aiModelVersion': '1'}",0.63403333333333
Muni Sreenivas Pydi,5y,What are the most fascinating incidents in ,"There’s one incident from the 60’s concerning Marvin Minsky
 and what may be considered the birth of Computer vision
. That was a time when the field of artificial intelligence was just getting started, a time when even the greatest of researchers were blissfully unaware of the challenges involved in making computers ‘see’ things.

In the summer of 1966, Marvin Minsky
 and Seymour Papert
 drafted a project proposal to use their “summer workers” (undergraduate interns) at MIT effectively. The goal was to hook up a camera to a computer and write programs that will detect objects from background areas, and then proceed to name the objects by matching them to a dictionary of known objects.

(The original summer project proposal, written by Seymour Papert[1])

Basically, Minsky wanted to SOLVE computer vision in ONE summer with a few undergrad INTERNS! I don’t know what they accomplished that summer, but it took hundreds of researchers more than half a century to achieve human-level performance in object detection and identification! And that too with computers that are at least a million times more powerful than what they had back in the 60’s!

Bonus Anecdote 1: Minsky’s summer team consisted of Gerald Jay Sussman
, Leslie Lamport
 and Richard Greenblatt
.

Bonus Anecdote 2: There’s an XKCD comic inspired from this incident.[2]

Footnotes

[1] http://dspace.mit.edu/bitstream/handle/1721.1/6125/AIM-100.pdf
[2] xkcd: Tasks","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ov1ic7xez0smla9t', 'title': 'What are the most fascinating incidents in', 'score': {'original': 0.9891, 'ai': 0.0109}, 'blocks': [{'text': 'There’s one incident from the 60’s concerning Marvin Minsky\n and what may be considered the birth of Computer vision\n. That was a time when the field of artificial intelligence was just getting started, a time when even the greatest of researchers were blissfully unaware of the challenges involved in making computers ‘see’ things.\n\nIn the summer of 1966, Marvin Minsky\n and Seymour Papert\n drafted a project proposal to use their “summer workers” (undergraduate interns) at MIT effectively. The goal was to hook up a camera to a computer and write programs that will detect objects from background areas, and then proceed to name the objects by matching them to a dictionary of known objects.\n\n(The original summer project proposal, written by Seymour Papert[1])\n\nBasically, Minsky wanted to SOLVE computer vision in ONE summer with a few undergrad INTERNS! I don’t know what they accomplished that summer, but it took hundreds of researchers more than half a century to achieve human-level performance in object detection and identification! And that too with computers that are at least a million times more powerful than what they had back in the 60’s!\n\nBonus Anecdote 1: Minsky’s summer team consisted of Gerald Jay Sussman\n, Leslie Lamport\n and Richard Greenblatt\n.\n\nBonus Anecdote 2: There’s an XKCD comic inspired from this incident.[2]\n\nFootnotes\n\n[1] http://dspace.mit.edu/bitstream/handle/1721.1/6125/AIM-100.pdf\n[2] xkcd: Tasks', 'result': {'fake': 0.0109, 'real': 0.9891}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995114, 'subscription': 0, 'content': 'There’s one incident from the 60’s concerning Marvin Minsky\n and what may be considered the birth of Computer vision\n. That was a time when the field of artificial intelligence was just getting started, a time when even the greatest of researchers were blissfully unaware of the challenges involved in making computers ‘see’ things.\n\nIn the summer of 1966, Marvin Minsky\n and Seymour Papert\n drafted a project proposal to use their “summer workers” (undergraduate interns) at MIT effectively. The goal was to hook up a camera to a computer and write programs that will detect objects from background areas, and then proceed to name the objects by matching them to a dictionary of known objects.\n\n(The original summer project proposal, written by Seymour Papert[1])\n\nBasically, Minsky wanted to SOLVE computer vision in ONE summer with a few undergrad INTERNS! I don’t know what they accomplished that summer, but it took hundreds of researchers more than half a century to achieve human-level performance in object detection and identification! And that too with computers that are at least a million times more powerful than what they had back in the 60’s!\n\nBonus Anecdote 1: Minsky’s summer team consisted of Gerald Jay Sussman\n, Leslie Lamport\n and Richard Greenblatt\n.\n\nBonus Anecdote 2: There’s an XKCD comic inspired from this incident.[2]\n\nFootnotes\n\n[1] http://dspace.mit.edu/bitstream/handle/1721.1/6125/AIM-100.pdf\n[2] xkcd: Tasks', 'aiModelVersion': '1'}",0.9891
Chris Nash,2y,Was Microsoft a giant company before the success of Windows 95?,"Oh, yes. Quite so.

Microsoft was a pretty successful company early on in the home computer revolution. They built BASIC interpreters for several early computers, including for the Apple II.

When IBM needed an operating system (OS) for their entry in the home computer market, the IBM PC, Bill Gates approached them and lied and said they had an OS for their device. He said it was called “DOS”, for disk operating system.

IBM bit and wanted a demonstration. Gates and Paul Allen already knew about an operating system by Seattle Computer Products (SCP). It was called 86-DOS written by a guy named Tim Paterson.

They already had a working relationship with SCP and approached them to market 86-DOS for $25,000. Later they bought full rights to it for $50,000 and renamed it MS-DOS.

MS-DOS startup prompt (image credit)

Here is where Gates and Microsoft got lucky. Gates said they’d license DOS to IBM, but they needed to be able to sell it to other customers as well. They’d rename it PC DOS for the IBM PC.

IBM was fine with that. They just needed an operating system for their computer; they didn’t care what Microsoft did with it afterwards.

This brings up an interesting question:

Why didn’t IBM write their own operating system?

IBM was in the computer business. Surely they’d written operating systems before. Why didn’t they write their own OS for their home computer?

Because they figured their home computer would be “small beans”.

About 1981, everyone was coming out with home computers. Texas Instruments, Commodore and even Timex were coming out with consumer-grade computers. IBM figured they should get in on the craze and release their own computer.

But they figured it would:

be a fad, and
wouldn’t be very popular

After all, what the heck did a normal person need a computer for? Balancing their checkbook?

But still, they wanted to cash in while people were willing to pay for it.

This is also why IBM developed their PC from commercial off-the-shelf (COTS) parts. They didn’t want to devote money to developing their own, custom hardware for something that would be a flash in the pan.

IBM also made their PC with an open architecture. Anyone could look and see how they were put together. They weren’t closed architecture like some computers (ahem, Apple).

The 1981 IBM PC: a heavy, beige box (image credit)

So they just wanted an operating system so they could start selling their entrant into the hot home computer market.

What few foresaw was that the PC was a hit. While miserable in many respects (four colors? Really?), IBM was a known brand. It was known the world over for its computers and business machines.

All of a sudden, businesses all over started standardizing on the IBM PC. Every office had to have it. It was the professional computer!

All of a sudden, IBM had a surprise hit! They weren’t stupid; they started producing more.

But there were problems. The PC was fine for office use, but it wasn’t very mobile:

What if a worker wanted to bring their work home with them?
Or on the road?

It was 28 pounds! And that’s not including that hefty monitor. And who wants to hassle with unhooking and hooking back up all those wires?

Plus it was pricey. The first IBM PC was US$1,565 (about $4,455 in 2021). That was steep, even for mid-level managers of the day. So it wasn’t especially practical for everyone to buy one for home use.

Since IBM had used COTS parts and an open architecture, competitors stepped in to fill the void and began developing IBM PC compatibles. They built cheaper systems with equivalent parts and even developed portable versions (Compaq) of their computers.

And Microsoft was there from the start, selling them MS-DOS which was nearly identical to PC DOS. So clone makers were raking in money, stealing some of IBM’s market. Microsoft reaped benefits too. While they didn’t make any money for each copy of PC DOS sold, they did for each copy of MS-DOS.

Microsoft made tons of money from DOS. And here’s where Microsoft got greedy. They said that clone makers had to pay them a fee for every computer they sold, whether it contained DOS or not.

This was clearly monopolistic, but since clone makers were being forced to pay a fee for every computer they sold, they just installed it by default. After all, they were getting charged for it anyway. Why not?

So it didn’t matter if customers had their own copy of DOS already or not, their PC was coming with a fresh install of it. Whether they liked it or not, they were getting charged for another copy of DOS, wrapped up in the purchase price.

Microsoft had other products, but DOS was a huge cash cow, so they kept improving it over the years. Eventually the Apple Macintosh came out and introduced a windowed interface to the market. Microsoft responded by developing their own windowing “overlays”, powered by DOS underneath. Microsoft Windows 1.0 and 2.0 were somewhat successful, but not earth-shattering.

But things changed with Windows 3.0. Suddenly, Windows was kinda good and usable! And it was compatible with all their existing DOS programs!

Microsoft Windows 3.0 (image credit)

Microsoft Windows 3.0 was widely adopted and Windows 3.1 was an even bigger hit. While not a revolutionary upgrade to Windows 3.0, it was a much-needed evolutionary upgrade.

Some of the features introduced with Windows 3.1 included:

Support for more sorts of Multimedia like MIDI and CDs
TrueType fonts
Workgroup networking
Program Manager
Super-VGA (800*600)
Introduction of CTRL+ALT+DELETE

Now Windows was even more useable and user-friendly. And was still compatible with all the existing DOS programs. Because, underneath, it was still just DOS. It was DOS with a pretty UI.

Windows 3.1 user interface was a polishing of the 3.0 interface (image credit)

By 1994, over 4 million copies of Windows 3.0 had been sold, and 3 million of Windows 3.1. Since Windows 3.1 cost about $149 in 1994, it alone grossed Microsoft about US$447,000,000 (about $1.2 billion in 2021).

But Windows 95 really was another revolutionary overhaul of the operating system, like Windows 3.0 was to DOS. It introduced DirectX, which was needed for truly fullscreen applications, like video games. It also introduced long, mixed-case filenames and was a true 32-bit operating system, as opposed to Windows 3.1, which was still 16-bit.

MS-DOS was still in it, but mostly as a compatibility layer instead of the primary OS.

Windows 95 (image credit)

And Microsoft marketed Windows 95 hard. Some may say they over-marketed it. Most commercials didn’t reference computers at all and many customers didn’t even know it was a computer operating system!

Microsoft sold 3,000,000 copies of Windows 95 in its first three days on the market. It would go on to sell 40 million copies its first year.

But yes, Microsoft was already a big player in the software world before Windows 95. Windows 95 was surely a big success for them—and their operating systems are still a huge cash cow—but Microsoft had other software products that were healthy at the time, such as Microsoft Word (for DOS) and several other product lines.

Update:

Joe Zbiciak pointed out a few minor errors in my answer here. Read his comments for the corrections.

Jerry Coffin also had some criticisms of my answer. You can read his comments below for his points.

Last update:

Several readers have posted “corrections” to my answer. That’s fine. Part of this came from memory, some comes from research. Some people have different opinions of why A caused B, and C implies D, and E didn’t really do F.

Even after 40 years, there’s still disagreement about what really happened. Fine.

But, I think everyone can agree that Microsoft was a giant company before Windows 95.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/a3hiyl2nd7bfmt1r', 'title': 'Was Microsoft a giant company before the success of Windows 95?', 'score': {'original': 0.96896666666667, 'ai': 0.031033333333333}, 'blocks': [{'text': 'Oh, yes. Quite so.\n\nMicrosoft was a pretty successful company early on in the home computer revolution. They built BASIC interpreters for several early computers, including for the Apple II.\n\nWhen IBM needed an operating system (OS) for their entry in the home computer market, the IBM PC, Bill Gates approached them and lied and said they had an OS for their device. He said it was called “DOS”, for disk operating system.\n\nIBM bit and wanted a demonstration. Gates and Paul Allen already knew about an operating system by Seattle Computer Products (SCP). It was called 86-DOS written by a guy named Tim Paterson.\n\nThey already had a working relationship with SCP and approached them to market 86-DOS for $25,000. Later they bought full rights to it for $50,000 and renamed it MS-DOS.\n\nMS-DOS startup prompt (image credit)\n\nHere is where Gates and Microsoft got lucky. Gates said they’d license DOS to IBM, but they needed to be able to sell it to other customers as well. They’d rename it PC DOS for the IBM PC.\n\nIBM was fine with that. They just needed an operating system for their computer; they didn’t care what Microsoft did with it afterwards.\n\nThis brings up an interesting question:\n\nWhy didn’t IBM write their own operating system?\n\nIBM was in the computer business. Surely they’d written operating systems before. Why didn’t they write their own OS for their home computer?\n\nBecause they figured their home computer would be “small beans”.\n\nAbout 1981, everyone was coming out with home computers. Texas Instruments, Commodore and even Timex were coming out with consumer-grade computers. IBM figured they should get in on the craze and release their own computer.\n\nBut they figured it would:\n\nbe a fad, and\nwouldn’t be very popular\n\nAfter all, what the heck did a normal person need a computer for? Balancing their checkbook?\n\nBut still, they wanted to cash in while people were willing to pay for it.\n\nThis is also why IBM developed their PC from commercial off-the-shelf (COTS) parts. They didn’t want to devote money to developing their own, custom hardware for something that would be a flash in the pan.\n\nIBM also made their PC with an open architecture. Anyone could look and see how they were put together. They weren’t closed architecture like some computers (ahem, Apple).\n\nThe 1981 IBM PC: a heavy, beige box (image credit)\n\nSo they just wanted an operating system so they could start selling their entrant into the hot home computer market.\n\nWhat few foresaw was that the PC was a hit. While miserable in many respects (four colors? Really?), IBM was a known brand. It was known the world over for its computers and business machines.\n\nAll of a sudden, businesses all over started standardizing on the IBM PC. Every office had to have it. It was the professional computer!\n\nAll of a sudden, IBM had a surprise hit! They weren’t stupid; they started producing more.\n\nBut there were problems. The PC was fine for office use, but it wasn’t very mobile:\n\nWhat if a worker wanted to bring their work home with them?\nOr on the road?\n\nIt was 28 pounds! And that’s not including that hefty monitor. And who wants', 'result': {'fake': 0.008, 'real': 0.992}, 'status': 'success'}, {'text': 'to hassle with unhooking and hooking back up all those wires?\n\nPlus it was pricey. The first IBM PC was US$1,565 (about $4,455 in 2021). That was steep, even for mid-level managers of the day. So it wasn’t especially practical for everyone to buy one for home use.\n\nSince IBM had used COTS parts and an open architecture, competitors stepped in to fill the void and began developing IBM PC compatibles. They built cheaper systems with equivalent parts and even developed portable versions (Compaq) of their computers.\n\nAnd Microsoft was there from the start, selling them MS-DOS which was nearly identical to PC DOS. So clone makers were raking in money, stealing some of IBM’s market. Microsoft reaped benefits too. While they didn’t make any money for each copy of PC DOS sold, they did for each copy of MS-DOS.\n\nMicrosoft made tons of money from DOS. And here’s where Microsoft got greedy. They said that clone makers had to pay them a fee for every computer they sold, whether it contained DOS or not.\n\nThis was clearly monopolistic, but since clone makers were being forced to pay a fee for every computer they sold, they just installed it by default. After all, they were getting charged for it anyway. Why not?\n\nSo it didn’t matter if customers had their own copy of DOS already or not, their PC was coming with a fresh install of it. Whether they liked it or not, they were getting charged for another copy of DOS, wrapped up in the purchase price.\n\nMicrosoft had other products, but DOS was a huge cash cow, so they kept improving it over the years. Eventually the Apple Macintosh came out and introduced a windowed interface to the market. Microsoft responded by developing their own windowing “overlays”, powered by DOS underneath. Microsoft Windows 1.0 and 2.0 were somewhat successful, but not earth-shattering.\n\nBut things changed with Windows 3.0. Suddenly, Windows was kinda good and usable! And it was compatible with all their existing DOS programs!\n\nMicrosoft Windows 3.0 (image credit)\n\nMicrosoft Windows 3.0 was widely adopted and Windows 3.1 was an even bigger hit. While not a revolutionary upgrade to Windows 3.0, it was a much-needed evolutionary upgrade.\n\nSome of the features introduced with Windows 3.1 included:\n\nSupport for more sorts of Multimedia like MIDI and CDs\nTrueType fonts\nWorkgroup networking\nProgram Manager\nSuper-VGA (800*600)\nIntroduction of CTRL+ALT+DELETE\n\nNow Windows was even more useable and user-friendly. And was still compatible with all the existing DOS programs. Because, underneath, it was still just DOS. It was DOS with a pretty UI.\n\nWindows 3.1 user interface was a polishing of the 3.0 interface (image credit)\n\nBy 1994, over 4 million copies of Windows 3.0 had been sold, and 3 million of Windows 3.1. Since Windows 3.1 cost about $149 in 1994, it alone grossed Microsoft about US$447,000,000 (about $1.2 billion in 2021).\n\nBut Windows 95 really was another revolutionary overhaul of the operating system, like Windows 3.0 was to DOS. It introduced DirectX, which was needed for truly fullscreen applications, like video games. It also introduced long, mixed-case filenames and was a true 32-bit operating system, as opposed to Windows 3.1, which was', 'result': {'fake': 0.5828, 'real': 0.4172}, 'status': 'success'}, {'text': 'still 16-bit.\n\nMS-DOS was still in it, but mostly as a compatibility layer instead of the primary OS.\n\nWindows 95 (image credit)\n\nAnd Microsoft marketed Windows 95 hard. Some may say they over-marketed it. Most commercials didn’t reference computers at all and many customers didn’t even know it was a computer operating system!\n\nMicrosoft sold 3,000,000 copies of Windows 95 in its first three days on the market. It would go on to sell 40 million copies its first year.\n\nBut yes, Microsoft was already a big player in the software world before Windows 95. Windows 95 was surely a big success for them—and their operating systems are still a huge cash cow—but Microsoft had other software products that were healthy at the time, such as Microsoft Word (for DOS) and several other product lines.\n\nUpdate:\n\nJoe Zbiciak pointed out a few minor errors in my answer here. Read his comments for the corrections.\n\nJerry Coffin also had some criticisms of my answer. You can read his comments below for his points.\n\nLast update:\n\nSeveral readers have posted “corrections” to my answer. That’s fine. Part of this came from memory, some comes from research. Some people have different opinions of why A caused B, and C implies D, and E didn’t really do F.\n\nEven after 40 years, there’s still disagreement about what really happened. Fine.\n\nBut, I think everyone can agree that Microsoft was a giant company before Windows 95.', 'result': {'fake': 0.9616, 'real': 0.0384}, 'status': 'success'}], 'credits_used': 13, 'credits': 1995101, 'subscription': 0, 'content': 'Oh, yes. Quite so.\n\nMicrosoft was a pretty successful company early on in the home computer revolution. They built BASIC interpreters for several early computers, including for the Apple II.\n\nWhen IBM needed an operating system (OS) for their entry in the home computer market, the IBM PC, Bill Gates approached them and lied and said they had an OS for their device. He said it was called “DOS”, for disk operating system.\n\nIBM bit and wanted a demonstration. Gates and Paul Allen already knew about an operating system by Seattle Computer Products (SCP). It was called 86-DOS written by a guy named Tim Paterson.\n\nThey already had a working relationship with SCP and approached them to market 86-DOS for $25,000. Later they bought full rights to it for $50,000 and renamed it MS-DOS.\n\nMS-DOS startup prompt (image credit)\n\nHere is where Gates and Microsoft got lucky. Gates said they’d license DOS to IBM, but they needed to be able to sell it to other customers as well. They’d rename it PC DOS for the IBM PC.\n\nIBM was fine with that. They just needed an operating system for their computer; they didn’t care what Microsoft did with it afterwards.\n\nThis brings up an interesting question:\n\nWhy didn’t IBM write their own operating system?\n\nIBM was in the computer business. Surely they’d written operating systems before. Why didn’t they write their own OS for their home computer?\n\nBecause they figured their home computer would be “small beans”.\n\nAbout 1981, everyone was coming out with home computers. Texas Instruments, Commodore and even Timex were coming out with consumer-grade computers. IBM figured they should get in on the craze and release their own computer.\n\nBut they figured it would:\n\nbe a fad, and\nwouldn’t be very popular\n\nAfter all, what the heck did a normal person need a computer for? Balancing their checkbook?\n\nBut still, they wanted to cash in while people were willing to pay for it.\n\nThis is also why IBM developed their PC from commercial off-the-shelf (COTS) parts. They didn’t want to devote money to developing their own, custom hardware for something that would be a flash in the pan.\n\nIBM also made their PC with an open architecture. Anyone could look and see how they were put together. They weren’t closed architecture like some computers (ahem, Apple).\n\nThe 1981 IBM PC: a heavy, beige box (image credit)\n\nSo they just wanted an operating system so they could start selling their entrant into the hot home computer market.\n\nWhat few foresaw was that the PC was a hit. While miserable in many respects (four colors? Really?), IBM was a known brand. It was known the world over for its computers and business machines.\n\nAll of a sudden, businesses all over started standardizing on the IBM PC. Every office had to have it. It was the professional computer!\n\nAll of a sudden, IBM had a surprise hit! They weren’t stupid; they started producing more.\n\nBut there were problems. The PC was fine for office use, but it wasn’t very mobile:\n\nWhat if a worker wanted to bring their work home with them?\nOr on the road?\n\nIt was 28 pounds! And that’s not including that hefty monitor. And who wants to hassle with unhooking and hooking back up all those wires?\n\nPlus it was pricey. The first IBM PC was US$1,565 (about $4,455 in 2021). That was steep, even for mid-level managers of the day. So it wasn’t especially practical for everyone to buy one for home use.\n\nSince IBM had used COTS parts and an open architecture, competitors stepped in to fill the void and began developing IBM PC compatibles. They built cheaper systems with equivalent parts and even developed portable versions (Compaq) of their computers.\n\nAnd Microsoft was there from the start, selling them MS-DOS which was nearly identical to PC DOS. So clone makers were raking in money, stealing some of IBM’s market. Microsoft reaped benefits too. While they didn’t make any money for each copy of PC DOS sold, they did for each copy of MS-DOS.\n\nMicrosoft made tons of money from DOS. And here’s where Microsoft got greedy. They said that clone makers had to pay them a fee for every computer they sold, whether it contained DOS or not.\n\nThis was clearly monopolistic, but since clone makers were being forced to pay a fee for every computer they sold, they just installed it by default. After all, they were getting charged for it anyway. Why not?\n\nSo it didn’t matter if customers had their own copy of DOS already or not, their PC was coming with a fresh install of it. Whether they liked it or not, they were getting charged for another copy of DOS, wrapped up in the purchase price.\n\nMicrosoft had other products, but DOS was a huge cash cow, so they kept improving it over the years. Eventually the Apple Macintosh came out and introduced a windowed interface to the market. Microsoft responded by developing their own windowing “overlays”, powered by DOS underneath. Microsoft Windows 1.0 and 2.0 were somewhat successful, but not earth-shattering.\n\nBut things changed with Windows 3.0. Suddenly, Windows was kinda good and usable! And it was compatible with all their existing DOS programs!\n\nMicrosoft Windows 3.0 (image credit)\n\nMicrosoft Windows 3.0 was widely adopted and Windows 3.1 was an even bigger hit. While not a revolutionary upgrade to Windows 3.0, it was a much-needed evolutionary upgrade.\n\nSome of the features introduced with Windows 3.1 included:\n\nSupport for more sorts of Multimedia like MIDI and CDs\nTrueType fonts\nWorkgroup networking\nProgram Manager\nSuper-VGA (800*600)\nIntroduction of CTRL+ALT+DELETE\n\nNow Windows was even more useable and user-friendly. And was still compatible with all the existing DOS programs. Because, underneath, it was still just DOS. It was DOS with a pretty UI.\n\nWindows 3.1 user interface was a polishing of the 3.0 interface (image credit)\n\nBy 1994, over 4 million copies of Windows 3.0 had been sold, and 3 million of Windows 3.1. Since Windows 3.1 cost about $149 in 1994, it alone grossed Microsoft about US$447,000,000 (about $1.2 billion in 2021).\n\nBut Windows 95 really was another revolutionary overhaul of the operating system, like Windows 3.0 was to DOS. It introduced DirectX, which was needed for truly fullscreen applications, like video games. It also introduced long, mixed-case filenames and was a true 32-bit operating system, as opposed to Windows 3.1, which was still 16-bit.\n\nMS-DOS was still in it, but mostly as a compatibility layer instead of the primary OS.\n\nWindows 95 (image credit)\n\nAnd Microsoft marketed Windows 95 hard. Some may say they over-marketed it. Most commercials didn’t reference computers at all and many customers didn’t even know it was a computer operating system!\n\nMicrosoft sold 3,000,000 copies of Windows 95 in its first three days on the market. It would go on to sell 40 million copies its first year.\n\nBut yes, Microsoft was already a big player in the software world before Windows 95. Windows 95 was surely a big success for them—and their operating systems are still a huge cash cow—but Microsoft had other software products that were healthy at the time, such as Microsoft Word (for DOS) and several other product lines.\n\nUpdate:\n\nJoe Zbiciak pointed out a few minor errors in my answer here. Read his comments for the corrections.\n\nJerry Coffin also had some criticisms of my answer. You can read his comments below for his points.\n\nLast update:\n\nSeveral readers have posted “corrections” to my answer. That’s fine. Part of this came from memory, some comes from research. Some people have different opinions of why A caused B, and C implies D, and E didn’t really do F.\n\nEven after 40 years, there’s still disagreement about what really happened. Fine.\n\nBut, I think everyone can agree that Microsoft was a giant company before Windows 95.', 'aiModelVersion': '1'}",0.96896666666667
Randy Wigginton,9y,Who are the unsung heroes in Apple's history?,"Unsung heroes are those that are rarely heard from or noticed.  The first two would have to be Mike Scott - Apple's first CEO, and Rod Holt, our analog guru that created the switching power supply, along with disk drives and other amazing contributions.  Rod was also our first Engineering manager.Mike Scott built Apple, guiding it to a very large company, then was asked to leave for personal reasons.  He is not even mentioned in the Jobs' movie, and mentions of him are few and far between.  How many people even know he was the first CEO?  Everyone seems to think that Markkula or Jobs was the CEO.Dick & Cliff Huston are  unsung heroes -- very few people even know their names.  However, they produced much of Apple's early hardware and software.Gene Carter - produced a great sales organization.  Jim Martindale - built the first production line, and was responsible for Apple II production for a very long time. Like any great success story, far more people worked behind the scenes than ever get credit. I would argue that Andy Hertzfeld is not an unsung hero - he was front and center in many articles.  This is not to minimize his contributions in any manner; I am simply addressing this question.  Andy was critical to the Mac.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/89xdpnygf3ehj7k6', 'title': ""Who are the unsung heroes in Apple's history?"", 'score': {'original': 0.9989, 'ai': 0.0011}, 'blocks': [{'text': ""Unsung heroes are those that are rarely heard from or noticed.  The first two would have to be Mike Scott - Apple's first CEO, and Rod Holt, our analog guru that created the switching power supply, along with disk drives and other amazing contributions.  Rod was also our first Engineering manager.Mike Scott built Apple, guiding it to a very large company, then was asked to leave for personal reasons.  He is not even mentioned in the Jobs' movie, and mentions of him are few and far between.  How many people even know he was the first CEO?  Everyone seems to think that Markkula or Jobs was the CEO.Dick & Cliff Huston are  unsung heroes -- very few people even know their names.  However, they produced much of Apple's early hardware and software.Gene Carter - produced a great sales organization.  Jim Martindale - built the first production line, and was responsible for Apple II production for a very long time. Like any great success story, far more people worked behind the scenes than ever get credit. I would argue that Andy Hertzfeld is not an unsung hero - he was front and center in many articles.  This is not to minimize his contributions in any manner; I am simply addressing this question.  Andy was critical to the Mac."", 'result': {'fake': 0.0013, 'real': 0.9987}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995098, 'subscription': 0, 'content': ""Unsung heroes are those that are rarely heard from or noticed.  The first two would have to be Mike Scott - Apple's first CEO, and Rod Holt, our analog guru that created the switching power supply, along with disk drives and other amazing contributions.  Rod was also our first Engineering manager.Mike Scott built Apple, guiding it to a very large company, then was asked to leave for personal reasons.  He is not even mentioned in the Jobs' movie, and mentions of him are few and far between.  How many people even know he was the first CEO?  Everyone seems to think that Markkula or Jobs was the CEO.Dick & Cliff Huston are  unsung heroes -- very few people even know their names.  However, they produced much of Apple's early hardware and software.Gene Carter - produced a great sales organization.  Jim Martindale - built the first production line, and was responsible for Apple II production for a very long time. Like any great success story, far more people worked behind the scenes than ever get credit. I would argue that Andy Hertzfeld is not an unsung hero - he was front and center in many articles.  This is not to minimize his contributions in any manner; I am simply addressing this question.  Andy was critical to the Mac."", 'aiModelVersion': '1'}",0.9989
Jesse Tov,4y,What are some ways that programming was better in the past?,"Almost everything is better today than when I started in the late 1980s, as many other answers here amply discuss. But there’s one thing I think was better back then: learning to program on the Apple II with Applesoft BASIC.

When I was a kid, classrooms were full of Apple IIe machines. They didn’t have hard disks (much less SSDs), so if you wanted to run a pre-written program, you had to put a floppy disk with that program on it in the floppy drive before turning on the computer; then it would boot off that disk and run the program. But what if you turned on an Apple II without any disks in the drives? It would load the BASIC interpreter that it had in ROM:

Today, when you turn on a computer, there are thousands of ways to make a program and millions of other things you can do instead. But the default behavior of this computer, when you turned it on without a program selected, was to ask you to type a program in instead. So we did!

BASIC is a terrible language for almost any purpose, and we have much better educational languages
 now. But the advantage was that the model was simple enough for young children to understand. You want to do input? Print a prompt and read a string from the user. You want to do graphics? How about 260-by-160 pixels with eight colors? It’s not OpenGL, but that’s the point—it’s easy. And since BASIC was available everywhere, there was no friction to trying it out.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/kmdjlrycox0eu52p', 'title': 'What are some ways that programming was better in the past?', 'score': {'original': 0.9991, 'ai': 0.0009}, 'blocks': [{'text': 'Almost everything is better today than when I started in the late 1980s, as many other answers here amply discuss. But there’s one thing I think was better back then: learning to program on the Apple II with Applesoft BASIC.\n\nWhen I was a kid, classrooms were full of Apple IIe machines. They didn’t have hard disks (much less SSDs), so if you wanted to run a pre-written program, you had to put a floppy disk with that program on it in the floppy drive before turning on the computer; then it would boot off that disk and run the program. But what if you turned on an Apple II without any disks in the drives? It would load the BASIC interpreter that it had in ROM:\n\nToday, when you turn on a computer, there are thousands of ways to make a program and millions of other things you can do instead. But the default behavior of this computer, when you turned it on without a program selected, was to ask you to type a program in instead. So we did!\n\nBASIC is a terrible language for almost any purpose, and we have much better educational languages\n now. But the advantage was that the model was simple enough for young children to understand. You want to do input? Print a prompt and read a string from the user. You want to do graphics? How about 260-by-160 pixels with eight colors? It’s not OpenGL, but that’s the point—it’s easy. And since BASIC was available everywhere, there was no friction to trying it out.', 'result': {'fake': 0.0009, 'real': 0.9991}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995095, 'subscription': 0, 'content': 'Almost everything is better today than when I started in the late 1980s, as many other answers here amply discuss. But there’s one thing I think was better back then: learning to program on the Apple II with Applesoft BASIC.\n\nWhen I was a kid, classrooms were full of Apple IIe machines. They didn’t have hard disks (much less SSDs), so if you wanted to run a pre-written program, you had to put a floppy disk with that program on it in the floppy drive before turning on the computer; then it would boot off that disk and run the program. But what if you turned on an Apple II without any disks in the drives? It would load the BASIC interpreter that it had in ROM:\n\nToday, when you turn on a computer, there are thousands of ways to make a program and millions of other things you can do instead. But the default behavior of this computer, when you turned it on without a program selected, was to ask you to type a program in instead. So we did!\n\nBASIC is a terrible language for almost any purpose, and we have much better educational languages\n now. But the advantage was that the model was simple enough for young children to understand. You want to do input? Print a prompt and read a string from the user. You want to do graphics? How about 260-by-160 pixels with eight colors? It’s not OpenGL, but that’s the point—it’s easy. And since BASIC was available everywhere, there was no friction to trying it out.', 'aiModelVersion': '1'}",0.9991
Susanna Viljanen,Updated 8mo,Which of the early computers were used by the military?,"The mechanical analogue computers, such as Admiralty Fire Control Table, Sokutekiban and Ford Rangekeeper. They were used as gunnery fire control central processing units, and they were far older than the electronic digital computers.

The Admiralty Fire Control Table onboard HMS Belfast. Its basic form was designed in 1919, and it was based on the previous Dreyer Fire Control Table

The fire control computer is located deep in the ship’s bilge in a shop called transmitting station. It is basically the “brain” of the ship. On HMS Belfast, it is next to the steering room where the ship’s helm is located.

The fire control computers took own speed, own heading, enemy speed, enemy heading, distance to enemy, wind, air pressure, temperature etc variables in, adjusted certain corrections, and calculated the elevation and azimuth for the own guns, the values indicating where the guns should be pointing at for the time of flight when the shells hit the enemy’s calculated position. The fire control officer then fired the guns as a broadside with an electric central firing key.

The enemy ship was sunk with sheer mathematics.

See also Dreyer Fire Control Table - The Dreadnought Project.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/wxkhrnipjo9eu2fs', 'title': 'Which of the early computers were used by the military?', 'score': {'original': 0.999, 'ai': 0.001}, 'blocks': [{'text': 'The mechanical analogue computers, such as Admiralty Fire Control Table, Sokutekiban and Ford Rangekeeper. They were used as gunnery fire control central processing units, and they were far older than the electronic digital computers.\n\nThe Admiralty Fire Control Table onboard HMS Belfast. Its basic form was designed in 1919, and it was based on the previous Dreyer Fire Control Table\n\nThe fire control computer is located deep in the ship’s bilge in a shop called transmitting station. It is basically the “brain” of the ship. On HMS Belfast, it is next to the steering room where the ship’s helm is located.\n\nThe fire control computers took own speed, own heading, enemy speed, enemy heading, distance to enemy, wind, air pressure, temperature etc variables in, adjusted certain corrections, and calculated the elevation and azimuth for the own guns, the values indicating where the guns should be pointing at for the time of flight when the shells hit the enemy’s calculated position. The fire control officer then fired the guns as a broadside with an electric central firing key.\n\nThe enemy ship was sunk with sheer mathematics.\n\nSee also Dreyer Fire Control Table - The Dreadnought Project.', 'result': {'fake': 0.001, 'real': 0.999}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995093, 'subscription': 0, 'content': 'The mechanical analogue computers, such as Admiralty Fire Control Table, Sokutekiban and Ford Rangekeeper. They were used as gunnery fire control central processing units, and they were far older than the electronic digital computers.\n\nThe Admiralty Fire Control Table onboard HMS Belfast. Its basic form was designed in 1919, and it was based on the previous Dreyer Fire Control Table\n\nThe fire control computer is located deep in the ship’s bilge in a shop called transmitting station. It is basically the “brain” of the ship. On HMS Belfast, it is next to the steering room where the ship’s helm is located.\n\nThe fire control computers took own speed, own heading, enemy speed, enemy heading, distance to enemy, wind, air pressure, temperature etc variables in, adjusted certain corrections, and calculated the elevation and azimuth for the own guns, the values indicating where the guns should be pointing at for the time of flight when the shells hit the enemy’s calculated position. The fire control officer then fired the guns as a broadside with an electric central firing key.\n\nThe enemy ship was sunk with sheer mathematics.\n\nSee also Dreyer Fire Control Table - The Dreadnought Project.', 'aiModelVersion': '1'}",0.999
Joshua Gross,2y,What was the last breakthrough in computer programming?,"I'll offer a reflection on Alan Kay's answer. Most of the work in the past 30 years has been to realize what the 70s promised with ideas, largely as hardware scaled several orders of magnitude. However, it wasn't fully realized. Some of this is due to a lack of understanding, and some due to practical considerations, and some due to the fact that these ideas were not perfect. Also, resources are limited, and change is hard. We have trillions of dollars invested in existing technology in the US alone.

There have been methodological innovations, but no major new tech or PL ideas. Kotlin is the hot language of today, and one reason is its support for coroutines. As an idea, coroutines have existed for more than 60 years. I could go on and on with examples, but in programming itself, innovation is largely at the edge, dealing with special problems. This doesn't make the work less important, but I cannot think of a programming language feature that did not exist in some form when I started more than two decades ago.

I've written many times that the hardest and most interesting problem in CS today is educating the mass of people who would like to learn to program. We have a fraction of the educators we need.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/wgl64m8h2oz9a570', 'title': 'What was the last breakthrough in computer programming?', 'score': {'original': 0.1621, 'ai': 0.8379}, 'blocks': [{'text': ""I'll offer a reflection on Alan Kay's answer. Most of the work in the past 30 years has been to realize what the 70s promised with ideas, largely as hardware scaled several orders of magnitude. However, it wasn't fully realized. Some of this is due to a lack of understanding, and some due to practical considerations, and some due to the fact that these ideas were not perfect. Also, resources are limited, and change is hard. We have trillions of dollars invested in existing technology in the US alone.\n\nThere have been methodological innovations, but no major new tech or PL ideas. Kotlin is the hot language of today, and one reason is its support for coroutines. As an idea, coroutines have existed for more than 60 years. I could go on and on with examples, but in programming itself, innovation is largely at the edge, dealing with special problems. This doesn't make the work less important, but I cannot think of a programming language feature that did not exist in some form when I started more than two decades ago.\n\nI've written many times that the hardest and most interesting problem in CS today is educating the mass of people who would like to learn to program. We have a fraction of the educators we need."", 'result': {'fake': 0.8379, 'real': 0.1621}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995090, 'subscription': 0, 'content': ""I'll offer a reflection on Alan Kay's answer. Most of the work in the past 30 years has been to realize what the 70s promised with ideas, largely as hardware scaled several orders of magnitude. However, it wasn't fully realized. Some of this is due to a lack of understanding, and some due to practical considerations, and some due to the fact that these ideas were not perfect. Also, resources are limited, and change is hard. We have trillions of dollars invested in existing technology in the US alone.\n\nThere have been methodological innovations, but no major new tech or PL ideas. Kotlin is the hot language of today, and one reason is its support for coroutines. As an idea, coroutines have existed for more than 60 years. I could go on and on with examples, but in programming itself, innovation is largely at the edge, dealing with special problems. This doesn't make the work less important, but I cannot think of a programming language feature that did not exist in some form when I started more than two decades ago.\n\nI've written many times that the hardest and most interesting problem in CS today is educating the mass of people who would like to learn to program. We have a fraction of the educators we need."", 'aiModelVersion': '1'}",0.1621
Tom Crosley,Updated 2y,What was the purpose of all of the blinking lights on the early computers (think Univac and Eniac) of the 1950s & 1960s? What information did they convey?,"The ENIAC didn’t really have a central console as such, it was a modular computer, taking up an entire room (as many first generation computers did). However unlike other machines, each of its ten accumulators were the size of large side by side refrigerators and had their own display — but there was no program counter in the original machine. Programming was done by wiring the boxes together in a particular order.

What you’re probably referring to is the central console of the UNIVAC I (1951), a first generation machine using vacuum tubes, which was the first commercially successful computer in the US:

The main console displayed the status of each of the hardware registers in the machine, as well as the contents of various buses. Using a console like this, one could observe the running of a program, and if the computer “crashed” and halted, it would display information useful to the programmer. One could also load data into any of the registers using the switches.

Most of these early computers had no operating system. Here’s a picture of a young Donald Knuth, famous for his The Art of Computer Programming series (which I had as textbooks when getting my MSCS degree) at the console of an IBM 650 (late-1950s), the first mass-produced computer in the world:

Even later mainframe computers still had consoles, even though they now had operating systems and were running multiple programs at the same time. Here’s the main console of an IBM 360/65 (1965), a second generation (discrete transistor) computer:

While in grad school, I had exclusive use of a used classic PDP-8 (built in the mid-1960s) for a couple of years that looked just like this one:

Hard to see because they’re white, but there is a row of switches along the bottom.

The front panel displayed the Program Counter, Memory Address, Memory Buffer, Accumulator, and Multiplier/Quotient registers. It was fun to watch it running.

I programmed it in assembly language using a Teletype ASR-35. Making changes was very slow, so if I had a bug I would sometimes “patch” the program using the front panel to modify the machine code in the core memory, making a note of the fix so I could update my source code later.

Since it used core memory which wasn’t erased when unpowered and I was the sole user, I could turn the computer off and come back the next day and resume with my patched program.

The very first computer that I bought myself, was a third-generation machine in 1971 that is considered the world’s first personal computer called the Kenbak-1. It was built using 7400 TTL logic, but did not use a microprocessor. It had a minimal front panel:

So only one register or bus could be displayed at a time, selected by incrementing one of the buttons on the right side.

When the first fourth-generation, microprocessor-based computers came out like the popular Altair 8800 (1974), they also sported a front panel:

However it was limited to displaying the contents of the address bus and data bus plus some status lights, because the registers were now internal to the 8080 chip and could not be displayed in real-time. One could still modify memory through the front panel, and indeed it was necessary to load a boot program there if one wanted, for example, to read in a program from external media.

I liked the instruction set of the Motorola 6800 much better than the Intel 8080 (most likely because the 6800 had a similar set of registers, two accumulators A and B and an index register X, to my earlier Kenbak). So I built a 6800-based Sphere-1 from a kit in 1975:

Yup! No front panel! AFAIK, it was the first computer with an integrated monitor and keyboard.

Instead of a front panel, it had a monitor program called the Programmer’s Development System (PDS), which one could use for debugging and would display the contents of the registers when the processor was stopped. No more real-time display though.

It also featured an editor in EPROM along with a mini-assembler. I didn’t like the editor very much, so I wrote my own and called it PIE for PDS Improved Editor. That eventually became a full-fledged word processor as a side project that I write about here.

As mentioned in that answer, my publisher gave me one of the first Apple ][ computers, and lo and behold, it didn’t have a front panel either:

Like the Sphere, it had a monitor program which would dump out the registers, along with the contents of memory. Here’s a part of a diagram from a later Apple IIgs manual showing the layout of part of its debugger:

Note the registers along the top. The lower case letters on the right “nvmxdizc” are the status bits.

So in general, no more blinkin’ lights. Kinda too bad. They were fun to watch. But now you can buy emulators of the early machines, with replicas of the front panels. Here’s a replica PDP 11/70 front panel, which you can buy for $255:

It is powered by a Raspberry Pi which you have to supply. It faithfully emulates a real PDP 11/70, which back in 1975 cost around $80,000 (equivalent to almost $400,000 today). Yes, it will run old copies of Unix just like the old days.

In my own work with microcontrollers, instead of a monitor program I use an Integrated Development System (IDE) running on a PC or laptop, which is connected to an EDBG (Embedded Debugger) on the development board via USB. The EDBG is actually a separate processor from the one running my code; they are connected by serial interface.

In the IDE, I can set breakpoints and single step through the generated assembly code if I wish and watch the registers change. Here is a sample display of the microcontroller’s registers (in this case, an AMR Cortex M0+):

No more simple program counter and a single accumulator like the PDP-8.

Frankly, I very very seldom have the need to do look at the CPU registers anymore. However I do use this interface to look at the contents of the hundreds of peripheral registers of the processor (ADC, CAN, DAC, DMA, I2C, GPIO, SPI, timers, USART, USB etc.). The CAN peripheral alone in this processor has 49 separate registers. A little too much to display on a front panel.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/skphd5jubm12a98r', 'title': 'What was the purpose of all of the blinking lights on the early computers (think Univac and Eniac) of the 1950s & 1960s? What information did they convey?', 'score': {'original': 0.80053333333333, 'ai': 0.19946666666667}, 'blocks': [{'text': 'The ENIAC didn’t really have a central console as such, it was a modular computer, taking up an entire room (as many first generation computers did). However unlike other machines, each of its ten accumulators were the size of large side by side refrigerators and had their own display — but there was no program counter in the original machine. Programming was done by wiring the boxes together in a particular order.\n\nWhat you’re probably referring to is the central console of the UNIVAC I (1951), a first generation machine using vacuum tubes, which was the first commercially successful computer in the US:\n\nThe main console displayed the status of each of the hardware registers in the machine, as well as the contents of various buses. Using a console like this, one could observe the running of a program, and if the computer “crashed” and halted, it would display information useful to the programmer. One could also load data into any of the registers using the switches.\n\nMost of these early computers had no operating system. Here’s a picture of a young Donald Knuth, famous for his The Art of Computer Programming series (which I had as textbooks when getting my MSCS degree) at the console of an IBM 650 (late-1950s), the first mass-produced computer in the world:\n\nEven later mainframe computers still had consoles, even though they now had operating systems and were running multiple programs at the same time. Here’s the main console of an IBM 360/65 (1965), a second generation (discrete transistor) computer:\n\nWhile in grad school, I had exclusive use of a used classic PDP-8 (built in the mid-1960s) for a couple of years that looked just like this one:\n\nHard to see because they’re white, but there is a row of switches along the bottom.\n\nThe front panel displayed the Program Counter, Memory Address, Memory Buffer, Accumulator, and Multiplier/Quotient registers. It was fun to watch it running.\n\nI programmed it in assembly language using a Teletype ASR-35. Making changes was very slow, so if I had a bug I would sometimes “patch” the program using the front panel to modify the machine code in the core memory, making a note of the fix so I could update my source code later.\n\nSince it used core memory which wasn’t erased when unpowered and I was the sole user, I could turn the computer off and come back the next day and resume with my patched program.\n\nThe very first computer that I bought myself, was a third-generation machine in 1971 that is considered the world’s first personal computer called the Kenbak-1. It was built using 7400 TTL logic, but did not use a microprocessor. It had a minimal front panel:\n\nSo only one register or bus could be displayed at a time, selected by incrementing one of the buttons on the right side.\n\nWhen the first fourth-generation, microprocessor-based computers came out like the popular Altair 8800 (1974), they also sported a front panel:\n\nHowever it was limited to displaying the contents of the address bus and data bus plus some status lights, because the registers were now internal to the 8080', 'result': {'fake': 0.0063, 'real': 0.9937}, 'status': 'success'}, {'text': 'chip and could not be displayed in real-time. One could still modify memory through the front panel, and indeed it was necessary to load a boot program there if one wanted, for example, to read in a program from external media.\n\nI liked the instruction set of the Motorola 6800 much better than the Intel 8080 (most likely because the 6800 had a similar set of registers, two accumulators A and B and an index register X, to my earlier Kenbak). So I built a 6800-based Sphere-1 from a kit in 1975:\n\nYup! No front panel! AFAIK, it was the first computer with an integrated monitor and keyboard.\n\nInstead of a front panel, it had a monitor program called the Programmer’s Development System (PDS), which one could use for debugging and would display the contents of the registers when the processor was stopped. No more real-time display though.\n\nIt also featured an editor in EPROM along with a mini-assembler. I didn’t like the editor very much, so I wrote my own and called it PIE for PDS Improved Editor. That eventually became a full-fledged word processor as a side project that I write about here.\n\nAs mentioned in that answer, my publisher gave me one of the first Apple ][ computers, and lo and behold, it didn’t have a front panel either:\n\nLike the Sphere, it had a monitor program which would dump out the registers, along with the contents of memory. Here’s a part of a diagram from a later Apple IIgs manual showing the layout of part of its debugger:\n\nNote the registers along the top. The lower case letters on the right “nvmxdizc” are the status bits.\n\nSo in general, no more blinkin’ lights. Kinda too bad. They were fun to watch. But now you can buy emulators of the early machines, with replicas of the front panels. Here’s a replica PDP 11/70 front panel, which you can buy for $255:\n\nIt is powered by a Raspberry Pi which you have to supply. It faithfully emulates a real PDP 11/70, which back in 1975 cost around $80,000 (equivalent to almost $400,000 today). Yes, it will run old copies of Unix just like the old days.\n\nIn my own work with microcontrollers, instead of a monitor program I use an Integrated Development System (IDE) running on a PC or laptop, which is connected to an EDBG (Embedded Debugger) on the development board via USB. The EDBG is actually a separate processor from the one running my code; they are connected by serial interface.\n\nIn the IDE, I can set breakpoints and single step through the generated assembly code if I wish and watch the registers change. Here is a sample display of the microcontroller’s registers (in this case, an AMR Cortex M0+):\n\nNo more simple program counter and a single accumulator like the PDP-8.\n\nFrankly, I very very seldom have the need to do look at the CPU registers anymore. However I do use this interface to look at the contents of the hundreds of peripheral registers of the processor (ADC, CAN, DAC, DMA, I2C, GPIO, SPI, timers, USART, USB etc.). The CAN', 'result': {'fake': 0.8094, 'real': 0.1906}, 'status': 'success'}, {'text': 'peripheral alone in this processor has 49 separate registers. A little too much to display on a front panel.', 'result': {'fake': 0.9984, 'real': 0.0016}, 'status': 'success'}], 'credits_used': 11, 'credits': 1995079, 'subscription': 0, 'content': 'The ENIAC didn’t really have a central console as such, it was a modular computer, taking up an entire room (as many first generation computers did). However unlike other machines, each of its ten accumulators were the size of large side by side refrigerators and had their own display — but there was no program counter in the original machine. Programming was done by wiring the boxes together in a particular order.\n\nWhat you’re probably referring to is the central console of the UNIVAC I (1951), a first generation machine using vacuum tubes, which was the first commercially successful computer in the US:\n\nThe main console displayed the status of each of the hardware registers in the machine, as well as the contents of various buses. Using a console like this, one could observe the running of a program, and if the computer “crashed” and halted, it would display information useful to the programmer. One could also load data into any of the registers using the switches.\n\nMost of these early computers had no operating system. Here’s a picture of a young Donald Knuth, famous for his The Art of Computer Programming series (which I had as textbooks when getting my MSCS degree) at the console of an IBM 650 (late-1950s), the first mass-produced computer in the world:\n\nEven later mainframe computers still had consoles, even though they now had operating systems and were running multiple programs at the same time. Here’s the main console of an IBM 360/65 (1965), a second generation (discrete transistor) computer:\n\nWhile in grad school, I had exclusive use of a used classic PDP-8 (built in the mid-1960s) for a couple of years that looked just like this one:\n\nHard to see because they’re white, but there is a row of switches along the bottom.\n\nThe front panel displayed the Program Counter, Memory Address, Memory Buffer, Accumulator, and Multiplier/Quotient registers. It was fun to watch it running.\n\nI programmed it in assembly language using a Teletype ASR-35. Making changes was very slow, so if I had a bug I would sometimes “patch” the program using the front panel to modify the machine code in the core memory, making a note of the fix so I could update my source code later.\n\nSince it used core memory which wasn’t erased when unpowered and I was the sole user, I could turn the computer off and come back the next day and resume with my patched program.\n\nThe very first computer that I bought myself, was a third-generation machine in 1971 that is considered the world’s first personal computer called the Kenbak-1. It was built using 7400 TTL logic, but did not use a microprocessor. It had a minimal front panel:\n\nSo only one register or bus could be displayed at a time, selected by incrementing one of the buttons on the right side.\n\nWhen the first fourth-generation, microprocessor-based computers came out like the popular Altair 8800 (1974), they also sported a front panel:\n\nHowever it was limited to displaying the contents of the address bus and data bus plus some status lights, because the registers were now internal to the 8080 chip and could not be displayed in real-time. One could still modify memory through the front panel, and indeed it was necessary to load a boot program there if one wanted, for example, to read in a program from external media.\n\nI liked the instruction set of the Motorola 6800 much better than the Intel 8080 (most likely because the 6800 had a similar set of registers, two accumulators A and B and an index register X, to my earlier Kenbak). So I built a 6800-based Sphere-1 from a kit in 1975:\n\nYup! No front panel! AFAIK, it was the first computer with an integrated monitor and keyboard.\n\nInstead of a front panel, it had a monitor program called the Programmer’s Development System (PDS), which one could use for debugging and would display the contents of the registers when the processor was stopped. No more real-time display though.\n\nIt also featured an editor in EPROM along with a mini-assembler. I didn’t like the editor very much, so I wrote my own and called it PIE for PDS Improved Editor. That eventually became a full-fledged word processor as a side project that I write about here.\n\nAs mentioned in that answer, my publisher gave me one of the first Apple ][ computers, and lo and behold, it didn’t have a front panel either:\n\nLike the Sphere, it had a monitor program which would dump out the registers, along with the contents of memory. Here’s a part of a diagram from a later Apple IIgs manual showing the layout of part of its debugger:\n\nNote the registers along the top. The lower case letters on the right “nvmxdizc” are the status bits.\n\nSo in general, no more blinkin’ lights. Kinda too bad. They were fun to watch. But now you can buy emulators of the early machines, with replicas of the front panels. Here’s a replica PDP 11/70 front panel, which you can buy for $255:\n\nIt is powered by a Raspberry Pi which you have to supply. It faithfully emulates a real PDP 11/70, which back in 1975 cost around $80,000 (equivalent to almost $400,000 today). Yes, it will run old copies of Unix just like the old days.\n\nIn my own work with microcontrollers, instead of a monitor program I use an Integrated Development System (IDE) running on a PC or laptop, which is connected to an EDBG (Embedded Debugger) on the development board via USB. The EDBG is actually a separate processor from the one running my code; they are connected by serial interface.\n\nIn the IDE, I can set breakpoints and single step through the generated assembly code if I wish and watch the registers change. Here is a sample display of the microcontroller’s registers (in this case, an AMR Cortex M0+):\n\nNo more simple program counter and a single accumulator like the PDP-8.\n\nFrankly, I very very seldom have the need to do look at the CPU registers anymore. However I do use this interface to look at the contents of the hundreds of peripheral registers of the processor (ADC, CAN, DAC, DMA, I2C, GPIO, SPI, timers, USART, USB etc.). The CAN peripheral alone in this processor has 49 separate registers. A little too much to display on a front panel.', 'aiModelVersion': '1'}",0.80053333333333
Dr. Balaji Viswanathan,Updated 9mo,"What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?","1. In Windows 95 and 98 you could hack the boot screen and shutdown screen and override with whatever bitmap you wanted. Back when I bought my first computer in 2000 I replaced most of Microsoft branding with bitmaps of my own branding. Thus, it will boot, shutdown, show screensaver all with my own bitmaps. At least one girl in college was impressed that I wrote my own operating system ;-)

2. In DOS, you could overwrite the graphics memory in a very easy and straightforward way. Thus, you could put your content anywhere in the screen. We would write all sorts of silly worms for this and distribute it as a TSR [Terminate and Stay Resident Program] in a floppy disk. When our friend opens up this exe in the floppy it will overtake his screen and whatever he types would be replaced with something funny.

Plenty of crazy stuff could be done through overwriting memory blocks. Mapping DOS Memory Allocation

We would screw our windows installations so badly with many of these botched programs that at one point, I used to format & reinstall my OS every week. Years later when I graduated, my first job was a developer in the core OS team at Microsoft Redmond. All my experimentation stopped ;-)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rcnspkmwhq4dabtv', 'title': 'What is a computer ""trick"" that in the past was cool, but today is unimpressive and trivial?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': '1. In Windows 95 and 98 you could hack the boot screen and shutdown screen and override with whatever bitmap you wanted. Back when I bought my first computer in 2000 I replaced most of Microsoft branding with bitmaps of my own branding. Thus, it will boot, shutdown, show screensaver all with my own bitmaps. At least one girl in college was impressed that I wrote my own operating system ;-)\n\n2. In DOS, you could overwrite the graphics memory in a very easy and straightforward way. Thus, you could put your content anywhere in the screen. We would write all sorts of silly worms for this and distribute it as a TSR [Terminate and Stay Resident Program] in a floppy disk. When our friend opens up this exe in the floppy it will overtake his screen and whatever he types would be replaced with something funny.\n\nPlenty of crazy stuff could be done through overwriting memory blocks. Mapping DOS Memory Allocation\n\nWe would screw our windows installations so badly with many of these botched programs that at one point, I used to format & reinstall my OS every week. Years later when I graduated, my first job was a developer in the core OS team at Microsoft Redmond. All my experimentation stopped ;-)', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1995076, 'subscription': 0, 'content': '1. In Windows 95 and 98 you could hack the boot screen and shutdown screen and override with whatever bitmap you wanted. Back when I bought my first computer in 2000 I replaced most of Microsoft branding with bitmaps of my own branding. Thus, it will boot, shutdown, show screensaver all with my own bitmaps. At least one girl in college was impressed that I wrote my own operating system ;-)\n\n2. In DOS, you could overwrite the graphics memory in a very easy and straightforward way. Thus, you could put your content anywhere in the screen. We would write all sorts of silly worms for this and distribute it as a TSR [Terminate and Stay Resident Program] in a floppy disk. When our friend opens up this exe in the floppy it will overtake his screen and whatever he types would be replaced with something funny.\n\nPlenty of crazy stuff could be done through overwriting memory blocks. Mapping DOS Memory Allocation\n\nWe would screw our windows installations so badly with many of these botched programs that at one point, I used to format & reinstall my OS every week. Years later when I graduated, my first job was a developer in the core OS team at Microsoft Redmond. All my experimentation stopped ;-)', 'aiModelVersion': '1'}",0.9998
Alan Kay,5y,What made APL programming so revolutionary?,"APL stands for “A Programming Language”, the title of the book in 1962 written by Ken Iverson about what initially was called “Iverson Notation”. Part of the reason for the “notation” label was that it was used extensively a number of years as “a notation” before it was implemented as “APL/360” (on the IBM 360 series of mainframes).

Ken Iverson was essentially a mathematician, but who also had a physics background, and trained under Howard Aiken at Harvard in close proximity with the various computers designed and built there, receiving his PhD in Applied Math, with a thesis on how to deal with very large sparse matrices.

He started to use mathematical tools to describe computations and computers, and soon found these to be lacking. This led to a number of inventions very much in the spirit of mathematics that allowed many more structures and operations to be easily defined and “notated”, many by “functional projection”.

One of the most interesting things about “Iverson Notation” at this stage was that not having an implementation greatly helped — IMO — what he tried to do at the descriptive level: there were no worries about whether this or that could be implemented at the time, or whether there would be enough computing capacity for speed or space to eventually implement the notation.

It was in this form that I and many of the other grad students of the mid-60s learned “Iverson”. My first CS course was from the legendary and wonderful Nick Metropolis, the main architect and builder of the Los Alamos computers, especially the “Maniac” series. Nick liked “Iverson”, and used it extensively for both hardware and software descriptions. A year or so later, Bob Barton in his notorious first course in “Systems Design”, required us to “get and read and learn Iverson’s book”.

To motivate what Ken Iverson decided to do, it is worth looking at the history of Maxwell’s Equations — 4 ideas (can be just 2 or 1) that will fit on a T-shirt. However, one of Maxwell’s main renditions was not in the form we are familiar with, but was expressed as 20 partial differential equations in x, y, z coordinates.

This is not a great T-shirt!

Helmholtz and especially Oliver Heaviside did a fair amount of work to use the definitional possibilities of mathematics to hide coordinate systems with vectors and details of the PDEs, with “operators” (Div, Curl, Gradient … “and all that”)
.

A terrific T-shirt!

You can think of the operators “gradient”: ∇, “divergence”: ∇•, “curl”: ∇×, as “meta”, that act a bit like macros to rewrite functions in a more complex “decorated form”.

The basic idea here is to get “whole ideas into one eyeful” by inventing notations and processes that can do this, and consequently requiring readers to learn the new notations fluently enough so there is a net benefit rather than just more noise.

When this is done well, the new “meta-stuff” becomes generally useful (like the grad, div, curl “and all that” above). An example in APL is the operator “.” , which is generalized inner product that can take any APL functions as arguments. For example, what we think of as “matrix multiplication” is +.* in APL (see inner product in APL)
.

People who learn math are quite willing to do this learning and gain the necessary fluency — but there’s considerable evidence that most computer folks are not at all willing to do lots of training in special tools that would make a difference in “being professional”.†

This has led to the idea that APL is not readable. In fact, it is both very readable and very compact. This is not to say that a face lift wouldn’t help — the standard notation for APL was derived to fit on an IBM Selectric golf ball typewriter terminal, and could be greatly improved today.

The second interesting idea in APL is “projection”. This is much more relatable today in an era of “map/reduce” than it was in the 60s or 70s, even though one could also write a good “mapping” function in Lisp (and it was also an “operator” because it could take a function as one of its arguments). In the early 70s, Unix happened, and Doug McIlroy invented “pipes programming” to allow in this case “data” to be passed through “functions” to be reformulated,

However, the big uses and extreme ranges of this way to program was explored earliest and most extensively in “Iverson Notation” and to a slightly less extent in the actual language “APL”.

Attaining fluency in APL as one of three or so main ways to think about programming “is good for one’s mind”. As in the later map/reduce, one “sends” a structure in parallel through a cascade of shaping functions and then a cascade of trimming and extracting functions to finally get a result. (One must suppress one’s imagination of just how big some of the intermediate structures might be getting … this is also good for one’s mind!)

There is real clarity to be gained for both writers and readers of APL.

A number of us in our research group at Parc liked APL quite a bit, and it was clear that much more could be done using polymorphic operations and the extension features of Smalltalk (only a few of these experiments emerged publically in the 80s). But, imagine gazillions of objects provided with “events, suggestions and hints”, etc.

As always, time has moved on (and programming language ideas move much slower — and programmers move almost not at all).

There are several modern APL-like languages today — such as J and K — but I would criticize them as being too much like the classic APL. It is possible to extract what is really great from APL and use it in new language designs without being so tied to the past. This would be a great project for some grad students of today: what does the APL-perspective mean today, and what kind of great programming language could be inspired by it? ††

† This seems rather like the disinclination of so many pop culture musicians to learn to read and write music notation, despite the tremendous advantages for doing so in many areas — and in fact what seems to be a disinclination in much of our culture for learning to fluently read and write the written form of their own language. It’s not that you can’t do art in “oral modes”, but that the possibilities for art are so expanded when literacy is added.

†† As an example, a looser more versatile version of this kind of programming can be done using dataflow between processes that themselves are made from projective mappings, and this could yield a very useful and beautiful language. This is what Dan Amelang and some of his colleagues did to make the Nile Language, which was especially aimed at “graphical mathematics and rendering”. In the STEPS project of some years ago, this allowed virtually all of 2.5D “personal computer” graphics — including rendering, compositing, filtering, curves, fills, masks, etc., to be defined and run in real-time in under 500 lines of code. This replaced an estimated 50,000 to 100,000 lines of C++. Because of the dataflow and the independence of the mappings, this was able to be set up so it could use as many cores as available to run the code. (And so forth.)

500 lines of code is only about 10 pages and it can be shown as an “eyeful” on a desktop screen:

This is partially low hanging fruit since mathematics does underlie computer graphics at all levels. The kinds of ideas that APL first brought to light allows “runnable mathematics” to be possible (and when it is possible, it is as wonderful as it gets!)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/t58xhi71gv6ymzks', 'title': 'What made APL programming so revolutionary?', 'score': {'original': 0.9507, 'ai': 0.0493}, 'blocks': [{'text': 'APL stands for “A Programming Language”, the title of the book in 1962 written by Ken Iverson about what initially was called “Iverson Notation”. Part of the reason for the “notation” label was that it was used extensively a number of years as “a notation” before it was implemented as “APL/360” (on the IBM 360 series of mainframes).\n\nKen Iverson was essentially a mathematician, but who also had a physics background, and trained under Howard Aiken at Harvard in close proximity with the various computers designed and built there, receiving his PhD in Applied Math, with a thesis on how to deal with very large sparse matrices.\n\nHe started to use mathematical tools to describe computations and computers, and soon found these to be lacking. This led to a number of inventions very much in the spirit of mathematics that allowed many more structures and operations to be easily defined and “notated”, many by “functional projection”.\n\nOne of the most interesting things about “Iverson Notation” at this stage was that not having an implementation greatly helped — IMO — what he tried to do at the descriptive level: there were no worries about whether this or that could be implemented at the time, or whether there would be enough computing capacity for speed or space to eventually implement the notation.\n\nIt was in this form that I and many of the other grad students of the mid-60s learned “Iverson”. My first CS course was from the legendary and wonderful Nick Metropolis, the main architect and builder of the Los Alamos computers, especially the “Maniac” series. Nick liked “Iverson”, and used it extensively for both hardware and software descriptions. A year or so later, Bob Barton in his notorious first course in “Systems Design”, required us to “get and read and learn Iverson’s book”.\n\nTo motivate what Ken Iverson decided to do, it is worth looking at the history of Maxwell’s Equations — 4 ideas (can be just 2 or 1) that will fit on a T-shirt. However, one of Maxwell’s main renditions was not in the form we are familiar with, but was expressed as 20 partial differential equations in x, y, z coordinates.\n\nThis is not a great T-shirt!\n\nHelmholtz and especially Oliver Heaviside did a fair amount of work to use the definitional possibilities of mathematics to hide coordinate systems with vectors and details of the PDEs, with “operators” (Div, Curl, Gradient … “and all that”)\n.\n\nA terrific T-shirt!\n\nYou can think of the operators “gradient”: ∇, “divergence”: ∇•, “curl”: ∇×, as “meta”, that act a bit like macros to rewrite functions in a more complex “decorated form”.\n\nThe basic idea here is to get “whole ideas into one eyeful” by inventing notations and processes that can do this, and consequently requiring readers to learn the new notations fluently enough so there is a net benefit rather than just more noise.\n\nWhen this is done well, the new “meta-stuff” becomes generally useful (like the grad, div, curl “and all that” above). An example in APL is the operator “.” , which is generalized inner product that can take any APL functions', 'result': {'fake': 0.0149, 'real': 0.9851}, 'status': 'success'}, {'text': 'as arguments. For example, what we think of as “matrix multiplication” is +.* in APL (see inner product in APL)\n.\n\nPeople who learn math are quite willing to do this learning and gain the necessary fluency — but there’s considerable evidence that most computer folks are not at all willing to do lots of training in special tools that would make a difference in “being professional”.†\n\nThis has led to the idea that APL is not readable. In fact, it is both very readable and very compact. This is not to say that a face lift wouldn’t help — the standard notation for APL was derived to fit on an IBM Selectric golf ball typewriter terminal, and could be greatly improved today.\n\nThe second interesting idea in APL is “projection”. This is much more relatable today in an era of “map/reduce” than it was in the 60s or 70s, even though one could also write a good “mapping” function in Lisp (and it was also an “operator” because it could take a function as one of its arguments). In the early 70s, Unix happened, and Doug McIlroy invented “pipes programming” to allow in this case “data” to be passed through “functions” to be reformulated,\n\nHowever, the big uses and extreme ranges of this way to program was explored earliest and most extensively in “Iverson Notation” and to a slightly less extent in the actual language “APL”.\n\nAttaining fluency in APL as one of three or so main ways to think about programming “is good for one’s mind”. As in the later map/reduce, one “sends” a structure in parallel through a cascade of shaping functions and then a cascade of trimming and extracting functions to finally get a result. (One must suppress one’s imagination of just how big some of the intermediate structures might be getting … this is also good for one’s mind!)\n\nThere is real clarity to be gained for both writers and readers of APL.\n\nA number of us in our research group at Parc liked APL quite a bit, and it was clear that much more could be done using polymorphic operations and the extension features of Smalltalk (only a few of these experiments emerged publically in the 80s). But, imagine gazillions of objects provided with “events, suggestions and hints”, etc.\n\nAs always, time has moved on (and programming language ideas move much slower — and programmers move almost not at all).\n\nThere are several modern APL-like languages today — such as J and K — but I would criticize them as being too much like the classic APL. It is possible to extract what is really great from APL and use it in new language designs without being so tied to the past. This would be a great project for some grad students of today: what does the APL-perspective mean today, and what kind of great programming language could be inspired by it? ††\n\n† This seems rather like the disinclination of so many pop culture musicians to learn to read and write music notation, despite the tremendous advantages for doing so in many areas — and in', 'result': {'fake': 0.0135, 'real': 0.9865}, 'status': 'success'}, {'text': 'fact what seems to be a disinclination in much of our culture for learning to fluently read and write the written form of their own language. It’s not that you can’t do art in “oral modes”, but that the possibilities for art are so expanded when literacy is added.\n\n†† As an example, a looser more versatile version of this kind of programming can be done using dataflow between processes that themselves are made from projective mappings, and this could yield a very useful and beautiful language. This is what Dan Amelang and some of his colleagues did to make the Nile Language, which was especially aimed at “graphical mathematics and rendering”. In the STEPS project of some years ago, this allowed virtually all of 2.5D “personal computer” graphics — including rendering, compositing, filtering, curves, fills, masks, etc., to be defined and run in real-time in under 500 lines of code. This replaced an estimated 50,000 to 100,000 lines of C++. Because of the dataflow and the independence of the mappings, this was able to be set up so it could use as many cores as available to run the code. (And so forth.)\n\n500 lines of code is only about 10 pages and it can be shown as an “eyeful” on a desktop screen:\n\nThis is partially low hanging fruit since mathematics does underlie computer graphics at all levels. The kinds of ideas that APL first brought to light allows “runnable mathematics” to be possible (and when it is possible, it is as wonderful as it gets!)', 'result': {'fake': 0.3379, 'real': 0.6621}, 'status': 'success'}], 'credits_used': 13, 'credits': 1995063, 'subscription': 0, 'content': 'APL stands for “A Programming Language”, the title of the book in 1962 written by Ken Iverson about what initially was called “Iverson Notation”. Part of the reason for the “notation” label was that it was used extensively a number of years as “a notation” before it was implemented as “APL/360” (on the IBM 360 series of mainframes).\n\nKen Iverson was essentially a mathematician, but who also had a physics background, and trained under Howard Aiken at Harvard in close proximity with the various computers designed and built there, receiving his PhD in Applied Math, with a thesis on how to deal with very large sparse matrices.\n\nHe started to use mathematical tools to describe computations and computers, and soon found these to be lacking. This led to a number of inventions very much in the spirit of mathematics that allowed many more structures and operations to be easily defined and “notated”, many by “functional projection”.\n\nOne of the most interesting things about “Iverson Notation” at this stage was that not having an implementation greatly helped — IMO — what he tried to do at the descriptive level: there were no worries about whether this or that could be implemented at the time, or whether there would be enough computing capacity for speed or space to eventually implement the notation.\n\nIt was in this form that I and many of the other grad students of the mid-60s learned “Iverson”. My first CS course was from the legendary and wonderful Nick Metropolis, the main architect and builder of the Los Alamos computers, especially the “Maniac” series. Nick liked “Iverson”, and used it extensively for both hardware and software descriptions. A year or so later, Bob Barton in his notorious first course in “Systems Design”, required us to “get and read and learn Iverson’s book”.\n\nTo motivate what Ken Iverson decided to do, it is worth looking at the history of Maxwell’s Equations — 4 ideas (can be just 2 or 1) that will fit on a T-shirt. However, one of Maxwell’s main renditions was not in the form we are familiar with, but was expressed as 20 partial differential equations in x, y, z coordinates.\n\nThis is not a great T-shirt!\n\nHelmholtz and especially Oliver Heaviside did a fair amount of work to use the definitional possibilities of mathematics to hide coordinate systems with vectors and details of the PDEs, with “operators” (Div, Curl, Gradient … “and all that”)\n.\n\nA terrific T-shirt!\n\nYou can think of the operators “gradient”: ∇, “divergence”: ∇•, “curl”: ∇×, as “meta”, that act a bit like macros to rewrite functions in a more complex “decorated form”.\n\nThe basic idea here is to get “whole ideas into one eyeful” by inventing notations and processes that can do this, and consequently requiring readers to learn the new notations fluently enough so there is a net benefit rather than just more noise.\n\nWhen this is done well, the new “meta-stuff” becomes generally useful (like the grad, div, curl “and all that” above). An example in APL is the operator “.” , which is generalized inner product that can take any APL functions as arguments. For example, what we think of as “matrix multiplication” is +.* in APL (see inner product in APL)\n.\n\nPeople who learn math are quite willing to do this learning and gain the necessary fluency — but there’s considerable evidence that most computer folks are not at all willing to do lots of training in special tools that would make a difference in “being professional”.†\n\nThis has led to the idea that APL is not readable. In fact, it is both very readable and very compact. This is not to say that a face lift wouldn’t help — the standard notation for APL was derived to fit on an IBM Selectric golf ball typewriter terminal, and could be greatly improved today.\n\nThe second interesting idea in APL is “projection”. This is much more relatable today in an era of “map/reduce” than it was in the 60s or 70s, even though one could also write a good “mapping” function in Lisp (and it was also an “operator” because it could take a function as one of its arguments). In the early 70s, Unix happened, and Doug McIlroy invented “pipes programming” to allow in this case “data” to be passed through “functions” to be reformulated,\n\nHowever, the big uses and extreme ranges of this way to program was explored earliest and most extensively in “Iverson Notation” and to a slightly less extent in the actual language “APL”.\n\nAttaining fluency in APL as one of three or so main ways to think about programming “is good for one’s mind”. As in the later map/reduce, one “sends” a structure in parallel through a cascade of shaping functions and then a cascade of trimming and extracting functions to finally get a result. (One must suppress one’s imagination of just how big some of the intermediate structures might be getting … this is also good for one’s mind!)\n\nThere is real clarity to be gained for both writers and readers of APL.\n\nA number of us in our research group at Parc liked APL quite a bit, and it was clear that much more could be done using polymorphic operations and the extension features of Smalltalk (only a few of these experiments emerged publically in the 80s). But, imagine gazillions of objects provided with “events, suggestions and hints”, etc.\n\nAs always, time has moved on (and programming language ideas move much slower — and programmers move almost not at all).\n\nThere are several modern APL-like languages today — such as J and K — but I would criticize them as being too much like the classic APL. It is possible to extract what is really great from APL and use it in new language designs without being so tied to the past. This would be a great project for some grad students of today: what does the APL-perspective mean today, and what kind of great programming language could be inspired by it? ††\n\n† This seems rather like the disinclination of so many pop culture musicians to learn to read and write music notation, despite the tremendous advantages for doing so in many areas — and in fact what seems to be a disinclination in much of our culture for learning to fluently read and write the written form of their own language. It’s not that you can’t do art in “oral modes”, but that the possibilities for art are so expanded when literacy is added.\n\n†† As an example, a looser more versatile version of this kind of programming can be done using dataflow between processes that themselves are made from projective mappings, and this could yield a very useful and beautiful language. This is what Dan Amelang and some of his colleagues did to make the Nile Language, which was especially aimed at “graphical mathematics and rendering”. In the STEPS project of some years ago, this allowed virtually all of 2.5D “personal computer” graphics — including rendering, compositing, filtering, curves, fills, masks, etc., to be defined and run in real-time in under 500 lines of code. This replaced an estimated 50,000 to 100,000 lines of C++. Because of the dataflow and the independence of the mappings, this was able to be set up so it could use as many cores as available to run the code. (And so forth.)\n\n500 lines of code is only about 10 pages and it can be shown as an “eyeful” on a desktop screen:\n\nThis is partially low hanging fruit since mathematics does underlie computer graphics at all levels. The kinds of ideas that APL first brought to light allows “runnable mathematics” to be possible (and when it is possible, it is as wonderful as it gets!)', 'aiModelVersion': '1'}",0.9507
Ciro Pabon,2y,What is the most significant tool that was invented in the early years of computing?,"Without a shadow of doubt, the compiler.

Few people realize that the fancy languages that we use to make computer programs have to be translated into actual machine code.

Without compilers there is nothing.

Every time you play COD, say thanks to Grace Hopper, admiral of the US Navy","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/adis7xkgh6qpluof', 'title': 'What is the most significant tool that was invented in the early years of computing?', 'score': {'original': 0.9372, 'ai': 0.0628}, 'blocks': [{'text': 'Without a shadow of doubt, the compiler.\n\nFew people realize that the fancy languages that we use to make computer programs have to be translated into actual machine code.\n\nWithout compilers there is nothing.\n\nEvery time you play COD, say thanks to Grace Hopper, admiral of the US Navy', 'result': {'fake': 0.0628, 'real': 0.9372}, 'status': 'success'}], 'credits_used': 1, 'credits': 1995062, 'subscription': 0, 'content': 'Without a shadow of doubt, the compiler.\n\nFew people realize that the fancy languages that we use to make computer programs have to be translated into actual machine code.\n\nWithout compilers there is nothing.\n\nEvery time you play COD, say thanks to Grace Hopper, admiral of the US Navy', 'aiModelVersion': '1'}",0.9372
Steve Baker,1y,Was the invention of computers inevitable?,"They were kindof invented three times.

Charles Babbage clearly invented a fully programmable mechanical computer - the “Analytical Engine” - but never got around to actually building one. A team called “Plan 5” are working towards building on for the London Science Museum…and they are of the view that his design would have worked.

After that though - his work was pretty much forgotten and there were no computers until Konrad Zuse built a series of working computers at the start of WWII in Germany.

But even his work was largely unknown when Alan Turing built his “code cracking” computer during WWII.

So clearly, the idea for what a computer can be has been independently conceived by at least THREE (and possibly four) people…each with little or no knowledge of the other.

That suggests to me that once we had modestly capable relays or thermionic valves or transistors - the invention would be inevitable.

Of course Babbage conceived of this using levers and gearwheels.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/j9gt8m4xw7pqu0co', 'title': 'Was the invention of computers inevitable?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'They were kindof invented three times.\n\nCharles Babbage clearly invented a fully programmable mechanical computer - the “Analytical Engine” - but never got around to actually building one. A team called “Plan 5” are working towards building on for the London Science Museum…and they are of the view that his design would have worked.\n\nAfter that though - his work was pretty much forgotten and there were no computers until Konrad Zuse built a series of working computers at the start of WWII in Germany.\n\nBut even his work was largely unknown when Alan Turing built his “code cracking” computer during WWII.\n\nSo clearly, the idea for what a computer can be has been independently conceived by at least THREE (and possibly four) people…each with little or no knowledge of the other.\n\nThat suggests to me that once we had modestly capable relays or thermionic valves or transistors - the invention would be inevitable.\n\nOf course Babbage conceived of this using levers and gearwheels.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 2, 'credits': 1995060, 'subscription': 0, 'content': 'They were kindof invented three times.\n\nCharles Babbage clearly invented a fully programmable mechanical computer - the “Analytical Engine” - but never got around to actually building one. A team called “Plan 5” are working towards building on for the London Science Museum…and they are of the view that his design would have worked.\n\nAfter that though - his work was pretty much forgotten and there were no computers until Konrad Zuse built a series of working computers at the start of WWII in Germany.\n\nBut even his work was largely unknown when Alan Turing built his “code cracking” computer during WWII.\n\nSo clearly, the idea for what a computer can be has been independently conceived by at least THREE (and possibly four) people…each with little or no knowledge of the other.\n\nThat suggests to me that once we had modestly capable relays or thermionic valves or transistors - the invention would be inevitable.\n\nOf course Babbage conceived of this using levers and gearwheels.', 'aiModelVersion': '1'}",0.9998
