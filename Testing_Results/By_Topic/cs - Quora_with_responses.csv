Authors,Time,Questions,Answers,api_response,originality_score
Thomas Cormen,3y,"As a computer science graduate, could you provide one practical application in your post education life of any coding problem like circular linked list, dining philosopher problem, etc.?","I live in semi-rural New Hampshire. Like many folks around here, I heat my home with wood during the colder months. I typically go through around three cords per year. (A cord is 128 cubic feet.) I buy “green” wood: wood that has not yet dried. It’s much less expensive than buying dried wood. I stack it and let it dry for two summers before I use it, so that it’s nice and dry by the time I burn it.

For several years, I would stack the wood in rows. One year, taking from the rows on the east side, and the next year taking from the rows on the west side. The problem was that I didn’t always get to the wood in the middle. I couldn’t just always stack from the west and take from the east, because eventually I’d run out of yard.

I needed a way to take the wood that had been drying the longest, but storing it in a bounded area.

In other words, a queue in bounded space.

Once I realized that was what I needed, it occurred to me that this is a standard computer science data structure.

So standard that it appears in Introduction to Algorithms.

A book that I co-authored.

And that is why for several years, I stacked my firewood as a queue in bounded space.

I stopped stacking that way after I built the World’s Greatest Woodshed: the Log Mahal. Here’s what the Log Mahal looks like, fully loaded (and with two of my three battery-powered snowblowers in summer storage).

It is currently holding seven cords. Yes, it has electricity. The lighting makes it so easy to load up the cart on a winter night. The vertical posts are 2x4s wedged against a beveled strip of plywood on the floor and attached at the top with velcro, so that they are easily installed and easily removed.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rxn86q73tl54uwd9', 'title': 'As a computer science graduate, could you provide one practical application in your post education life of any coding problem like circular linked list, dining philosopher problem, etc.?', 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': 'I live in semi-rural New Hampshire. Like many folks around here, I heat my home with wood during the colder months. I typically go through around three cords per year. (A cord is 128 cubic feet.) I buy “green” wood: wood that has not yet dried. It’s much less expensive than buying dried wood. I stack it and let it dry for two summers before I use it, so that it’s nice and dry by the time I burn it.\n\nFor several years, I would stack the wood in rows. One year, taking from the rows on the east side, and the next year taking from the rows on the west side. The problem was that I didn’t always get to the wood in the middle. I couldn’t just always stack from the west and take from the east, because eventually I’d run out of yard.\n\nI needed a way to take the wood that had been drying the longest, but storing it in a bounded area.\n\nIn other words, a queue in bounded space.\n\nOnce I realized that was what I needed, it occurred to me that this is a standard computer science data structure.\n\nSo standard that it appears in Introduction to Algorithms.\n\nA book that I co-authored.\n\nAnd that is why for several years, I stacked my firewood as a queue in bounded space.\n\nI stopped stacking that way after I built the World’s Greatest Woodshed: the Log Mahal. Here’s what the Log Mahal looks like, fully loaded (and with two of my three battery-powered snowblowers in summer storage).\n\nIt is currently holding seven cords. Yes, it has electricity. The lighting makes it so easy to load up the cart on a winter night. The vertical posts are 2x4s wedged against a beveled strip of plywood on the floor and attached at the top with velcro, so that they are easily installed and easily removed.', 'result': {'fake': 0.0007, 'real': 0.9993}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986201, 'subscription': 0, 'content': 'I live in semi-rural New Hampshire. Like many folks around here, I heat my home with wood during the colder months. I typically go through around three cords per year. (A cord is 128 cubic feet.) I buy “green” wood: wood that has not yet dried. It’s much less expensive than buying dried wood. I stack it and let it dry for two summers before I use it, so that it’s nice and dry by the time I burn it.\n\nFor several years, I would stack the wood in rows. One year, taking from the rows on the east side, and the next year taking from the rows on the west side. The problem was that I didn’t always get to the wood in the middle. I couldn’t just always stack from the west and take from the east, because eventually I’d run out of yard.\n\nI needed a way to take the wood that had been drying the longest, but storing it in a bounded area.\n\nIn other words, a queue in bounded space.\n\nOnce I realized that was what I needed, it occurred to me that this is a standard computer science data structure.\n\nSo standard that it appears in Introduction to Algorithms.\n\nA book that I co-authored.\n\nAnd that is why for several years, I stacked my firewood as a queue in bounded space.\n\nI stopped stacking that way after I built the World’s Greatest Woodshed: the Log Mahal. Here’s what the Log Mahal looks like, fully loaded (and with two of my three battery-powered snowblowers in summer storage).\n\nIt is currently holding seven cords. Yes, it has electricity. The lighting makes it so easy to load up the cart on a winter night. The vertical posts are 2x4s wedged against a beveled strip of plywood on the floor and attached at the top with velcro, so that they are easily installed and easily removed.', 'aiModelVersion': '1'}",0.9993
Matthew Bates,2y,Are you worried about the possible effects of artificial intelligence?,"Not so much for me, since I plan on not being around when the worst of it comes to pass, but for my kids and my students. By “worst of it,” I mean the lack of low-skill or even medium-skill career paths.

I often think about what jobs could be performed by a machine, if only we had slightly more intelligent machines. Driving will be the next big shoe to drop, once automated driving is perfected. Food and beverage prep will also be largely automated in my lifetime.

Currently, two of the best career paths for non-college graduates are delivery driver and barista. Those doors are closing rapidly, and none are opening to take their place.

Here is a painful truth that I’ve seen firsthand as a teacher: not every student is college material. In fact, I’d argue that the majority of students aren’t college material. The majority of students aren’t trade school material, either. There is a nontrivial percentage of people in the country whose only job skill is the ability to (usually) show up to work on time and follow some basic directions. And machines are very good at taking those types of jobs.

Think about this: Starbucks has almost 350,000 employees. Around here, they start at just under $16 an hour. It’s a great way for reliable, friendly people to make some money. The average Starbucks has around 85 employees.

Then this comes along:

A fully-automated Starbucks kiosk. Imagine if Starbucks could get that 85-employees-per-site average down to just five or so, just to make sure the machine was running properly. Starbucks’ owners (I am one… I have a dozen shares) would be thrilled! So would a lot of customers, who’d rather not talk to human anyway.

But that would be one less career path for the reliable, motivated, friendly, but otherwise unskilled crowd.

Improvements in A.I. are just going to make inequalities worse. The bell curve is turning into an “M” curve, and the people on the left hump of the “M” won’t sit back and accept their fate easily. Nor should they. There needs to be viable paths into the middle and upper classes for people of all ability levels. You should be able to work your way out of poverty, but A.I. makes that more difficult with every passing advancement.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/cqnwrhj0mld7fotb', 'title': 'Are you worried about the possible effects of artificial intelligence?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'Not so much for me, since I plan on not being around when the worst of it comes to pass, but for my kids and my students. By “worst of it,” I mean the lack of low-skill or even medium-skill career paths.\n\nI often think about what jobs could be performed by a machine, if only we had slightly more intelligent machines. Driving will be the next big shoe to drop, once automated driving is perfected. Food and beverage prep will also be largely automated in my lifetime.\n\nCurrently, two of the best career paths for non-college graduates are delivery driver and barista. Those doors are closing rapidly, and none are opening to take their place.\n\nHere is a painful truth that I’ve seen firsthand as a teacher: not every student is college material. In fact, I’d argue that the majority of students aren’t college material. The majority of students aren’t trade school material, either. There is a nontrivial percentage of people in the country whose only job skill is the ability to (usually) show up to work on time and follow some basic directions. And machines are very good at taking those types of jobs.\n\nThink about this: Starbucks has almost 350,000 employees. Around here, they start at just under $16 an hour. It’s a great way for reliable, friendly people to make some money. The average Starbucks has around 85 employees.\n\nThen this comes along:\n\nA fully-automated Starbucks kiosk. Imagine if Starbucks could get that 85-employees-per-site average down to just five or so, just to make sure the machine was running properly. Starbucks’ owners (I am one… I have a dozen shares) would be thrilled! So would a lot of customers, who’d rather not talk to human anyway.\n\nBut that would be one less career path for the reliable, motivated, friendly, but otherwise unskilled crowd.\n\nImprovements in A.I. are just going to make inequalities worse. The bell curve is turning into an “M” curve, and the people on the left hump of the “M” won’t sit back and accept their fate easily. Nor should they. There needs to be viable paths into the middle and upper classes for people of all ability levels. You should be able to work your way out of poverty, but A.I. makes that more difficult with every passing advancement.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986197, 'subscription': 0, 'content': 'Not so much for me, since I plan on not being around when the worst of it comes to pass, but for my kids and my students. By “worst of it,” I mean the lack of low-skill or even medium-skill career paths.\n\nI often think about what jobs could be performed by a machine, if only we had slightly more intelligent machines. Driving will be the next big shoe to drop, once automated driving is perfected. Food and beverage prep will also be largely automated in my lifetime.\n\nCurrently, two of the best career paths for non-college graduates are delivery driver and barista. Those doors are closing rapidly, and none are opening to take their place.\n\nHere is a painful truth that I’ve seen firsthand as a teacher: not every student is college material. In fact, I’d argue that the majority of students aren’t college material. The majority of students aren’t trade school material, either. There is a nontrivial percentage of people in the country whose only job skill is the ability to (usually) show up to work on time and follow some basic directions. And machines are very good at taking those types of jobs.\n\nThink about this: Starbucks has almost 350,000 employees. Around here, they start at just under $16 an hour. It’s a great way for reliable, friendly people to make some money. The average Starbucks has around 85 employees.\n\nThen this comes along:\n\nA fully-automated Starbucks kiosk. Imagine if Starbucks could get that 85-employees-per-site average down to just five or so, just to make sure the machine was running properly. Starbucks’ owners (I am one… I have a dozen shares) would be thrilled! So would a lot of customers, who’d rather not talk to human anyway.\n\nBut that would be one less career path for the reliable, motivated, friendly, but otherwise unskilled crowd.\n\nImprovements in A.I. are just going to make inequalities worse. The bell curve is turning into an “M” curve, and the people on the left hump of the “M” won’t sit back and accept their fate easily. Nor should they. There needs to be viable paths into the middle and upper classes for people of all ability levels. You should be able to work your way out of poverty, but A.I. makes that more difficult with every passing advancement.', 'aiModelVersion': '1'}",0.9997
Alon Amit,3y,"In what sense is it said that Conway's Game of Life exhibits computation universality? Is it possible to use it directly to solve a real problem, such as computing the sum of 1 + 1? Is there a computable procedure for translating a problem into GoL?","It is possible to represent bits of memory, basic logic and states in a suitable Game of Life configuration. Yes, there’s a computable procedure to translate any Turing Machine (in other words, any computer program) to a Life configuration which would evolve in the the same way as the program would get run.

“Is it possible to use it directly to solve a real problem such as computing the sum 1+1”: computing the sum 1+1 is not a real problem. A program that merely prints “2” solves that problem. A real problem is “given the binary representation of two natural numbers, produce the binary representation of their sum”.

Yes, such a program can be implemented as a Game of Life configuration. Yes, it would output “10” (the binary representation of 2) given the input “1,1”. No, this isn’t recommended to actually carry out: it would be terribly slow. But, in principle, it is perfectly doable.

This is a sample Life configuration implementing a simpler program which takes a string of 1’s and doubles its length. You can think of it as “doubling in unary notation”. The details of its operation are in this paper
 by Paul Rendell.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/v2dife1plwkz9s0u', 'title': ""In what sense is it said that Conway's Game of Life exhibits computation universality? Is it possible to use it directly to solve a real problem, such as computing the sum of 1 + 1? Is there a computable procedure for translating a problem into GoL?"", 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'It is possible to represent bits of memory, basic logic and states in a suitable Game of Life configuration. Yes, there’s a computable procedure to translate any Turing Machine (in other words, any computer program) to a Life configuration which would evolve in the the same way as the program would get run.\n\n“Is it possible to use it directly to solve a real problem such as computing the sum 1+1”: computing the sum 1+1 is not a real problem. A program that merely prints “2” solves that problem. A real problem is “given the binary representation of two natural numbers, produce the binary representation of their sum”.\n\nYes, such a program can be implemented as a Game of Life configuration. Yes, it would output “10” (the binary representation of 2) given the input “1,1”. No, this isn’t recommended to actually carry out: it would be terribly slow. But, in principle, it is perfectly doable.\n\nThis is a sample Life configuration implementing a simpler program which takes a string of 1’s and doubles its length. You can think of it as “doubling in unary notation”. The details of its operation are in this paper\n by Paul Rendell.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986195, 'subscription': 0, 'content': 'It is possible to represent bits of memory, basic logic and states in a suitable Game of Life configuration. Yes, there’s a computable procedure to translate any Turing Machine (in other words, any computer program) to a Life configuration which would evolve in the the same way as the program would get run.\n\n“Is it possible to use it directly to solve a real problem such as computing the sum 1+1”: computing the sum 1+1 is not a real problem. A program that merely prints “2” solves that problem. A real problem is “given the binary representation of two natural numbers, produce the binary representation of their sum”.\n\nYes, such a program can be implemented as a Game of Life configuration. Yes, it would output “10” (the binary representation of 2) given the input “1,1”. No, this isn’t recommended to actually carry out: it would be terribly slow. But, in principle, it is perfectly doable.\n\nThis is a sample Life configuration implementing a simpler program which takes a string of 1’s and doubles its length. You can think of it as “doubling in unary notation”. The details of its operation are in this paper\n by Paul Rendell.', 'aiModelVersion': '1'}",0.9996
Imtiaz Mohammad,4y,How can you set yourself apart when everyone is doing machine learning or data science in 2019?,"Great question.

There are some excellent videos on YouTube by 3Blue1Brown on Machine Learning and other areas. Study the four images below and see if you observe something.

There are 4 chapters dedicated to Deep Learning. At this point

First chapter has 4M views
Second chapter has 1.5M views
Third chapter has 1M views
Fourth chapter has 0.6M views

Clearly, as the content got deeper into algorithms (chapters 2 & 3) and mathematics (chapter 4), most viewers simply disappeared.

Most students of hard sciences are like that. They scratch the surface and move on. They offer

no competition to the small set of dedicated students.
no value to the employers.

If you want to stand apart, have the perseverance to drill deeper into the hard math that enables Machine Learning.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/zb8iynflu9ervx6q', 'title': 'How can you set yourself apart when everyone is doing machine learning or data science in 2019?', 'score': {'original': 0.9976, 'ai': 0.0024}, 'blocks': [{'text': 'Great question.\n\nThere are some excellent videos on YouTube by 3Blue1Brown on Machine Learning and other areas. Study the four images below and see if you observe something.\n\nThere are 4 chapters dedicated to Deep Learning. At this point\n\nFirst chapter has 4M views\nSecond chapter has 1.5M views\nThird chapter has 1M views\nFourth chapter has 0.6M views\n\nClearly, as the content got deeper into algorithms (chapters 2 & 3) and mathematics (chapter 4), most viewers simply disappeared.\n\nMost students of hard sciences are like that. They scratch the surface and move on. They offer\n\nno competition to the small set of dedicated students.\nno value to the employers.\n\nIf you want to stand apart, have the perseverance to drill deeper into the hard math that enables Machine Learning.', 'result': {'fake': 0.0022, 'real': 0.9978}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986193, 'subscription': 0, 'content': 'Great question.\n\nThere are some excellent videos on YouTube by 3Blue1Brown on Machine Learning and other areas. Study the four images below and see if you observe something.\n\nThere are 4 chapters dedicated to Deep Learning. At this point\n\nFirst chapter has 4M views\nSecond chapter has 1.5M views\nThird chapter has 1M views\nFourth chapter has 0.6M views\n\nClearly, as the content got deeper into algorithms (chapters 2 & 3) and mathematics (chapter 4), most viewers simply disappeared.\n\nMost students of hard sciences are like that. They scratch the surface and move on. They offer\n\nno competition to the small set of dedicated students.\nno value to the employers.\n\nIf you want to stand apart, have the perseverance to drill deeper into the hard math that enables Machine Learning.', 'aiModelVersion': '1'}",0.9976
Dr Jo,4y,Why can't we create a computer system (or device) which cannot be hacked?,"Some good answers, but as nobody has done this so far, let’s trot out the answer:

There is almost always a “side-channel” attack against security. In his much-maligned book on cryptography, Bruce Schneier describes a lot of modern security as driving a stake into the ground and hoping that the attacker will walk into it. This is embarrassingly close to the truth.

‘Experts’ are actually quite bad at seeing the ‘obvious’ attack, because they are so focused on the technical details of their solution. Intruders have the luxury of a more broad perspective—and a criminal mind that gets over little moral issues like crushed fingers.

There is a close parallel between achieving ‘good’ security and the problem of finding ‘truth’ in Science. Good science consists of assertions that we’ve tested to the best of our ability and not yet shown to be wrong. Secure computer systems are just those that have been quite well tested—and not yet hacked.

My 2c, Dr Jo.

xkcd: Security","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/b6ywqgi5cf9vj2xp', 'title': ""Why can't we create a computer system (or device) which cannot be hacked?"", 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': 'Some good answers, but as nobody has done this so far, let’s trot out the answer:\n\nThere is almost always a “side-channel” attack against security. In his much-maligned book on cryptography, Bruce Schneier describes a lot of modern security as driving a stake into the ground and hoping that the attacker will walk into it. This is embarrassingly close to the truth.\n\n‘Experts’ are actually quite bad at seeing the ‘obvious’ attack, because they are so focused on the technical details of their solution. Intruders have the luxury of a more broad perspective—and a criminal mind that gets over little moral issues like crushed fingers.\n\nThere is a close parallel between achieving ‘good’ security and the problem of finding ‘truth’ in Science. Good science consists of assertions that we’ve tested to the best of our ability and not yet shown to be wrong. Secure computer systems are just those that have been quite well tested—and not yet hacked.\n\nMy 2c, Dr Jo.\n\nxkcd: Security', 'result': {'fake': 0.0007, 'real': 0.9993}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986191, 'subscription': 0, 'content': 'Some good answers, but as nobody has done this so far, let’s trot out the answer:\n\nThere is almost always a “side-channel” attack against security. In his much-maligned book on cryptography, Bruce Schneier describes a lot of modern security as driving a stake into the ground and hoping that the attacker will walk into it. This is embarrassingly close to the truth.\n\n‘Experts’ are actually quite bad at seeing the ‘obvious’ attack, because they are so focused on the technical details of their solution. Intruders have the luxury of a more broad perspective—and a criminal mind that gets over little moral issues like crushed fingers.\n\nThere is a close parallel between achieving ‘good’ security and the problem of finding ‘truth’ in Science. Good science consists of assertions that we’ve tested to the best of our ability and not yet shown to be wrong. Secure computer systems are just those that have been quite well tested—and not yet hacked.\n\nMy 2c, Dr Jo.\n\nxkcd: Security', 'aiModelVersion': '1'}",0.9993
Joe Zbiciak,4y,"Why is quicksort more common than heapsort, even though they have equal time complexities?","Benchmark them! I’m serious, benchmark them!

Write yourself up a Heapsort and a Quicksort. Copy the classic versions of each out of your textbook.

Then measure how long each takes to sort a random array of N elements for several values of N. To save time, double N each time. Be sure to let N get fairly large—say, at least 
2
24
224
, or maybe 
2
28
228
 if you’re up for it.

And, just to be sure, have each sort several randomized arrays of each size. After all, even with randomized arrays, you might get a “lucky” input.

You’ll find that both grow at 
O
(
N
lg
N
)
O(Nlg⁡N)
. I think you’ll also find that, pretty consistently, Quicksort is faster.

I did this experiment waaay back in 1994 when I took my data structures and algorithms class, and sure enough, Quicksort was noticeably faster, even back then, on much simpler hardware. I haven’t run the experiment recently, but I suspect on modern machines the effect is more pronounced.

The big-Oh time complexity measure isn’t everything!

Why is that? The time complexity measure ignores constant factors.

Spend some time meditating on why that might be.

Hint: What’s your data locality look like, and what do your inner and outer loops look like for each?

You’ll also notice that I said “use randomized arrays” with your textbook Quicksort. You might consult your textbook for why I suggested that.

In practice, the pure, classic Quicksort isn’t all that common either. Usually, they doll it up with various improvements, such as:

Randomized pivot selection.
Select pivot from a median of multiple potential pivots.
Hybrid sorts that use selection, merge, insertion, or even heap sort once the partition size gets below a certain threshold.

That’s if they use a Quicksort variant at all. Timsort
 is pretty popular these days, I hear.

(And, to be fair, there’s modified and improved versions of Heapsort as well. Real world modern practical implementations don’t tend to use the original classic textbook algorithms from the 1950s and 1960s directly. We’ve learned a lot since then.)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/67wl93mu8q5doyhe', 'title': 'Why is quicksort more common than heapsort, even though they have equal time complexities?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'Benchmark them! I’m serious, benchmark them!\n\nWrite yourself up a Heapsort and a Quicksort. Copy the classic versions of each out of your textbook.\n\nThen measure how long each takes to sort a random array of N elements for several values of N. To save time, double N each time. Be sure to let N get fairly large—say, at least \n2\n24\n224\n, or maybe \n2\n28\n228\n if you’re up for it.\n\nAnd, just to be sure, have each sort several randomized arrays of each size. After all, even with randomized arrays, you might get a “lucky” input.\n\nYou’ll find that both grow at \nO\n(\nN\nlg\nN\n)\nO(Nlg\u2061N)\n. I think you’ll also find that, pretty consistently, Quicksort is faster.\n\nI did this experiment waaay back in 1994 when I took my data structures and algorithms class, and sure enough, Quicksort was noticeably faster, even back then, on much simpler hardware. I haven’t run the experiment recently, but I suspect on modern machines the effect is more pronounced.\n\nThe big-Oh time complexity measure isn’t everything!\n\nWhy is that? The time complexity measure ignores constant factors.\n\nSpend some time meditating on why that might be.\n\nHint: What’s your data locality look like, and what do your inner and outer loops look like for each?\n\nYou’ll also notice that I said “use randomized arrays” with your textbook Quicksort. You might consult your textbook for why I suggested that.\n\nIn practice, the pure, classic Quicksort isn’t all that common either. Usually, they doll it up with various improvements, such as:\n\nRandomized pivot selection.\nSelect pivot from a median of multiple potential pivots.\nHybrid sorts that use selection, merge, insertion, or even heap sort once the partition size gets below a certain threshold.\n\nThat’s if they use a Quicksort variant at all. Timsort\n is pretty popular these days, I hear.\n\n(And, to be fair, there’s modified and improved versions of Heapsort as well. Real world modern practical implementations don’t tend to use the original classic textbook algorithms from the 1950s and 1960s directly. We’ve learned a lot since then.)', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986187, 'subscription': 0, 'content': 'Benchmark them! I’m serious, benchmark them!\n\nWrite yourself up a Heapsort and a Quicksort. Copy the classic versions of each out of your textbook.\n\nThen measure how long each takes to sort a random array of N elements for several values of N. To save time, double N each time. Be sure to let N get fairly large—say, at least \n2\n24\n224\n, or maybe \n2\n28\n228\n if you’re up for it.\n\nAnd, just to be sure, have each sort several randomized arrays of each size. After all, even with randomized arrays, you might get a “lucky” input.\n\nYou’ll find that both grow at \nO\n(\nN\nlg\nN\n)\nO(Nlg\u2061N)\n. I think you’ll also find that, pretty consistently, Quicksort is faster.\n\nI did this experiment waaay back in 1994 when I took my data structures and algorithms class, and sure enough, Quicksort was noticeably faster, even back then, on much simpler hardware. I haven’t run the experiment recently, but I suspect on modern machines the effect is more pronounced.\n\nThe big-Oh time complexity measure isn’t everything!\n\nWhy is that? The time complexity measure ignores constant factors.\n\nSpend some time meditating on why that might be.\n\nHint: What’s your data locality look like, and what do your inner and outer loops look like for each?\n\nYou’ll also notice that I said “use randomized arrays” with your textbook Quicksort. You might consult your textbook for why I suggested that.\n\nIn practice, the pure, classic Quicksort isn’t all that common either. Usually, they doll it up with various improvements, such as:\n\nRandomized pivot selection.\nSelect pivot from a median of multiple potential pivots.\nHybrid sorts that use selection, merge, insertion, or even heap sort once the partition size gets below a certain threshold.\n\nThat’s if they use a Quicksort variant at all. Timsort\n is pretty popular these days, I hear.\n\n(And, to be fair, there’s modified and improved versions of Heapsort as well. Real world modern practical implementations don’t tend to use the original classic textbook algorithms from the 1950s and 1960s directly. We’ve learned a lot since then.)', 'aiModelVersion': '1'}",0.9999
Nikhil Jadhav,Updated 6y,Does India have computers?,"Unfortunately no. We have-

A very efficient Mars Orbiter
A technologically advanced Rocket launch center.
64 Domestic, 22 International, 32 Defense and 16 private airports
3rd largest military in the world with 1,325,000 active personnel.
IT Services is the fastest growing segment within the Indian IT-ITES sector. India’s IT service exports generated a revenue of US$ 66.0 billion in year 2016-17. (Please note that this picture was not clicked in India as we don’t have any computers. Credit- Avaneesh Shetye)
An economy that is the sixth-largest
 in the world measured by nominal GDP
 and the third-largest
 by purchasing power parity
 (PPP).

We have all the things mentioned here and many more. But we don’t have a damned computer.

Edit-

We also have ‘Param- India’s very own indigenous supercomputer.’

But since this is not the type of computer you were looking for, it doesn’t count.

So the answer still remains NO.

Thanks for the edit User-9654800801082532045.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/csnyu38b7jrv1wqo', 'title': 'Does India have computers?', 'score': {'original': 0.9978, 'ai': 0.0022}, 'blocks': [{'text': 'Unfortunately no. We have-\n\nA very efficient Mars Orbiter\nA technologically advanced Rocket launch center.\n64 Domestic, 22 International, 32 Defense and 16 private airports\n3rd largest military in the world with 1,325,000 active personnel.\nIT Services is the fastest growing segment within the Indian IT-ITES sector. India’s IT service exports generated a revenue of US$ 66.0 billion in year 2016-17. (Please note that this picture was not clicked in India as we don’t have any computers. Credit- Avaneesh Shetye)\nAn economy that is the sixth-largest\n in the world measured by nominal GDP\n and the third-largest\n by purchasing power parity\n (PPP).\n\nWe have all the things mentioned here and many more. But we don’t have a damned computer.\n\nEdit-\n\nWe also have ‘Param- India’s very own indigenous supercomputer.’\n\nBut since this is not the type of computer you were looking for, it doesn’t count.\n\nSo the answer still remains NO.\n\nThanks for the edit User-9654800801082532045.', 'result': {'fake': 0.0022, 'real': 0.9978}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986185, 'subscription': 0, 'content': 'Unfortunately no. We have-\n\nA very efficient Mars Orbiter\nA technologically advanced Rocket launch center.\n64 Domestic, 22 International, 32 Defense and 16 private airports\n3rd largest military in the world with 1,325,000 active personnel.\nIT Services is the fastest growing segment within the Indian IT-ITES sector. India’s IT service exports generated a revenue of US$ 66.0 billion in year 2016-17. (Please note that this picture was not clicked in India as we don’t have any computers. Credit- Avaneesh Shetye)\nAn economy that is the sixth-largest\n in the world measured by nominal GDP\n and the third-largest\n by purchasing power parity\n (PPP).\n\nWe have all the things mentioned here and many more. But we don’t have a damned computer.\n\nEdit-\n\nWe also have ‘Param- India’s very own indigenous supercomputer.’\n\nBut since this is not the type of computer you were looking for, it doesn’t count.\n\nSo the answer still remains NO.\n\nThanks for the edit User-9654800801082532045.', 'aiModelVersion': '1'}",0.9978
Steve Baker,Updated 5y,What is an issue that computer science is facing?,"The end of Moore’s law is a good one.

Almost since the dawn of computing, we’ve had this idea (called “Moore’s Law”) that the number of transistors on a chip will double every 18 months.

However, we all know that there must be a limit to this. There is no way to build a transistor that is smaller than (say) an atom.

But as of today - the “law” has held pretty solid.

However, there are signs of it slowing down - and Intel claims that it’ll end sometime around 2029.

When that happens, computers will no longer get cheaper and faster as time goes on.

This will be a weird thing to get used to.

We’ve all grown to expect this steady growth - and without it, our industry will become a very different place.

For example - will cellphone manufacturers continue to persuade people to replace their phones every few years if the replacements are not better or cheaper than they were before?

The likely answer is that software will have to get better and smarter - and more of the calculations will have to be done by remote servers. We kinda see that trend happening already - but it’s going to have to go much further.

I’d also expect to see some architectural changes in computer design - things like more emphasis on processors like GPU’s that are massively parallel collections of relatively simple machines.

It’s going to be an interesting time to live through.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ykpaozinfe61du0j', 'title': 'What is an issue that computer science is facing?', 'score': {'original': 0.9925, 'ai': 0.0075}, 'blocks': [{'text': 'The end of Moore’s law is a good one.\n\nAlmost since the dawn of computing, we’ve had this idea (called “Moore’s Law”) that the number of transistors on a chip will double every 18 months.\n\nHowever, we all know that there must be a limit to this. There is no way to build a transistor that is smaller than (say) an atom.\n\nBut as of today - the “law” has held pretty solid.\n\nHowever, there are signs of it slowing down - and Intel claims that it’ll end sometime around 2029.\n\nWhen that happens, computers will no longer get cheaper and faster as time goes on.\n\nThis will be a weird thing to get used to.\n\nWe’ve all grown to expect this steady growth - and without it, our industry will become a very different place.\n\nFor example - will cellphone manufacturers continue to persuade people to replace their phones every few years if the replacements are not better or cheaper than they were before?\n\nThe likely answer is that software will have to get better and smarter - and more of the calculations will have to be done by remote servers. We kinda see that trend happening already - but it’s going to have to go much further.\n\nI’d also expect to see some architectural changes in computer design - things like more emphasis on processors like GPU’s that are massively parallel collections of relatively simple machines.\n\nIt’s going to be an interesting time to live through.', 'result': {'fake': 0.0075, 'real': 0.9925}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986182, 'subscription': 0, 'content': 'The end of Moore’s law is a good one.\n\nAlmost since the dawn of computing, we’ve had this idea (called “Moore’s Law”) that the number of transistors on a chip will double every 18 months.\n\nHowever, we all know that there must be a limit to this. There is no way to build a transistor that is smaller than (say) an atom.\n\nBut as of today - the “law” has held pretty solid.\n\nHowever, there are signs of it slowing down - and Intel claims that it’ll end sometime around 2029.\n\nWhen that happens, computers will no longer get cheaper and faster as time goes on.\n\nThis will be a weird thing to get used to.\n\nWe’ve all grown to expect this steady growth - and without it, our industry will become a very different place.\n\nFor example - will cellphone manufacturers continue to persuade people to replace their phones every few years if the replacements are not better or cheaper than they were before?\n\nThe likely answer is that software will have to get better and smarter - and more of the calculations will have to be done by remote servers. We kinda see that trend happening already - but it’s going to have to go much further.\n\nI’d also expect to see some architectural changes in computer design - things like more emphasis on processors like GPU’s that are massively parallel collections of relatively simple machines.\n\nIt’s going to be an interesting time to live through.', 'aiModelVersion': '1'}",0.9925
Alon Amit,3y,Have you seen the latest solution to the traveling salesman problem?,"Yup, sure did. What do you want to know about it? The actual paper is A (Slightly) Improved Approximation Algorithm for Metric TSP. To clarify what the progress is, here is a quick summary. The context is metric TSP, which is TSP when the distances or costs are known to satisfy the triangle inequality. This is an important subclass of the general TSP problem, but it’s not the TSP problem in full generality. A further context is approximation. Of all the possible tours of a given TSP graph there is one which is the absolute best (shortest, cheapest. There may be multiple tours with the exact same cost; this doesn’t matter much.) We know that finding this absolute best is NP-hard, so we’re wondering whether it might be easier to find a tour that is not too far from being the absolute best. If you want to get very close to the absolute best (not more than 0.7% away, say), the problem is still NP-hard. If you are ok with a 3232\frac{3}{2}-approximation, meaning you could be 50% off the optimal tour, then there are efficient algorithms for doing that that have been known since 1976. The final context is randomization. We can sometimes build good random algorithms for doing something without necessarily knowing how to make them deterministic (though some folks believe that all randomized algorithms can be efficiently derandomized). In this case, we are hoping for a random algorithm for which we can prove that the expected quality of the produced tour is better than 50% off the mark. This is what Karlin, Klein and Oveis Gharan have achieved: a random polynomial-time algorithm which produces a TSP tour with an expected cost that is within 32−ϵ32−ϵ\frac{3}{2}-\epsilon of the optimal tour, for some small absolute constant ϵϵ\epsilon. The authors believe the algorithm actually achieves a 4343\frac{4}{3}-approximation, but they can’t prove it yet. People who work on combinatorial optimization in various guises are familiar with the situation where a certain constant magnitude, like this 3232\frac{3}{2}, stands for decades and over time creates the impression of an impenetrable fortification. It’s therefore exciting when people manage to breach that wall, even if by a small amount. The guaranteed value of ϵϵ\epsilon in this paper is 10−3610−3610^{-36}, which is tiny, but it’s still a breach. So, yeah. Fun!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2lm14rh3uasz8xwy', 'title': 'Have you seen the latest solution to the traveling salesman problem?', 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': 'Yup, sure did. What do you want to know about it? The actual paper is A (Slightly) Improved Approximation Algorithm for Metric TSP. To clarify what the progress is, here is a quick summary. The context is metric TSP, which is TSP when the distances or costs are known to satisfy the triangle inequality. This is an important subclass of the general TSP problem, but it’s not the TSP problem in full generality. A further context is approximation. Of all the possible tours of a given TSP graph there is one which is the absolute best (shortest, cheapest. There may be multiple tours with the exact same cost; this doesn’t matter much.) We know that finding this absolute best is NP-hard, so we’re wondering whether it might be easier to find a tour that is not too far from being the absolute best. If you want to get very close to the absolute best (not more than 0.7% away, say), the problem is still NP-hard. If you are ok with a 3232\\frac{3}{2}-approximation, meaning you could be 50% off the optimal tour, then there are efficient algorithms for doing that that have been known since 1976. The final context is randomization. We can sometimes build good random algorithms for doing something without necessarily knowing how to make them deterministic (though some folks believe that all randomized algorithms can be efficiently derandomized). In this case, we are hoping for a random algorithm for which we can prove that the expected quality of the produced tour is better than 50% off the mark. This is what Karlin, Klein and Oveis Gharan have achieved: a random polynomial-time algorithm which produces a TSP tour with an expected cost that is within 32−ϵ32−ϵ\\frac{3}{2}-\\epsilon of the optimal tour, for some small absolute constant ϵϵ\\epsilon. The authors believe the algorithm actually achieves a 4343\\frac{4}{3}-approximation, but they can’t prove it yet. People who work on combinatorial optimization in various guises are familiar with the situation where a certain constant magnitude, like this 3232\\frac{3}{2}, stands for decades and over time creates the impression of an impenetrable fortification. It’s therefore exciting when people manage to breach that wall, even if by a small amount. The guaranteed value of ϵϵ\\epsilon in this paper is 10−3610−3610^{-36}, which is tiny, but it’s still a breach. So, yeah. Fun!', 'result': {'fake': 0.0007, 'real': 0.9993}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986178, 'subscription': 0, 'content': 'Yup, sure did. What do you want to know about it? The actual paper is A (Slightly) Improved Approximation Algorithm for Metric TSP. To clarify what the progress is, here is a quick summary. The context is metric TSP, which is TSP when the distances or costs are known to satisfy the triangle inequality. This is an important subclass of the general TSP problem, but it’s not the TSP problem in full generality. A further context is approximation. Of all the possible tours of a given TSP graph there is one which is the absolute best (shortest, cheapest. There may be multiple tours with the exact same cost; this doesn’t matter much.) We know that finding this absolute best is NP-hard, so we’re wondering whether it might be easier to find a tour that is not too far from being the absolute best. If you want to get very close to the absolute best (not more than 0.7% away, say), the problem is still NP-hard. If you are ok with a 3232\\frac{3}{2}-approximation, meaning you could be 50% off the optimal tour, then there are efficient algorithms for doing that that have been known since 1976. The final context is randomization. We can sometimes build good random algorithms for doing something without necessarily knowing how to make them deterministic (though some folks believe that all randomized algorithms can be efficiently derandomized). In this case, we are hoping for a random algorithm for which we can prove that the expected quality of the produced tour is better than 50% off the mark. This is what Karlin, Klein and Oveis Gharan have achieved: a random polynomial-time algorithm which produces a TSP tour with an expected cost that is within 32−ϵ32−ϵ\\frac{3}{2}-\\epsilon of the optimal tour, for some small absolute constant ϵϵ\\epsilon. The authors believe the algorithm actually achieves a 4343\\frac{4}{3}-approximation, but they can’t prove it yet. People who work on combinatorial optimization in various guises are familiar with the situation where a certain constant magnitude, like this 3232\\frac{3}{2}, stands for decades and over time creates the impression of an impenetrable fortification. It’s therefore exciting when people manage to breach that wall, even if by a small amount. The guaranteed value of ϵϵ\\epsilon in this paper is 10−3610−3610^{-36}, which is tiny, but it’s still a breach. So, yeah. Fun!', 'aiModelVersion': '1'}",0.9993
Prateek Jain,Updated 7y,What is the most frustrating thing about being a computer programmer?,"I was travelling from Moscow to New Delhi and the conversation with my fellow passenger was like this:

Me: I am working for XYZ company and I work as a tester.

(His TV was not working and he constantly asked the stewards for help, but they were unable to do anything. So he moves to me and said)

He: Can't you do something like they do in a Hollywood movie, just write some code, hack the system and make my TV work...?

Me: You have a lot of expectation which even my managers don't have with me :P","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2guy64vo7cbe3nkm', 'title': 'What is the most frustrating thing about being a computer programmer?', 'score': {'original': 0.9977, 'ai': 0.0023}, 'blocks': [{'text': ""I was travelling from Moscow to New Delhi and the conversation with my fellow passenger was like this:\n\nMe: I am working for XYZ company and I work as a tester.\n\n(His TV was not working and he constantly asked the stewards for help, but they were unable to do anything. So he moves to me and said)\n\nHe: Can't you do something like they do in a Hollywood movie, just write some code, hack the system and make my TV work...?\n\nMe: You have a lot of expectation which even my managers don't have with me :P"", 'result': {'fake': 0.0023, 'real': 0.9977}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986177, 'subscription': 0, 'content': ""I was travelling from Moscow to New Delhi and the conversation with my fellow passenger was like this:\n\nMe: I am working for XYZ company and I work as a tester.\n\n(His TV was not working and he constantly asked the stewards for help, but they were unable to do anything. So he moves to me and said)\n\nHe: Can't you do something like they do in a Hollywood movie, just write some code, hack the system and make my TV work...?\n\nMe: You have a lot of expectation which even my managers don't have with me :P"", 'aiModelVersion': '1'}",0.9977
Mats Bauer,Updated 3y,How do computer programmers think differently from others?,"They see things differently. About a week ago, I was trying to end my day with some Netflix. On logging in, I got the beautiful message: “There is no account with the given e-mail address”. Well, I thought, that’s odd, maybe you didn’t pay. Checked my payment records via PayPal - there it was, paid my 10,90€ only six days ago.

Checked my e-mails, and found these e-mails in my spam folder:

My account status went from “New Login with different mail, to password changed, to e-mail changed, to language changed from German to English, to language changed from English to Spanish”.

I contacted the Netflix support and asked. They told me they didn’t know, and that they assumed that my account was hacked.

And there, as a programmer, I started wondering. My password has a form similar to: Hasjdi23891 ! 39?. Cracking this password using brute force would take a) years and years and b) would probably be stopped by Netflix after 50 threading cycles, assuming a DDoS attack.

I only use this password for Netflix, so there is no way it could have been lost somewhere else. That meant that the password must have somehow been taken or intercepted in the database traffic.

Next, I wondered, how it was possible for someone, after logging in to my account, and triggering a “new login” warning by Netflix, to change the main Email address, phone number, language, name, account type (it was changed to the most expensive plan) and account names, while keeping my payment settings.

After reviewing the given login logs by Netflix, I found the final IP address and phone number. I traced both to a location in Colombia, where a fixed endpoint was. This meant, that the hacker didn’t even bother to use a proxy (hidemyass), VPN tunnel or Tor. So how was it possible for him to hack my password, that (I hope) is transferred and stored hashed, when not even knowing how to hide his location? I did a little research and found some references to sites on the dark web, selling login information for Netflix accounts for only a couple of dollars each.

After I contacted the Netflix support, which was most likely some outsourced firm somewhere other than Palo Alto, I noticed that it took the support guy less than 30 seconds to change the registered e-mail address to mine, reset the password, change language and account type and regrant me access. This is most likely done via a support team database access site with limited access rights. The most obvious reason for the loss of my Netflix account, without me having to approve anything (email change, password change, phone number change, account type change), was that this was done via this support platform, as with him doing so, I received the same emails as above (“Your email address was changed”, etc.).

This flaw, that could have easily been prevented by having me approve any changes via email or phone, Netflix could have saved me about 45 minutes of my life.

Edit:

Thanks for the great feedback. As many of you wrote in the comments, the same Problem is found with accounts at EA and Spotify. It seems like the companies don’t think it’s a big deal, losing account information to a stranger, because it costs only $10 per month. But how about enabling a security feature (that can be deactivated if wanted), that disables the ability to change passwords or email addresses for 24 hours after a new login? If the algorithms are advanced enough to find these wrong logins, why not prevent those people from changing your data and stealing your data? Starting with private account names, my phone number and my previously watched list?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/f7vy6e4rxjcn3khs', 'title': 'How do computer programmers think differently from others?', 'score': {'original': 0.75065, 'ai': 0.24935}, 'blocks': [{'text': 'They see things differently. About a week ago, I was trying to end my day with some Netflix. On logging in, I got the beautiful message: “There is no account with the given e-mail address”. Well, I thought, that’s odd, maybe you didn’t pay. Checked my payment records via PayPal - there it was, paid my 10,90€ only six days ago.\n\nChecked my e-mails, and found these e-mails in my spam folder:\n\nMy account status went from “New Login with different mail, to password changed, to e-mail changed, to language changed from German to English, to language changed from English to Spanish”.\n\nI contacted the Netflix support and asked. They told me they didn’t know, and that they assumed that my account was hacked.\n\nAnd there, as a programmer, I started wondering. My password has a form similar to: Hasjdi23891 ! 39?. Cracking this password using brute force would take a) years and years and b) would probably be stopped by Netflix after 50 threading cycles, assuming a DDoS attack.\n\nI only use this password for Netflix, so there is no way it could have been lost somewhere else. That meant that the password must have somehow been taken or intercepted in the database traffic.\n\nNext, I wondered, how it was possible for someone, after logging in to my account, and triggering a “new login” warning by Netflix, to change the main Email address, phone number, language, name, account type (it was changed to the most expensive plan) and account names, while keeping my payment settings.\n\nAfter reviewing the given login logs by Netflix, I found the final IP address and phone number. I traced both to a location in Colombia, where a fixed endpoint was. This meant, that the hacker didn’t even bother to use a proxy (hidemyass), VPN tunnel or Tor. So how was it possible for him to hack my password, that (I hope) is transferred and stored hashed, when not even knowing how to hide his location? I did a little research and found some references to sites on the dark web, selling login information for Netflix accounts for only a couple of dollars each.\n\nAfter I contacted the Netflix support, which was most likely some outsourced firm somewhere other than Palo Alto, I noticed that it took the support guy less than 30 seconds to change the registered e-mail address to mine, reset the password, change language and account type and regrant me access. This is most likely done via a support team database access site with limited access rights. The most obvious reason for the loss of my Netflix account, without me having to approve anything (email change, password change, phone number change, account type change), was that this was done via this support platform, as with him doing so, I received the same emails as above (“Your email address was changed”, etc.).\n\nThis flaw, that could have easily been prevented by having me approve any changes via email or phone, Netflix could have saved me about 45 minutes of my life.\n\nEdit:\n\nThanks for the great feedback. As many of you wrote in the comments, the', 'result': {'fake': 0.206, 'real': 0.794}, 'status': 'success'}, {'text': 'same Problem is found with accounts at EA and Spotify. It seems like the companies don’t think it’s a big deal, losing account information to a stranger, because it costs only $10 per month. But how about enabling a security feature (that can be deactivated if wanted), that disables the ability to change passwords or email addresses for 24 hours after a new login? If the algorithms are advanced enough to find these wrong logins, why not prevent those people from changing your data and stealing your data? Starting with private account names, my phone number and my previously watched list?', 'result': {'fake': 0.9154, 'real': 0.0846}, 'status': 'success'}], 'credits_used': 7, 'credits': 1986170, 'subscription': 0, 'content': 'They see things differently. About a week ago, I was trying to end my day with some Netflix. On logging in, I got the beautiful message: “There is no account with the given e-mail address”. Well, I thought, that’s odd, maybe you didn’t pay. Checked my payment records via PayPal - there it was, paid my 10,90€ only six days ago.\n\nChecked my e-mails, and found these e-mails in my spam folder:\n\nMy account status went from “New Login with different mail, to password changed, to e-mail changed, to language changed from German to English, to language changed from English to Spanish”.\n\nI contacted the Netflix support and asked. They told me they didn’t know, and that they assumed that my account was hacked.\n\nAnd there, as a programmer, I started wondering. My password has a form similar to: Hasjdi23891 ! 39?. Cracking this password using brute force would take a) years and years and b) would probably be stopped by Netflix after 50 threading cycles, assuming a DDoS attack.\n\nI only use this password for Netflix, so there is no way it could have been lost somewhere else. That meant that the password must have somehow been taken or intercepted in the database traffic.\n\nNext, I wondered, how it was possible for someone, after logging in to my account, and triggering a “new login” warning by Netflix, to change the main Email address, phone number, language, name, account type (it was changed to the most expensive plan) and account names, while keeping my payment settings.\n\nAfter reviewing the given login logs by Netflix, I found the final IP address and phone number. I traced both to a location in Colombia, where a fixed endpoint was. This meant, that the hacker didn’t even bother to use a proxy (hidemyass), VPN tunnel or Tor. So how was it possible for him to hack my password, that (I hope) is transferred and stored hashed, when not even knowing how to hide his location? I did a little research and found some references to sites on the dark web, selling login information for Netflix accounts for only a couple of dollars each.\n\nAfter I contacted the Netflix support, which was most likely some outsourced firm somewhere other than Palo Alto, I noticed that it took the support guy less than 30 seconds to change the registered e-mail address to mine, reset the password, change language and account type and regrant me access. This is most likely done via a support team database access site with limited access rights. The most obvious reason for the loss of my Netflix account, without me having to approve anything (email change, password change, phone number change, account type change), was that this was done via this support platform, as with him doing so, I received the same emails as above (“Your email address was changed”, etc.).\n\nThis flaw, that could have easily been prevented by having me approve any changes via email or phone, Netflix could have saved me about 45 minutes of my life.\n\nEdit:\n\nThanks for the great feedback. As many of you wrote in the comments, the same Problem is found with accounts at EA and Spotify. It seems like the companies don’t think it’s a big deal, losing account information to a stranger, because it costs only $10 per month. But how about enabling a security feature (that can be deactivated if wanted), that disables the ability to change passwords or email addresses for 24 hours after a new login? If the algorithms are advanced enough to find these wrong logins, why not prevent those people from changing your data and stealing your data? Starting with private account names, my phone number and my previously watched list?', 'aiModelVersion': '1'}",0.75065
Mike,Updated 2y,Is there a way to boot a computer without a CPU?,"Some folks answering this question are simply far too young… or I’m too old.

Strictly speaking, no, you need the CPU to actually boot the computer, but you can load the boot loader code into memory in some computers without a CPU.

The term boot is short for bootstrap loader. The bootstrap loader is the very first program a computer must execute. It has just one purpose: To load the next program into memory.

A long time ago…

Computers didn’t have any non-volatile storage. When you turned the computer off it lost the entire contents of all of its the memory.

When you turned the computer on it was a big dumb paper weight. The processor was “halted”.

On the front panel there was a set of switches and sometimes a button. Those switches allowed you to set addresses and assembly op codes in binary. You would then push the “load” button — sometimes it was another switch with a spring to return it.

You would use the switches to program the bootstrap loader into memory. By its very nature the bootstrap loader had to be small. Programming it with those switches was an arduous process.

The switches were eventually replaced on large mainframe computers but when the first “personal” computers came to market — they had switches on the front.

Photo attribution: IMSAI 8080 computer

So, you could load the bootstrap loader without the CPU. In fact, you had to load the bootstrap loader without the CPU because the CPU had no instructions.

Once you loaded the bootstrap loader you would move another switch from “halt” to “run” and only then would the CPU begin to execute the program just loaded with the switches.

Typically the bootstrap loader consisted of a very simple IO driver code and a few instructions to load the first real, useful program into memory. Often the next program would be the “monitor”. This was a small program that provided keyboard IO and some type of output — often an RS232-C serial port. It would let you modify memory, load programs, and tell the CPU to run the program. You could only run one program at a time. There was no OS in the earliest personal computer systems.

After that you’d load a BASIC interpreter and then you could run BASIC programs. It was great fun!

It was common to use the term boot to describe the action of moving the run/halt switch to run after the bootstrap loader was programmed. So, again, strictly speaking, you need the CPU to actually boot the computer, but in some computers you can load the bootstrap loader into memory without a CPU.

Also, just a quick note about “core memory”; that is memory that relies on magnetic storage of data and which was the dominant form of program and data storage in computers sold between the mid 1950s until 1970. Some commercial computers employing core memory continued in operation into the 1990s. The robustness of this type of memory lead to its continued use throughout the 70s and at least part of the 80s for military and space applications.

What most people think of when they mention core memory is the computer’s main core memory. This type of core memory is mostly non-volatile. However, a key exception is that the read process is destructive and results in all bits in the word being set to zero. The memory controller had to rewrite each location after a read operation. If power was lost mid-read operation that location would be corrupted. This might or might not corrupt the program or constant data. Those unfamiliar with core memory might be amused to know it’s capacity was measured in 
k
b
i
t
/
f
t
3
kbit/ft3
.

A different type of core memory, known as “rope core memory” did not have destructive reads. It provided the function of ROM and carried a pre-programmed bootstrap loader in some systems. It had to be “programmed” during assembly by running a wire through a core for a 1 or around a core for a 0. Rope core memory was exceedingly robust and insensitive to SEUs and EMP. The program for the Apollo Guidance Computer was stored in rope core memory.

EDIT: Thanks for all the views and upvotes. I’m glad you enjoyed my answer.

EDIT2: Added an opening sentence to clarify that you can’t actually boot the computer without a CPU. Thanks to Alexander Serebriansky for the kind suggestion.

EDIT3: Added a note clarifying the degree to which core memory was non-volatile.

EDIT4: Added photo attribution.

EDIT5: Corrected core memory superscript.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/tfy8mki1bp942z5g', 'title': 'Is there a way to boot a computer without a CPU?', 'score': {'original': 0.5085, 'ai': 0.4915}, 'blocks': [{'text': 'Some folks answering this question are simply far too young… or I’m too old.\n\nStrictly speaking, no, you need the CPU to actually boot the computer, but you can load the boot loader code into memory in some computers without a CPU.\n\nThe term boot is short for bootstrap loader. The bootstrap loader is the very first program a computer must execute. It has just one purpose: To load the next program into memory.\n\nA long time ago…\n\nComputers didn’t have any non-volatile storage. When you turned the computer off it lost the entire contents of all of its the memory.\n\nWhen you turned the computer on it was a big dumb paper weight. The processor was “halted”.\n\nOn the front panel there was a set of switches and sometimes a button. Those switches allowed you to set addresses and assembly op codes in binary. You would then push the “load” button — sometimes it was another switch with a spring to return it.\n\nYou would use the switches to program the bootstrap loader into memory. By its very nature the bootstrap loader had to be small. Programming it with those switches was an arduous process.\n\nThe switches were eventually replaced on large mainframe computers but when the first “personal” computers came to market — they had switches on the front.\n\nPhoto attribution: IMSAI 8080 computer\n\nSo, you could load the bootstrap loader without the CPU. In fact, you had to load the bootstrap loader without the CPU because the CPU had no instructions.\n\nOnce you loaded the bootstrap loader you would move another switch from “halt” to “run” and only then would the CPU begin to execute the program just loaded with the switches.\n\nTypically the bootstrap loader consisted of a very simple IO driver code and a few instructions to load the first real, useful program into memory. Often the next program would be the “monitor”. This was a small program that provided keyboard IO and some type of output — often an RS232-C serial port. It would let you modify memory, load programs, and tell the CPU to run the program. You could only run one program at a time. There was no OS in the earliest personal computer systems.\n\nAfter that you’d load a BASIC interpreter and then you could run BASIC programs. It was great fun!\n\nIt was common to use the term boot to describe the action of moving the run/halt switch to run after the bootstrap loader was programmed. So, again, strictly speaking, you need the CPU to actually boot the computer, but in some computers you can load the bootstrap loader into memory without a CPU.\n\nAlso, just a quick note about “core memory”; that is memory that relies on magnetic storage of data and which was the dominant form of program and data storage in computers sold between the mid 1950s until 1970. Some commercial computers employing core memory continued in operation into the 1990s. The robustness of this type of memory lead to its continued use throughout the 70s and at least part of the 80s for military and space applications.\n\nWhat most people think of when they', 'result': {'fake': 0.0379, 'real': 0.9621}, 'status': 'success'}, {'text': 'mention core memory is the computer’s main core memory. This type of core memory is mostly non-volatile. However, a key exception is that the read process is destructive and results in all bits in the word being set to zero. The memory controller had to rewrite each location after a read operation. If power was lost mid-read operation that location would be corrupted. This might or might not corrupt the program or constant data. Those unfamiliar with core memory might be amused to know it’s capacity was measured in \nk\nb\ni\nt\n/\nf\nt\n3\nkbit/ft3\n.\n\nA different type of core memory, known as “rope core memory” did not have destructive reads. It provided the function of ROM and carried a pre-programmed bootstrap loader in some systems. It had to be “programmed” during assembly by running a wire through a core for a 1 or around a core for a 0. Rope core memory was exceedingly robust and insensitive to SEUs and EMP. The program for the Apollo Guidance Computer was stored in rope core memory.\n\nEDIT: Thanks for all the views and upvotes. I’m glad you enjoyed my answer.\n\nEDIT2: Added an opening sentence to clarify that you can’t actually boot the computer without a CPU. Thanks to Alexander Serebriansky for the kind suggestion.\n\nEDIT3: Added a note clarifying the degree to which core memory was non-volatile.\n\nEDIT4: Added photo attribution.\n\nEDIT5: Corrected core memory superscript.', 'result': {'fake': 0.1566, 'real': 0.8434}, 'status': 'success'}], 'credits_used': 8, 'credits': 1986162, 'subscription': 0, 'content': 'Some folks answering this question are simply far too young… or I’m too old.\n\nStrictly speaking, no, you need the CPU to actually boot the computer, but you can load the boot loader code into memory in some computers without a CPU.\n\nThe term boot is short for bootstrap loader. The bootstrap loader is the very first program a computer must execute. It has just one purpose: To load the next program into memory.\n\nA long time ago…\n\nComputers didn’t have any non-volatile storage. When you turned the computer off it lost the entire contents of all of its the memory.\n\nWhen you turned the computer on it was a big dumb paper weight. The processor was “halted”.\n\nOn the front panel there was a set of switches and sometimes a button. Those switches allowed you to set addresses and assembly op codes in binary. You would then push the “load” button — sometimes it was another switch with a spring to return it.\n\nYou would use the switches to program the bootstrap loader into memory. By its very nature the bootstrap loader had to be small. Programming it with those switches was an arduous process.\n\nThe switches were eventually replaced on large mainframe computers but when the first “personal” computers came to market — they had switches on the front.\n\nPhoto attribution: IMSAI 8080 computer\n\nSo, you could load the bootstrap loader without the CPU. In fact, you had to load the bootstrap loader without the CPU because the CPU had no instructions.\n\nOnce you loaded the bootstrap loader you would move another switch from “halt” to “run” and only then would the CPU begin to execute the program just loaded with the switches.\n\nTypically the bootstrap loader consisted of a very simple IO driver code and a few instructions to load the first real, useful program into memory. Often the next program would be the “monitor”. This was a small program that provided keyboard IO and some type of output — often an RS232-C serial port. It would let you modify memory, load programs, and tell the CPU to run the program. You could only run one program at a time. There was no OS in the earliest personal computer systems.\n\nAfter that you’d load a BASIC interpreter and then you could run BASIC programs. It was great fun!\n\nIt was common to use the term boot to describe the action of moving the run/halt switch to run after the bootstrap loader was programmed. So, again, strictly speaking, you need the CPU to actually boot the computer, but in some computers you can load the bootstrap loader into memory without a CPU.\n\nAlso, just a quick note about “core memory”; that is memory that relies on magnetic storage of data and which was the dominant form of program and data storage in computers sold between the mid 1950s until 1970. Some commercial computers employing core memory continued in operation into the 1990s. The robustness of this type of memory lead to its continued use throughout the 70s and at least part of the 80s for military and space applications.\n\nWhat most people think of when they mention core memory is the computer’s main core memory. This type of core memory is mostly non-volatile. However, a key exception is that the read process is destructive and results in all bits in the word being set to zero. The memory controller had to rewrite each location after a read operation. If power was lost mid-read operation that location would be corrupted. This might or might not corrupt the program or constant data. Those unfamiliar with core memory might be amused to know it’s capacity was measured in \nk\nb\ni\nt\n/\nf\nt\n3\nkbit/ft3\n.\n\nA different type of core memory, known as “rope core memory” did not have destructive reads. It provided the function of ROM and carried a pre-programmed bootstrap loader in some systems. It had to be “programmed” during assembly by running a wire through a core for a 1 or around a core for a 0. Rope core memory was exceedingly robust and insensitive to SEUs and EMP. The program for the Apollo Guidance Computer was stored in rope core memory.\n\nEDIT: Thanks for all the views and upvotes. I’m glad you enjoyed my answer.\n\nEDIT2: Added an opening sentence to clarify that you can’t actually boot the computer without a CPU. Thanks to Alexander Serebriansky for the kind suggestion.\n\nEDIT3: Added a note clarifying the degree to which core memory was non-volatile.\n\nEDIT4: Added photo attribution.\n\nEDIT5: Corrected core memory superscript.', 'aiModelVersion': '1'}",0.5085
Michelle - MAD PIRATE QUEEN,2y,The symbol for the save button on computers got carried forward to this day long beyond the thing it's based on (floppy disk) becoming obsolete. What other concepts do we have today that are represented by things that are obsolete?,"I was talking to my partner about this the other day. A couple of carryovers I/we still use:

* When you signal for the bill, you do that thing where you fake sign something in the air while looking at the waiter with that ‘look’ and mouthing “the bill, I need the bill, please.” But these days, we don't even sign anything. I just ‘Tap and Go’.
* I still use the term “cheque” when I haven’t used a cheque in 15+ years.
* I make the hang-loose hand sign and hold it to my ear when talking about phone calls…


…because that’s what phones used to look like to me. Well not like that (in the picture). It looks bad in the picture because I’m a sh!tty artist. You should know that by now. If you want something accurate, google it… The hang loose bit came from here [ https://en.wikipedia.org/wiki/Shaka_sign ].

Now cell phones are flat but if you put a flat hand against your tipped head, you look like you're making the un...

Access this answer and support the author as a Quora+ subscriber
Access all answers reserved by 
Michelle - MAD PIRATE QUEEN
 for Quora+ subscribers
Access exclusive answers from thousands more participating creators in Quora+
Browse ad‑free and support creators
Start free trial
Learn more","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/12bpk70ex6rwds8j', 'title': ""The symbol for the save button on computers got carried forward to this day long beyond the thing it's based on (floppy disk) becoming obsolete. What other concepts do we have today that are represented by things that are obsolete?"", 'score': {'original': 0.9982, 'ai': 0.0018}, 'blocks': [{'text': ""I was talking to my partner about this the other day. A couple of carryovers I/we still use:\n\n* When you signal for the bill, you do that thing where you fake sign something in the air while looking at the waiter with that ‘look’ and mouthing “the bill, I need the bill, please.” But these days, we don't even sign anything. I just ‘Tap and Go’.\n* I still use the term “cheque” when I haven’t used a cheque in 15+ years.\n* I make the hang-loose hand sign and hold it to my ear when talking about phone calls…\n\n\n…because that’s what phones used to look like to me. Well not like that (in the picture). It looks bad in the picture because I’m a sh!tty artist. You should know that by now. If you want something accurate, google it… The hang loose bit came from here [ https://en.wikipedia.org/wiki/Shaka_sign ].\n\nNow cell phones are flat but if you put a flat hand against your tipped head, you look like you're making the un...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nMichelle - MAD PIRATE QUEEN\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more"", 'result': {'fake': 0.0018, 'real': 0.9982}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986159, 'subscription': 0, 'content': ""I was talking to my partner about this the other day. A couple of carryovers I/we still use:\n\n* When you signal for the bill, you do that thing where you fake sign something in the air while looking at the waiter with that ‘look’ and mouthing “the bill, I need the bill, please.” But these days, we don't even sign anything. I just ‘Tap and Go’.\n* I still use the term “cheque” when I haven’t used a cheque in 15+ years.\n* I make the hang-loose hand sign and hold it to my ear when talking about phone calls…\n\n\n…because that’s what phones used to look like to me. Well not like that (in the picture). It looks bad in the picture because I’m a sh!tty artist. You should know that by now. If you want something accurate, google it… The hang loose bit came from here [ https://en.wikipedia.org/wiki/Shaka_sign ].\n\nNow cell phones are flat but if you put a flat hand against your tipped head, you look like you're making the un...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nMichelle - MAD PIRATE QUEEN\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more"", 'aiModelVersion': '1'}",0.9982
Joseph Hornsey,Updated 2y,What is the most ridiculous example of made-up computer jargon from TV and film?,"I can think of a few…. maybe not ‘jargon’, per se, but stupidity nonetheless.

Before I begin, let me show you an example of someone doing it right:

In The Matrix series, they show real-world tools such as Nmap being used to do the things those tools are actually designed to do.

Now, the stupidity?

I can’t find the quote, but I’ll never forget watching the show “24” and they were in a situation where a surveillance camera didn’t show the correct angle.

The computer girl said, “I’ll datamine the Ethernet packets”. Because, you know, the network is a giant database and the camera actually captures ALL angles, but only shows you the ones you don’t want. Gotta go mine them packets.

But, the all-time ‘takes the cake’ example of TV show computer idiocy is from NCIS:

So, here’s the situation…. A bad guy has hacked into the NCIS computer and the computer genius girl can’t kick the bad guy out. So, the computer genius guy says, “I’ll help!” and starts using the keyboard at the same time.

There’s a video on YouTube if you want to watch it. It’s called “2 Idiots 1 Keyboard
”

How does that tense, OMG situation end? Well, the fearless leader (who knows nothing about computers) unplugs the monitor. Whew! Problem solved and crisis averted!

This is one of those situations where one can’t help but wonder why in the hell a big-budget TV show like NCIS couldn’t afford to, or simply didn’t consider, hiring a consultant to tell them they were doing something that stupid.

It is the single most ridiculous thing I’ve ever seen with computers on TV. And, believe me, that is saying a LOT.

EDIT #2 - Okay, so this has been bothering me ever since Hans Egloff linked this in a comment below:

Now, I can’t decide whether what I posted is the worst of all time, or if the scene above is the worst of all time. Thanks a lot, Micke Friberg, for planting that seed of doubt. LOL

EDIT #1 - Thank you all for the upvotes! 2.5K is by far the most I’ve ever received. Also, thank you for the comments!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/qcg21x4ij9dp0lto', 'title': 'What is the most ridiculous example of made-up computer jargon from TV and film?', 'score': {'original': 0.9988, 'ai': 0.0012}, 'blocks': [{'text': 'I can think of a few…. maybe not ‘jargon’, per se, but stupidity nonetheless.\n\nBefore I begin, let me show you an example of someone doing it right:\n\nIn The Matrix series, they show real-world tools such as Nmap being used to do the things those tools are actually designed to do.\n\nNow, the stupidity?\n\nI can’t find the quote, but I’ll never forget watching the show “24” and they were in a situation where a surveillance camera didn’t show the correct angle.\n\nThe computer girl said, “I’ll datamine the Ethernet packets”. Because, you know, the network is a giant database and the camera actually captures ALL angles, but only shows you the ones you don’t want. Gotta go mine them packets.\n\nBut, the all-time ‘takes the cake’ example of TV show computer idiocy is from NCIS:\n\nSo, here’s the situation…. A bad guy has hacked into the NCIS computer and the computer genius girl can’t kick the bad guy out. So, the computer genius guy says, “I’ll help!” and starts using the keyboard at the same time.\n\nThere’s a video on YouTube if you want to watch it. It’s called “2 Idiots 1 Keyboard\n”\n\nHow does that tense, OMG situation end? Well, the fearless leader (who knows nothing about computers) unplugs the monitor. Whew! Problem solved and crisis averted!\n\nThis is one of those situations where one can’t help but wonder why in the hell a big-budget TV show like NCIS couldn’t afford to, or simply didn’t consider, hiring a consultant to tell them they were doing something that stupid.\n\nIt is the single most ridiculous thing I’ve ever seen with computers on TV. And, believe me, that is saying a LOT.\n\nEDIT #2 - Okay, so this has been bothering me ever since Hans Egloff linked this in a comment below:\n\nNow, I can’t decide whether what I posted is the worst of all time, or if the scene above is the worst of all time. Thanks a lot, Micke Friberg, for planting that seed of doubt. LOL\n\nEDIT #1 - Thank you all for the upvotes! 2.5K is by far the most I’ve ever received. Also, thank you for the comments!', 'result': {'fake': 0.0012, 'real': 0.9988}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986155, 'subscription': 0, 'content': 'I can think of a few…. maybe not ‘jargon’, per se, but stupidity nonetheless.\n\nBefore I begin, let me show you an example of someone doing it right:\n\nIn The Matrix series, they show real-world tools such as Nmap being used to do the things those tools are actually designed to do.\n\nNow, the stupidity?\n\nI can’t find the quote, but I’ll never forget watching the show “24” and they were in a situation where a surveillance camera didn’t show the correct angle.\n\nThe computer girl said, “I’ll datamine the Ethernet packets”. Because, you know, the network is a giant database and the camera actually captures ALL angles, but only shows you the ones you don’t want. Gotta go mine them packets.\n\nBut, the all-time ‘takes the cake’ example of TV show computer idiocy is from NCIS:\n\nSo, here’s the situation…. A bad guy has hacked into the NCIS computer and the computer genius girl can’t kick the bad guy out. So, the computer genius guy says, “I’ll help!” and starts using the keyboard at the same time.\n\nThere’s a video on YouTube if you want to watch it. It’s called “2 Idiots 1 Keyboard\n”\n\nHow does that tense, OMG situation end? Well, the fearless leader (who knows nothing about computers) unplugs the monitor. Whew! Problem solved and crisis averted!\n\nThis is one of those situations where one can’t help but wonder why in the hell a big-budget TV show like NCIS couldn’t afford to, or simply didn’t consider, hiring a consultant to tell them they were doing something that stupid.\n\nIt is the single most ridiculous thing I’ve ever seen with computers on TV. And, believe me, that is saying a LOT.\n\nEDIT #2 - Okay, so this has been bothering me ever since Hans Egloff linked this in a comment below:\n\nNow, I can’t decide whether what I posted is the worst of all time, or if the scene above is the worst of all time. Thanks a lot, Micke Friberg, for planting that seed of doubt. LOL\n\nEDIT #1 - Thank you all for the upvotes! 2.5K is by far the most I’ve ever received. Also, thank you for the comments!', 'aiModelVersion': '1'}",0.9988
Joe Zbiciak,2y,Why is my C++ vending machine calculating the change in the wrong way?,"I’m not going to debug your program. But, I am going to give you a big hint for a major problem I see in your code.

This:

if (foo < bar) 
   foo = foo - bar; 
   bar = bar % 42; 

Means the same as this:

if (foo < bar) 
{ 
   foo = foo - bar; 
} 
bar = bar % 42; 

It does not mean this:

if (foo < bar) 
{ 
   foo = foo - bar; 
   bar = bar % 42; 
} 

Python figures out blocks with indentation. C++ requires curly braces for blocks with more than one statement. I don’t know whether you have previous experience with Python. If you do, be very aware of this difference. It applies wherever you need to group multiple statements into a block of code (i.e. if/else, for, while, switch, functions, etc.)

Now, take a look at your code, and see how you might repair it based on that observation.

Good luck!

Addendum: I also haven’t looked closely at what moneyLeftDouble is, but some of that looks dubious.

Your code will be easier to follow and easier to get right if you convert all money amounts to cents and store them in integers, so you never have any fractional values to worry about.

You’ll just need to multiply user inputs by 100, and divide any monetary amounts by 100.0 before printing. (Don’t forget the “.0” if you store your monetary amounts in int.)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/7b69m1i8gh4atecz', 'title': 'Why is my C++ vending machine calculating the change in the wrong way?', 'score': {'original': 0.9943, 'ai': 0.0057}, 'blocks': [{'text': 'I’m not going to debug your program. But, I am going to give you a big hint for a major problem I see in your code.\n\nThis:\n\nif (foo < bar)\xa0\n   foo = foo - bar;\xa0\n   bar = bar % 42;\xa0\n\nMeans the same as this:\n\nif (foo < bar)\xa0\n{\xa0\n   foo = foo - bar;\xa0\n}\xa0\nbar = bar % 42;\xa0\n\nIt does not mean this:\n\nif (foo < bar)\xa0\n{\xa0\n   foo = foo - bar;\xa0\n   bar = bar % 42;\xa0\n}\xa0\n\nPython figures out blocks with indentation. C++ requires curly braces for blocks with more than one statement. I don’t know whether you have previous experience with Python. If you do, be very aware of this difference. It applies wherever you need to group multiple statements into a block of code (i.e. if/else, for, while, switch, functions, etc.)\n\nNow, take a look at your code, and see how you might repair it based on that observation.\n\nGood luck!\n\nAddendum: I also haven’t looked closely at what moneyLeftDouble is, but some of that looks dubious.\n\nYour code will be easier to follow and easier to get right if you convert all money amounts to cents and store them in integers, so you never have any fractional values to worry about.\n\nYou’ll just need to multiply user inputs by 100, and divide any monetary amounts by 100.0 before printing. (Don’t forget the “.0” if you store your monetary amounts in int.)', 'result': {'fake': 0.0056, 'real': 0.9944}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986152, 'subscription': 0, 'content': 'I’m not going to debug your program. But, I am going to give you a big hint for a major problem I see in your code.\n\nThis:\n\nif (foo < bar)\xa0\n   foo = foo - bar;\xa0\n   bar = bar % 42;\xa0\n\nMeans the same as this:\n\nif (foo < bar)\xa0\n{\xa0\n   foo = foo - bar;\xa0\n}\xa0\nbar = bar % 42;\xa0\n\nIt does not mean this:\n\nif (foo < bar)\xa0\n{\xa0\n   foo = foo - bar;\xa0\n   bar = bar % 42;\xa0\n}\xa0\n\nPython figures out blocks with indentation. C++ requires curly braces for blocks with more than one statement. I don’t know whether you have previous experience with Python. If you do, be very aware of this difference. It applies wherever you need to group multiple statements into a block of code (i.e. if/else, for, while, switch, functions, etc.)\n\nNow, take a look at your code, and see how you might repair it based on that observation.\n\nGood luck!\n\nAddendum: I also haven’t looked closely at what moneyLeftDouble is, but some of that looks dubious.\n\nYour code will be easier to follow and easier to get right if you convert all money amounts to cents and store them in integers, so you never have any fractional values to worry about.\n\nYou’ll just need to multiply user inputs by 100, and divide any monetary amounts by 100.0 before printing. (Don’t forget the “.0” if you store your monetary amounts in int.)', 'aiModelVersion': '1'}",0.9943
Ted Mayberry,Updated 1y,How would a 1980s computer scientist react to today’s modern computers?,"In the early 80’s, I used IBM punch cards to run my programs through the computer.

I remember the instructor holding one up and stating these will become obsolete and we should stick one in a drawer sometime because years later, people won’t even know what it is. I had a hard time believing him.

Well, I don’t remember what capacity that mainframe I used back in the day that parsed out jobs from hundreds of different users, but I suspect my desktop likely exceeds that computer’s capacity.

In other words, if I visited myself in the 80’s with just my smart phone, well, I’d say the 80’s version of me would be blown away.

Edit: Had no idea so many would comment.

I’m pretty sure my cell phone has more computing capacity over the mainframe I used in the 80’s. But, since I don’t really know what the specs of the mainframe back in the 80’s, well, I figured softening my statement was for the best. People on this board can get brutal when you don’t have your facts straight. Then again, many can be brutal because they’re idiots.
Reading through the comments, reminded me of the parameter that we set in case we accidentally programed an infinite loop. Back then, the standard was 2 seconds, if the program took more than 2 seconds, it would halt the job and spit out whatever you got. My dad got his engineering degree in the early 70’s and used the same interface, punch cards to a mainframe. His infinite loop parameter was 2 minutes.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/wx21y095vk8o4pzc', 'title': 'How would a 1980s computer scientist react to today’s modern computers?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'In the early 80’s, I used IBM punch cards to run my programs through the computer.\n\nI remember the instructor holding one up and stating these will become obsolete and we should stick one in a drawer sometime because years later, people won’t even know what it is. I had a hard time believing him.\n\nWell, I don’t remember what capacity that mainframe I used back in the day that parsed out jobs from hundreds of different users, but I suspect my desktop likely exceeds that computer’s capacity.\n\nIn other words, if I visited myself in the 80’s with just my smart phone, well, I’d say the 80’s version of me would be blown away.\n\nEdit: Had no idea so many would comment.\n\nI’m pretty sure my cell phone has more computing capacity over the mainframe I used in the 80’s. But, since I don’t really know what the specs of the mainframe back in the 80’s, well, I figured softening my statement was for the best. People on this board can get brutal when you don’t have your facts straight. Then again, many can be brutal because they’re idiots.\nReading through the comments, reminded me of the parameter that we set in case we accidentally programed an infinite loop. Back then, the standard was 2 seconds, if the program took more than 2 seconds, it would halt the job and spit out whatever you got. My dad got his engineering degree in the early 70’s and used the same interface, punch cards to a mainframe. His infinite loop parameter was 2 minutes.', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986149, 'subscription': 0, 'content': 'In the early 80’s, I used IBM punch cards to run my programs through the computer.\n\nI remember the instructor holding one up and stating these will become obsolete and we should stick one in a drawer sometime because years later, people won’t even know what it is. I had a hard time believing him.\n\nWell, I don’t remember what capacity that mainframe I used back in the day that parsed out jobs from hundreds of different users, but I suspect my desktop likely exceeds that computer’s capacity.\n\nIn other words, if I visited myself in the 80’s with just my smart phone, well, I’d say the 80’s version of me would be blown away.\n\nEdit: Had no idea so many would comment.\n\nI’m pretty sure my cell phone has more computing capacity over the mainframe I used in the 80’s. But, since I don’t really know what the specs of the mainframe back in the 80’s, well, I figured softening my statement was for the best. People on this board can get brutal when you don’t have your facts straight. Then again, many can be brutal because they’re idiots.\nReading through the comments, reminded me of the parameter that we set in case we accidentally programed an infinite loop. Back then, the standard was 2 seconds, if the program took more than 2 seconds, it would halt the job and spit out whatever you got. My dad got his engineering degree in the early 70’s and used the same interface, punch cards to a mainframe. His infinite loop parameter was 2 minutes.', 'aiModelVersion': '1'}",0.9999
Alon Amit,5y,"If you discover something like P=NP and have solid proof, where do you bring the proof without it being stolen/to claim credit?","You have a Quora account. Quora has blogs. Post it to your blog. It’s dated, it has your name on it, done.

Or post it on Facebook. Or Medium. Or Wordpress. Or tweetstorm it. Or record a video of yourself explaining it and upload to YouTube. Or instagram. Or of all the above simultaneously.

It’s the digital age. Publishing and establishing ownership over something is the easiest thing in the world. Having an actual proof of P=NP, you may find, is a bigger hurdle.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/73d6tfijm1l5nwur', 'title': 'If you discover something like P=NP and have solid proof, where do you bring the proof without it being stolen/to claim credit?', 'score': {'original': 0.9964, 'ai': 0.0036}, 'blocks': [{'text': 'You have a Quora account. Quora has blogs. Post it to your blog. It’s dated, it has your name on it, done.\n\nOr post it on Facebook. Or Medium. Or Wordpress. Or tweetstorm it. Or record a video of yourself explaining it and upload to YouTube. Or instagram. Or of all the above simultaneously.\n\nIt’s the digital age. Publishing and establishing ownership over something is the easiest thing in the world. Having an actual proof of P=NP, you may find, is a bigger hurdle.', 'result': {'fake': 0.0036, 'real': 0.9964}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986148, 'subscription': 0, 'content': 'You have a Quora account. Quora has blogs. Post it to your blog. It’s dated, it has your name on it, done.\n\nOr post it on Facebook. Or Medium. Or Wordpress. Or tweetstorm it. Or record a video of yourself explaining it and upload to YouTube. Or instagram. Or of all the above simultaneously.\n\nIt’s the digital age. Publishing and establishing ownership over something is the easiest thing in the world. Having an actual proof of P=NP, you may find, is a bigger hurdle.', 'aiModelVersion': '1'}",0.9964
Jeff Erickson,5y,How can I be so sure I will get a good job with my computer science degree? Other students in my classes are 10x smarter than me.,"Let me echo Mercedes May.

Nobody cares how smart you are, or how smart other students in your classes are. Well, okay, you seem to care, but you’re the only one. Well, okay, there are probably a few jerks who care because they think they’re the smart ones, but they’re idiots; just ignore them.

(It’s also simply not true that other students in your classes are 10x smarter than you, but the utter irrelevance of the comparison is far more important than its simple lack of truth.)

What people care about is what you can actually do. What value do you bring to the table? In particular, what can you do that other people can’t?

No, no, I saw you thinking “nothing”. Stop that. At absolute worst, the answer is “nothing yet”.

You’re in a unique position to learn skills that can help you answer that question with something that you actually care about. No, no, not “being as smart as those other people”—what do you want to do for yourself with the skills you learn getting your degree? What kind of computer science do you like? What aspects and/or applications of computing inspire you? (Programming tools? Algorithms? Networking? Machine learning? User interface design? Games and movies? Sociology? Education? Biology or medicine? Music? Public policy?) What are you eager to work hard to become better at? (Not “as good as those people”, just “better”.)

It’s okay not to have an answer yet, and it’s okay to change your mind over time. Look around; try a bunch of things; talk to lots of people; be open to change your mind; say yes and. This is one of the things college is for. (Not just college, but it is a lot easier in college.)

Then aim to get better at whatever it is you actually care about. Ask people who do what you care about (students, professors, recruiters, neighbors) how to get involved. Listen. Follow their advice. Work hard. Don’t worry about what other people are doing; this is about you.

Then look for jobs that let you do the kind of thing you like to do. And look, hey, these are exactly the jobs you’ve prepared yourself for!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/mowqp5alnv01s2de', 'title': 'How can I be so sure I will get a good job with my computer science degree? Other students in my classes are 10x smarter than me.', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Let me echo Mercedes May.\n\nNobody cares how smart you are, or how smart other students in your classes are. Well, okay, you seem to care, but you’re the only one. Well, okay, there are probably a few jerks who care because they think they’re the smart ones, but they’re idiots; just ignore them.\n\n(It’s also simply not true that other students in your classes are 10x smarter than you, but the utter irrelevance of the comparison is far more important than its simple lack of truth.)\n\nWhat people care about is what you can actually do. What value do you bring to the table? In particular, what can you do that other people can’t?\n\nNo, no, I saw you thinking “nothing”. Stop that. At absolute worst, the answer is “nothing yet”.\n\nYou’re in a unique position to learn skills that can help you answer that question with something that you actually care about. No, no, not “being as smart as those other people”—what do you want to do for yourself with the skills you learn getting your degree? What kind of computer science do you like? What aspects and/or applications of computing inspire you? (Programming tools? Algorithms? Networking? Machine learning? User interface design? Games and movies? Sociology? Education? Biology or medicine? Music? Public policy?) What are you eager to work hard to become better at? (Not “as good as those people”, just “better”.)\n\nIt’s okay not to have an answer yet, and it’s okay to change your mind over time. Look around; try a bunch of things; talk to lots of people; be open to change your mind; say yes and. This is one of the things college is for. (Not just college, but it is a lot easier in college.)\n\nThen aim to get better at whatever it is you actually care about. Ask people who do what you care about (students, professors, recruiters, neighbors) how to get involved. Listen. Follow their advice. Work hard. Don’t worry about what other people are doing; this is about you.\n\nThen look for jobs that let you do the kind of thing you like to do. And look, hey, these are exactly the jobs you’ve prepared yourself for!', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986144, 'subscription': 0, 'content': 'Let me echo Mercedes May.\n\nNobody cares how smart you are, or how smart other students in your classes are. Well, okay, you seem to care, but you’re the only one. Well, okay, there are probably a few jerks who care because they think they’re the smart ones, but they’re idiots; just ignore them.\n\n(It’s also simply not true that other students in your classes are 10x smarter than you, but the utter irrelevance of the comparison is far more important than its simple lack of truth.)\n\nWhat people care about is what you can actually do. What value do you bring to the table? In particular, what can you do that other people can’t?\n\nNo, no, I saw you thinking “nothing”. Stop that. At absolute worst, the answer is “nothing yet”.\n\nYou’re in a unique position to learn skills that can help you answer that question with something that you actually care about. No, no, not “being as smart as those other people”—what do you want to do for yourself with the skills you learn getting your degree? What kind of computer science do you like? What aspects and/or applications of computing inspire you? (Programming tools? Algorithms? Networking? Machine learning? User interface design? Games and movies? Sociology? Education? Biology or medicine? Music? Public policy?) What are you eager to work hard to become better at? (Not “as good as those people”, just “better”.)\n\nIt’s okay not to have an answer yet, and it’s okay to change your mind over time. Look around; try a bunch of things; talk to lots of people; be open to change your mind; say yes and. This is one of the things college is for. (Not just college, but it is a lot easier in college.)\n\nThen aim to get better at whatever it is you actually care about. Ask people who do what you care about (students, professors, recruiters, neighbors) how to get involved. Listen. Follow their advice. Work hard. Don’t worry about what other people are doing; this is about you.\n\nThen look for jobs that let you do the kind of thing you like to do. And look, hey, these are exactly the jobs you’ve prepared yourself for!', 'aiModelVersion': '1'}",0.9998
Jean-Marie Valheur,2y,Who is your favorite historical figure and why?,"I’ve always been both impressed and terrified at the same time by Roman general, consul and dictator Lucius Cornelius Sulla. His rise to power and the boldness of his decisions set the stage for the end of the Roman republic and would later inspire Julius Caesar in his crossing of the Rubicon. A man whose epitaph read only: “No better friend, no worse enemy”.

Sulla was an odd character — as a young boy, his father died, his mother abandoned him and he was left alone in the care of a rich but distant stepmother. In the end, he had to fend entirely for himself. So he disappeared into the lower classes of Roman life. The shady, criminal, debauched places. Some say he made his money as a gigolo. Others say he was a pimp.

Fact remains, Sulla was popular. A great lover with little to no moral qualms, his best friend were actors — then considered the lowest of the low! — crossdressers, prostitutes and swindlers. He was reportedly exceptionally handsome and well-endowed, his face strikingly beautiful with the palest of skins and a thick mop of golden-red hair. In his early twenties, he managed to inherit money from both his stepmother and a rich courtesan, rumor had it he seduced both to secure the fortune. He then entered the political arena.

Sulla allied himself with the famous Roman general Gaius Marius. He started a military campaign, joined the senate and marched with Marius to Gaul. He fought the barbarians for years. And managed to make a name for himself. When one day he achieved the highest and loftiest of positions any Roman could ever reach, that of consul, he was over the moon!

As an aside, here’s a little picture I found of someone reconstructing Marius and Sulla from their busts and descriptions. Old Sulla vaguely… reminds me of someone, hmmm. Anyway, Sulla becomes consul, he’s in his early fifties, and he gets the command against a worthy adversary, Mithridates of Pontus. He deserves it, too — he has managed to squish the Italian rebels that threatened the empire just months earlier. Marius, meanwhile, cannot handle that he is no longer the top dog. So he stages a coup in the senate through bribes and calling in favors, thus cheating his former officer of his rightful command.

Sulla is furious! How dare that old man steal his glory and attempt to gain the command for his own selfish means? He makes a powerful, and daring decision… he marches on to Rome, at the head of his five legions. Marches on, calls on to the senate and has his command reinstated. Forces Marius to flee with his tail between his legs and rides off on a glorious Pontic adventure into the sunset…

As Sulla is out, Marius and his allies once more return to Rome, granting Marius an unheard of seventh consulship that makes him into an eternal legend. By now the old general seems to have gone mad and he butchers many of Sulla’s allies, and pretty much everyone he holds a grudge against. He then dies of a stroke at the height of his power, only a few weeks into office. Soon, Sulla returns to the Italian peninsula, having kicked some serious Pontic ass, now once again marching onto Rome at the head of his armies. He meets what remains of the Marian forces outside the gates and decisively defeats them in battle.

Now he is the undisputed champion of Rome, Sulla once again marches in. Declares himself dictator of the city and takes full control. He massacres his enemies, and all those who betrayed or belittled him in spite of his decades of service to Rome. Nails their heads to the rostra, on the walls of the city and near its gates.

Once, people snickered at Lucius Cornelius Sulla. Now, soon, they snicker once again, as he starts wandering around the city like a sad old man, back crooked, without bodyguards protecting him. People assume the massacres might have been orchestrated by those next to him, maybe the now-old general was unaware? People began to let down their guard around him. He hears a lot of things he wasn’t meant to hear. He plays Rome and its citizens like a fiddle, revealing himself to still be strong, capable and decisive after a few weeks of gathering intel.

He acts with terrifying swiftness. Restores old laws, abolishes new ones. Limits the powers of the senate to take away a consul’s rightful command, reforms laws, tries, and succeeds, in rebuilding the economy, and manages to appoint people who are actually competent for the most part. After just two years in power, Sulla decided to retire from public life, surrendering his powers to the senate and retreating to his Italian villa. He left Rome with a full treasury and a lot of ruthlessly ambitious people without a head.

In old age, Sulla wrote his memoirs, and lived a debased and scandalous life, once more surrounded by the very people he started his early years with — pimps, actors, whores and swindlers. He shared a bedroom with his very young, very pregnant fifth wife and the Greek actor Metrobius, both of them his lovers, making no secret of the fact. He died shortly after finishing his memoirs. Rome’s biggest badboy made Mark Antony seem like a choirboy.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/l1au2socgni5wzjk', 'title': 'Who is your favorite historical figure and why?', 'score': {'original': 0.77, 'ai': 0.23}, 'blocks': [{'text': 'I’ve always been both impressed and terrified at the same time by Roman general, consul and dictator Lucius Cornelius Sulla. His rise to power and the boldness of his decisions set the stage for the end of the Roman republic and would later inspire Julius Caesar in his crossing of the Rubicon. A man whose epitaph read only: “No better friend, no worse enemy”.\n\nSulla was an odd character — as a young boy, his father died, his mother abandoned him and he was left alone in the care of a rich but distant stepmother. In the end, he had to fend entirely for himself. So he disappeared into the lower classes of Roman life. The shady, criminal, debauched places. Some say he made his money as a gigolo. Others say he was a pimp.\n\nFact remains, Sulla was popular. A great lover with little to no moral qualms, his best friend were actors — then considered the lowest of the low! — crossdressers, prostitutes and swindlers. He was reportedly exceptionally handsome and well-endowed, his face strikingly beautiful with the palest of skins and a thick mop of golden-red hair. In his early twenties, he managed to inherit money from both his stepmother and a rich courtesan, rumor had it he seduced both to secure the fortune. He then entered the political arena.\n\nSulla allied himself with the famous Roman general Gaius Marius. He started a military campaign, joined the senate and marched with Marius to Gaul. He fought the barbarians for years. And managed to make a name for himself. When one day he achieved the highest and loftiest of positions any Roman could ever reach, that of consul, he was over the moon!\n\nAs an aside, here’s a little picture I found of someone reconstructing Marius and Sulla from their busts and descriptions. Old Sulla vaguely… reminds me of someone, hmmm. Anyway, Sulla becomes consul, he’s in his early fifties, and he gets the command against a worthy adversary, Mithridates of Pontus. He deserves it, too — he has managed to squish the Italian rebels that threatened the empire just months earlier. Marius, meanwhile, cannot handle that he is no longer the top dog. So he stages a coup in the senate through bribes and calling in favors, thus cheating his former officer of his rightful command.\n\nSulla is furious! How dare that old man steal his glory and attempt to gain the command for his own selfish means? He makes a powerful, and daring decision… he marches on to Rome, at the head of his five legions. Marches on, calls on to the senate and has his command reinstated. Forces Marius to flee with his tail between his legs and rides off on a glorious Pontic adventure into the sunset…\n\nAs Sulla is out, Marius and his allies once more return to Rome, granting Marius an unheard of seventh consulship that makes him into an eternal legend. By now the old general seems to have gone mad and he butchers many of Sulla’s allies, and pretty much everyone he holds a grudge against. He then', 'result': {'fake': 0.0082, 'real': 0.9918}, 'status': 'success'}, {'text': 'dies of a stroke at the height of his power, only a few weeks into office. Soon, Sulla returns to the Italian peninsula, having kicked some serious Pontic ass, now once again marching onto Rome at the head of his armies. He meets what remains of the Marian forces outside the gates and decisively defeats them in battle.\n\nNow he is the undisputed champion of Rome, Sulla once again marches in. Declares himself dictator of the city and takes full control. He massacres his enemies, and all those who betrayed or belittled him in spite of his decades of service to Rome. Nails their heads to the rostra, on the walls of the city and near its gates.\n\nOnce, people snickered at Lucius Cornelius Sulla. Now, soon, they snicker once again, as he starts wandering around the city like a sad old man, back crooked, without bodyguards protecting him. People assume the massacres might have been orchestrated by those next to him, maybe the now-old general was unaware? People began to let down their guard around him. He hears a lot of things he wasn’t meant to hear. He plays Rome and its citizens like a fiddle, revealing himself to still be strong, capable and decisive after a few weeks of gathering intel.\n\nHe acts with terrifying swiftness. Restores old laws, abolishes new ones. Limits the powers of the senate to take away a consul’s rightful command, reforms laws, tries, and succeeds, in rebuilding the economy, and manages to appoint people who are actually competent for the most part. After just two years in power, Sulla decided to retire from public life, surrendering his powers to the senate and retreating to his Italian villa. He left Rome with a full treasury and a lot of ruthlessly ambitious people without a head.\n\nIn old age, Sulla wrote his memoirs, and lived a debased and scandalous life, once more surrounded by the very people he started his early years with — pimps, actors, whores and swindlers. He shared a bedroom with his very young, very pregnant fifth wife and the Greek actor Metrobius, both of them his lovers, making no secret of the fact. He died shortly after finishing his memoirs. Rome’s biggest badboy made Mark Antony seem like a choirboy.', 'result': {'fake': 0.0454, 'real': 0.9546}, 'status': 'success'}], 'credits_used': 9, 'credits': 1986135, 'subscription': 0, 'content': 'I’ve always been both impressed and terrified at the same time by Roman general, consul and dictator Lucius Cornelius Sulla. His rise to power and the boldness of his decisions set the stage for the end of the Roman republic and would later inspire Julius Caesar in his crossing of the Rubicon. A man whose epitaph read only: “No better friend, no worse enemy”.\n\nSulla was an odd character — as a young boy, his father died, his mother abandoned him and he was left alone in the care of a rich but distant stepmother. In the end, he had to fend entirely for himself. So he disappeared into the lower classes of Roman life. The shady, criminal, debauched places. Some say he made his money as a gigolo. Others say he was a pimp.\n\nFact remains, Sulla was popular. A great lover with little to no moral qualms, his best friend were actors — then considered the lowest of the low! — crossdressers, prostitutes and swindlers. He was reportedly exceptionally handsome and well-endowed, his face strikingly beautiful with the palest of skins and a thick mop of golden-red hair. In his early twenties, he managed to inherit money from both his stepmother and a rich courtesan, rumor had it he seduced both to secure the fortune. He then entered the political arena.\n\nSulla allied himself with the famous Roman general Gaius Marius. He started a military campaign, joined the senate and marched with Marius to Gaul. He fought the barbarians for years. And managed to make a name for himself. When one day he achieved the highest and loftiest of positions any Roman could ever reach, that of consul, he was over the moon!\n\nAs an aside, here’s a little picture I found of someone reconstructing Marius and Sulla from their busts and descriptions. Old Sulla vaguely… reminds me of someone, hmmm. Anyway, Sulla becomes consul, he’s in his early fifties, and he gets the command against a worthy adversary, Mithridates of Pontus. He deserves it, too — he has managed to squish the Italian rebels that threatened the empire just months earlier. Marius, meanwhile, cannot handle that he is no longer the top dog. So he stages a coup in the senate through bribes and calling in favors, thus cheating his former officer of his rightful command.\n\nSulla is furious! How dare that old man steal his glory and attempt to gain the command for his own selfish means? He makes a powerful, and daring decision… he marches on to Rome, at the head of his five legions. Marches on, calls on to the senate and has his command reinstated. Forces Marius to flee with his tail between his legs and rides off on a glorious Pontic adventure into the sunset…\n\nAs Sulla is out, Marius and his allies once more return to Rome, granting Marius an unheard of seventh consulship that makes him into an eternal legend. By now the old general seems to have gone mad and he butchers many of Sulla’s allies, and pretty much everyone he holds a grudge against. He then dies of a stroke at the height of his power, only a few weeks into office. Soon, Sulla returns to the Italian peninsula, having kicked some serious Pontic ass, now once again marching onto Rome at the head of his armies. He meets what remains of the Marian forces outside the gates and decisively defeats them in battle.\n\nNow he is the undisputed champion of Rome, Sulla once again marches in. Declares himself dictator of the city and takes full control. He massacres his enemies, and all those who betrayed or belittled him in spite of his decades of service to Rome. Nails their heads to the rostra, on the walls of the city and near its gates.\n\nOnce, people snickered at Lucius Cornelius Sulla. Now, soon, they snicker once again, as he starts wandering around the city like a sad old man, back crooked, without bodyguards protecting him. People assume the massacres might have been orchestrated by those next to him, maybe the now-old general was unaware? People began to let down their guard around him. He hears a lot of things he wasn’t meant to hear. He plays Rome and its citizens like a fiddle, revealing himself to still be strong, capable and decisive after a few weeks of gathering intel.\n\nHe acts with terrifying swiftness. Restores old laws, abolishes new ones. Limits the powers of the senate to take away a consul’s rightful command, reforms laws, tries, and succeeds, in rebuilding the economy, and manages to appoint people who are actually competent for the most part. After just two years in power, Sulla decided to retire from public life, surrendering his powers to the senate and retreating to his Italian villa. He left Rome with a full treasury and a lot of ruthlessly ambitious people without a head.\n\nIn old age, Sulla wrote his memoirs, and lived a debased and scandalous life, once more surrounded by the very people he started his early years with — pimps, actors, whores and swindlers. He shared a bedroom with his very young, very pregnant fifth wife and the Greek actor Metrobius, both of them his lovers, making no secret of the fact. He died shortly after finishing his memoirs. Rome’s biggest badboy made Mark Antony seem like a choirboy.', 'aiModelVersion': '1'}",0.77
Wes Winn,3y,"According to the book Talent is overrated, every field has classic guides that will always repay study, as a way of reviewing fundamental skills. What would these guides be for Computer Science?","Walk into a room full of software developers and talk about some “taboo” topic, and you’ll get a couple shrugs and grumbles. Walk in and scream,

“EMACS IS BETTER THAN VIM!!!”

And be ready to have that on your tombstone.

I’m kidding of course, mostly, but when it comes to what things are best, this field has some opinionated people, and you won’t find some perfect agreed upon guide.

There are some that are very frequently referred to:

But even then, there are understandable criticisms of each.

Algorithms was dry enough to make the Sahara look like a waterfall, Design Patterns was morphed into a book called Head First Design Patterns, which I recommend twice as often, etc.

Then there’s subject specific books. The dragon books is kind of the de facto standard for Compilers, but then Compilers is often just an elective of a CS program, despite being really fascinating (and oddly useful from time to time).

Credit to Monica S Lam, who contributed in later versions.

You can be a good software developer and never read any of these books.

Generally, you’ll come across the concepts in one place or another, and if you’re going to spend your life in academia, they really are good reads, but information repays itself differently to different people in different forms.

I will say that people like Knuth or in application, people like Uncle Bob, have lessons that virtually everyone will learn with any time or success in this field, but it doesn’t come with a requirement to read a giant tome.

As people get better at everything in the world, we also get better at understanding learning and teaching. For many of us, there are simply more digestible forms of information out there.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/q7ay5cuxowgej138', 'title': 'According to the book Talent is overrated, every field has classic guides that will always repay study, as a way of reviewing fundamental skills. What would these guides be for Computer Science?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'Walk into a room full of software developers and talk about some “taboo” topic, and you’ll get a couple shrugs and grumbles. Walk in and scream,\n\n“EMACS IS BETTER THAN VIM!!!”\n\nAnd be ready to have that on your tombstone.\n\nI’m kidding of course, mostly, but when it comes to what things are best, this field has some opinionated people, and you won’t find some perfect agreed upon guide.\n\nThere are some that are very frequently referred to:\n\nBut even then, there are understandable criticisms of each.\n\nAlgorithms was dry enough to make the Sahara look like a waterfall, Design Patterns was morphed into a book called Head First Design Patterns, which I recommend twice as often, etc.\n\nThen there’s subject specific books. The dragon books is kind of the de facto standard for Compilers, but then Compilers is often just an elective of a CS program, despite being really fascinating (and oddly useful from time to time).\n\nCredit to Monica S Lam, who contributed in later versions.\n\nYou can be a good software developer and never read any of these books.\n\nGenerally, you’ll come across the concepts in one place or another, and if you’re going to spend your life in academia, they really are good reads, but information repays itself differently to different people in different forms.\n\nI will say that people like Knuth or in application, people like Uncle Bob, have lessons that virtually everyone will learn with any time or success in this field, but it doesn’t come with a requirement to read a giant tome.\n\nAs people get better at everything in the world, we also get better at understanding learning and teaching. For many of us, there are simply more digestible forms of information out there.', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986132, 'subscription': 0, 'content': 'Walk into a room full of software developers and talk about some “taboo” topic, and you’ll get a couple shrugs and grumbles. Walk in and scream,\n\n“EMACS IS BETTER THAN VIM!!!”\n\nAnd be ready to have that on your tombstone.\n\nI’m kidding of course, mostly, but when it comes to what things are best, this field has some opinionated people, and you won’t find some perfect agreed upon guide.\n\nThere are some that are very frequently referred to:\n\nBut even then, there are understandable criticisms of each.\n\nAlgorithms was dry enough to make the Sahara look like a waterfall, Design Patterns was morphed into a book called Head First Design Patterns, which I recommend twice as often, etc.\n\nThen there’s subject specific books. The dragon books is kind of the de facto standard for Compilers, but then Compilers is often just an elective of a CS program, despite being really fascinating (and oddly useful from time to time).\n\nCredit to Monica S Lam, who contributed in later versions.\n\nYou can be a good software developer and never read any of these books.\n\nGenerally, you’ll come across the concepts in one place or another, and if you’re going to spend your life in academia, they really are good reads, but information repays itself differently to different people in different forms.\n\nI will say that people like Knuth or in application, people like Uncle Bob, have lessons that virtually everyone will learn with any time or success in this field, but it doesn’t come with a requirement to read a giant tome.\n\nAs people get better at everything in the world, we also get better at understanding learning and teaching. For many of us, there are simply more digestible forms of information out there.', 'aiModelVersion': '1'}",0.9999
Nick Pappas,Updated 5y,When does programming stop being fun?,"When you find your self in a situation like this:

or this:

Or when THIS happens.

Or of course, when eventually this inevitable crap happens:

closely related to this one below.

Or when you have a boss who first tells you something like this:

and then goes ahead and assigns to you a task like this:

and FINALLY….:
When they tell you that you have to maintain uncommented code that looks like this:","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/qe9ot3py8nl1xgsz', 'title': 'When does programming stop being fun?', 'score': {'original': 0.9966, 'ai': 0.0034}, 'blocks': [{'text': 'When you find your self in a situation like this:\n\nor this:\n\nOr when THIS happens.\n\nOr of course, when eventually this inevitable crap happens:\n\nclosely related to this one below.\n\nOr when you have a boss who first tells you something like this:\n\nand then goes ahead and assigns to you a task like this:\n\nand FINALLY….:\nWhen they tell you that you have to maintain uncommented code that looks like this:', 'result': {'fake': 0.0034, 'real': 0.9966}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986131, 'subscription': 0, 'content': 'When you find your self in a situation like this:\n\nor this:\n\nOr when THIS happens.\n\nOr of course, when eventually this inevitable crap happens:\n\nclosely related to this one below.\n\nOr when you have a boss who first tells you something like this:\n\nand then goes ahead and assigns to you a task like this:\n\nand FINALLY….:\nWhen they tell you that you have to maintain uncommented code that looks like this:', 'aiModelVersion': '1'}",0.9966
Jeff Sturm,11mo,Why was it so hard for Unix to run on a 386 CPU before Linux?,"It wasn’t. There were commercial ports to the i386 available before 1990. When I worked in a Radio Shack retail store in the late 1980’s, our store operating system ran on a Tandy 4000 computer running SCO Xenix, a flavor of Unix. It supported two POS terminals connected by a serial RS-232 link and the main console where we would enter the day’s cash balances at the close of business, among other management tasks.

Essentially, one of these:

https://worldradiohistory.com/hd2/IDX-CATALOGS/IDX/Radio-Shack/Radio-Shack-1988-OCR-Page-0175.pdf

The Tandy 4000 had one megabyte RAM standard and could be upgraded to a maximum of 16MB. But these, and the other early 80386 systems from PC clone manufacturers, are what made Unix possible on the desktop for “reasonable” cost (probably around $4k all in).

Altos sold 80386 systems running AT&T System V Unix, and later 80486 which were software compatible but higher performance (up to 33 MHz). I supported these early in my career with up to 20 users running accounting software.

But the commercial Unix releases were usually out of reach of the home computer hobbyist. From memory, a base license for SCO Xenix could be $795 retail (EDIT: Xenix was available for $595 per the Radio Shack ad above), and that’s without upgrades for TCP/IP networking or development tools.

Many hobbyists in the 1980’s were frustrated with the limitations of MS-DOS and wanted a true multitasking, multiuser operating system on their home computers. Most wouldn’t pay for the commercial licenses from AT&T licensees, so there was a small, but growing interest in having a Unix clone OS available for inexpensive home computers.

That was how Linux got its start, when Linus Torvalds, then a computer science student in Finland, began work on his kernel project that would eventually allow him to host GNU ports of common Unix utilities and software from the Free Software Foundation. But Linux wasn’t the first—though as an ambitious project, it was among the best. Linux importantly had support for virtual memory on the 80386, allowing for a unified buffer cache and demand-paged memory that yielded high performance of applications ported from Unix to GNU/Linux, even on small memory systems.

Before Linux there was Minix, which also ran on the 8088-based IBM PC, but it was more of a teaching tool than a practical operating system.

The Mark-Williams company saw an opportunity and created a commercial Unix clone called Coherent, initially on the 16-bit 80286 CPU, later ported to the 80386. Coherent had inadequate support for virtual memory and lacked a sufficient developer community as it was closed source. That company went out of business as they could not compete with Linux or any of the BSD ports then.

Speaking of BSD, the 386BSD project was a port of the Berkeley Software Distribution’s Unix system (created and maintained initially at the University of California, Berkeley campus) to the 80386, and it could be freely downloaded by hobbyists in 1992 around the time Linux was in early development. I can remember downloading a boot floppy that could run a live 386BSD distribution, downloading software packages over a dial-up modem. Very very slow to download, but it worked!

(386BSD has been discontinued as a project. It split into other projects such as NetBSD or FreeBSD which are still maintained today. However, unlike Linux developers, the BSD team had access to AT&T licensed source code for Unix. AT&T filed a lawsuit against the BSDi, a company the produced one of the early ports of BSD to the 80386, alleging that the free code contained proprietary software licensed by Unix System Laboratories, a subsidiary of AT&T. The lawsuits were settled by 1994, but in the meantime, Linux got a sizeable head start, as it was unencumbered by the lawsuits.)

Strictly speaking the various Linux and BSD releases could not be called “Unix” as they were not a licensed product. But they all strove to be Unix compatible and made it easy to port Unix software often without changes. In some cases, they were even binary compatible with Unix, allowing a Linux user to run software released for Unix without modification.

By the mid-1990’s the BSD distributions along with GNU/Linux were stable and successful on Intel-based personal computers. I ran a Linux distribution on my 80486-based home computer and used it every day. It wasn’t called “Unix” but it was close enough for us, and the name “Unix” has began to fade with history as today it is commonly (and incorrectly) simply called “Linux”.

In short, Linux wasn’t the first or only port to the 80386, but it saw early success due to a combination of factors such as a high-quality implementation that would rival commercial releases, and luck (due to the AT&T lawsuit).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/95zt2hwam06civsy', 'title': 'Why was it so hard for Unix to run on a 386 CPU before Linux?', 'score': {'original': 0.92885, 'ai': 0.07115}, 'blocks': [{'text': 'It wasn’t. There were commercial ports to the i386 available before 1990. When I worked in a Radio Shack retail store in the late 1980’s, our store operating system ran on a Tandy 4000 computer running SCO Xenix, a flavor of Unix. It supported two POS terminals connected by a serial RS-232 link and the main console where we would enter the day’s cash balances at the close of business, among other management tasks.\n\nEssentially, one of these:\n\nhttps://worldradiohistory.com/hd2/IDX-CATALOGS/IDX/Radio-Shack/Radio-Shack-1988-OCR-Page-0175.pdf\n\nThe Tandy 4000 had one megabyte RAM standard and could be upgraded to a maximum of 16MB. But these, and the other early 80386 systems from PC clone manufacturers, are what made Unix possible on the desktop for “reasonable” cost (probably around $4k all in).\n\nAltos sold 80386 systems running AT&T System V Unix, and later 80486 which were software compatible but higher performance (up to 33 MHz). I supported these early in my career with up to 20 users running accounting software.\n\nBut the commercial Unix releases were usually out of reach of the home computer hobbyist. From memory, a base license for SCO Xenix could be $795 retail (EDIT: Xenix was available for $595 per the Radio Shack ad above), and that’s without upgrades for TCP/IP networking or development tools.\n\nMany hobbyists in the 1980’s were frustrated with the limitations of MS-DOS and wanted a true multitasking, multiuser operating system on their home computers. Most wouldn’t pay for the commercial licenses from AT&T licensees, so there was a small, but growing interest in having a Unix clone OS available for inexpensive home computers.\n\nThat was how Linux got its start, when Linus Torvalds, then a computer science student in Finland, began work on his kernel project that would eventually allow him to host GNU ports of common Unix utilities and software from the Free Software Foundation. But Linux wasn’t the first—though as an ambitious project, it was among the best. Linux importantly had support for virtual memory on the 80386, allowing for a unified buffer cache and demand-paged memory that yielded high performance of applications ported from Unix to GNU/Linux, even on small memory systems.\n\nBefore Linux there was Minix, which also ran on the 8088-based IBM PC, but it was more of a teaching tool than a practical operating system.\n\nThe Mark-Williams company saw an opportunity and created a commercial Unix clone called Coherent, initially on the 16-bit 80286 CPU, later ported to the 80386. Coherent had inadequate support for virtual memory and lacked a sufficient developer community as it was closed source. That company went out of business as they could not compete with Linux or any of the BSD ports then.\n\nSpeaking of BSD, the 386BSD project was a port of the Berkeley Software Distribution’s Unix system (created and maintained initially at the University of California, Berkeley campus) to the 80386, and it could be freely downloaded by hobbyists in 1992 around the time Linux was in early development. I can remember downloading a boot floppy that could run a live 386BSD distribution, downloading software packages over a dial-up modem. Very very slow to download, but it worked!\n\n(386BSD', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}, {'text': 'has been discontinued as a project. It split into other projects such as NetBSD or FreeBSD which are still maintained today. However, unlike Linux developers, the BSD team had access to AT&T licensed source code for Unix. AT&T filed a lawsuit against the BSDi, a company the produced one of the early ports of BSD to the 80386, alleging that the free code contained proprietary software licensed by Unix System Laboratories, a subsidiary of AT&T. The lawsuits were settled by 1994, but in the meantime, Linux got a sizeable head start, as it was unencumbered by the lawsuits.)\n\nStrictly speaking the various Linux and BSD releases could not be called “Unix” as they were not a licensed product. But they all strove to be Unix compatible and made it easy to port Unix software often without changes. In some cases, they were even binary compatible with Unix, allowing a Linux user to run software released for Unix without modification.\n\nBy the mid-1990’s the BSD distributions along with GNU/Linux were stable and successful on Intel-based personal computers. I ran a Linux distribution on my 80486-based home computer and used it every day. It wasn’t called “Unix” but it was close enough for us, and the name “Unix” has began to fade with history as today it is commonly (and incorrectly) simply called “Linux”.\n\nIn short, Linux wasn’t the first or only port to the 80386, but it saw early success due to a combination of factors such as a high-quality implementation that would rival commercial releases, and luck (due to the AT&T lawsuit).', 'result': {'fake': 0.1565, 'real': 0.8435}, 'status': 'success'}], 'credits_used': 8, 'credits': 1986123, 'subscription': 0, 'content': 'It wasn’t. There were commercial ports to the i386 available before 1990. When I worked in a Radio Shack retail store in the late 1980’s, our store operating system ran on a Tandy 4000 computer running SCO Xenix, a flavor of Unix. It supported two POS terminals connected by a serial RS-232 link and the main console where we would enter the day’s cash balances at the close of business, among other management tasks.\n\nEssentially, one of these:\n\nhttps://worldradiohistory.com/hd2/IDX-CATALOGS/IDX/Radio-Shack/Radio-Shack-1988-OCR-Page-0175.pdf\n\nThe Tandy 4000 had one megabyte RAM standard and could be upgraded to a maximum of 16MB. But these, and the other early 80386 systems from PC clone manufacturers, are what made Unix possible on the desktop for “reasonable” cost (probably around $4k all in).\n\nAltos sold 80386 systems running AT&T System V Unix, and later 80486 which were software compatible but higher performance (up to 33 MHz). I supported these early in my career with up to 20 users running accounting software.\n\nBut the commercial Unix releases were usually out of reach of the home computer hobbyist. From memory, a base license for SCO Xenix could be $795 retail (EDIT: Xenix was available for $595 per the Radio Shack ad above), and that’s without upgrades for TCP/IP networking or development tools.\n\nMany hobbyists in the 1980’s were frustrated with the limitations of MS-DOS and wanted a true multitasking, multiuser operating system on their home computers. Most wouldn’t pay for the commercial licenses from AT&T licensees, so there was a small, but growing interest in having a Unix clone OS available for inexpensive home computers.\n\nThat was how Linux got its start, when Linus Torvalds, then a computer science student in Finland, began work on his kernel project that would eventually allow him to host GNU ports of common Unix utilities and software from the Free Software Foundation. But Linux wasn’t the first—though as an ambitious project, it was among the best. Linux importantly had support for virtual memory on the 80386, allowing for a unified buffer cache and demand-paged memory that yielded high performance of applications ported from Unix to GNU/Linux, even on small memory systems.\n\nBefore Linux there was Minix, which also ran on the 8088-based IBM PC, but it was more of a teaching tool than a practical operating system.\n\nThe Mark-Williams company saw an opportunity and created a commercial Unix clone called Coherent, initially on the 16-bit 80286 CPU, later ported to the 80386. Coherent had inadequate support for virtual memory and lacked a sufficient developer community as it was closed source. That company went out of business as they could not compete with Linux or any of the BSD ports then.\n\nSpeaking of BSD, the 386BSD project was a port of the Berkeley Software Distribution’s Unix system (created and maintained initially at the University of California, Berkeley campus) to the 80386, and it could be freely downloaded by hobbyists in 1992 around the time Linux was in early development. I can remember downloading a boot floppy that could run a live 386BSD distribution, downloading software packages over a dial-up modem. Very very slow to download, but it worked!\n\n(386BSD has been discontinued as a project. It split into other projects such as NetBSD or FreeBSD which are still maintained today. However, unlike Linux developers, the BSD team had access to AT&T licensed source code for Unix. AT&T filed a lawsuit against the BSDi, a company the produced one of the early ports of BSD to the 80386, alleging that the free code contained proprietary software licensed by Unix System Laboratories, a subsidiary of AT&T. The lawsuits were settled by 1994, but in the meantime, Linux got a sizeable head start, as it was unencumbered by the lawsuits.)\n\nStrictly speaking the various Linux and BSD releases could not be called “Unix” as they were not a licensed product. But they all strove to be Unix compatible and made it easy to port Unix software often without changes. In some cases, they were even binary compatible with Unix, allowing a Linux user to run software released for Unix without modification.\n\nBy the mid-1990’s the BSD distributions along with GNU/Linux were stable and successful on Intel-based personal computers. I ran a Linux distribution on my 80486-based home computer and used it every day. It wasn’t called “Unix” but it was close enough for us, and the name “Unix” has began to fade with history as today it is commonly (and incorrectly) simply called “Linux”.\n\nIn short, Linux wasn’t the first or only port to the 80386, but it saw early success due to a combination of factors such as a high-quality implementation that would rival commercial releases, and luck (due to the AT&T lawsuit).', 'aiModelVersion': '1'}",0.92885
Anders Kaseorg,Updated 8y,Should array indices start at 0 or 1?,"Array indices should start at 0. This is not just an efficiency hack for ancient computers, or a reflection of the underlying memory model, or some other kind of historical accident—forget all of that. Zero-based indexing actually simplifies array-related math for the programmer, and simpler math leads to fewer bugs. Here are some examples.

Suppose you’re writing a hash table that maps each integer key to one of n buckets. If your array of buckets is indexed starting at 0, you can write bucket = key mod n; but if it’s indexed starting at 1, you have to write bucket = (key mod n) + 1.
Suppose you’re writing code to serialize a rectangular array of pixels, with width w and height h, to a file (which we’ll think of as a one-dimensional array of length w*h). With 0-indexed arrays, pixel (x, y) goes into position y*w + x; with 1-indexed arrays, pixel (x, y) goes into position y*w + x - w.
Suppose you want to put the letters ‘A’ through ‘Z’ into an array of length 26, and you have a function ord that maps a character to its ASCII value. With 0-indexed arrays, the character c is put at index ord(c) - ord(‘A’); with 1-indexed arrays, it’s put at index ord(c) - ord(‘A’) + 1.


It’s in fact one-based indexing that’s the historical accident—human languages needed numbers for “first”, “second”, etc. before we had invented zero. For a practical example of the kinds of problems this accident leads to, consider how the 1800s—well, no, actually, the period from January 1, 1801 through December 31, 1900—came to be known as the “19th century”.

Along similar lines, see my answer to Why are Python ranges half-open instead of closed?.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/juir6hklcdtyo72m', 'title': 'Should array indices start at 0 or 1?', 'score': {'original': 0.9991, 'ai': 0.0009}, 'blocks': [{'text': 'Array indices should start at 0. This is not just an efficiency hack for ancient computers, or a reflection of the underlying memory model, or some other kind of historical accident—forget all of that. Zero-based indexing actually simplifies array-related math for the programmer, and simpler math leads to fewer bugs. Here are some examples.\n\nSuppose you’re writing a hash table that maps each integer key to one of n buckets. If your array of buckets is indexed starting at 0, you can write bucket = key mod n; but if it’s indexed starting at 1, you have to write bucket = (key mod n) + 1.\nSuppose you’re writing code to serialize a rectangular array of pixels, with width w and height h, to a file (which we’ll think of as a one-dimensional array of length w*h). With 0-indexed arrays, pixel (x, y) goes into position y*w + x; with 1-indexed arrays, pixel (x, y) goes into position y*w + x - w.\nSuppose you want to put the letters ‘A’ through ‘Z’ into an array of length 26, and you have a function ord that maps a character to its ASCII value. With 0-indexed arrays, the character c is put at index ord(c) - ord(‘A’); with 1-indexed arrays, it’s put at index ord(c) - ord(‘A’) + 1.\n\n\nIt’s in fact one-based indexing that’s the historical accident—human languages needed numbers for “first”, “second”, etc. before we had invented zero. For a practical example of the kinds of problems this accident leads to, consider how the 1800s—well, no, actually, the period from January 1, 1801 through December 31, 1900—came to be known as the “19th century”.\n\nAlong similar lines, see my answer to Why are Python ranges half-open instead of closed?.', 'result': {'fake': 0.0009, 'real': 0.9991}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986120, 'subscription': 0, 'content': 'Array indices should start at 0. This is not just an efficiency hack for ancient computers, or a reflection of the underlying memory model, or some other kind of historical accident—forget all of that. Zero-based indexing actually simplifies array-related math for the programmer, and simpler math leads to fewer bugs. Here are some examples.\n\nSuppose you’re writing a hash table that maps each integer key to one of n buckets. If your array of buckets is indexed starting at 0, you can write bucket = key mod n; but if it’s indexed starting at 1, you have to write bucket = (key mod n) + 1.\nSuppose you’re writing code to serialize a rectangular array of pixels, with width w and height h, to a file (which we’ll think of as a one-dimensional array of length w*h). With 0-indexed arrays, pixel (x, y) goes into position y*w + x; with 1-indexed arrays, pixel (x, y) goes into position y*w + x - w.\nSuppose you want to put the letters ‘A’ through ‘Z’ into an array of length 26, and you have a function ord that maps a character to its ASCII value. With 0-indexed arrays, the character c is put at index ord(c) - ord(‘A’); with 1-indexed arrays, it’s put at index ord(c) - ord(‘A’) + 1.\n\n\nIt’s in fact one-based indexing that’s the historical accident—human languages needed numbers for “first”, “second”, etc. before we had invented zero. For a practical example of the kinds of problems this accident leads to, consider how the 1800s—well, no, actually, the period from January 1, 1801 through December 31, 1900—came to be known as the “19th century”.\n\nAlong similar lines, see my answer to Why are Python ranges half-open instead of closed?.', 'aiModelVersion': '1'}",0.9991
John Byrd,5y,What is the most difficult concept to grasp in object oriented programming (OOP)? How did you finally understand it?,"That computers don’t know anything about objects.

Nor do they know about classes, or methods, or members, or subclasses, or virtual functions, or pure functions, or any of that new-school jazz.

Computers don’t care.

Computers just know that there is this stuff called memory, and memory contains a bunch of numbers in it.

We humans invented object-oriented programming, in order to keep ourselves sane.

Time was, we just wrote a bunch of instructions to the computer, in the computer’s own language. Look at this memory location, do the instruction there, load another memory location, add one to it, store that value in memory there, yada yada.

Computers were fine with that.

But we weren’t fine with that. Once you added more than one programmer to a project, people couldn’t keep straight which memory locations did what, and which code did what, and who was responsible for what.

High-level languages like C helped some. But we ridiculous humans still wanted all our code to access all our data at all times, with each section of the program accessing each section of memory willy-nilly.

So we were constantly creating bugs, because we couldn’t keep all that complexity straight.

And it only got worse, the more programmers that were added to a project.

So we created rules for humans to follow, and enforced them through this wacky new human concept of object-oriented programming.

If you strip off all the funky language about what an object is and what a class is, the soul of object-oriented programming is just this:

A particular type of data should only be accessible, to a specific set of functions that operate on it.

Now this is not a computer rule. This is a human rule. Sure, you can break this rule anytime you want (with C).

But if we all agree to follow this rule, then if we do have a bug in our code, we usually have a pretty good idea where to find it. We don’t have to traipse all over our own code base, to find the one function that happens to inadvertently overwrite the wrong memory.

Each programmer has their own area of responsibility. Keeping data accessible only to the functions that operate on it, means that the audio programmer will only access audio data, and the graphics programmer will only access graphics data.

So, since we have agreed to follow this rule, we can now add a bunch of programmers to a project, without having them all constantly undoing one another’s work! What a novel concept.

Understanding virtual method tables
, and their relationship to classes, is probably the second-most difficult concept in object-oriented programming.

But once you’ve got that down, you can actually deduce almost all of how object-oriented programming works.

How did I understand object-oriented programming finally? Well, I wrote spaghetti code for about ten years, like everyone else did. And often I couldn’t go back and read my own code and figure out what the hell I was trying to do.

So in 1990, I was taking a compiler class at Harvard, and a few people were talking about this wacky new human concept of object-oriented programming. So I wrote a small compiler for an object-oriented language.

Anyway, once those concepts baked themselves into my brain, my whole approach to data abstraction changed. I started thinking in objects.

It was only living without object-oriented programming, that I could see the advantages of using it.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/uhgrf50vxjastpye', 'title': 'What is the most difficult concept to grasp in object oriented programming (OOP)? How did you finally understand it?', 'score': {'original': 0.9686, 'ai': 0.0314}, 'blocks': [{'text': 'That computers don’t know anything about objects.\n\nNor do they know about classes, or methods, or members, or subclasses, or virtual functions, or pure functions, or any of that new-school jazz.\n\nComputers don’t care.\n\nComputers just know that there is this stuff called memory, and memory contains a bunch of numbers in it.\n\nWe humans invented object-oriented programming, in order to keep ourselves sane.\n\nTime was, we just wrote a bunch of instructions to the computer, in the computer’s own language. Look at this memory location, do the instruction there, load another memory location, add one to it, store that value in memory there, yada yada.\n\nComputers were fine with that.\n\nBut we weren’t fine with that. Once you added more than one programmer to a project, people couldn’t keep straight which memory locations did what, and which code did what, and who was responsible for what.\n\nHigh-level languages like C helped some. But we ridiculous humans still wanted all our code to access all our data at all times, with each section of the program accessing each section of memory willy-nilly.\n\nSo we were constantly creating bugs, because we couldn’t keep all that complexity straight.\n\nAnd it only got worse, the more programmers that were added to a project.\n\nSo we created rules for humans to follow, and enforced them through this wacky new human concept of object-oriented programming.\n\nIf you strip off all the funky language about what an object is and what a class is, the soul of object-oriented programming is just this:\n\nA particular type of data should only be accessible, to a specific set of functions that operate on it.\n\nNow this is not a computer rule. This is a human rule. Sure, you can break this rule anytime you want (with C).\n\nBut if we all agree to follow this rule, then if we do have a bug in our code, we usually have a pretty good idea where to find it. We don’t have to traipse all over our own code base, to find the one function that happens to inadvertently overwrite the wrong memory.\n\nEach programmer has their own area of responsibility. Keeping data accessible only to the functions that operate on it, means that the audio programmer will only access audio data, and the graphics programmer will only access graphics data.\n\nSo, since we have agreed to follow this rule, we can now add a bunch of programmers to a project, without having them all constantly undoing one another’s work! What a novel concept.\n\nUnderstanding virtual method tables\n, and their relationship to classes, is probably the second-most difficult concept in object-oriented programming.\n\nBut once you’ve got that down, you can actually deduce almost all of how object-oriented programming works.\n\nHow did I understand object-oriented programming finally? Well, I wrote spaghetti code for about ten years, like everyone else did. And often I couldn’t go back and read my own code and figure out what the hell I was trying to do.\n\nSo in 1990, I was taking a compiler class at Harvard, and a few people were talking about this wacky new human concept of object-oriented programming. So I wrote a small compiler for', 'result': {'fake': 0.0068, 'real': 0.9932}, 'status': 'success'}, {'text': 'an object-oriented language.\n\nAnyway, once those concepts baked themselves into my brain, my whole approach to data abstraction changed. I started thinking in objects.\n\nIt was only living without object-oriented programming, that I could see the advantages of using it.', 'result': {'fake': 0.1452, 'real': 0.8548}, 'status': 'success'}], 'credits_used': 6, 'credits': 1986114, 'subscription': 0, 'content': 'That computers don’t know anything about objects.\n\nNor do they know about classes, or methods, or members, or subclasses, or virtual functions, or pure functions, or any of that new-school jazz.\n\nComputers don’t care.\n\nComputers just know that there is this stuff called memory, and memory contains a bunch of numbers in it.\n\nWe humans invented object-oriented programming, in order to keep ourselves sane.\n\nTime was, we just wrote a bunch of instructions to the computer, in the computer’s own language. Look at this memory location, do the instruction there, load another memory location, add one to it, store that value in memory there, yada yada.\n\nComputers were fine with that.\n\nBut we weren’t fine with that. Once you added more than one programmer to a project, people couldn’t keep straight which memory locations did what, and which code did what, and who was responsible for what.\n\nHigh-level languages like C helped some. But we ridiculous humans still wanted all our code to access all our data at all times, with each section of the program accessing each section of memory willy-nilly.\n\nSo we were constantly creating bugs, because we couldn’t keep all that complexity straight.\n\nAnd it only got worse, the more programmers that were added to a project.\n\nSo we created rules for humans to follow, and enforced them through this wacky new human concept of object-oriented programming.\n\nIf you strip off all the funky language about what an object is and what a class is, the soul of object-oriented programming is just this:\n\nA particular type of data should only be accessible, to a specific set of functions that operate on it.\n\nNow this is not a computer rule. This is a human rule. Sure, you can break this rule anytime you want (with C).\n\nBut if we all agree to follow this rule, then if we do have a bug in our code, we usually have a pretty good idea where to find it. We don’t have to traipse all over our own code base, to find the one function that happens to inadvertently overwrite the wrong memory.\n\nEach programmer has their own area of responsibility. Keeping data accessible only to the functions that operate on it, means that the audio programmer will only access audio data, and the graphics programmer will only access graphics data.\n\nSo, since we have agreed to follow this rule, we can now add a bunch of programmers to a project, without having them all constantly undoing one another’s work! What a novel concept.\n\nUnderstanding virtual method tables\n, and their relationship to classes, is probably the second-most difficult concept in object-oriented programming.\n\nBut once you’ve got that down, you can actually deduce almost all of how object-oriented programming works.\n\nHow did I understand object-oriented programming finally? Well, I wrote spaghetti code for about ten years, like everyone else did. And often I couldn’t go back and read my own code and figure out what the hell I was trying to do.\n\nSo in 1990, I was taking a compiler class at Harvard, and a few people were talking about this wacky new human concept of object-oriented programming. So I wrote a small compiler for an object-oriented language.\n\nAnyway, once those concepts baked themselves into my brain, my whole approach to data abstraction changed. I started thinking in objects.\n\nIt was only living without object-oriented programming, that I could see the advantages of using it.', 'aiModelVersion': '1'}",0.9686
Tom Crosley,Updated 9mo,What is the most impressive thing you have ever coded in assembly language?,"Back in 1986, Apple was about to release the Apple IIgs, the “gs” standing for enhanced graphics and sound capabilities compared to the earlier Apple II computers. It used a new processor called the 65C816, which was a 16-bit version of the 6502 used in earlier machines.

Apple wanted a demo that would show off the new capabilities. So they hired me (I had a small consulting business with one employee besides myself) to write an 8 minute sales demo
 that would also be used at the launch event for the new machine. The graphics look pretty primitive today, but this was state of the art for a home computer some 30 years ago.

The problem was that, several months before the launch there were no high-level languages yet available — a C compiler was being written, but it wasn’t ready yet. So we had to write the entire program in assembly language. No choice.

I was primarily hired because even though the 65816 chip was brand new, I was already familiar with it since I had consulted with the chip maker (Western Design Center) on the instruction set before it was finalized (they added an instruction on my recommendation that would make compiler-generated code more efficient).

I wrote all of the code for the animation in assembly, and my employee (Ron Lichty) wrote the code for the sound. I’m not a graphics artist, so they hired someone else to actually create all of the individual graphic elements.

So I first wrote a graphics editor that ran on an existing Apple II that allowed the artist to move these sprites around on a “stage”, and then the motion and timing would be captured as a script which would run in the demo. My editor has eight different planes, so the artist could place one element in back of another, and later my graphics engine would hide parts of the sprites to create the desired effect. (For example in a scene where several large items — including a floor lamp — are being placed inside a file drawer, the artist didn’t have to keep creating smaller and smaller versions of the lamp.)

For the rotating picture of the IIgs, I had to build a turntable and then shoot multiple photos of the machine, one every 22.5 degrees to get 16 frames of animation.

I then wrote the graphics engine to run the demo in 65C816 assembly language. I wrote about 20,000 lines of assembly code in three months — averaging something like 250 to 350 debugged lines of code a day. Meanwhile Ron was writing the sound engine.

It was pretty intense — the last week or so when we were going through final testing and with Apple employees, we literally moved into Apple headquarters in Cupertino, sleeping in sleeping bags at night in their office, working 18+ hours a day, and having virtually all of our food brought in.

Afterwards, Ron and I coauthored a two-part article for a leading Apple II magazine on our efforts to create the Apple IIgs demo. As a result, they sent a photographer to my office and our picture was put on the front cover.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pakerqdwyst91l3z', 'title': 'What is the most impressive thing you have ever coded in assembly language?', 'score': {'original': 0.49995, 'ai': 0.50005}, 'blocks': [{'text': 'Back in 1986, Apple was about to release the Apple IIgs, the “gs” standing for enhanced graphics and sound capabilities compared to the earlier Apple II computers. It used a new processor called the 65C816, which was a 16-bit version of the 6502 used in earlier machines.\n\nApple wanted a demo that would show off the new capabilities. So they hired me (I had a small consulting business with one employee besides myself) to write an 8 minute sales demo\n that would also be used at the launch event for the new machine. The graphics look pretty primitive today, but this was state of the art for a home computer some 30 years ago.\n\nThe problem was that, several months before the launch there were no high-level languages yet available — a C compiler was being written, but it wasn’t ready yet. So we had to write the entire program in assembly language. No choice.\n\nI was primarily hired because even though the 65816 chip was brand new, I was already familiar with it since I had consulted with the chip maker (Western Design Center) on the instruction set before it was finalized (they added an instruction on my recommendation that would make compiler-generated code more efficient).\n\nI wrote all of the code for the animation in assembly, and my employee (Ron Lichty) wrote the code for the sound. I’m not a graphics artist, so they hired someone else to actually create all of the individual graphic elements.\n\nSo I first wrote a graphics editor that ran on an existing Apple II that allowed the artist to move these sprites around on a “stage”, and then the motion and timing would be captured as a script which would run in the demo. My editor has eight different planes, so the artist could place one element in back of another, and later my graphics engine would hide parts of the sprites to create the desired effect. (For example in a scene where several large items — including a floor lamp — are being placed inside a file drawer, the artist didn’t have to keep creating smaller and smaller versions of the lamp.)\n\nFor the rotating picture of the IIgs, I had to build a turntable and then shoot multiple photos of the machine, one every 22.5 degrees to get 16 frames of animation.\n\nI then wrote the graphics engine to run the demo in 65C816 assembly language. I wrote about 20,000 lines of assembly code in three months — averaging something like 250 to 350 debugged lines of code a day. Meanwhile Ron was writing the sound engine.\n\nIt was pretty intense — the last week or so when we were going through final testing and with Apple employees, we literally moved into Apple headquarters in Cupertino, sleeping in sleeping bags at night in their office, working 18+ hours a day, and having virtually all of our food brought in.\n\nAfterwards, Ron and I coauthored a two-part article for a leading Apple II magazine on our efforts to create the Apple IIgs demo. As a result, they sent a photographer to my', 'result': {'fake': 0.1194, 'real': 0.8806}, 'status': 'success'}, {'text': 'office and our picture was put on the front cover.', 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 6, 'credits': 1986108, 'subscription': 0, 'content': 'Back in 1986, Apple was about to release the Apple IIgs, the “gs” standing for enhanced graphics and sound capabilities compared to the earlier Apple II computers. It used a new processor called the 65C816, which was a 16-bit version of the 6502 used in earlier machines.\n\nApple wanted a demo that would show off the new capabilities. So they hired me (I had a small consulting business with one employee besides myself) to write an 8 minute sales demo\n that would also be used at the launch event for the new machine. The graphics look pretty primitive today, but this was state of the art for a home computer some 30 years ago.\n\nThe problem was that, several months before the launch there were no high-level languages yet available — a C compiler was being written, but it wasn’t ready yet. So we had to write the entire program in assembly language. No choice.\n\nI was primarily hired because even though the 65816 chip was brand new, I was already familiar with it since I had consulted with the chip maker (Western Design Center) on the instruction set before it was finalized (they added an instruction on my recommendation that would make compiler-generated code more efficient).\n\nI wrote all of the code for the animation in assembly, and my employee (Ron Lichty) wrote the code for the sound. I’m not a graphics artist, so they hired someone else to actually create all of the individual graphic elements.\n\nSo I first wrote a graphics editor that ran on an existing Apple II that allowed the artist to move these sprites around on a “stage”, and then the motion and timing would be captured as a script which would run in the demo. My editor has eight different planes, so the artist could place one element in back of another, and later my graphics engine would hide parts of the sprites to create the desired effect. (For example in a scene where several large items — including a floor lamp — are being placed inside a file drawer, the artist didn’t have to keep creating smaller and smaller versions of the lamp.)\n\nFor the rotating picture of the IIgs, I had to build a turntable and then shoot multiple photos of the machine, one every 22.5 degrees to get 16 frames of animation.\n\nI then wrote the graphics engine to run the demo in 65C816 assembly language. I wrote about 20,000 lines of assembly code in three months — averaging something like 250 to 350 debugged lines of code a day. Meanwhile Ron was writing the sound engine.\n\nIt was pretty intense — the last week or so when we were going through final testing and with Apple employees, we literally moved into Apple headquarters in Cupertino, sleeping in sleeping bags at night in their office, working 18+ hours a day, and having virtually all of our food brought in.\n\nAfterwards, Ron and I coauthored a two-part article for a leading Apple II magazine on our efforts to create the Apple IIgs demo. As a result, they sent a photographer to my office and our picture was put on the front cover.', 'aiModelVersion': '1'}",0.49995
S M,6y,What is a neural network in layman’s terms?,"Let’s detect sarcasm. Very simple problem, right? (I just went meta.) Okay. Let’s look at a couple of sarcastic product reviews. Intuitively, if a review has a positive sentiment but a low rating, then it’s probably sarcastic. Examples:“I was tired of getting hit on by beautiful women. After I bought this jacket, problem solved!” (Rating: 0.5/5)“Great burrito, now actually try cooking the beans.” (Rating: 1/5)You may have noticed that the sentiment of the reviews are positive (“problem solved”, “great”), but the ratings are low. That seems like a sign of sarcasm. Now that we suspect there is some relationship between {sentiment, rating} and {sarcasm}, we list down some data points: Sentiment (+1 for positive, 0 for neutral, -1 for negative), Rating (0 to 5), Sarcasm (1 for Yes, 0 for No) (Sentiment, Rating, Sarcasm)(1, 0.5, 1)(1, 1, 1)(1, 5, 0)(-1, 4, 1)(-1, 1, 0)... and a few thousand more. So, to find out the actual relationship, we want to work on sentiment and rating values to somehow get the value of sarcasm. We will use layers as steps to move from inputs to output. Let’s look at the first example (1, 0.5, 1):Each line in that network has a weight. We will use those weights to calculate the values in the circles in the hidden layer and the output layer (which we hope will be ‘1’). Initially we assign weights randomly:Now we have our initial stupid neural network. Let’s see what the output will be. At each circle (aka “neuron”) in the hidden and output layer, we multiply their inputs with the corresponding weights and sum up the results. Hidden Layer 1st Neuron = (1∗0.2)+(0.5∗0.4)=0.4(1∗0.2)+(0.5∗0.4)=0.4(1 * 0.2) + (0.5 * 0.4) = 0.4  Hidden Layer 2nd Neuron =(1∗0.3)+(0.5∗0.6)=0.6(1∗0.3)+(0.5∗0.6)=0.6 (1 * 0.3) + (0.5 * 0.6) = 0.6  Hidden Layer 3rd Neuron =(1∗0.4)+(0.5∗0.7)=0.75(1∗0.4)+(0.5∗0.7)=0.75 (1 * 0.4) + (0.5 * 0.7) = 0.75 Also, we want the output (Sarcasm) to be a number between 0 and 1 (because nothing else makes sense). We do this by using a magic function on the output layer, that reduces any given number to a number between 0 and 1. Any function we use at any neuron is called the activation function and in this case, we use the sigmoid function on the output layer. Final Layer = (0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4 * 0.3) + (0.6 * 0.4) + (0.75 * 0.5) = 0.735 Output = sigmoid(0.735)sigmoid(0.735)sigmoid(0.735) =0.324=0.324= 0.324 So, we have an output 0.324. But we were expecting 1! So, what do we do? We change the weights slightly to nudge the output towards the correct value. We do this using a method called Back propagation, which is explained in this blog. We repeat this thousands of times covering all the training data, changing the weights slightly every time. Eventually, we’ll get the ‘right’ weights which will best predict sarcasm, given sentiment and rating. That’s it! Most applications of neural networks that you see, are variations of the above neural network with changes in:The structure of inputs and outputs (duh).The number of hidden layers/neurons.How the neurons are connected.The training process.The activation function.… and some other hyper parameters. And in case you haven’t noticed, logistic regression is just a one-layer neural network. Whaaaaaa More importantly, can we all take a moment here to appreciate how perfectly circular those circles in my diagrams are? :)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pcgkodsyihqu47f0', 'title': 'What is a neural network in layman’s terms?', 'score': {'original': 0.9752, 'ai': 0.0248}, 'blocks': [{'text': 'Let’s detect sarcasm. Very simple problem, right? (I just went meta.) Okay. Let’s look at a couple of sarcastic product reviews. Intuitively, if a review has a positive sentiment but a low rating, then it’s probably sarcastic. Examples:“I was tired of getting hit on by beautiful women. After I bought this jacket, problem solved!” (Rating: 0.5/5)“Great burrito, now actually try cooking the beans.” (Rating: 1/5)You may have noticed that the sentiment of the reviews are positive (“problem solved”, “great”), but the ratings are low. That seems like a sign of sarcasm. Now that we suspect there is some relationship between {sentiment, rating} and {sarcasm}, we list down some data points: Sentiment (+1 for positive, 0 for neutral, -1 for negative), Rating (0 to 5), Sarcasm (1 for Yes, 0 for No) (Sentiment, Rating, Sarcasm)(1, 0.5, 1)(1, 1, 1)(1, 5, 0)(-1, 4, 1)(-1, 1, 0)... and a few thousand more. So, to find out the actual relationship, we want to work on sentiment and rating values to somehow get the value of sarcasm. We will use layers as steps to move from inputs to output. Let’s look at the first example (1, 0.5, 1):Each line in that network has a weight. We will use those weights to calculate the values in the circles in the hidden layer and the output layer (which we hope will be ‘1’). Initially we assign weights randomly:Now we have our initial stupid neural network. Let’s see what the output will be. At each circle (aka “neuron”) in the hidden and output layer, we multiply their inputs with the corresponding weights and sum up the results. Hidden Layer 1st Neuron = (1∗0.2)+(0.5∗0.4)=0.4(1∗0.2)+(0.5∗0.4)=0.4(1 * 0.2) + (0.5 * 0.4) = 0.4  Hidden Layer 2nd Neuron =(1∗0.3)+(0.5∗0.6)=0.6(1∗0.3)+(0.5∗0.6)=0.6 (1 * 0.3) + (0.5 * 0.6) = 0.6  Hidden Layer 3rd Neuron =(1∗0.4)+(0.5∗0.7)=0.75(1∗0.4)+(0.5∗0.7)=0.75 (1 * 0.4) + (0.5 * 0.7) = 0.75 Also, we want the output (Sarcasm) to be a number between 0 and 1 (because nothing else makes sense). We do this by using a magic function on the output layer, that reduces any given number to a number between 0 and 1. Any function we use at any neuron is called the activation function and in this case, we use the sigmoid function on the output layer. Final Layer = (0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4 * 0.3) + (0.6 * 0.4) + (0.75 * 0.5) = 0.735 Output = sigmoid(0.735)sigmoid(0.735)sigmoid(0.735) =0.324=0.324= 0.324 So, we have an output 0.324. But we were expecting 1! So, what do we do? We change the weights slightly to nudge the output towards the correct value. We do this using a method called Back propagation, which is explained in this blog. We repeat this thousands of times covering all the training data, changing the weights slightly every time. Eventually, we’ll get the ‘right’ weights which will best predict sarcasm, given sentiment and rating. That’s it! Most applications of neural networks that you see, are variations of the above neural network with changes in:The structure of inputs and outputs (duh).The number of hidden layers/neurons.How the neurons are connected.The', 'result': {'fake': 0.0326, 'real': 0.9674}, 'status': 'success'}, {'text': 'training process.The activation function.… and some other hyper parameters. And in case you haven’t noticed, logistic regression is just a one-layer neural network. Whaaaaaa More importantly, can we all take a moment here to appreciate how perfectly circular those circles in my diagrams are? :)', 'result': {'fake': 0.0116, 'real': 0.9884}, 'status': 'success'}], 'credits_used': 6, 'credits': 1986102, 'subscription': 0, 'content': 'Let’s detect sarcasm. Very simple problem, right? (I just went meta.) Okay. Let’s look at a couple of sarcastic product reviews. Intuitively, if a review has a positive sentiment but a low rating, then it’s probably sarcastic. Examples:“I was tired of getting hit on by beautiful women. After I bought this jacket, problem solved!” (Rating: 0.5/5)“Great burrito, now actually try cooking the beans.” (Rating: 1/5)You may have noticed that the sentiment of the reviews are positive (“problem solved”, “great”), but the ratings are low. That seems like a sign of sarcasm. Now that we suspect there is some relationship between {sentiment, rating} and {sarcasm}, we list down some data points: Sentiment (+1 for positive, 0 for neutral, -1 for negative), Rating (0 to 5), Sarcasm (1 for Yes, 0 for No) (Sentiment, Rating, Sarcasm)(1, 0.5, 1)(1, 1, 1)(1, 5, 0)(-1, 4, 1)(-1, 1, 0)... and a few thousand more. So, to find out the actual relationship, we want to work on sentiment and rating values to somehow get the value of sarcasm. We will use layers as steps to move from inputs to output. Let’s look at the first example (1, 0.5, 1):Each line in that network has a weight. We will use those weights to calculate the values in the circles in the hidden layer and the output layer (which we hope will be ‘1’). Initially we assign weights randomly:Now we have our initial stupid neural network. Let’s see what the output will be. At each circle (aka “neuron”) in the hidden and output layer, we multiply their inputs with the corresponding weights and sum up the results. Hidden Layer 1st Neuron = (1∗0.2)+(0.5∗0.4)=0.4(1∗0.2)+(0.5∗0.4)=0.4(1 * 0.2) + (0.5 * 0.4) = 0.4  Hidden Layer 2nd Neuron =(1∗0.3)+(0.5∗0.6)=0.6(1∗0.3)+(0.5∗0.6)=0.6 (1 * 0.3) + (0.5 * 0.6) = 0.6  Hidden Layer 3rd Neuron =(1∗0.4)+(0.5∗0.7)=0.75(1∗0.4)+(0.5∗0.7)=0.75 (1 * 0.4) + (0.5 * 0.7) = 0.75 Also, we want the output (Sarcasm) to be a number between 0 and 1 (because nothing else makes sense). We do this by using a magic function on the output layer, that reduces any given number to a number between 0 and 1. Any function we use at any neuron is called the activation function and in this case, we use the sigmoid function on the output layer. Final Layer = (0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4 * 0.3) + (0.6 * 0.4) + (0.75 * 0.5) = 0.735 Output = sigmoid(0.735)sigmoid(0.735)sigmoid(0.735) =0.324=0.324= 0.324 So, we have an output 0.324. But we were expecting 1! So, what do we do? We change the weights slightly to nudge the output towards the correct value. We do this using a method called Back propagation, which is explained in this blog. We repeat this thousands of times covering all the training data, changing the weights slightly every time. Eventually, we’ll get the ‘right’ weights which will best predict sarcasm, given sentiment and rating. That’s it! Most applications of neural networks that you see, are variations of the above neural network with changes in:The structure of inputs and outputs (duh).The number of hidden layers/neurons.How the neurons are connected.The training process.The activation function.… and some other hyper parameters. And in case you haven’t noticed, logistic regression is just a one-layer neural network. Whaaaaaa More importantly, can we all take a moment here to appreciate how perfectly circular those circles in my diagrams are? :)', 'aiModelVersion': '1'}",0.9752
Thomas Cormen,7y,"Is CLRS really an ""introduction""? If so, what's next?","Yes, Introduction to Algorithms really is an introductory text—on algorithms. It is not, and was never intended to be, an introductory text for computer science. We assume that the reader has some programming experience, including recursion, and knows how to read and write rigorous mathematical proofs. The discrete mathematics facts needed to analyze the algorithms in the book appear in the appendices.

As an algorithms text, it starts with insertion sort, one of the simplest of sorting algorithms. The algorithms and data structures covered in the first few sections are, for the most part, among the most basic. I think of the material in the first six parts of the book (except for the starred sections, which we consider graduate-level material) as part of the canon of computer science.

I understand that some people consider the book to be beyond introductory. We do go deeply into some material, and we pull no punches in the mathematics. From where the material starts, however, the book is most definitely an introduction.

What’s next? Don Knuth’s The Art of Computer Programming, which I consider the greatest books (it is a multi-volume set) computer science has ever had or ever will have.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0yh1zuncdfbepg2k', 'title': 'Is CLRS really an ""introduction""? If so, what\'s next?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': 'Yes, Introduction to Algorithms really is an introductory text—on algorithms. It is not, and was never intended to be, an introductory text for computer science. We assume that the reader has some programming experience, including recursion, and knows how to read and write rigorous mathematical proofs. The discrete mathematics facts needed to analyze the algorithms in the book appear in the appendices.\n\nAs an algorithms text, it starts with insertion sort, one of the simplest of sorting algorithms. The algorithms and data structures covered in the first few sections are, for the most part, among the most basic. I think of the material in the first six parts of the book (except for the starred sections, which we consider graduate-level material) as part of the canon of computer science.\n\nI understand that some people consider the book to be beyond introductory. We do go deeply into some material, and we pull no punches in the mathematics. From where the material starts, however, the book is most definitely an introduction.\n\nWhat’s next? Don Knuth’s The Art of Computer Programming, which I consider the greatest books (it is a multi-volume set) computer science has ever had or ever will have.', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986099, 'subscription': 0, 'content': 'Yes, Introduction to Algorithms really is an introductory text—on algorithms. It is not, and was never intended to be, an introductory text for computer science. We assume that the reader has some programming experience, including recursion, and knows how to read and write rigorous mathematical proofs. The discrete mathematics facts needed to analyze the algorithms in the book appear in the appendices.\n\nAs an algorithms text, it starts with insertion sort, one of the simplest of sorting algorithms. The algorithms and data structures covered in the first few sections are, for the most part, among the most basic. I think of the material in the first six parts of the book (except for the starred sections, which we consider graduate-level material) as part of the canon of computer science.\n\nI understand that some people consider the book to be beyond introductory. We do go deeply into some material, and we pull no punches in the mathematics. From where the material starts, however, the book is most definitely an introduction.\n\nWhat’s next? Don Knuth’s The Art of Computer Programming, which I consider the greatest books (it is a multi-volume set) computer science has ever had or ever will have.', 'aiModelVersion': '1'}",0.9995
Rakesh Singh,Updated 6y,What are some real life examples of deadlock?,"10th NCERT Science Book - Doppler Effect (You will study this in higher classes)

11th NCERT Science Book - Doppler Effect (You have studied this in earlier classes)

Edit:

The new NCERT of 11th class has Doppler Effect explained in 1000+ words. Congrats kids!

Link - http://ncert.nic.in/ncerts/l/keph207.pdf","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/d3bpaftk51mue6s4', 'title': 'What are some real life examples of deadlock?', 'score': {'original': 0.9873, 'ai': 0.0127}, 'blocks': [{'text': '10th NCERT Science Book - Doppler Effect (You will study this in higher classes)\n\n11th NCERT Science Book - Doppler Effect (You have studied this in earlier classes)\n\nEdit:\n\nThe new NCERT of 11th class has Doppler Effect explained in 1000+ words. Congrats kids!\n\nLink - http://ncert.nic.in/ncerts/l/keph207.pdf', 'result': {'fake': 0.0127, 'real': 0.9873}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986098, 'subscription': 0, 'content': '10th NCERT Science Book - Doppler Effect (You will study this in higher classes)\n\n11th NCERT Science Book - Doppler Effect (You have studied this in earlier classes)\n\nEdit:\n\nThe new NCERT of 11th class has Doppler Effect explained in 1000+ words. Congrats kids!\n\nLink - http://ncert.nic.in/ncerts/l/keph207.pdf', 'aiModelVersion': '1'}",0.9873
Mercedes R. Lackey,Updated 4y,"Are Terminator and Skynet just a ripoff or extrapolation of ""I Have No Mouth, and I Must Scream"" by Harlan Ellison?","I loved Harlan, but he tended to use the term “ripped off” far too frequently.

You can’t copyright an idea. You can’t trademark an idea. However, the Terminator franchise was indeed inspired by a “The Outer Limits” episode he wrote, and they acknowledged that fact.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/hqcai6rfy4eb5zgd', 'title': 'Are Terminator and Skynet just a ripoff or extrapolation of ""I Have No Mouth, and I Must Scream"" by Harlan Ellison?', 'score': {'original': 0.8372, 'ai': 0.1628}, 'blocks': [{'text': 'I loved Harlan, but he tended to use the term “ripped off” far too frequently.\n\nYou can’t copyright an idea. You can’t trademark an idea. However, the Terminator franchise was indeed inspired by a “The Outer Limits” episode he wrote, and they acknowledged that fact.', 'result': {'fake': 0.1628, 'real': 0.8372}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986097, 'subscription': 0, 'content': 'I loved Harlan, but he tended to use the term “ripped off” far too frequently.\n\nYou can’t copyright an idea. You can’t trademark an idea. However, the Terminator franchise was indeed inspired by a “The Outer Limits” episode he wrote, and they acknowledged that fact.', 'aiModelVersion': '1'}",0.8372
Alon Amit,Updated 2y,What common misunderstandings do people have about data structures and algorithms?,"Here are some more or less common misunderstandings and misconceptions:

The study of data structures and algorithms is only good for coding interviews and competitions. (In fact, while “Real world” coding has many aspects that aren’t data structures and algorithms, a deep understanding of data structures and algorithms is useful and practical in very real situations.)
“NP” means “non-polynomial”. (It doesn’t. It means “non-deterministic polynomial time”.)
NP problems are hard. (No, NP problems are easy. Problems not in NP are much harder.)
There’s a “best” algorithm for X. (There never is. There may be for particular instances of a problem, in particular circumstances, with clearly defined computational resources. This is never the case when such questions or assertions are made.)
Quantum computers are fast because they instantly check all possible combinations, like in parallel universes or something. (This is one of the most pervasive, consistent, utterly false misconceptions in the history of algorithms.)
Anything recursive is Dynamic Programming. (It’s not.)
My problem is NP-complete, so it’s hopeless. (it may be, but it may not. It depends on many things.)
SAT problems cannot be solved. (They sure can, and SAT solvers are incredibly useful in very practical, real-life situations.)
Sorting is 
O
(
n
log
n
)
O(nlog⁡n)
. (Sorting is an algorithmic task. Tasks aren’t 
O
O
 of anything. The time-complexity of a specific algorithm is a function of its input size 
n
n
, and we often approximate this function using big-Oh notation.)
Shorter programs are more efficient. (The correlation between performance and number of lines of code is very, very weak. A few lines of code can be horribly inefficient, or just mildly inefficient, compared with more lines that express a smarter algorithm.)
In machine learning, simpler models are better because Occam’s razor. (This may be true for two models that are otherwise equivalent, but if a more complex model is demonstrably more accurate, don’t automatically throw it away for a simpler and less accurate alternative. This is sometimes the right choice, but often it’s not.)
Governments are racing to build quantum computers because the first to succeed will be able to hack into any system it wants. (This was actually stated by several generally respectable news publications. It’s nonsense.)
Once we prove the Riemann Hypothesis, we’ll be able to factor numbers more efficiently, or find primes more efficiently, or break the internet. (No, No and No.)
I’m good at algorithms, but not so much at data structures. (No you’re not. The two are inexorably linked.)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/gmt9pyhqfdk4axzj', 'title': 'What common misunderstandings do people have about data structures and algorithms?', 'score': {'original': 0.9992, 'ai': 0.0008}, 'blocks': [{'text': 'Here are some more or less common misunderstandings and misconceptions:\n\nThe study of data structures and algorithms is only good for coding interviews and competitions. (In fact, while “Real world” coding has many aspects that aren’t data structures and algorithms, a deep understanding of data structures and algorithms is useful and practical in very real situations.)\n“NP” means “non-polynomial”. (It doesn’t. It means “non-deterministic polynomial time”.)\nNP problems are hard. (No, NP problems are easy. Problems not in NP are much harder.)\nThere’s a “best” algorithm for X. (There never is. There may be for particular instances of a problem, in particular circumstances, with clearly defined computational resources. This is never the case when such questions or assertions are made.)\nQuantum computers are fast because they instantly check all possible combinations, like in parallel universes or something. (This is one of the most pervasive, consistent, utterly false misconceptions in the history of algorithms.)\nAnything recursive is Dynamic Programming. (It’s not.)\nMy problem is NP-complete, so it’s hopeless. (it may be, but it may not. It depends on many things.)\nSAT problems cannot be solved. (They sure can, and SAT solvers are incredibly useful in very practical, real-life situations.)\nSorting is \nO\n(\nn\nlog\nn\n)\nO(nlog\u2061n)\n. (Sorting is an algorithmic task. Tasks aren’t \nO\nO\n of anything. The time-complexity of a specific algorithm is a function of its input size \nn\nn\n, and we often approximate this function using big-Oh notation.)\nShorter programs are more efficient. (The correlation between performance and number of lines of code is very, very weak. A few lines of code can be horribly inefficient, or just mildly inefficient, compared with more lines that express a smarter algorithm.)\nIn machine learning, simpler models are better because Occam’s razor. (This may be true for two models that are otherwise equivalent, but if a more complex model is demonstrably more accurate, don’t automatically throw it away for a simpler and less accurate alternative. This is sometimes the right choice, but often it’s not.)\nGovernments are racing to build quantum computers because the first to succeed will be able to hack into any system it wants. (This was actually stated by several generally respectable news publications. It’s nonsense.)\nOnce we prove the Riemann Hypothesis, we’ll be able to factor numbers more efficiently, or find primes more efficiently, or break the internet. (No, No and No.)\nI’m good at algorithms, but not so much at data structures. (No you’re not. The two are inexorably linked.)', 'result': {'fake': 0.0008, 'real': 0.9992}, 'status': 'success'}], 'credits_used': 5, 'credits': 1986092, 'subscription': 0, 'content': 'Here are some more or less common misunderstandings and misconceptions:\n\nThe study of data structures and algorithms is only good for coding interviews and competitions. (In fact, while “Real world” coding has many aspects that aren’t data structures and algorithms, a deep understanding of data structures and algorithms is useful and practical in very real situations.)\n“NP” means “non-polynomial”. (It doesn’t. It means “non-deterministic polynomial time”.)\nNP problems are hard. (No, NP problems are easy. Problems not in NP are much harder.)\nThere’s a “best” algorithm for X. (There never is. There may be for particular instances of a problem, in particular circumstances, with clearly defined computational resources. This is never the case when such questions or assertions are made.)\nQuantum computers are fast because they instantly check all possible combinations, like in parallel universes or something. (This is one of the most pervasive, consistent, utterly false misconceptions in the history of algorithms.)\nAnything recursive is Dynamic Programming. (It’s not.)\nMy problem is NP-complete, so it’s hopeless. (it may be, but it may not. It depends on many things.)\nSAT problems cannot be solved. (They sure can, and SAT solvers are incredibly useful in very practical, real-life situations.)\nSorting is \nO\n(\nn\nlog\nn\n)\nO(nlog\u2061n)\n. (Sorting is an algorithmic task. Tasks aren’t \nO\nO\n of anything. The time-complexity of a specific algorithm is a function of its input size \nn\nn\n, and we often approximate this function using big-Oh notation.)\nShorter programs are more efficient. (The correlation between performance and number of lines of code is very, very weak. A few lines of code can be horribly inefficient, or just mildly inefficient, compared with more lines that express a smarter algorithm.)\nIn machine learning, simpler models are better because Occam’s razor. (This may be true for two models that are otherwise equivalent, but if a more complex model is demonstrably more accurate, don’t automatically throw it away for a simpler and less accurate alternative. This is sometimes the right choice, but often it’s not.)\nGovernments are racing to build quantum computers because the first to succeed will be able to hack into any system it wants. (This was actually stated by several generally respectable news publications. It’s nonsense.)\nOnce we prove the Riemann Hypothesis, we’ll be able to factor numbers more efficiently, or find primes more efficiently, or break the internet. (No, No and No.)\nI’m good at algorithms, but not so much at data structures. (No you’re not. The two are inexorably linked.)', 'aiModelVersion': '1'}",0.9992
Alan Kay,Updated 3y,Is there a programming language that uses past and future tense?,"Some very confident (why?) answers here about “no, there aren’t programming languages with past tense”. Or “there isn’t any reason for such a thing”.

But take a look at Lucid, by Wadge and Ashcroft. Its variables are histories of the values, so the various kinds of past tense can be used. Lucid (programming language) - Wikipedia

Also, there have been a number of experiments and proposals for “world-lines” in programming, especially “good old real AI” programming. These go all the way back to John McCarthy’s “Situations, Actions, and Causal Laws” papers in the 60s. A “situation” is a past whole state of a system, and it is reached by a “past tense” operation.

These ideas are important on a number of fronts (for example, think of the many ways that a “general UNDO” at any granularity could vastly help programming and debugging). Both databases (usually) and user interfaces (sometimes) have unlimited undoes and looks at the historical past, but this is also very useful for finer grained programming, AI, etc.

Histories and tenses can be implemented in languages with a decent meta-framework (e.g. Lisp, Smalltalk, etc.). Here is a paper about such an experiment: http://www.vpri.org/pdf/tr2011001_final_worlds.pdf","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/l8sy0e43xob5mht6', 'title': 'Is there a programming language that uses past and future tense?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Some very confident (why?) answers here about “no, there aren’t programming languages with past tense”. Or “there isn’t any reason for such a thing”.\n\nBut take a look at Lucid, by Wadge and Ashcroft. Its variables are histories of the values, so the various kinds of past tense can be used. Lucid (programming language) - Wikipedia\n\nAlso, there have been a number of experiments and proposals for “world-lines” in programming, especially “good old real AI” programming. These go all the way back to John McCarthy’s “Situations, Actions, and Causal Laws” papers in the 60s. A “situation” is a past whole state of a system, and it is reached by a “past tense” operation.\n\nThese ideas are important on a number of fronts (for example, think of the many ways that a “general UNDO” at any granularity could vastly help programming and debugging). Both databases (usually) and user interfaces (sometimes) have unlimited undoes and looks at the historical past, but this is also very useful for finer grained programming, AI, etc.\n\nHistories and tenses can be implemented in languages with a decent meta-framework (e.g. Lisp, Smalltalk, etc.). Here is a paper about such an experiment: http://www.vpri.org/pdf/tr2011001_final_worlds.pdf', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986089, 'subscription': 0, 'content': 'Some very confident (why?) answers here about “no, there aren’t programming languages with past tense”. Or “there isn’t any reason for such a thing”.\n\nBut take a look at Lucid, by Wadge and Ashcroft. Its variables are histories of the values, so the various kinds of past tense can be used. Lucid (programming language) - Wikipedia\n\nAlso, there have been a number of experiments and proposals for “world-lines” in programming, especially “good old real AI” programming. These go all the way back to John McCarthy’s “Situations, Actions, and Causal Laws” papers in the 60s. A “situation” is a past whole state of a system, and it is reached by a “past tense” operation.\n\nThese ideas are important on a number of fronts (for example, think of the many ways that a “general UNDO” at any granularity could vastly help programming and debugging). Both databases (usually) and user interfaces (sometimes) have unlimited undoes and looks at the historical past, but this is also very useful for finer grained programming, AI, etc.\n\nHistories and tenses can be implemented in languages with a decent meta-framework (e.g. Lisp, Smalltalk, etc.). Here is a paper about such an experiment: http://www.vpri.org/pdf/tr2011001_final_worlds.pdf', 'aiModelVersion': '1'}",0.9998
Brett Bergan,2y,What are the benefits of using a smaller processor size during CPU manufacturing?,"Let’s go into business together. We’re going to make a an eight core ARM CPU at 14nm that is 10mm x 10mm.

This is how our hypothetical CPU turns out on silicon with a 12″ wafer.

Our projected defect density of one failure point per square cm is sadly a realistic one, leaving us with more bad CPU dies than good ones.

If we design our 8-core ARM CPU with the modularity to disable a pair of CPU cores, we can recover about half of our defective processors and turn them into 6-core processors, letting us get 400 usable dies rather than only 230. Maybe we can also salvage some quad cores as well.

Perhaps we end up with

8-core processors: 230
6-core processors: 175
4-core processors: 95

Now, watch what happens when we use a 15 x 15 CPU die.

Almost every CPU die has a defect. There are only 40 usable ones. Many of the others can be salvaged, but the proportion of dies with multiple defects goes up, meaning those will almost certainly be scrapped. In all we may end up with 150 good processors.

If a single 12″ wafer costs us $20,000, we’d be paying an average $133 for each good piece of silicon, added to the cost of manufacturing it into a functional product. That would be a disaster.

With the 10x10 processor, we end up with 500 good pieces of silicon. The average price drops to $40. If every CPU we sell is $99 or more, we still make a profit and stay in business.

Let’s look at one more possibility.

This is a 72mm­­² chiplet with eight cores. There is nothing on these chiplets that is not redundant, so every chip with a defect can be used. You put the 412 good dies on your 8-core CPU, and the ones with a defect on your 6-core CPU. The rare chiplet with two defects can go on a 4-core CPU. You basically get 800 good chiplets from a 12″ wafer, at a cost of $25 each.

Do the same thing with your “system agent” on a lower cost fab somewhere else and you have solved your yield problems with the added manufacturing cost of putting two (or three) chiplets on one CPU.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/s865dtbvqel70o3w', 'title': 'What are the benefits of using a smaller processor size during CPU manufacturing?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'Let’s go into business together. We’re going to make a an eight core ARM CPU at 14nm that is 10mm x 10mm.\n\nThis is how our hypothetical CPU turns out on silicon with a 12″ wafer.\n\nOur projected defect density of one failure point per square cm is sadly a realistic one, leaving us with more bad CPU dies than good ones.\n\nIf we design our 8-core ARM CPU with the modularity to disable a pair of CPU cores, we can recover about half of our defective processors and turn them into 6-core processors, letting us get 400 usable dies rather than only 230. Maybe we can also salvage some quad cores as well.\n\nPerhaps we end up with\n\n8-core processors: 230\n6-core processors: 175\n4-core processors: 95\n\nNow, watch what happens when we use a 15 x 15 CPU die.\n\nAlmost every CPU die has a defect. There are only 40 usable ones. Many of the others can be salvaged, but the proportion of dies with multiple defects goes up, meaning those will almost certainly be scrapped. In all we may end up with 150 good processors.\n\nIf a single 12″ wafer costs us $20,000, we’d be paying an average $133 for each good piece of silicon, added to the cost of manufacturing it into a functional product. That would be a disaster.\n\nWith the 10x10 processor, we end up with 500 good pieces of silicon. The average price drops to $40. If every CPU we sell is $99 or more, we still make a profit and stay in business.\n\nLet’s look at one more possibility.\n\nThis is a 72mm\xad\xad² chiplet with eight cores. There is nothing on these chiplets that is not redundant, so every chip with a defect can be used. You put the 412 good dies on your 8-core CPU, and the ones with a defect on your 6-core CPU. The rare chiplet with two defects can go on a 4-core CPU. You basically get 800 good chiplets from a 12″ wafer, at a cost of $25 each.\n\nDo the same thing with your “system agent” on a lower cost fab somewhere else and you have solved your yield problems with the added manufacturing cost of putting two (or three) chiplets on one CPU.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986085, 'subscription': 0, 'content': 'Let’s go into business together. We’re going to make a an eight core ARM CPU at 14nm that is 10mm x 10mm.\n\nThis is how our hypothetical CPU turns out on silicon with a 12″ wafer.\n\nOur projected defect density of one failure point per square cm is sadly a realistic one, leaving us with more bad CPU dies than good ones.\n\nIf we design our 8-core ARM CPU with the modularity to disable a pair of CPU cores, we can recover about half of our defective processors and turn them into 6-core processors, letting us get 400 usable dies rather than only 230. Maybe we can also salvage some quad cores as well.\n\nPerhaps we end up with\n\n8-core processors: 230\n6-core processors: 175\n4-core processors: 95\n\nNow, watch what happens when we use a 15 x 15 CPU die.\n\nAlmost every CPU die has a defect. There are only 40 usable ones. Many of the others can be salvaged, but the proportion of dies with multiple defects goes up, meaning those will almost certainly be scrapped. In all we may end up with 150 good processors.\n\nIf a single 12″ wafer costs us $20,000, we’d be paying an average $133 for each good piece of silicon, added to the cost of manufacturing it into a functional product. That would be a disaster.\n\nWith the 10x10 processor, we end up with 500 good pieces of silicon. The average price drops to $40. If every CPU we sell is $99 or more, we still make a profit and stay in business.\n\nLet’s look at one more possibility.\n\nThis is a 72mm\xad\xad² chiplet with eight cores. There is nothing on these chiplets that is not redundant, so every chip with a defect can be used. You put the 412 good dies on your 8-core CPU, and the ones with a defect on your 6-core CPU. The rare chiplet with two defects can go on a 4-core CPU. You basically get 800 good chiplets from a 12″ wafer, at a cost of $25 each.\n\nDo the same thing with your “system agent” on a lower cost fab somewhere else and you have solved your yield problems with the added manufacturing cost of putting two (or three) chiplets on one CPU.', 'aiModelVersion': '1'}",0.9997
Geoffrey Widdison,2y,The symbol for the save button on computers got carried forward to this day long beyond the thing it's based on (floppy disk) becoming obsolete. What other concepts do we have today that are represented by things that are obsolete?,"A set of real time indicators will often be colloquially referred to as a “dashboard"", even if it's on a computer. Most people know that term comes from the dashboard of a motor vehicle.

What most people don't know is that the term “dashboard” itself actually comes from the horse-and-buggy era. If horses were going fast on muddy streets, they'd “dash”, or fling, mud back with their hooves and often hit the driver. So it became common to place a wooden board in front of the driver to catch as much mud as possible.

That name has now jumped technologies twice, from buggies to cars, and from cars to computers, keeping the name, but almost totally divorced from its original meaning.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/i2v80tlu7xerqd39', 'title': ""The symbol for the save button on computers got carried forward to this day long beyond the thing it's based on (floppy disk) becoming obsolete. What other concepts do we have today that are represented by things that are obsolete?"", 'score': {'original': 0.9942, 'ai': 0.0058}, 'blocks': [{'text': 'A set of real time indicators will often be colloquially referred to as a “dashboard"", even if it\'s on a computer. Most people know that term comes from the dashboard of a motor vehicle.\n\nWhat most people don\'t know is that the term “dashboard” itself actually comes from the horse-and-buggy era. If horses were going fast on muddy streets, they\'d “dash”, or fling, mud back with their hooves and often hit the driver. So it became common to place a wooden board in front of the driver to catch as much mud as possible.\n\nThat name has now jumped technologies twice, from buggies to cars, and from cars to computers, keeping the name, but almost totally divorced from its original meaning.', 'result': {'fake': 0.0097, 'real': 0.9903}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986083, 'subscription': 0, 'content': 'A set of real time indicators will often be colloquially referred to as a “dashboard"", even if it\'s on a computer. Most people know that term comes from the dashboard of a motor vehicle.\n\nWhat most people don\'t know is that the term “dashboard” itself actually comes from the horse-and-buggy era. If horses were going fast on muddy streets, they\'d “dash”, or fling, mud back with their hooves and often hit the driver. So it became common to place a wooden board in front of the driver to catch as much mud as possible.\n\nThat name has now jumped technologies twice, from buggies to cars, and from cars to computers, keeping the name, but almost totally divorced from its original meaning.', 'aiModelVersion': '1'}",0.9942
Giuseppe Frisella,1y,Is a disk capable of storing information for billions of years possible?,"Yes. A small disc has been invented that can store up to 360 TB in very little space, for a period of time estimated in the billions of years.

Using a very intense pulsed laser that lasts a quadrillionth of a second, scientists were able to record data into a quartz glass disc in the form of the coordinates, size and orientation of tiny holes. Five pieces of information per hole, hence the name 5D optical storage.

Various human works have been encoded in these discs, including the Universal Declaration of Human Rights, the Magna Carta, the Bible and the writings of Isaac Newton.

This system, extremely resilient and high in storage capacity, but difficult and expensive to write and read, will not replace those used in today's consumer electronics. Also because once incised, they cannot be modified.

They are rather to be used as emergency backups, given their longevity and ability to withstand temperatures of thousands of degrees.

Reading the disc with an optical microscope and a polarizer is easier than writing on it, so in the future this technology could find applications in libraries and archives where only reading information is necessary.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/1o9v743cnjsklwqd', 'title': 'Is a disk capable of storing information for billions of years possible?', 'score': {'original': 0.9681, 'ai': 0.0319}, 'blocks': [{'text': ""Yes. A small disc has been invented that can store up to 360 TB in very little space, for a period of time estimated in the billions of years.\n\nUsing a very intense pulsed laser that lasts a quadrillionth of a second, scientists were able to record data into a quartz glass disc in the form of the coordinates, size and orientation of tiny holes. Five pieces of information per hole, hence the name 5D optical storage.\n\nVarious human works have been encoded in these discs, including the Universal Declaration of Human Rights, the Magna Carta, the Bible and the writings of Isaac Newton.\n\nThis system, extremely resilient and high in storage capacity, but difficult and expensive to write and read, will not replace those used in today's consumer electronics. Also because once incised, they cannot be modified.\n\nThey are rather to be used as emergency backups, given their longevity and ability to withstand temperatures of thousands of degrees.\n\nReading the disc with an optical microscope and a polarizer is easier than writing on it, so in the future this technology could find applications in libraries and archives where only reading information is necessary."", 'result': {'fake': 0.0319, 'real': 0.9681}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986081, 'subscription': 0, 'content': ""Yes. A small disc has been invented that can store up to 360 TB in very little space, for a period of time estimated in the billions of years.\n\nUsing a very intense pulsed laser that lasts a quadrillionth of a second, scientists were able to record data into a quartz glass disc in the form of the coordinates, size and orientation of tiny holes. Five pieces of information per hole, hence the name 5D optical storage.\n\nVarious human works have been encoded in these discs, including the Universal Declaration of Human Rights, the Magna Carta, the Bible and the writings of Isaac Newton.\n\nThis system, extremely resilient and high in storage capacity, but difficult and expensive to write and read, will not replace those used in today's consumer electronics. Also because once incised, they cannot be modified.\n\nThey are rather to be used as emergency backups, given their longevity and ability to withstand temperatures of thousands of degrees.\n\nReading the disc with an optical microscope and a polarizer is easier than writing on it, so in the future this technology could find applications in libraries and archives where only reading information is necessary."", 'aiModelVersion': '1'}",0.9681
Ben Podgursky,4y,"In computer science, what is the ""two generals problem""?","There is no algorithm by which two agents, separated by time and unreliable means of communication, can ever come up with a plan that requires both agents to act on the plan and guarantees success.

The original formulation was:

“Two generals are on opposite sides of a city. If they both attack at dawn, the attack will succeed. If either of them attacks without support, the attack will fail with great loss of life. Messengers sent between the camps are sometimes intercepted and lost.

Can they communicate with each other and come up with a plan which guarantees that they will either both attack at dawn, or neither will attack?”

And the answer is no, there is no such algorithm. Your only choice is to increase the reliability of your signal-passing and hope for the best.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/mi36quj8ldhxp5f0', 'title': 'In computer science, what is the ""two generals problem""?', 'score': {'original': 0.5168, 'ai': 0.4832}, 'blocks': [{'text': 'There is no algorithm by which two agents, separated by time and unreliable means of communication, can ever come up with a plan that requires both agents to act on the plan and guarantees success.\n\nThe original formulation was:\n\n“Two generals are on opposite sides of a city. If they both attack at dawn, the attack will succeed. If either of them attacks without support, the attack will fail with great loss of life. Messengers sent between the camps are sometimes intercepted and lost.\n\nCan they communicate with each other and come up with a plan which guarantees that they will either both attack at dawn, or neither will attack?”\n\nAnd the answer is no, there is no such algorithm. Your only choice is to increase the reliability of your signal-passing and hope for the best.', 'result': {'fake': 0.4832, 'real': 0.5168}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986079, 'subscription': 0, 'content': 'There is no algorithm by which two agents, separated by time and unreliable means of communication, can ever come up with a plan that requires both agents to act on the plan and guarantees success.\n\nThe original formulation was:\n\n“Two generals are on opposite sides of a city. If they both attack at dawn, the attack will succeed. If either of them attacks without support, the attack will fail with great loss of life. Messengers sent between the camps are sometimes intercepted and lost.\n\nCan they communicate with each other and come up with a plan which guarantees that they will either both attack at dawn, or neither will attack?”\n\nAnd the answer is no, there is no such algorithm. Your only choice is to increase the reliability of your signal-passing and hope for the best.', 'aiModelVersion': '1'}",0.5168
Brian Bi,11y,Have there been any new brilliant computer science algorithms in last 10 years?,"I believe just last year it was found that the maximum network flow problem can be solved in O(VE)O(VE)O(VE) time. This bound was first achieved for dense graphs, i.e., E∈Ω(V2)E∈Ω(V2)E \in \Omega(V^2), by the relabel-to-front algorithm, a particular version of the push-preflow method, which runs in O(V3)O(V3)O(V^3) time. This algorithm was published by Goldberg and Tarjan in 1986, and you can find it in CLRS. In 1994, King, Rao, and Tarjan gave an algorithm that runs in O(VElog(V)/log(EVlogV))O(VElog⁡(V)/log⁡(EVlog⁡V))O\left(VE \log(V)/\log\left(\frac{E}{V \log V}\right)\right) time. This reduces to O(VE)O(VE)O(VE) for all graphs that are not too sparse, i.e., E∈Ω(V1+ϵ)E∈Ω(V1+ϵ)E \in \Omega(V^{1+\epsilon}). (Graphs with constant or logarithmic average degree are still a bit too sparse.) Finally, in 2012, Orlin put the final piece in place, finding an algorithm that runs in O(VE+E31/16log2V)O(VE+E31/16log2⁡V)O(VE + E^{31/16} \log^2 V). For E∈O(V16/15−ϵ)E∈O(V16/15−ϵ)E \in O(V^{16/15-\epsilon}), that is, graphs that are sparse enough, this reduces to O(VE)O(VE)O(VE). By combining the algorithm of King, Rao, and Tarjan, and Orlin's algorithm, we see that maximum network flow can be solved in O(VE)O(VE)O(VE) time. I think that's pretty awesome.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/sxrbe4cqnoldw5i2', 'title': 'Have there been any new brilliant computer science algorithms in last 10 years?', 'score': {'original': 0.0317, 'ai': 0.9683}, 'blocks': [{'text': ""I believe just last year it was found that the maximum network flow problem can be solved in O(VE)O(VE)O(VE) time. This bound was first achieved for dense graphs, i.e., E∈Ω(V2)E∈Ω(V2)E \\in \\Omega(V^2), by the relabel-to-front algorithm, a particular version of the push-preflow method, which runs in O(V3)O(V3)O(V^3) time. This algorithm was published by Goldberg and Tarjan in 1986, and you can find it in CLRS. In 1994, King, Rao, and Tarjan gave an algorithm that runs in O(VElog(V)/log(EVlogV))O(VElog\u2061(V)/log\u2061(EVlog\u2061V))O\\left(VE \\log(V)/\\log\\left(\\frac{E}{V \\log V}\\right)\\right) time. This reduces to O(VE)O(VE)O(VE) for all graphs that are not too sparse, i.e., E∈Ω(V1+ϵ)E∈Ω(V1+ϵ)E \\in \\Omega(V^{1+\\epsilon}). (Graphs with constant or logarithmic average degree are still a bit too sparse.) Finally, in 2012, Orlin put the final piece in place, finding an algorithm that runs in O(VE+E31/16log2V)O(VE+E31/16log2\u2061V)O(VE + E^{31/16} \\log^2 V). For E∈O(V16/15−ϵ)E∈O(V16/15−ϵ)E \\in O(V^{16/15-\\epsilon}), that is, graphs that are sparse enough, this reduces to O(VE)O(VE)O(VE). By combining the algorithm of King, Rao, and Tarjan, and Orlin's algorithm, we see that maximum network flow can be solved in O(VE)O(VE)O(VE) time. I think that's pretty awesome."", 'result': {'fake': 0.9683, 'real': 0.0317}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986076, 'subscription': 0, 'content': ""I believe just last year it was found that the maximum network flow problem can be solved in O(VE)O(VE)O(VE) time. This bound was first achieved for dense graphs, i.e., E∈Ω(V2)E∈Ω(V2)E \\in \\Omega(V^2), by the relabel-to-front algorithm, a particular version of the push-preflow method, which runs in O(V3)O(V3)O(V^3) time. This algorithm was published by Goldberg and Tarjan in 1986, and you can find it in CLRS. In 1994, King, Rao, and Tarjan gave an algorithm that runs in O(VElog(V)/log(EVlogV))O(VElog\u2061(V)/log\u2061(EVlog\u2061V))O\\left(VE \\log(V)/\\log\\left(\\frac{E}{V \\log V}\\right)\\right) time. This reduces to O(VE)O(VE)O(VE) for all graphs that are not too sparse, i.e., E∈Ω(V1+ϵ)E∈Ω(V1+ϵ)E \\in \\Omega(V^{1+\\epsilon}). (Graphs with constant or logarithmic average degree are still a bit too sparse.) Finally, in 2012, Orlin put the final piece in place, finding an algorithm that runs in O(VE+E31/16log2V)O(VE+E31/16log2\u2061V)O(VE + E^{31/16} \\log^2 V). For E∈O(V16/15−ϵ)E∈O(V16/15−ϵ)E \\in O(V^{16/15-\\epsilon}), that is, graphs that are sparse enough, this reduces to O(VE)O(VE)O(VE). By combining the algorithm of King, Rao, and Tarjan, and Orlin's algorithm, we see that maximum network flow can be solved in O(VE)O(VE)O(VE) time. I think that's pretty awesome."", 'aiModelVersion': '1'}",0.0317
Dr Jo,Dec 12,"What is the significance of the Turing test in the field of artificial intelligence, and how does it contribute to our understanding of machine intelligence?","Hi Kabir

None—and it doesn’t. It’s a hindrance. Let me explain.

Alan Turing was a bloody genius. Put aside the contribution he made to British code-breaking during World War II; he had a remarkable grasp what could be done with the technology of the time—but more than this, he had a beautiful and clear vision of the principles of computing.

If you’ve played with building Turing machines, for example, then you’ll immediately grok my meaning. Genius! Superficially, the Turing test is also rather appealing.

First, the Turing Test

A machine ‘passes the Turing test’ if a human observer is unable to distinguish between a real human and the ‘intelligent’ machine, after a substantial interaction with both.
∗
∗
 In his 1950 paper on Computing Machinery and Intelligence,[1] Turing limited this interaction to text alone. There’s some disagreement about the precise formulation of the Test, [2] but the principle seems simple, doesn’t it? Not so fast.

The Turing test is ‘Unscience’

Let’s start in the 1930s, when the Logical Positivists were all the rage. They saw science as meticulous building of scientific constructs on a stable foundation. There are ground truths that you can assume, and move on. Heck, some people still think like this.

Unfortunately for them, life doesn’t work this way. You can have a model that works—that passes every test—and then some bugger comes along with a test that reveals foundational flaws in your model. This doesn’t necessarily mean that you have to abandon using your model for many or even most things, but it does suggest that you need to tread cautiously. Your foundation isn’t actually foundational at all. It’s more like winter ice—and in some places, the ice may be thin.

In fact, the more complex the thing you’re testing, the less assured you should be that even ‘comprehensive’ testing has validated your “model”. And what could be more complex than ‘intelligence’? In other words, however you reasonably define intelligence, it would be unscientific to attempt to prove that it passes some test, give it your stamp of approval, and move on.

By the 1950s, most philosophers had moved on—as had not a few scientists. It seems that either Turing didn’t get the memo, or that he didn’t see the implications of the failure of Logical Positivism for things like testing models and indeed devices.

But before we dip into the philosophy, let’s look at a practical example.

Practically

Look at the image above. It was generated by Leonardo|DiffusionXL with the prompt “A photograph of a robot shaking hands with Alan Turing, in black and white. The hands of both are clearly visible.”

I think it’s not a bad representation—although perhaps the Turing has a slight flavour of Benedict Cumberbatch,
†
†
 and his right hand is a bit weird; the robot is pretty stereotyped (although not quite Rossum’s Universal Robots :)

But I chose that image from among four. The other three were along these lines…

Not only did the generator stuff up Turing’s right eye—look at the hands, too. Eugh! Hands are something that current image generators get almost right more by luck than anything else. (I won’t use the term ‘judgement’ here, for reasons explored later).

If you look at the first image, you might be so smitten as to believe that image generation has come of age. Wow! But a better test of fidelity is surely where the AI under-performs. This is at present all-too-easy to identify. As an aside, here are a few things that you might easily imagine, but I find the current crop of image generators struggle with:

“A photograph of a horse riding an astronaut on the Moon”
“A hippopotamus with eight legs”
“A cat hanging by its claws from a giant tongue. The cat is on the left, the tongue is on the right.”

Try these. It’s fun to explore the boundaries, isn’t it?

Don’t get me wrong

The above handshake is still pretty impressive—and things will improve. All I’m doing here is illustrating the point that I’ve already made: finding demonstrations that assert the presence of a given capability is daft, particularly in the field of AI. But this practical application of the thinking that refutes Logical Positivism has two other, major implications. Let’s look at them briefly.

Data ‘science’ mostly isn’t

I’m afraid I tend to bang on a bit about Science. But looking through multiple recent posts on Quora that discuss ‘Science’, I can’t help but feel that there are a lot of crypto-logical positivists still hidden among us. So let me once again give my definition of Science, so that you can shoot at it.
‡
‡
 (Karl Popper worked most of this out in the 1930s, in his book Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwissenschaft).

Science starts with a problem. We propose an explanatory model that not only describes the problem, but is predictive. This prediction doesn’t have to be perfect, just better than random chance, and ideally better than other competing models.
We then test our model. We test the logic—’cos we often stuff up the internal workings; once we’re happy with this, we then go out in reality and test the model to see how it really performs. Most models fail easily.
If a model succeeds—hooray—we provisionally accept it as ‘true’. But we continue back to step (1) again, testing and refining and trying to break or supplant our model. This is good Science. There should be a Darwinian elimination of less favourable models.

Contrast this with a lot of modern approaches to ‘data science’. This often involves garnering vast amounts of data, running it through some sort of black box, pulling out features and then trying to use the outputs in reality. Can you see the huge problems with this approach? Yes, there are many, but the biggest is often missed completely—that this too is Unscience. The meta-model is wrong!

Spotting a winner

Our second implication of the oracular “let’s spot a winner” approach is even more disturbing. Strangely, it was also worked out in the 1930s, by an American called Walter Shewhart. Busy times, the 1930s. Shewhart was struggling with quality control problems on assembly lines. The story is complex, and I won’t delve deeply into it, but the bottom line can be illustrated quite easily by an analogy.

Let’s say you’re running a breakfast diner, and you have a problem with burnt toast. One approach is to deploy a complex system of toast evaluation, with burnt toast sent back—or perhaps you might employ a dedicated toast-scraper. More sensible is not to burn the toast in the first place!

Get the process right. The analogy is obvious: if you’re making a product, especially a complex product, rather than extensively testing it at the end of the assembly line to be “sure that it works” (Hint: based on evaluation, you can’t be sure), design the quality you desire into the product from the start. This can and indeed should be iterative—just like good science.

(Poe/StableDiffusion: “A photograph of an eight-legged hippopotamus, walking on winter ice.” Note not just the inability to count, but the shape of the legs. It’s not just human extremities these bots struggle with).

Applying this to AI

If you listen to real experts on AI (like Geoffrey Hinton) and AI companies’ frontmen (yep, mostly male and charming, like Sam Altman), you’ll often get the assurance that the bot is “really thinking”, just like us. For example, Sam has said that we too are ‘stochastic parrots’,[3] and Hinton has even more recently claimed that LLMs “understand and have empathy”.[4]

Which is bullshit. Surely, you can produce isolated examples that seem to demonstrate this—but this is Unscience. These models have vast domains of incompetence. It’s still trivially easy to show how incompetent these models are. But even if this demonstration becomes challenging, we still can’t assert competence.

More to the point, what are these domains? I’ve already hinted at this. Yes, they pretty much can’t count (although some have attached extra, prosthetic limbs to enable this); but more fundamentally, they have no meta-cognition. They struggle to generalise from one domain to another, and importantly, cannot reliably do either causal inference or counterfactual reasoning. And despite this, Hinton is also claiming that—effectively—they have a limbic system!

These are not just things you can graft on—the maths behind causality and counterfactuals was only worked out in the 1990s, it’s subtle, and it’s unreasonable to assume that the required functionality will somehow just emerge. Solid models of emotion? Even more challenging.

Sure, you can scrape the toast—pick up the ‘defective patterns of behaviour’, and put on extra prosthetics to try to make your LLM ‘behave’. But designing in the right functionality—well, that’s the charm. We’re simply not there yet. As Gary Marcus has pointed out repeatedly, it appears unlikely that current models will ever get there, simply by extension.

And even then, you shouldn’t rely on a demonstration of ‘intelligent performance’. That’s the most unconvincing thing ever.

It’s just Unscience.

My 2c, Dr Jo.

∗
∗
 I’m not going to explore silly and pointless digressions like Searle’s Chinese Room argument, ‘qualia’, dualism, and other philosophical claptrap.
§
§
 Let’s also lay aside whether the evaluator should be a computer, how long you should bash on for, and how smart the benchmark human being should be.

†
†
 I have significant prosopagnosia, so don’t take my word for this.

‡
‡
 Metacognitively <grin>

§
§
 (Waits for explosion from philosophers who haven’t quite caught up).

Footnotes

[1] Computing Machinery and Intelligence - Wikipedia
[2] Turing test - Wikipedia
[3] Dr Jo's answer to What is a ""stochastic parrot""?
[4] Geoffrey Hinton: Large Language Models in Medicine. They Understand and Have Empathy","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/wjn8amlupfz0hk17', 'title': 'What is the significance of the Turing test in the field of artificial intelligence, and how does it contribute to our understanding of machine intelligence?', 'score': {'original': 0.788, 'ai': 0.212}, 'blocks': [{'text': 'Hi Kabir\n\nNone—and it doesn’t. It’s a hindrance. Let me explain.\n\nAlan Turing was a bloody genius. Put aside the contribution he made to British code-breaking during World War II; he had a remarkable grasp what could be done with the technology of the time—but more than this, he had a beautiful and clear vision of the principles of computing.\n\nIf you’ve played with building Turing machines, for example, then you’ll immediately grok my meaning. Genius! Superficially, the Turing test is also rather appealing.\n\nFirst, the Turing Test\n\nA machine ‘passes the Turing test’ if a human observer is unable to distinguish between a real human and the ‘intelligent’ machine, after a substantial interaction with both.\n∗\n∗\n In his 1950 paper on Computing Machinery and Intelligence,[1] Turing limited this interaction to text alone. There’s some disagreement about the precise formulation of the Test, [2] but the principle seems simple, doesn’t it? Not so fast.\n\nThe Turing test is ‘Unscience’\n\nLet’s start in the 1930s, when the Logical Positivists were all the rage. They saw science as meticulous building of scientific constructs on a stable foundation. There are ground truths that you can assume, and move on. Heck, some people still think like this.\n\nUnfortunately for them, life doesn’t work this way. You can have a model that works—that passes every test—and then some bugger comes along with a test that reveals foundational flaws in your model. This doesn’t necessarily mean that you have to abandon using your model for many or even most things, but it does suggest that you need to tread cautiously. Your foundation isn’t actually foundational at all. It’s more like winter ice—and in some places, the ice may be thin.\n\nIn fact, the more complex the thing you’re testing, the less assured you should be that even ‘comprehensive’ testing has validated your “model”. And what could be more complex than ‘intelligence’? In other words, however you reasonably define intelligence, it would be unscientific to attempt to prove that it passes some test, give it your stamp of approval, and move on.\n\nBy the 1950s, most philosophers had moved on—as had not a few scientists. It seems that either Turing didn’t get the memo, or that he didn’t see the implications of the failure of Logical Positivism for things like testing models and indeed devices.\n\nBut before we dip into the philosophy, let’s look at a practical example.\n\nPractically\n\nLook at the image above. It was generated by Leonardo|DiffusionXL with the prompt “A photograph of a robot shaking hands with Alan Turing, in black and white. The hands of both are clearly visible.”\n\nI think it’s not a bad representation—although perhaps the Turing has a slight flavour of Benedict Cumberbatch,\n†\n†\n and his right hand is a bit weird; the robot is pretty stereotyped (although not quite Rossum’s Universal Robots :)\n\nBut I chose that image from among four. The other three were along these lines…\n\nNot only did the generator stuff up Turing’s right eye—look at the hands, too. Eugh! Hands are something that current image generators get almost right more by luck than anything else. (I won’t use the term ‘judgement’ here, for reasons explored', 'result': {'fake': 0.0051, 'real': 0.9949}, 'status': 'success'}, {'text': 'later).\n\nIf you look at the first image, you might be so smitten as to believe that image generation has come of age. Wow! But a better test of fidelity is surely where the AI under-performs. This is at present all-too-easy to identify. As an aside, here are a few things that you might easily imagine, but I find the current crop of image generators struggle with:\n\n“A photograph of a horse riding an astronaut on the Moon”\n“A hippopotamus with eight legs”\n“A cat hanging by its claws from a giant tongue. The cat is on the left, the tongue is on the right.”\n\nTry these. It’s fun to explore the boundaries, isn’t it?\n\nDon’t get me wrong\n\nThe above handshake is still pretty impressive—and things will improve. All I’m doing here is illustrating the point that I’ve already made: finding demonstrations that assert the presence of a given capability is daft, particularly in the field of AI. But this practical application of the thinking that refutes Logical Positivism has two other, major implications. Let’s look at them briefly.\n\nData ‘science’ mostly isn’t\n\nI’m afraid I tend to bang on a bit about Science. But looking through multiple recent posts on Quora that discuss ‘Science’, I can’t help but feel that there are a lot of crypto-logical positivists still hidden among us. So let me once again give my definition of Science, so that you can shoot at it.\n‡\n‡\n (Karl Popper worked most of this out in the 1930s, in his book Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwissenschaft).\n\nScience starts with a problem. We propose an explanatory model that not only describes the problem, but is predictive. This prediction doesn’t have to be perfect, just better than random chance, and ideally better than other competing models.\nWe then test our model. We test the logic—’cos we often stuff up the internal workings; once we’re happy with this, we then go out in reality and test the model to see how it really performs. Most models fail easily.\nIf a model succeeds—hooray—we provisionally accept it as ‘true’. But we continue back to step (1) again, testing and refining and trying to break or supplant our model. This is good Science. There should be a Darwinian elimination of less favourable models.\n\nContrast this with a lot of modern approaches to ‘data science’. This often involves garnering vast amounts of data, running it through some sort of black box, pulling out features and then trying to use the outputs in reality. Can you see the huge problems with this approach? Yes, there are many, but the biggest is often missed completely—that this too is Unscience. The meta-model is wrong!\n\nSpotting a winner\n\nOur second implication of the oracular “let’s spot a winner” approach is even more disturbing. Strangely, it was also worked out in the 1930s, by an American called Walter Shewhart. Busy times, the 1930s. Shewhart was struggling with quality control problems on assembly lines. The story is complex, and I won’t delve deeply into it, but the bottom line can be illustrated quite easily by an analogy.\n\nLet’s say you’re running a breakfast diner, and you', 'result': {'fake': 0.1802, 'real': 0.8198}, 'status': 'success'}, {'text': 'have a problem with burnt toast. One approach is to deploy a complex system of toast evaluation, with burnt toast sent back—or perhaps you might employ a dedicated toast-scraper. More sensible is not to burn the toast in the first place!\n\nGet the process right. The analogy is obvious: if you’re making a product, especially a complex product, rather than extensively testing it at the end of the assembly line to be “sure that it works” (Hint: based on evaluation, you can’t be sure), design the quality you desire into the product from the start. This can and indeed should be iterative—just like good science.\n\n(Poe/StableDiffusion: “A photograph of an eight-legged hippopotamus, walking on winter ice.” Note not just the inability to count, but the shape of the legs. It’s not just human extremities these bots struggle with).\n\nApplying this to AI\n\nIf you listen to real experts on AI (like Geoffrey Hinton) and AI companies’ frontmen (yep, mostly male and charming, like Sam Altman), you’ll often get the assurance that the bot is “really thinking”, just like us. For example, Sam has said that we too are ‘stochastic parrots’,[3] and Hinton has even more recently claimed that LLMs “understand and have empathy”.[4]\n\nWhich is bullshit. Surely, you can produce isolated examples that seem to demonstrate this—but this is Unscience. These models have vast domains of incompetence. It’s still trivially easy to show how incompetent these models are. But even if this demonstration becomes challenging, we still can’t assert competence.\n\nMore to the point, what are these domains? I’ve already hinted at this. Yes, they pretty much can’t count (although some have attached extra, prosthetic limbs to enable this); but more fundamentally, they have no meta-cognition. They struggle to generalise from one domain to another, and importantly, cannot reliably do either causal inference or counterfactual reasoning. And despite this, Hinton is also claiming that—effectively—they have a limbic system!\n\nThese are not just things you can graft on—the maths behind causality and counterfactuals was only worked out in the 1990s, it’s subtle, and it’s unreasonable to assume that the required functionality will somehow just emerge. Solid models of emotion? Even more challenging.\n\nSure, you can scrape the toast—pick up the ‘defective patterns of behaviour’, and put on extra prosthetics to try to make your LLM ‘behave’. But designing in the right functionality—well, that’s the charm. We’re simply not there yet. As Gary Marcus has pointed out repeatedly, it appears unlikely that current models will ever get there, simply by extension.\n\nAnd even then, you shouldn’t rely on a demonstration of ‘intelligent performance’. That’s the most unconvincing thing ever.\n\nIt’s just Unscience.\n\nMy 2c, Dr Jo.\n\n∗\n∗\n I’m not going to explore silly and pointless digressions like Searle’s Chinese Room argument, ‘qualia’, dualism, and other philosophical claptrap.\n§\n§\n Let’s also lay aside whether the evaluator should be a computer, how long you should bash on for, and how smart the benchmark human being should be.\n\n†\n†\n I have significant prosopagnosia, so don’t take my word for this.\n\n‡\n‡\n Metacognitively <grin>\n\n§\n§\n (Waits for explosion from philosophers who haven’t quite caught up).\n\nFootnotes\n\n[1] Computing Machinery and Intelligence - Wikipedia\n[2] Turing test -', 'result': {'fake': 0.0591, 'real': 0.9409}, 'status': 'success'}, {'text': 'Wikipedia\n[3] Dr Jo\'s answer to What is a ""stochastic parrot""?\n[4] Geoffrey Hinton: Large Language Models in Medicine. They Understand and Have Empathy', 'result': {'fake': 0.2667, 'real': 0.7333}, 'status': 'success'}], 'credits_used': 17, 'credits': 1986059, 'subscription': 0, 'content': 'Hi Kabir\n\nNone—and it doesn’t. It’s a hindrance. Let me explain.\n\nAlan Turing was a bloody genius. Put aside the contribution he made to British code-breaking during World War II; he had a remarkable grasp what could be done with the technology of the time—but more than this, he had a beautiful and clear vision of the principles of computing.\n\nIf you’ve played with building Turing machines, for example, then you’ll immediately grok my meaning. Genius! Superficially, the Turing test is also rather appealing.\n\nFirst, the Turing Test\n\nA machine ‘passes the Turing test’ if a human observer is unable to distinguish between a real human and the ‘intelligent’ machine, after a substantial interaction with both.\n∗\n∗\n In his 1950 paper on Computing Machinery and Intelligence,[1] Turing limited this interaction to text alone. There’s some disagreement about the precise formulation of the Test, [2] but the principle seems simple, doesn’t it? Not so fast.\n\nThe Turing test is ‘Unscience’\n\nLet’s start in the 1930s, when the Logical Positivists were all the rage. They saw science as meticulous building of scientific constructs on a stable foundation. There are ground truths that you can assume, and move on. Heck, some people still think like this.\n\nUnfortunately for them, life doesn’t work this way. You can have a model that works—that passes every test—and then some bugger comes along with a test that reveals foundational flaws in your model. This doesn’t necessarily mean that you have to abandon using your model for many or even most things, but it does suggest that you need to tread cautiously. Your foundation isn’t actually foundational at all. It’s more like winter ice—and in some places, the ice may be thin.\n\nIn fact, the more complex the thing you’re testing, the less assured you should be that even ‘comprehensive’ testing has validated your “model”. And what could be more complex than ‘intelligence’? In other words, however you reasonably define intelligence, it would be unscientific to attempt to prove that it passes some test, give it your stamp of approval, and move on.\n\nBy the 1950s, most philosophers had moved on—as had not a few scientists. It seems that either Turing didn’t get the memo, or that he didn’t see the implications of the failure of Logical Positivism for things like testing models and indeed devices.\n\nBut before we dip into the philosophy, let’s look at a practical example.\n\nPractically\n\nLook at the image above. It was generated by Leonardo|DiffusionXL with the prompt “A photograph of a robot shaking hands with Alan Turing, in black and white. The hands of both are clearly visible.”\n\nI think it’s not a bad representation—although perhaps the Turing has a slight flavour of Benedict Cumberbatch,\n†\n†\n and his right hand is a bit weird; the robot is pretty stereotyped (although not quite Rossum’s Universal Robots :)\n\nBut I chose that image from among four. The other three were along these lines…\n\nNot only did the generator stuff up Turing’s right eye—look at the hands, too. Eugh! Hands are something that current image generators get almost right more by luck than anything else. (I won’t use the term ‘judgement’ here, for reasons explored later).\n\nIf you look at the first image, you might be so smitten as to believe that image generation has come of age. Wow! But a better test of fidelity is surely where the AI under-performs. This is at present all-too-easy to identify. As an aside, here are a few things that you might easily imagine, but I find the current crop of image generators struggle with:\n\n“A photograph of a horse riding an astronaut on the Moon”\n“A hippopotamus with eight legs”\n“A cat hanging by its claws from a giant tongue. The cat is on the left, the tongue is on the right.”\n\nTry these. It’s fun to explore the boundaries, isn’t it?\n\nDon’t get me wrong\n\nThe above handshake is still pretty impressive—and things will improve. All I’m doing here is illustrating the point that I’ve already made: finding demonstrations that assert the presence of a given capability is daft, particularly in the field of AI. But this practical application of the thinking that refutes Logical Positivism has two other, major implications. Let’s look at them briefly.\n\nData ‘science’ mostly isn’t\n\nI’m afraid I tend to bang on a bit about Science. But looking through multiple recent posts on Quora that discuss ‘Science’, I can’t help but feel that there are a lot of crypto-logical positivists still hidden among us. So let me once again give my definition of Science, so that you can shoot at it.\n‡\n‡\n (Karl Popper worked most of this out in the 1930s, in his book Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwissenschaft).\n\nScience starts with a problem. We propose an explanatory model that not only describes the problem, but is predictive. This prediction doesn’t have to be perfect, just better than random chance, and ideally better than other competing models.\nWe then test our model. We test the logic—’cos we often stuff up the internal workings; once we’re happy with this, we then go out in reality and test the model to see how it really performs. Most models fail easily.\nIf a model succeeds—hooray—we provisionally accept it as ‘true’. But we continue back to step (1) again, testing and refining and trying to break or supplant our model. This is good Science. There should be a Darwinian elimination of less favourable models.\n\nContrast this with a lot of modern approaches to ‘data science’. This often involves garnering vast amounts of data, running it through some sort of black box, pulling out features and then trying to use the outputs in reality. Can you see the huge problems with this approach? Yes, there are many, but the biggest is often missed completely—that this too is Unscience. The meta-model is wrong!\n\nSpotting a winner\n\nOur second implication of the oracular “let’s spot a winner” approach is even more disturbing. Strangely, it was also worked out in the 1930s, by an American called Walter Shewhart. Busy times, the 1930s. Shewhart was struggling with quality control problems on assembly lines. The story is complex, and I won’t delve deeply into it, but the bottom line can be illustrated quite easily by an analogy.\n\nLet’s say you’re running a breakfast diner, and you have a problem with burnt toast. One approach is to deploy a complex system of toast evaluation, with burnt toast sent back—or perhaps you might employ a dedicated toast-scraper. More sensible is not to burn the toast in the first place!\n\nGet the process right. The analogy is obvious: if you’re making a product, especially a complex product, rather than extensively testing it at the end of the assembly line to be “sure that it works” (Hint: based on evaluation, you can’t be sure), design the quality you desire into the product from the start. This can and indeed should be iterative—just like good science.\n\n(Poe/StableDiffusion: “A photograph of an eight-legged hippopotamus, walking on winter ice.” Note not just the inability to count, but the shape of the legs. It’s not just human extremities these bots struggle with).\n\nApplying this to AI\n\nIf you listen to real experts on AI (like Geoffrey Hinton) and AI companies’ frontmen (yep, mostly male and charming, like Sam Altman), you’ll often get the assurance that the bot is “really thinking”, just like us. For example, Sam has said that we too are ‘stochastic parrots’,[3] and Hinton has even more recently claimed that LLMs “understand and have empathy”.[4]\n\nWhich is bullshit. Surely, you can produce isolated examples that seem to demonstrate this—but this is Unscience. These models have vast domains of incompetence. It’s still trivially easy to show how incompetent these models are. But even if this demonstration becomes challenging, we still can’t assert competence.\n\nMore to the point, what are these domains? I’ve already hinted at this. Yes, they pretty much can’t count (although some have attached extra, prosthetic limbs to enable this); but more fundamentally, they have no meta-cognition. They struggle to generalise from one domain to another, and importantly, cannot reliably do either causal inference or counterfactual reasoning. And despite this, Hinton is also claiming that—effectively—they have a limbic system!\n\nThese are not just things you can graft on—the maths behind causality and counterfactuals was only worked out in the 1990s, it’s subtle, and it’s unreasonable to assume that the required functionality will somehow just emerge. Solid models of emotion? Even more challenging.\n\nSure, you can scrape the toast—pick up the ‘defective patterns of behaviour’, and put on extra prosthetics to try to make your LLM ‘behave’. But designing in the right functionality—well, that’s the charm. We’re simply not there yet. As Gary Marcus has pointed out repeatedly, it appears unlikely that current models will ever get there, simply by extension.\n\nAnd even then, you shouldn’t rely on a demonstration of ‘intelligent performance’. That’s the most unconvincing thing ever.\n\nIt’s just Unscience.\n\nMy 2c, Dr Jo.\n\n∗\n∗\n I’m not going to explore silly and pointless digressions like Searle’s Chinese Room argument, ‘qualia’, dualism, and other philosophical claptrap.\n§\n§\n Let’s also lay aside whether the evaluator should be a computer, how long you should bash on for, and how smart the benchmark human being should be.\n\n†\n†\n I have significant prosopagnosia, so don’t take my word for this.\n\n‡\n‡\n Metacognitively <grin>\n\n§\n§\n (Waits for explosion from philosophers who haven’t quite caught up).\n\nFootnotes\n\n[1] Computing Machinery and Intelligence - Wikipedia\n[2] Turing test - Wikipedia\n[3] Dr Jo\'s answer to What is a ""stochastic parrot""?\n[4] Geoffrey Hinton: Large Language Models in Medicine. They Understand and Have Empathy', 'aiModelVersion': '1'}",0.788
Gayle Laakmann McDowell,10y,What's the best way to explain big-O notation in laymen's terms?,"This is a true story.

In 2009, a South African company named The Unlimited grew frustrated by their ISP’s slow internet and made news by comically showing just how bad it is. They “raced” a carrier pigeon against their ISP. The pigeon had a USB stick affixed to its leg and was taught to fly to an office, 50 miles away. Meanwhile, the company transferred this same data over the internet to this same office. The pigeon won -- by a long shot.

What a joke this ISP was, right? A BIRD could transfer data faster than them. A bird!

Their internet may or may not have been slow, but this experiment doesn't say much. No matter how fast or slow your internet is, you can select an amount of data that will allow the internet or a pigeon to win.

Here's why:

How long does it take a pigeon to fly 50 miles with a 10 GB USB stick attached to its leg? Let's say it takes about 3 hours. Great.

Now, how long does it take to transfer 10 GB on the internet? Let's say you have pretty fast internet, and 10 GB only took 30 minutes. Okay, then transfer 100 GB and you know it will take more than 3 hours.

How long does it take that same pigeon to ""transfer"" 100 GB? Still 3 hours. The pigeon's transfer speed doesn't depend on the amount of data. (USB sticks are pretty light but can fit a ton of data.)

So, just like that: the pigeon beat the internet!

The pigeon's transfer time is constant. The internet's transfer time is proportional to the amount of data: twice the data will take about twice as much time.

In Big-O time, we'd say that the pigeon takes O(1) time. This means that the time it takes to transfer N gigabytes varies proportionally with 1. That is, it doesn't vary at all.

The internet's transfer speed is O(N). This means that the amount of time it takes varies proportionally with N.

Now, what if you had something that was O(N^2)? This would mean that the time varies with the size of N squared.

A real life example of an O(N^2) problem would be the time it takes to paint a square wall of length N (note: N is the length of the wall, not the area of the wall). If I make the edge of my square twice as long, the area of the square wall increases 4x.

Big-O offers an equation to describe how the time of a procedure changes relative to its input. It describes the trend. It does not define exactly how long it takes, as a procedure with a larger big-O time than another procedure could be faster on specific inputs.

Note: If you’ve taken an algorithms class, you might remember that, technically, big-O refers to an upper-bound. Anything that is O(N) could also be said to be O(N^2). To describe the exact runtime, we should be using big-theta.

However, outside of an algorithms class, this distinction has basically been forgotten about. People use big-O when they should really be using big-theta.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/74g5ht2mvyod0lqb', 'title': ""What's the best way to explain big-O notation in laymen's terms?"", 'score': {'original': 0.46985, 'ai': 0.53015}, 'blocks': [{'text': 'This is a true story.\n\nIn 2009, a South African company named The Unlimited grew frustrated by their ISP’s slow internet and made news by comically showing just how bad it is. They “raced” a carrier pigeon against their ISP. The pigeon had a USB stick affixed to its leg and was taught to fly to an office, 50 miles away. Meanwhile, the company transferred this same data over the internet to this same office. The pigeon won -- by a long shot.\n\nWhat a joke this ISP was, right? A BIRD could transfer data faster than them. A bird!\n\nTheir internet may or may not have been slow, but this experiment doesn\'t say much. No matter how fast or slow your internet is, you can select an amount of data that will allow the internet or a pigeon to win.\n\nHere\'s why:\n\nHow long does it take a pigeon to fly 50 miles with a 10 GB USB stick attached to its leg? Let\'s say it takes about 3 hours. Great.\n\nNow, how long does it take to transfer 10 GB on the internet? Let\'s say you have pretty fast internet, and 10 GB only took 30 minutes. Okay, then transfer 100 GB and you know it will take more than 3 hours.\n\nHow long does it take that same pigeon to ""transfer"" 100 GB? Still 3 hours. The pigeon\'s transfer speed doesn\'t depend on the amount of data. (USB sticks are pretty light but can fit a ton of data.)\n\nSo, just like that: the pigeon beat the internet!\n\nThe pigeon\'s transfer time is constant. The internet\'s transfer time is proportional to the amount of data: twice the data will take about twice as much time.\n\nIn Big-O time, we\'d say that the pigeon takes O(1) time. This means that the time it takes to transfer N gigabytes varies proportionally with 1. That is, it doesn\'t vary at all.\n\nThe internet\'s transfer speed is O(N). This means that the amount of time it takes varies proportionally with N.\n\nNow, what if you had something that was O(N^2)? This would mean that the time varies with the size of N squared.\n\nA real life example of an O(N^2) problem would be the time it takes to paint a square wall of length N (note: N is the length of the wall, not the area of the wall). If I make the edge of my square twice as long, the area of the square wall increases 4x.\n\nBig-O offers an equation to describe how the time of a procedure changes relative to its input. It describes the trend. It does not define exactly how long it takes, as a procedure with a larger big-O time than another procedure could be faster on specific inputs.\n\nNote: If you’ve taken an algorithms class, you might remember that, technically, big-O refers to an upper-bound. Anything that is O(N) could also be said to be O(N^2). To describe the exact runtime, we should be using big-theta.\n\nHowever, outside of an algorithms class, this distinction has basically been forgotten about. People use big-O when they should really be using big-theta.', 'result': {'fake': 0.0013, 'real': 0.9987}, 'status': 'success'}], 'credits_used': 6, 'credits': 1986053, 'subscription': 0, 'content': 'This is a true story.\n\nIn 2009, a South African company named The Unlimited grew frustrated by their ISP’s slow internet and made news by comically showing just how bad it is. They “raced” a carrier pigeon against their ISP. The pigeon had a USB stick affixed to its leg and was taught to fly to an office, 50 miles away. Meanwhile, the company transferred this same data over the internet to this same office. The pigeon won -- by a long shot.\n\nWhat a joke this ISP was, right? A BIRD could transfer data faster than them. A bird!\n\nTheir internet may or may not have been slow, but this experiment doesn\'t say much. No matter how fast or slow your internet is, you can select an amount of data that will allow the internet or a pigeon to win.\n\nHere\'s why:\n\nHow long does it take a pigeon to fly 50 miles with a 10 GB USB stick attached to its leg? Let\'s say it takes about 3 hours. Great.\n\nNow, how long does it take to transfer 10 GB on the internet? Let\'s say you have pretty fast internet, and 10 GB only took 30 minutes. Okay, then transfer 100 GB and you know it will take more than 3 hours.\n\nHow long does it take that same pigeon to ""transfer"" 100 GB? Still 3 hours. The pigeon\'s transfer speed doesn\'t depend on the amount of data. (USB sticks are pretty light but can fit a ton of data.)\n\nSo, just like that: the pigeon beat the internet!\n\nThe pigeon\'s transfer time is constant. The internet\'s transfer time is proportional to the amount of data: twice the data will take about twice as much time.\n\nIn Big-O time, we\'d say that the pigeon takes O(1) time. This means that the time it takes to transfer N gigabytes varies proportionally with 1. That is, it doesn\'t vary at all.\n\nThe internet\'s transfer speed is O(N). This means that the amount of time it takes varies proportionally with N.\n\nNow, what if you had something that was O(N^2)? This would mean that the time varies with the size of N squared.\n\nA real life example of an O(N^2) problem would be the time it takes to paint a square wall of length N (note: N is the length of the wall, not the area of the wall). If I make the edge of my square twice as long, the area of the square wall increases 4x.\n\nBig-O offers an equation to describe how the time of a procedure changes relative to its input. It describes the trend. It does not define exactly how long it takes, as a procedure with a larger big-O time than another procedure could be faster on specific inputs.\n\nNote: If you’ve taken an algorithms class, you might remember that, technically, big-O refers to an upper-bound. Anything that is O(N) could also be said to be O(N^2). To describe the exact runtime, we should be using big-theta.\n\nHowever, outside of an algorithms class, this distinction has basically been forgotten about. People use big-O when they should really be using big-theta.', 'aiModelVersion': '1'}",0.46985
Mark Phaedrus,4y,How does software that runs traffic lights avoid fatal errors while consumer programs crash the time?,"Here are some reasons why traffic light software is quite stable:

It’s a largely static field. The vast majority of intersections call for the same, simple requirements and situations. Most governments don’t want traffic control software that does some sort of amazing new thing, in the way that consumers of more typical types of software often do. New features of traffic control software are much rarer.
A given piece of traffic control software runs on a very small, very predictable, very carefully-tested number of hardware configurations. That immediately gets rid of the huge number of difficult-to-diagnose-and-debug problems that can occur when you combine some set of the literally billions of combinations of PC components that can make up, say, a consumer Windows PC.
With almost no exceptions, traffic control software is the only thing that’s running on hardware used for traffic control. That immediately gets rid of a huge majority of the difficult-to-diagnose-and-debug “This only happens when Program A is doing X and Y while Program B is simultaneously doing Z” problems that occur on consumer PCs.
As a result of those factors, the job of testing traffic control software is at least two or three orders of magnitude easier than the job of testing a complex piece of consumer software. The reduced number of possible hardware and software configurations means that you can test each combination in much greater detail than would be possible — let alone cost-effective — for consumer PCs.

And despite all this, traffic control software does sometimes crash or malfunction. So why don’t we hear about horrible crashes caused by malfunctioning software turning the lights green in all directions? Because of one more crucial factor:

Fancy traffic control software and hardware is almost never hooked directly up to the lights. Everything typically goes through a “watchdog”, a much more conservatively-designed and rarely-modified piece of hardware. The watchdog hardware is designed to enforce a few very basic, very well understood rules like:
“At a typical intersection where two roads intersect, at least one road’s traffic signals must be red at any given moment. Both roads must never be shown a yellow or green signal at the same time.”
“A pedestrian signal cannot be in a WALK or flashing DON’T WALK state if the vehicular signal for the road being crossed is not red.”
“Each direction of traffic must receive a green signal at least once every three minutes.”
If the main traffic control system ever does something that violates one of these constraints — if it tries to display a conflicting set of signals, or if it hangs and leaves the signals in the same state for too long — then the watchdog takes over, disconnects the main traffic control system from the lights, and sets everything to flashing red/flashing DON’T WALK instead. Typically it stays this way until a traffic engineer checks and resets the equipment.

So traffic light software simply doesn’t operate in anything remotely the same way as consumer software. That means that it’s generally not particularly useful to compare them.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/otu5pewrhqbfz37y', 'title': 'How does software that runs traffic lights avoid fatal errors while consumer programs crash the time?', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'Here are some reasons why traffic light software is quite stable:\n\nIt’s a largely static field. The vast majority of intersections call for the same, simple requirements and situations. Most governments don’t want traffic control software that does some sort of amazing new thing, in the way that consumers of more typical types of software often do. New features of traffic control software are much rarer.\nA given piece of traffic control software runs on a very small, very predictable, very carefully-tested number of hardware configurations. That immediately gets rid of the huge number of difficult-to-diagnose-and-debug problems that can occur when you combine some set of the literally billions of combinations of PC components that can make up, say, a consumer Windows PC.\nWith almost no exceptions, traffic control software is the only thing that’s running on hardware used for traffic control. That immediately gets rid of a huge majority of the difficult-to-diagnose-and-debug “This only happens when Program A is doing X and Y while Program B is simultaneously doing Z” problems that occur on consumer PCs.\nAs a result of those factors, the job of testing traffic control software is at least two or three orders of magnitude easier than the job of testing a complex piece of consumer software. The reduced number of possible hardware and software configurations means that you can test each combination in much greater detail than would be possible — let alone cost-effective — for consumer PCs.\n\nAnd despite all this, traffic control software does sometimes crash or malfunction. So why don’t we hear about horrible crashes caused by malfunctioning software turning the lights green in all directions? Because of one more crucial factor:\n\nFancy traffic control software and hardware is almost never hooked directly up to the lights. Everything typically goes through a “watchdog”, a much more conservatively-designed and rarely-modified piece of hardware. The watchdog hardware is designed to enforce a few very basic, very well understood rules like:\n“At a typical intersection where two roads intersect, at least one road’s traffic signals must be red at any given moment. Both roads must never be shown a yellow or green signal at the same time.”\n“A pedestrian signal cannot be in a WALK or flashing DON’T WALK state if the vehicular signal for the road being crossed is not red.”\n“Each direction of traffic must receive a green signal at least once every three minutes.”\nIf the main traffic control system ever does something that violates one of these constraints — if it tries to display a conflicting set of signals, or if it hangs and leaves the signals in the same state for too long — then the watchdog takes over, disconnects the main traffic control system from the lights, and sets everything to flashing red/flashing DON’T WALK instead. Typically it stays this way until a traffic engineer checks and resets the equipment.\n\nSo traffic light software simply doesn’t operate in anything remotely the same way as consumer software. That means that it’s generally not particularly useful to compare them.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 6, 'credits': 1986047, 'subscription': 0, 'content': 'Here are some reasons why traffic light software is quite stable:\n\nIt’s a largely static field. The vast majority of intersections call for the same, simple requirements and situations. Most governments don’t want traffic control software that does some sort of amazing new thing, in the way that consumers of more typical types of software often do. New features of traffic control software are much rarer.\nA given piece of traffic control software runs on a very small, very predictable, very carefully-tested number of hardware configurations. That immediately gets rid of the huge number of difficult-to-diagnose-and-debug problems that can occur when you combine some set of the literally billions of combinations of PC components that can make up, say, a consumer Windows PC.\nWith almost no exceptions, traffic control software is the only thing that’s running on hardware used for traffic control. That immediately gets rid of a huge majority of the difficult-to-diagnose-and-debug “This only happens when Program A is doing X and Y while Program B is simultaneously doing Z” problems that occur on consumer PCs.\nAs a result of those factors, the job of testing traffic control software is at least two or three orders of magnitude easier than the job of testing a complex piece of consumer software. The reduced number of possible hardware and software configurations means that you can test each combination in much greater detail than would be possible — let alone cost-effective — for consumer PCs.\n\nAnd despite all this, traffic control software does sometimes crash or malfunction. So why don’t we hear about horrible crashes caused by malfunctioning software turning the lights green in all directions? Because of one more crucial factor:\n\nFancy traffic control software and hardware is almost never hooked directly up to the lights. Everything typically goes through a “watchdog”, a much more conservatively-designed and rarely-modified piece of hardware. The watchdog hardware is designed to enforce a few very basic, very well understood rules like:\n“At a typical intersection where two roads intersect, at least one road’s traffic signals must be red at any given moment. Both roads must never be shown a yellow or green signal at the same time.”\n“A pedestrian signal cannot be in a WALK or flashing DON’T WALK state if the vehicular signal for the road being crossed is not red.”\n“Each direction of traffic must receive a green signal at least once every three minutes.”\nIf the main traffic control system ever does something that violates one of these constraints — if it tries to display a conflicting set of signals, or if it hangs and leaves the signals in the same state for too long — then the watchdog takes over, disconnects the main traffic control system from the lights, and sets everything to flashing red/flashing DON’T WALK instead. Typically it stays this way until a traffic engineer checks and resets the equipment.\n\nSo traffic light software simply doesn’t operate in anything remotely the same way as consumer software. That means that it’s generally not particularly useful to compare them.', 'aiModelVersion': '1'}",0.9996
Alon Amit,2y,Can all NP problems be solved in exponential time?,"Yes. This is formally expressed as NP ⊆⊆\subseteq EXPTIME and the reason it’s so is because instances in NP have polynomial-length certificates that can be checked in polynomial time, so one may simply enumerate over all possible certificates and see if any of them checks out. In fact, it is also known that NP ⊆⊆\subseteq PSPACE ⊆⊆\subseteq EXPTIME. As is often the case in complexity theory, those containments aren’t known to be strict. For all we know, it is possible that NP=EXPTIME, though I don’t think many people expect that to be true.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/p9ziemq5jcybhv6u', 'title': 'Can all NP problems be solved in exponential time?', 'score': {'original': 0.9856, 'ai': 0.0144}, 'blocks': [{'text': 'Yes. This is formally expressed as NP ⊆⊆\\subseteq EXPTIME and the reason it’s so is because instances in NP have polynomial-length certificates that can be checked in polynomial time, so one may simply enumerate over all possible certificates and see if any of them checks out. In fact, it is also known that NP ⊆⊆\\subseteq PSPACE ⊆⊆\\subseteq EXPTIME. As is often the case in complexity theory, those containments aren’t known to be strict. For all we know, it is possible that NP=EXPTIME, though I don’t think many people expect that to be true.', 'result': {'fake': 0.0144, 'real': 0.9856}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986046, 'subscription': 0, 'content': 'Yes. This is formally expressed as NP ⊆⊆\\subseteq EXPTIME and the reason it’s so is because instances in NP have polynomial-length certificates that can be checked in polynomial time, so one may simply enumerate over all possible certificates and see if any of them checks out. In fact, it is also known that NP ⊆⊆\\subseteq PSPACE ⊆⊆\\subseteq EXPTIME. As is often the case in complexity theory, those containments aren’t known to be strict. For all we know, it is possible that NP=EXPTIME, though I don’t think many people expect that to be true.', 'aiModelVersion': '1'}",0.9856
Steve Baker,1y,Is the computing power of the latest smart phones greater than that of the computers behind NASA's 1969 moon landing mission (Apollo 11)?,"The Apollo computer had a clock speed of about 1 MHz…and it needed 4 clock cycles to do a lot of it’s operations.

The iPhone 14 has a clock speed of 3,460 MHz an does most operations in one cycle. But it has

* 6 CPU cores.
* 5 GPU cores.
* 16 Neural Engine cores.


…it’s hard to say exactly how much faster that makes it - because it’s doubtful that ...

Access this answer and support the author as a Quora+ subscriber
Access all answers reserved by 
Steve Baker
 for Quora+ subscribers
Access exclusive answers from thousands more participating creators in Quora+
Browse ad‑free and support creators
Start free trial
Learn more","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/06jz3xqwol9ye2v5', 'title': ""Is the computing power of the latest smart phones greater than that of the computers behind NASA's 1969 moon landing mission (Apollo 11)?"", 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'The Apollo computer had a clock speed of about 1 MHz…and it needed 4 clock cycles to do a lot of it’s operations.\n\nThe iPhone 14 has a clock speed of 3,460 MHz an does most operations in one cycle. But it has\n\n* 6 CPU cores.\n* 5 GPU cores.\n* 16 Neural Engine cores.\n\n\n…it’s hard to say exactly how much faster that makes it - because it’s doubtful that ...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nSteve Baker\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986044, 'subscription': 0, 'content': 'The Apollo computer had a clock speed of about 1 MHz…and it needed 4 clock cycles to do a lot of it’s operations.\n\nThe iPhone 14 has a clock speed of 3,460 MHz an does most operations in one cycle. But it has\n\n* 6 CPU cores.\n* 5 GPU cores.\n* 16 Neural Engine cores.\n\n\n…it’s hard to say exactly how much faster that makes it - because it’s doubtful that ...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nSteve Baker\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'aiModelVersion': '1'}",0.9997
Petr Titera,Updated 4y,How do computers know that a second has passed?,"Because of this

that’s quartz crystal. Quarts has a property called Piezoelectricity
. Crystals with this property generate a charge when pressed or modify their shape when charge is applied to them.

This property allows you to create a Crystal oscillator
 which oscillates in regular intervals.

Each computer has several crystal oscillators generating different pulses which drive the computer.

To know when a second passes, you only need to know the frequency (i.e. number of oscillations per second) of that crystal. You then simply count these oscillations.

Modern computers have special chips for this. They are commonly called Real-time clocks
. These chips contain everything necessary to keep track of time: oscillator and counter. They are also battery-backed so they continue to run even if the computer is switched-off and disconnected from power.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/khvad9s4myeu5grw', 'title': 'How do computers know that a second has passed?', 'score': {'original': 0.9515, 'ai': 0.0485}, 'blocks': [{'text': 'Because of this\n\nthat’s quartz crystal. Quarts has a property called Piezoelectricity\n. Crystals with this property generate a charge when pressed or modify their shape when charge is applied to them.\n\nThis property allows you to create a Crystal oscillator\n which oscillates in regular intervals.\n\nEach computer has several crystal oscillators generating different pulses which drive the computer.\n\nTo know when a second passes, you only need to know the frequency (i.e. number of oscillations per second) of that crystal. You then simply count these oscillations.\n\nModern computers have special chips for this. They are commonly called Real-time clocks\n. These chips contain everything necessary to keep track of time: oscillator and counter. They are also battery-backed so they continue to run even if the computer is switched-off and disconnected from power.', 'result': {'fake': 0.0485, 'real': 0.9515}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986042, 'subscription': 0, 'content': 'Because of this\n\nthat’s quartz crystal. Quarts has a property called Piezoelectricity\n. Crystals with this property generate a charge when pressed or modify their shape when charge is applied to them.\n\nThis property allows you to create a Crystal oscillator\n which oscillates in regular intervals.\n\nEach computer has several crystal oscillators generating different pulses which drive the computer.\n\nTo know when a second passes, you only need to know the frequency (i.e. number of oscillations per second) of that crystal. You then simply count these oscillations.\n\nModern computers have special chips for this. They are commonly called Real-time clocks\n. These chips contain everything necessary to keep track of time: oscillator and counter. They are also battery-backed so they continue to run even if the computer is switched-off and disconnected from power.', 'aiModelVersion': '1'}",0.9515
Glyn Williams,Updated 7y,Is the iPhone 6 more powerful than Apollo 11?,"The computer on board the Apollo 11 was pretty feeble by modern standards. Not just weaker than the iPhone6 but weaker than the oldest slowest phone CPU. And weaker than the earliest home microcomputers.

The CPU and GPU in a modern iPhone is actually capable of absurd amounts of computation.

This is a Cray XMP supercomputer, which was used for scientific work in the 80s. If I did my calculations right, the iPhone GPU is equivalent to about 300 of them!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6x0clw3ua2sztqei', 'title': 'Is the iPhone 6 more powerful than Apollo 11?', 'score': {'original': 0.9887, 'ai': 0.0113}, 'blocks': [{'text': 'The computer on board the Apollo 11 was pretty feeble by modern standards. Not just weaker than the iPhone6 but weaker than the oldest slowest phone CPU. And weaker than the earliest home microcomputers.\n\nThe CPU and GPU in a modern iPhone is actually capable of absurd amounts of computation.\n\nThis is a Cray XMP supercomputer, which was used for scientific work in the 80s. If I did my calculations right, the iPhone GPU is equivalent to about 300 of them!', 'result': {'fake': 0.0113, 'real': 0.9887}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986041, 'subscription': 0, 'content': 'The computer on board the Apollo 11 was pretty feeble by modern standards. Not just weaker than the iPhone6 but weaker than the oldest slowest phone CPU. And weaker than the earliest home microcomputers.\n\nThe CPU and GPU in a modern iPhone is actually capable of absurd amounts of computation.\n\nThis is a Cray XMP supercomputer, which was used for scientific work in the 80s. If I did my calculations right, the iPhone GPU is equivalent to about 300 of them!', 'aiModelVersion': '1'}",0.9887
Mark,Updated 1y,What is one random thing you know about a computer that most people don’t?,"The first home computers were REALLY basic.

My first computer was a Tandy TRS-80 Model 1, bought in 1978.

Please admire the sophisticated cardboard box monitor stand and the huge cassette data storage archive. The calculator was needed because the computer was rubbish at actually calculating things. And hey - I still have that mug 43 years later. The little orange box to the left of the cassette deck was an audio splitter so the deck’s output went to the computer and to headphones. This will be explained shortly in a dramatic twist related to data storage. Oooh. You’re so excited now.

It had 4K RAM. Yes, 4,096 bytes, not megabytes. Which helps explain why programmers used 2-digit years dates and led to the Millennium Bug of 2000. Wasting 2 bytes of RAM to add “19” to a year was an extravagant waste of precious memory. I wrote myself a text editor (in Z80 machine code), but it consumed about a half of my RAM, so my text documents could only be REALLY small.
The screen display was 25 x 80 characters. All UPPERCASE (because having lowercase was another extravagant luxury). And it was monochrome, naturally.
Hard disk drive? ROFL. Easier to buy a unicorn. Floppy disk drive? I would’ve had to sell a house to buy a FDD back then. We had cassettes to store our data and programs on. Like real men. If you wanted to store more than one file on a cassette, you had to fast-forward and listen for where one hiss ended and the next began. All cassettes were adorned with hand-written sticky labels listing the tape-counter values of each of the programs stored on the tape.
The operating system was in ROM and could never be updated. And the OS was Microsoft BASIC.
It goes without saying that it didn’t do multitasking. It was barely able to singletask.
It didn’t have a fancy battery-backed real-time clock. Every time it was turned on, I had to enter the date and time manually. So, nearly every document had the same 1 January date (because I couldn’t be bothered setting the clock every day).
Commercial programs were as rare as hen’s teeth, and of course there was no internet. If you wanted a program, you wrote it yourself (and couldn’t share it, even if you could find another citizen in your state who had any computer at all).

Those were the days.

——

EDIT 2022–06–03 - I never thought this thread would blow up like this. It only goes to show many of us old geeks are still alive.

For the amusement of elderly citizens out there, I humbly present a letter from the Tandy district manager, no less, in 1978. You remember letters - paper, stamps, envelopes, waiting by the mailbox etc.

I’m glad he mentioned all the “Soft Wear” to be had.

And if you needed more convincing that a TRS-80 was the best computer value available today!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/vhj3f2pw4qcori1u', 'title': 'What is one random thing you know about a computer that most people don’t?', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'The first home computers were REALLY basic.\n\nMy first computer was a Tandy TRS-80 Model 1, bought in 1978.\n\nPlease admire the sophisticated cardboard box monitor stand and the huge cassette data storage archive. The calculator was needed because the computer was rubbish at actually calculating things. And hey - I still have that mug 43 years later. The little orange box to the left of the cassette deck was an audio splitter so the deck’s output went to the computer and to headphones. This will be explained shortly in a dramatic twist related to data storage. Oooh. You’re so excited now.\n\nIt had 4K RAM. Yes, 4,096 bytes, not megabytes. Which helps explain why programmers used 2-digit years dates and led to the Millennium Bug of 2000. Wasting 2 bytes of RAM to add “19” to a year was an extravagant waste of precious memory. I wrote myself a text editor (in Z80 machine code), but it consumed about a half of my RAM, so my text documents could only be REALLY small.\nThe screen display was 25 x 80 characters. All UPPERCASE (because having lowercase was another extravagant luxury). And it was monochrome, naturally.\nHard disk drive? ROFL. Easier to buy a unicorn. Floppy disk drive? I would’ve had to sell a house to buy a FDD back then. We had cassettes to store our data and programs on. Like real men. If you wanted to store more than one file on a cassette, you had to fast-forward and listen for where one hiss ended and the next began. All cassettes were adorned with hand-written sticky labels listing the tape-counter values of each of the programs stored on the tape.\nThe operating system was in ROM and could never be updated. And the OS was Microsoft BASIC.\nIt goes without saying that it didn’t do multitasking. It was barely able to singletask.\nIt didn’t have a fancy battery-backed real-time clock. Every time it was turned on, I had to enter the date and time manually. So, nearly every document had the same 1 January date (because I couldn’t be bothered setting the clock every day).\nCommercial programs were as rare as hen’s teeth, and of course there was no internet. If you wanted a program, you wrote it yourself (and couldn’t share it, even if you could find another citizen in your state who had any computer at all).\n\nThose were the days.\n\n——\n\nEDIT 2022–06–03 - I never thought this thread would blow up like this. It only goes to show many of us old geeks are still alive.\n\nFor the amusement of elderly citizens out there, I humbly present a letter from the Tandy district manager, no less, in 1978. You remember letters - paper, stamps, envelopes, waiting by the mailbox etc.\n\nI’m glad he mentioned all the “Soft Wear” to be had.\n\nAnd if you needed more convincing that a TRS-80 was the best computer value available today!', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 5, 'credits': 1986036, 'subscription': 0, 'content': 'The first home computers were REALLY basic.\n\nMy first computer was a Tandy TRS-80 Model 1, bought in 1978.\n\nPlease admire the sophisticated cardboard box monitor stand and the huge cassette data storage archive. The calculator was needed because the computer was rubbish at actually calculating things. And hey - I still have that mug 43 years later. The little orange box to the left of the cassette deck was an audio splitter so the deck’s output went to the computer and to headphones. This will be explained shortly in a dramatic twist related to data storage. Oooh. You’re so excited now.\n\nIt had 4K RAM. Yes, 4,096 bytes, not megabytes. Which helps explain why programmers used 2-digit years dates and led to the Millennium Bug of 2000. Wasting 2 bytes of RAM to add “19” to a year was an extravagant waste of precious memory. I wrote myself a text editor (in Z80 machine code), but it consumed about a half of my RAM, so my text documents could only be REALLY small.\nThe screen display was 25 x 80 characters. All UPPERCASE (because having lowercase was another extravagant luxury). And it was monochrome, naturally.\nHard disk drive? ROFL. Easier to buy a unicorn. Floppy disk drive? I would’ve had to sell a house to buy a FDD back then. We had cassettes to store our data and programs on. Like real men. If you wanted to store more than one file on a cassette, you had to fast-forward and listen for where one hiss ended and the next began. All cassettes were adorned with hand-written sticky labels listing the tape-counter values of each of the programs stored on the tape.\nThe operating system was in ROM and could never be updated. And the OS was Microsoft BASIC.\nIt goes without saying that it didn’t do multitasking. It was barely able to singletask.\nIt didn’t have a fancy battery-backed real-time clock. Every time it was turned on, I had to enter the date and time manually. So, nearly every document had the same 1 January date (because I couldn’t be bothered setting the clock every day).\nCommercial programs were as rare as hen’s teeth, and of course there was no internet. If you wanted a program, you wrote it yourself (and couldn’t share it, even if you could find another citizen in your state who had any computer at all).\n\nThose were the days.\n\n——\n\nEDIT 2022–06–03 - I never thought this thread would blow up like this. It only goes to show many of us old geeks are still alive.\n\nFor the amusement of elderly citizens out there, I humbly present a letter from the Tandy district manager, no less, in 1978. You remember letters - paper, stamps, envelopes, waiting by the mailbox etc.\n\nI’m glad he mentioned all the “Soft Wear” to be had.\n\nAnd if you needed more convincing that a TRS-80 was the best computer value available today!', 'aiModelVersion': '1'}",0.9996
Rick Hogan,Updated 4y,How do I know if I need more RAM?,"As mentioned previously, the Performance tab in Windows Task Manager will show you how much of your RAM is actually being used.

Here is an example from my computer, with Outlook open, and several open tabs in Google Chrome:

As you can see, I am currently using less than half of my 16 GB of RAM, so I am in good shape.

On the other hand, here is what I see on a computer with only 4 GB of RAM, with Outlook and Chrome open:

You can immediately see that a very high percentage of the RAM is in use on this computer. That’s a sign that this computer could use more RAM. However, you might now be wondering why one computer is using 6.6 GB of RAM to do pretty much the same thing the other computer is doing, while using only 3.5 GB of RAM. Here’s where things get a bit more complicated.

Windows uses something called Virtual Memory. This is a method where Windows divides up the available RAM among the many processes running in the system. It keeps things in RAM when they are actively being used, and moves things out of RAM and onto the disk when they are not actively being used, in order to free up RAM for other things that are actively being used. The less RAM you have available, the more time Windows spends moving things back and forth from RAM to disk and from disk back to RAM. This is what really slows things down when you don’t have enough RAM. You can see how much of a problem this is by using Resource Monitor:

The column “Hard Faults/sec” under the Memory section of Resource Monitor shows how frequently processes are looking for some data to be in RAM, only to find that the needed data has been moved out of RAM and onto the disk. Every time there is a hard fault, the running process has to stop and wait for Windows to make room in RAM, by moving some other data out of RAM, and then move the needed data back into RAM from the disk, before the process can continue. If you constantly see a high rate of Hard Faults/sec, you can tell for sure that your computer is being slowed down by insufficient RAM.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/245rpxv1sfmn3aoc', 'title': 'How do I know if I need more RAM?', 'score': {'original': 0.9387, 'ai': 0.0613}, 'blocks': [{'text': 'As mentioned previously, the Performance tab in Windows Task Manager will show you how much of your RAM is actually being used.\n\nHere is an example from my computer, with Outlook open, and several open tabs in Google Chrome:\n\nAs you can see, I am currently using less than half of my 16 GB of RAM, so I am in good shape.\n\nOn the other hand, here is what I see on a computer with only 4 GB of RAM, with Outlook and Chrome open:\n\nYou can immediately see that a very high percentage of the RAM is in use on this computer. That’s a sign that this computer could use more RAM. However, you might now be wondering why one computer is using 6.6 GB of RAM to do pretty much the same thing the other computer is doing, while using only 3.5 GB of RAM. Here’s where things get a bit more complicated.\n\nWindows uses something called Virtual Memory. This is a method where Windows divides up the available RAM among the many processes running in the system. It keeps things in RAM when they are actively being used, and moves things out of RAM and onto the disk when they are not actively being used, in order to free up RAM for other things that are actively being used. The less RAM you have available, the more time Windows spends moving things back and forth from RAM to disk and from disk back to RAM. This is what really slows things down when you don’t have enough RAM. You can see how much of a problem this is by using Resource Monitor:\n\nThe column “Hard Faults/sec” under the Memory section of Resource Monitor shows how frequently processes are looking for some data to be in RAM, only to find that the needed data has been moved out of RAM and onto the disk. Every time there is a hard fault, the running process has to stop and wait for Windows to make room in RAM, by moving some other data out of RAM, and then move the needed data back into RAM from the disk, before the process can continue. If you constantly see a high rate of Hard Faults/sec, you can tell for sure that your computer is being slowed down by insufficient RAM.', 'result': {'fake': 0.0613, 'real': 0.9387}, 'status': 'success'}], 'credits_used': 4, 'credits': 1986032, 'subscription': 0, 'content': 'As mentioned previously, the Performance tab in Windows Task Manager will show you how much of your RAM is actually being used.\n\nHere is an example from my computer, with Outlook open, and several open tabs in Google Chrome:\n\nAs you can see, I am currently using less than half of my 16 GB of RAM, so I am in good shape.\n\nOn the other hand, here is what I see on a computer with only 4 GB of RAM, with Outlook and Chrome open:\n\nYou can immediately see that a very high percentage of the RAM is in use on this computer. That’s a sign that this computer could use more RAM. However, you might now be wondering why one computer is using 6.6 GB of RAM to do pretty much the same thing the other computer is doing, while using only 3.5 GB of RAM. Here’s where things get a bit more complicated.\n\nWindows uses something called Virtual Memory. This is a method where Windows divides up the available RAM among the many processes running in the system. It keeps things in RAM when they are actively being used, and moves things out of RAM and onto the disk when they are not actively being used, in order to free up RAM for other things that are actively being used. The less RAM you have available, the more time Windows spends moving things back and forth from RAM to disk and from disk back to RAM. This is what really slows things down when you don’t have enough RAM. You can see how much of a problem this is by using Resource Monitor:\n\nThe column “Hard Faults/sec” under the Memory section of Resource Monitor shows how frequently processes are looking for some data to be in RAM, only to find that the needed data has been moved out of RAM and onto the disk. Every time there is a hard fault, the running process has to stop and wait for Windows to make room in RAM, by moving some other data out of RAM, and then move the needed data back into RAM from the disk, before the process can continue. If you constantly see a high rate of Hard Faults/sec, you can tell for sure that your computer is being slowed down by insufficient RAM.', 'aiModelVersion': '1'}",0.9387
Mark Gritter,5y,"Didn't the person who wrote world's first compiler have to compile it somehow? Did they compile it at all, and if they did, how did they do that?","The first compiler was actually written in the language it compiled, but strangely, this language does not have a name. This work formed one of the very first dissertations in computer science, by Corrado Böhm
 in 1951. You can read an English translation here: http://www.itu.dk/people/sestoft/boehmthesis/boehm.pdf
 The compiler’s flow control is given as a graph:

Unfortunately, this compiler was not implemented on a real computer! It targeted an abstract machine, and the thesis does not contain the result of compiling the compiler. So, no, the very first compiler was not compiled at all.

The first commercially available compiler was for the language FORTRAN, in 1957, and was written in assembly by a team of programmers. But this was quickly followed by the first self-hosting compiler, for ALGOL, in 1958. (See History of compiler construction - Wikipedia
.) It was bootstrapped from an initial assembly-language version to a compiler written in ALGOL. Effectively this means the compiler was written twice, once in assembly (in simplified form) and then in the high-level language.

Programmers today typically use a different high-level language rather than assembly. Rust, for example, was bootstrapped from an original implementation written in OCaml; now the Rust compiler is written in Rust.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/aibu8c390gdkt1hf', 'title': ""Didn't the person who wrote world's first compiler have to compile it somehow? Did they compile it at all, and if they did, how did they do that?"", 'score': {'original': 0.9991, 'ai': 0.0009}, 'blocks': [{'text': 'The first compiler was actually written in the language it compiled, but strangely, this language does not have a name. This work formed one of the very first dissertations in computer science, by Corrado Böhm\n in 1951. You can read an English translation here: http://www.itu.dk/people/sestoft/boehmthesis/boehm.pdf\n The compiler’s flow control is given as a graph:\n\nUnfortunately, this compiler was not implemented on a real computer! It targeted an abstract machine, and the thesis does not contain the result of compiling the compiler. So, no, the very first compiler was not compiled at all.\n\nThe first commercially available compiler was for the language FORTRAN, in 1957, and was written in assembly by a team of programmers. But this was quickly followed by the first self-hosting compiler, for ALGOL, in 1958. (See History of compiler construction - Wikipedia\n.) It was bootstrapped from an initial assembly-language version to a compiler written in ALGOL. Effectively this means the compiler was written twice, once in assembly (in simplified form) and then in the high-level language.\n\nProgrammers today typically use a different high-level language rather than assembly. Rust, for example, was bootstrapped from an original implementation written in OCaml; now the Rust compiler is written in Rust.', 'result': {'fake': 0.0009, 'real': 0.9991}, 'status': 'success'}], 'credits_used': 3, 'credits': 1986029, 'subscription': 0, 'content': 'The first compiler was actually written in the language it compiled, but strangely, this language does not have a name. This work formed one of the very first dissertations in computer science, by Corrado Böhm\n in 1951. You can read an English translation here: http://www.itu.dk/people/sestoft/boehmthesis/boehm.pdf\n The compiler’s flow control is given as a graph:\n\nUnfortunately, this compiler was not implemented on a real computer! It targeted an abstract machine, and the thesis does not contain the result of compiling the compiler. So, no, the very first compiler was not compiled at all.\n\nThe first commercially available compiler was for the language FORTRAN, in 1957, and was written in assembly by a team of programmers. But this was quickly followed by the first self-hosting compiler, for ALGOL, in 1958. (See History of compiler construction - Wikipedia\n.) It was bootstrapped from an initial assembly-language version to a compiler written in ALGOL. Effectively this means the compiler was written twice, once in assembly (in simplified form) and then in the high-level language.\n\nProgrammers today typically use a different high-level language rather than assembly. Rust, for example, was bootstrapped from an original implementation written in OCaml; now the Rust compiler is written in Rust.', 'aiModelVersion': '1'}",0.9991
Alan Kay,Updated 5y,What are the Seven Wonders of computer science?,"I love this question — in no small part because I don’t think a really correct/exclusive answer is possible.

I have a particular set of meanings for the term “computer science” — the questioner might have meant to say “computing”. A few of my items might be taken from “computing” rather than from “computer science” per se.

The things I have found to be astonishing and amazing (and shocking) are:

Turing’s notion of machines that can simulate machines completely by interpreting their descriptions (exhibiting the programmable computer as “a language machine” and a “meta-language machine” — along with this is the simplicity of what is required to do so (a great book is Marvin Minsky’s “Computation: Finite and Infinite Machines”). Turing’s approach is much more of a “real CS” approach compared to Goedel’s earlier methods, and soon led to a large number of important next steps.
<this one is tentative at this point> How simple (a) it is to design a whole computer from just one kind of logical element (e.g. “NOT-BOTH”), especially when compared (b) to how Russell and Whitehead struggled to “bootstrap mathematics, etc., from logic at the turn of the last century. (This is one of those “Point of View is Worth 80 IQ Points” …)
Lisp, and McCarthy’s general approach to “mathematical theories of computation” and having languages that can act as their own metalanguage. One of the great cornucopias of our field.
Sketchpad by Ivan Sutherland for so many reasons, including: the approach to interactive computer graphics and the simulations of the graphic relationships, the “object-oriented” approach to definition and deriving new kinds of things (including “masters” and making instances from masters), enormous virtual worlds that are windowed on the display, the use of goal-directed programming with the system solving the simultaneous goals in real-time, etc. And more, including the demonstration that a simulated computer on a computer need look nothing like the underlying hardware or any “normal” idea of “computer”.
The big Shannon et al. ideas about how to have imperfect things be organized in systems that are much more perfectly behaved even if the organizational mechanisms are themselves noisy. Includes all forms of “noise”, “representations”, “communications”, “machines”, etc. and poking deeply into Biology and how living things work. Nice implications for “stochastic computing” of many kinds which are needed more and more as things scale.
The deep implications of “symbolic computation” (now a very un-funded area) for being able to move from the trivialities of “data” (no matter how voluminous”) to the profundities and powers of “Meaning”. This used to be called “AI” and now has to be called “real AI” or “strong AI” (it would be much better under a less loaded term: how about “Flexible Competence”?)
The Internet. Certainly the best thing done by my research community, and the first real essay into the kinds of scaling and stabilities that all computer science should be trying to understand and improve. This was a great invention and development process in all ways, and — by looking at Biology, which inspired but we really couldn’t use — it had a reasonable chance to work. That it was able to scale stably over more than 10 (maybe 11) orders of magnitude, as indeed planned, is still kind of amazing to me (even though it should have). Judging from most software systems today not being organized like the Internet, one is forced into the opinion that most computerists don’t understand it, why it is great (and maybe don’t even think of it as the fruits of “real computer science” because it just works so much better and more reliably than most other attempted artifacts in the field).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pqin91270yebm6gw', 'title': 'What are the Seven Wonders of computer science?', 'score': {'original': 0.8118, 'ai': 0.1882}, 'blocks': [{'text': 'I love this question — in no small part because I don’t think a really correct/exclusive answer is possible.\n\nI have a particular set of meanings for the term “computer science” — the questioner might have meant to say “computing”. A few of my items might be taken from “computing” rather than from “computer science” per se.\n\nThe things I have found to be astonishing and amazing (and shocking) are:\n\nTuring’s notion of machines that can simulate machines completely by interpreting their descriptions (exhibiting the programmable computer as “a language machine” and a “meta-language machine” — along with this is the simplicity of what is required to do so (a great book is Marvin Minsky’s “Computation: Finite and Infinite Machines”). Turing’s approach is much more of a “real CS” approach compared to Goedel’s earlier methods, and soon led to a large number of important next steps.\n<this one is tentative at this point> How simple (a) it is to design a whole computer from just one kind of logical element (e.g. “NOT-BOTH”), especially when compared (b) to how Russell and Whitehead struggled to “bootstrap mathematics, etc., from logic at the turn of the last century. (This is one of those “Point of View is Worth 80 IQ Points” …)\nLisp, and McCarthy’s general approach to “mathematical theories of computation” and having languages that can act as their own metalanguage. One of the great cornucopias of our field.\nSketchpad by Ivan Sutherland for so many reasons, including: the approach to interactive computer graphics and the simulations of the graphic relationships, the “object-oriented” approach to definition and deriving new kinds of things (including “masters” and making instances from masters), enormous virtual worlds that are windowed on the display, the use of goal-directed programming with the system solving the simultaneous goals in real-time, etc. And more, including the demonstration that a simulated computer on a computer need look nothing like the underlying hardware or any “normal” idea of “computer”.\nThe big Shannon et al. ideas about how to have imperfect things be organized in systems that are much more perfectly behaved even if the organizational mechanisms are themselves noisy. Includes all forms of “noise”, “representations”, “communications”, “machines”, etc. and poking deeply into Biology and how living things work. Nice implications for “stochastic computing” of many kinds which are needed more and more as things scale.\nThe deep implications of “symbolic computation” (now a very un-funded area) for being able to move from the trivialities of “data” (no matter how voluminous”) to the profundities and powers of “Meaning”. This used to be called “AI” and now has to be called “real AI” or “strong AI” (it would be much better under a less loaded term: how about “Flexible Competence”?)\nThe Internet. Certainly the best thing done by my research community, and the first real essay into the kinds of scaling and stabilities that all computer science should be trying to understand and improve. This was a great invention and development process in all ways, and — by looking at Biology, which inspired but we really couldn’t use — it had a reasonable chance to work.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}, {'text': 'That it was able to scale stably over more than 10 (maybe 11) orders of magnitude, as indeed planned, is still kind of amazing to me (even though it should have). Judging from most software systems today not being organized like the Internet, one is forced into the opinion that most computerists don’t understand it, why it is great (and maybe don’t even think of it as the fruits of “real computer science” because it just works so much better and more reliably than most other attempted artifacts in the field).', 'result': {'fake': 0.0044, 'real': 0.9956}, 'status': 'success'}], 'credits_used': 7, 'credits': 1986022, 'subscription': 0, 'content': 'I love this question — in no small part because I don’t think a really correct/exclusive answer is possible.\n\nI have a particular set of meanings for the term “computer science” — the questioner might have meant to say “computing”. A few of my items might be taken from “computing” rather than from “computer science” per se.\n\nThe things I have found to be astonishing and amazing (and shocking) are:\n\nTuring’s notion of machines that can simulate machines completely by interpreting their descriptions (exhibiting the programmable computer as “a language machine” and a “meta-language machine” — along with this is the simplicity of what is required to do so (a great book is Marvin Minsky’s “Computation: Finite and Infinite Machines”). Turing’s approach is much more of a “real CS” approach compared to Goedel’s earlier methods, and soon led to a large number of important next steps.\n<this one is tentative at this point> How simple (a) it is to design a whole computer from just one kind of logical element (e.g. “NOT-BOTH”), especially when compared (b) to how Russell and Whitehead struggled to “bootstrap mathematics, etc., from logic at the turn of the last century. (This is one of those “Point of View is Worth 80 IQ Points” …)\nLisp, and McCarthy’s general approach to “mathematical theories of computation” and having languages that can act as their own metalanguage. One of the great cornucopias of our field.\nSketchpad by Ivan Sutherland for so many reasons, including: the approach to interactive computer graphics and the simulations of the graphic relationships, the “object-oriented” approach to definition and deriving new kinds of things (including “masters” and making instances from masters), enormous virtual worlds that are windowed on the display, the use of goal-directed programming with the system solving the simultaneous goals in real-time, etc. And more, including the demonstration that a simulated computer on a computer need look nothing like the underlying hardware or any “normal” idea of “computer”.\nThe big Shannon et al. ideas about how to have imperfect things be organized in systems that are much more perfectly behaved even if the organizational mechanisms are themselves noisy. Includes all forms of “noise”, “representations”, “communications”, “machines”, etc. and poking deeply into Biology and how living things work. Nice implications for “stochastic computing” of many kinds which are needed more and more as things scale.\nThe deep implications of “symbolic computation” (now a very un-funded area) for being able to move from the trivialities of “data” (no matter how voluminous”) to the profundities and powers of “Meaning”. This used to be called “AI” and now has to be called “real AI” or “strong AI” (it would be much better under a less loaded term: how about “Flexible Competence”?)\nThe Internet. Certainly the best thing done by my research community, and the first real essay into the kinds of scaling and stabilities that all computer science should be trying to understand and improve. This was a great invention and development process in all ways, and — by looking at Biology, which inspired but we really couldn’t use — it had a reasonable chance to work. That it was able to scale stably over more than 10 (maybe 11) orders of magnitude, as indeed planned, is still kind of amazing to me (even though it should have). Judging from most software systems today not being organized like the Internet, one is forced into the opinion that most computerists don’t understand it, why it is great (and maybe don’t even think of it as the fruits of “real computer science” because it just works so much better and more reliably than most other attempted artifacts in the field).', 'aiModelVersion': '1'}",0.8118
E. Boulesteix,Updated 1y,Why do CPUs need a clock? Why can’t they just do things “as fast as possible” without worrying about how fast they are doing it?,"They don't, strictly speaking. It just makes things easier.

Ever heard about those old-fashioned river locks?

(Image source
)

They're essentially just elevators for boats. You can both pump water in or out of the lock and open the doors on either side. Properly operated, it allows your boat to clear a gap of a few meters. I'd really recommend watching a video about their operation.

Traversing a lock is done in steps: first you match the internal water level with the side you're on, then you open the door on your side. You then enter into the lock, close the door, and either raise or drop the internal water level to match the other side of the lock. You then open the door and leave the lock on the other side.

This all takes time, and depending on the lock, its state and a myriad of other factors, it might take more or less of it.

That's all good, but what happens if you have many locks back to back?

(Image from article
)

And what happens if you have a constant stream of boats wanting to go from one end to the other?

Two boats cannot be in one lock at a time. You must wait for the next lock to be clear before a new boat can enter. The question here is the following: how do you ensure property traffic control, avoiding collisions without throwing throughput out the window?

The “clocked"" version of this problem boils down to the following: you know that it only takes so much time for a boat to clear a lock, even in the worst possible case. Call it 10 minutes for example's sake.

To ensure good throughput, you'll let all the boats move in lockstep every 10 minutes. Every 10 minutes exactly, each boat will move to the next lock simultaneously. This ensures that even the slowest boat will have cleared the lock, so there won't be any collisions. Simple, although it may not necessarily guarantee the best throughput as you pointed out. Crucially though, it's effectively “blind"": you don't need to keep track of where boats are in the system if you give them sufficient time between steps.

The “clock-less"" version of the problem involves each boat signaling the one(s) before it once it's cleared a lock, which it will only be able to do once the lock in front of it has also been cleared. You might also imagine a system where each lock houses a “traffic control"" person who can see adjacent locks and instructs boats on when to enter and exit the lock.

On the face of it, this second system trades simplicity for throughput. It's already pretty apparent that the asynchronous version requires a massively more complex control system to function. Maybe it'll be faster, but there's a cost to it.

Back to CPUs for a bit: there have been designs for asynchronous (clockless) CPUs before. A simple Google search will return many hits on the subject, mainly in the form of research papers.

Usually, such designs promise increased performance and efficiency compared to synchronous (clocked) designs. However, to my knowledge the success of these designs has been limited. I don't think there have been more than a handful of low-power processors built using such a design.

Part of the problem is the increased complexity inherent to such a design: instead of having a clock signal which you propagate around, you need inter-stage interlocks between pipeline stages to control the flow of information through the processor. There's a lot more to keep track of, meaning more wires, more transistors, and more complexity.

To my knowledge too, it wouldn't necessarily help all that much: for one, throughput is not the limiting factor when it comes to modern processor performance. Actually feeding work to the machine (“feeding the beast"") is, as most high-performance chips spend most of their time idling.

Additionally, modern designs use dynamic logic to save power and area compared to static logic. Dynamic logic essentially requires a “clock"" to refresh it's cells regularly. I'm not saying you couldn't build an asynchronous design with dynamic logic, but it might pose additional constraints and eat into any potential benefits of the design type.

But I'd simply argue: Intel and AMD (and others) are constantly implementing new somewhat-experimental tricks to get more performance or efficiency from their designs. I'd imagine they probably considered a clockless CPU before, and rejected the idea.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/aowgylust1ehqkvj', 'title': 'Why do CPUs need a clock? Why can’t they just do things “as fast as possible” without worrying about how fast they are doing it?', 'score': {'original': 0.7524, 'ai': 0.2476}, 'blocks': [{'text': 'They don\'t, strictly speaking. It just makes things easier.\n\nEver heard about those old-fashioned river locks?\n\n(Image source\n)\n\nThey\'re essentially just elevators for boats. You can both pump water in or out of the lock and open the doors on either side. Properly operated, it allows your boat to clear a gap of a few meters. I\'d really recommend watching a video about their operation.\n\nTraversing a lock is done in steps: first you match the internal water level with the side you\'re on, then you open the door on your side. You then enter into the lock, close the door, and either raise or drop the internal water level to match the other side of the lock. You then open the door and leave the lock on the other side.\n\nThis all takes time, and depending on the lock, its state and a myriad of other factors, it might take more or less of it.\n\nThat\'s all good, but what happens if you have many locks back to back?\n\n(Image from article\n)\n\nAnd what happens if you have a constant stream of boats wanting to go from one end to the other?\n\nTwo boats cannot be in one lock at a time. You must wait for the next lock to be clear before a new boat can enter. The question here is the following: how do you ensure property traffic control, avoiding collisions without throwing throughput out the window?\n\nThe “clocked"" version of this problem boils down to the following: you know that it only takes so much time for a boat to clear a lock, even in the worst possible case. Call it 10 minutes for example\'s sake.\n\nTo ensure good throughput, you\'ll let all the boats move in lockstep every 10 minutes. Every 10 minutes exactly, each boat will move to the next lock simultaneously. This ensures that even the slowest boat will have cleared the lock, so there won\'t be any collisions. Simple, although it may not necessarily guarantee the best throughput as you pointed out. Crucially though, it\'s effectively “blind"": you don\'t need to keep track of where boats are in the system if you give them sufficient time between steps.\n\nThe “clock-less"" version of the problem involves each boat signaling the one(s) before it once it\'s cleared a lock, which it will only be able to do once the lock in front of it has also been cleared. You might also imagine a system where each lock houses a “traffic control"" person who can see adjacent locks and instructs boats on when to enter and exit the lock.\n\nOn the face of it, this second system trades simplicity for throughput. It\'s already pretty apparent that the asynchronous version requires a massively more complex control system to function. Maybe it\'ll be faster, but there\'s a cost to it.\n\nBack to CPUs for a bit: there have been designs for asynchronous (clockless) CPUs before. A simple Google search will return many hits on the subject, mainly in the form of research papers.\n\nUsually, such designs promise increased performance and efficiency compared to synchronous (clocked) designs. However, to my knowledge the success of these', 'result': {'fake': 0.7963, 'real': 0.2037}, 'status': 'success'}, {'text': 'designs has been limited. I don\'t think there have been more than a handful of low-power processors built using such a design.\n\nPart of the problem is the increased complexity inherent to such a design: instead of having a clock signal which you propagate around, you need inter-stage interlocks between pipeline stages to control the flow of information through the processor. There\'s a lot more to keep track of, meaning more wires, more transistors, and more complexity.\n\nTo my knowledge too, it wouldn\'t necessarily help all that much: for one, throughput is not the limiting factor when it comes to modern processor performance. Actually feeding work to the machine (“feeding the beast"") is, as most high-performance chips spend most of their time idling.\n\nAdditionally, modern designs use dynamic logic to save power and area compared to static logic. Dynamic logic essentially requires a “clock"" to refresh it\'s cells regularly. I\'m not saying you couldn\'t build an asynchronous design with dynamic logic, but it might pose additional constraints and eat into any potential benefits of the design type.\n\nBut I\'d simply argue: Intel and AMD (and others) are constantly implementing new somewhat-experimental tricks to get more performance or efficiency from their designs. I\'d imagine they probably considered a clockless CPU before, and rejected the idea.', 'result': {'fake': 0.1074, 'real': 0.8926}, 'status': 'success'}], 'credits_used': 8, 'credits': 1986014, 'subscription': 0, 'content': 'They don\'t, strictly speaking. It just makes things easier.\n\nEver heard about those old-fashioned river locks?\n\n(Image source\n)\n\nThey\'re essentially just elevators for boats. You can both pump water in or out of the lock and open the doors on either side. Properly operated, it allows your boat to clear a gap of a few meters. I\'d really recommend watching a video about their operation.\n\nTraversing a lock is done in steps: first you match the internal water level with the side you\'re on, then you open the door on your side. You then enter into the lock, close the door, and either raise or drop the internal water level to match the other side of the lock. You then open the door and leave the lock on the other side.\n\nThis all takes time, and depending on the lock, its state and a myriad of other factors, it might take more or less of it.\n\nThat\'s all good, but what happens if you have many locks back to back?\n\n(Image from article\n)\n\nAnd what happens if you have a constant stream of boats wanting to go from one end to the other?\n\nTwo boats cannot be in one lock at a time. You must wait for the next lock to be clear before a new boat can enter. The question here is the following: how do you ensure property traffic control, avoiding collisions without throwing throughput out the window?\n\nThe “clocked"" version of this problem boils down to the following: you know that it only takes so much time for a boat to clear a lock, even in the worst possible case. Call it 10 minutes for example\'s sake.\n\nTo ensure good throughput, you\'ll let all the boats move in lockstep every 10 minutes. Every 10 minutes exactly, each boat will move to the next lock simultaneously. This ensures that even the slowest boat will have cleared the lock, so there won\'t be any collisions. Simple, although it may not necessarily guarantee the best throughput as you pointed out. Crucially though, it\'s effectively “blind"": you don\'t need to keep track of where boats are in the system if you give them sufficient time between steps.\n\nThe “clock-less"" version of the problem involves each boat signaling the one(s) before it once it\'s cleared a lock, which it will only be able to do once the lock in front of it has also been cleared. You might also imagine a system where each lock houses a “traffic control"" person who can see adjacent locks and instructs boats on when to enter and exit the lock.\n\nOn the face of it, this second system trades simplicity for throughput. It\'s already pretty apparent that the asynchronous version requires a massively more complex control system to function. Maybe it\'ll be faster, but there\'s a cost to it.\n\nBack to CPUs for a bit: there have been designs for asynchronous (clockless) CPUs before. A simple Google search will return many hits on the subject, mainly in the form of research papers.\n\nUsually, such designs promise increased performance and efficiency compared to synchronous (clocked) designs. However, to my knowledge the success of these designs has been limited. I don\'t think there have been more than a handful of low-power processors built using such a design.\n\nPart of the problem is the increased complexity inherent to such a design: instead of having a clock signal which you propagate around, you need inter-stage interlocks between pipeline stages to control the flow of information through the processor. There\'s a lot more to keep track of, meaning more wires, more transistors, and more complexity.\n\nTo my knowledge too, it wouldn\'t necessarily help all that much: for one, throughput is not the limiting factor when it comes to modern processor performance. Actually feeding work to the machine (“feeding the beast"") is, as most high-performance chips spend most of their time idling.\n\nAdditionally, modern designs use dynamic logic to save power and area compared to static logic. Dynamic logic essentially requires a “clock"" to refresh it\'s cells regularly. I\'m not saying you couldn\'t build an asynchronous design with dynamic logic, but it might pose additional constraints and eat into any potential benefits of the design type.\n\nBut I\'d simply argue: Intel and AMD (and others) are constantly implementing new somewhat-experimental tricks to get more performance or efficiency from their designs. I\'d imagine they probably considered a clockless CPU before, and rejected the idea.', 'aiModelVersion': '1'}",0.7524
Alon Amit,9y,What is the best book to explore the depth of the P versus NP problem?,"I highly recommend Barak and Arora's ""Computational Complexity: A Modern Approach"".


It starts with the very basics and proceeds to cover several advanced topics such as derandomization and the PCP theorem, including a complete proof thereof.

The book obviously doesn't cover everything that would count as ""the depth of P vs NP"". There are countless papers that explore the problem in many different directions. As far as books go, and without focusing on one specialized sub-domain, I think it's one of the best options around.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/t2w87xfbr1uzikhy', 'title': 'What is the best book to explore the depth of the P versus NP problem?', 'score': {'original': 0.95, 'ai': 0.05}, 'blocks': [{'text': 'I highly recommend Barak and Arora\'s ""Computational Complexity: A Modern Approach"".\n\n\nIt starts with the very basics and proceeds to cover several advanced topics such as derandomization and the PCP theorem, including a complete proof thereof.\n\nThe book obviously doesn\'t cover everything that would count as ""the depth of P vs NP"". There are countless papers that explore the problem in many different directions. As far as books go, and without focusing on one specialized sub-domain, I think it\'s one of the best options around.', 'result': {'fake': 0.2721, 'real': 0.7279}, 'status': 'success'}], 'credits_used': 1, 'credits': 1986013, 'subscription': 0, 'content': 'I highly recommend Barak and Arora\'s ""Computational Complexity: A Modern Approach"".\n\n\nIt starts with the very basics and proceeds to cover several advanced topics such as derandomization and the PCP theorem, including a complete proof thereof.\n\nThe book obviously doesn\'t cover everything that would count as ""the depth of P vs NP"". There are countless papers that explore the problem in many different directions. As far as books go, and without focusing on one specialized sub-domain, I think it\'s one of the best options around.', 'aiModelVersion': '1'}",0.95
Brett Bergan,Updated 3y,Why do people want to build a PC nowadays instead of buying it fully built?,"It’s like the old quip about the Model-T attributed to Henry Ford:

“You can get it in any color you want—as long as it’s black.”

Any PC manufacturer is going to use as many commodity-grade components as they can get away with for a reasonable margin of profit. The lid goes on and the average consumer won’t look inside until something breaks.

There is nothing wrong with 8GB of plain-green DDR4–2400, an unbranded SATA SSD, and a generic GTX 1650. This PC will run just fine on a 60Hz monitor. But then a seller like HP will turn around and try to sell it to you for $756.42 at Walmart when it has $450 worth of components inside.

This is a perfectly fine all-purpose computer that will run any game at 1080p. You’re essentially paying $300 for a clever green box.

Some people are okay with that, but there are others who want to get a lot better performance from a $750 PC. Everyone is free to choose their preference.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/4fl5wak2syjmp18q', 'title': 'Why do people want to build a PC nowadays instead of buying it fully built?', 'score': {'original': 0.1029, 'ai': 0.8971}, 'blocks': [{'text': 'It’s like the old quip about the Model-T attributed to Henry Ford:\n\n“You can get it in any color you want—as long as it’s black.”\n\nAny PC manufacturer is going to use as many commodity-grade components as they can get away with for a reasonable margin of profit. The lid goes on and the average consumer won’t look inside until something breaks.\n\nThere is nothing wrong with 8GB of plain-green DDR4–2400, an unbranded SATA SSD, and a generic GTX 1650. This PC will run just fine on a 60Hz monitor. But then a seller like HP will turn around and try to sell it to you for $756.42 at Walmart when it has $450 worth of components inside.\n\nThis is a perfectly fine all-purpose computer that will run any game at 1080p. You’re essentially paying $300 for a clever green box.\n\nSome people are okay with that, but there are others who want to get a lot better performance from a $750 PC. Everyone is free to choose their preference.', 'result': {'fake': 0.8971, 'real': 0.1029}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986011, 'subscription': 0, 'content': 'It’s like the old quip about the Model-T attributed to Henry Ford:\n\n“You can get it in any color you want—as long as it’s black.”\n\nAny PC manufacturer is going to use as many commodity-grade components as they can get away with for a reasonable margin of profit. The lid goes on and the average consumer won’t look inside until something breaks.\n\nThere is nothing wrong with 8GB of plain-green DDR4–2400, an unbranded SATA SSD, and a generic GTX 1650. This PC will run just fine on a 60Hz monitor. But then a seller like HP will turn around and try to sell it to you for $756.42 at Walmart when it has $450 worth of components inside.\n\nThis is a perfectly fine all-purpose computer that will run any game at 1080p. You’re essentially paying $300 for a clever green box.\n\nSome people are okay with that, but there are others who want to get a lot better performance from a $750 PC. Everyone is free to choose their preference.', 'aiModelVersion': '1'}",0.1029
Jerry Rufener,Updated 3y,How was the first programming language created without an operating system?,"Computers don’t necessarily have to have operating systems!!!!

An extremely popular computer was the PDP8 series of computers introduced in 1965 and a development the earlier PDP5. They were made by Digital Equipment Corporation (DEC) of Maynard, MA. They were THE minicomputer for many years. It had several OS’s available but were often sold/used without an OS - in fact without a disk. I have personally used them that way.

Here is a picture of one of the more popular versions, the PDP8/e. (By Florian Schäffer - Own work, CC BY-SA 4.0, File:Digital pdp8-e2.jpg
)

Notice the switches. To use it you sat in front of it and, using those switches, toggled in the RIM (Read In Mode) loader which was (as I recall) about 20 instructions long. You entered the 12 bit address using the switches, you pressed LOAD ADDR, you entered your first 12 bit instruction, you pressed DEP - that instruction was loaded at the specified memory address and address was incremented. You loaded your next instruction, you pressed DEP again … repeated until all the instructions were loaded. Now you entered the starting address of the loader, pressed LOAD ADDR and then pressed RUN. The RIM loader would execute. Its job was to load the BIN (Binary) loader, on paper tape, from your ASR-33 teletype (33 character per second). The BIN loaded would then load a program from a binary paper tape from the ASR-33.

If it was a cold start you probably read in editor - which allowed you to enter your source code at the, again, ASR-33. The editor would then print out a listing of your program on the ASR-33 and a paper tape of the source code. The source code was most likely in assembler. You then loaded the assembly program using - you guessed it - the ASR33.

The assembler read the source code from the paper tape using the ASR33. If all went well, the assembler would punch out a paper tape in object format. More likely - the first few times - you got errors and had to go back to the editor to fix them. You goal was to produce a paper tape with object code on it.

You would then load the linker and link all of the object modules you produced together - along with any libraries. This could produce more errors which sent you back several steps.

Once you had a clean binary, you loaded in using the BIN loader and the debugging fun began! No symbolic debug - you put debugging instructions into your own code.

Fun huh???

There were disks available - very expensive - very small. Also available was “drum” memory - it is like a disk but built like an Edison cylindrical phonograph. More common were tapes - both 7 and 9 track IBM compatible mag-tapes could be had - if you had the money. More common was something called DECtape - which was actually pretty slick and found on, I think, most PPD8’s. You could get an high speed paper tape reader (100 cps). You could get a ROM loader with RIM on it, but it took and entire slot in the computer, to save the toggling in process. Why didn’t everyone get these items? The computer alone cost $10,000 - for a base model. A disk would at least double the price. You can see where that goes.

Now if you were with a large organization they would have one machine with all the bells and whistles for the developer. They then distributed their application (and updates) on paper tape to machines in the field that were bare bones. I worked on one veneer processing system that was exactly that way. I was with DEC - the OEM (the company that developed the software) bought PDP8s from DEC - they built the electrical/electronic interface to control the veneer manufacturing equipment. They sold it to the end user along with the software.

While you could buy a computer directly from DEC, most were sold via the OEM route. They were installed and used without an OS.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ij8n1ykgmh5zf3v2', 'title': 'How was the first programming language created without an operating system?', 'score': {'original': 0.99245, 'ai': 0.00755}, 'blocks': [{'text': 'Computers don’t necessarily have to have operating systems!!!!\n\nAn extremely popular computer was the PDP8 series of computers introduced in 1965 and a development the earlier PDP5. They were made by Digital Equipment Corporation (DEC) of Maynard, MA. They were THE minicomputer for many years. It had several OS’s available but were often sold/used without an OS - in fact without a disk. I have personally used them that way.\n\nHere is a picture of one of the more popular versions, the PDP8/e. (By Florian Schäffer - Own work, CC BY-SA 4.0, File:Digital pdp8-e2.jpg\n)\n\nNotice the switches. To use it you sat in front of it and, using those switches, toggled in the RIM (Read In Mode) loader which was (as I recall) about 20 instructions long. You entered the 12 bit address using the switches, you pressed LOAD ADDR, you entered your first 12 bit instruction, you pressed DEP - that instruction was loaded at the specified memory address and address was incremented. You loaded your next instruction, you pressed DEP again … repeated until all the instructions were loaded. Now you entered the starting address of the loader, pressed LOAD ADDR and then pressed RUN. The RIM loader would execute. Its job was to load the BIN (Binary) loader, on paper tape, from your ASR-33 teletype (33 character per second). The BIN loaded would then load a program from a binary paper tape from the ASR-33.\n\nIf it was a cold start you probably read in editor - which allowed you to enter your source code at the, again, ASR-33. The editor would then print out a listing of your program on the ASR-33 and a paper tape of the source code. The source code was most likely in assembler. You then loaded the assembly program using - you guessed it - the ASR33.\n\nThe assembler read the source code from the paper tape using the ASR33. If all went well, the assembler would punch out a paper tape in object format. More likely - the first few times - you got errors and had to go back to the editor to fix them. You goal was to produce a paper tape with object code on it.\n\nYou would then load the linker and link all of the object modules you produced together - along with any libraries. This could produce more errors which sent you back several steps.\n\nOnce you had a clean binary, you loaded in using the BIN loader and the debugging fun began! No symbolic debug - you put debugging instructions into your own code.\n\nFun huh???\n\nThere were disks available - very expensive - very small. Also available was “drum” memory - it is like a disk but built like an Edison cylindrical phonograph. More common were tapes - both 7 and 9 track IBM compatible mag-tapes could be had - if you had the money. More common was something called DECtape - which was actually pretty slick and found on, I think, most PPD8’s. You could get an high speed paper tape reader (100 cps). You could get a ROM loader with RIM', 'result': {'fake': 0.0167, 'real': 0.9833}, 'status': 'success'}, {'text': 'on it, but it took and entire slot in the computer, to save the toggling in process. Why didn’t everyone get these items? The computer alone cost $10,000 - for a base model. A disk would at least double the price. You can see where that goes.\n\nNow if you were with a large organization they would have one machine with all the bells and whistles for the developer. They then distributed their application (and updates) on paper tape to machines in the field that were bare bones. I worked on one veneer processing system that was exactly that way. I was with DEC - the OEM (the company that developed the software) bought PDP8s from DEC - they built the electrical/electronic interface to control the veneer manufacturing equipment. They sold it to the end user along with the software.\n\nWhile you could buy a computer directly from DEC, most were sold via the OEM route. They were installed and used without an OS.', 'result': {'fake': 0.0717, 'real': 0.9283}, 'status': 'success'}], 'credits_used': 7, 'credits': 1986004, 'subscription': 0, 'content': 'Computers don’t necessarily have to have operating systems!!!!\n\nAn extremely popular computer was the PDP8 series of computers introduced in 1965 and a development the earlier PDP5. They were made by Digital Equipment Corporation (DEC) of Maynard, MA. They were THE minicomputer for many years. It had several OS’s available but were often sold/used without an OS - in fact without a disk. I have personally used them that way.\n\nHere is a picture of one of the more popular versions, the PDP8/e. (By Florian Schäffer - Own work, CC BY-SA 4.0, File:Digital pdp8-e2.jpg\n)\n\nNotice the switches. To use it you sat in front of it and, using those switches, toggled in the RIM (Read In Mode) loader which was (as I recall) about 20 instructions long. You entered the 12 bit address using the switches, you pressed LOAD ADDR, you entered your first 12 bit instruction, you pressed DEP - that instruction was loaded at the specified memory address and address was incremented. You loaded your next instruction, you pressed DEP again … repeated until all the instructions were loaded. Now you entered the starting address of the loader, pressed LOAD ADDR and then pressed RUN. The RIM loader would execute. Its job was to load the BIN (Binary) loader, on paper tape, from your ASR-33 teletype (33 character per second). The BIN loaded would then load a program from a binary paper tape from the ASR-33.\n\nIf it was a cold start you probably read in editor - which allowed you to enter your source code at the, again, ASR-33. The editor would then print out a listing of your program on the ASR-33 and a paper tape of the source code. The source code was most likely in assembler. You then loaded the assembly program using - you guessed it - the ASR33.\n\nThe assembler read the source code from the paper tape using the ASR33. If all went well, the assembler would punch out a paper tape in object format. More likely - the first few times - you got errors and had to go back to the editor to fix them. You goal was to produce a paper tape with object code on it.\n\nYou would then load the linker and link all of the object modules you produced together - along with any libraries. This could produce more errors which sent you back several steps.\n\nOnce you had a clean binary, you loaded in using the BIN loader and the debugging fun began! No symbolic debug - you put debugging instructions into your own code.\n\nFun huh???\n\nThere were disks available - very expensive - very small. Also available was “drum” memory - it is like a disk but built like an Edison cylindrical phonograph. More common were tapes - both 7 and 9 track IBM compatible mag-tapes could be had - if you had the money. More common was something called DECtape - which was actually pretty slick and found on, I think, most PPD8’s. You could get an high speed paper tape reader (100 cps). You could get a ROM loader with RIM on it, but it took and entire slot in the computer, to save the toggling in process. Why didn’t everyone get these items? The computer alone cost $10,000 - for a base model. A disk would at least double the price. You can see where that goes.\n\nNow if you were with a large organization they would have one machine with all the bells and whistles for the developer. They then distributed their application (and updates) on paper tape to machines in the field that were bare bones. I worked on one veneer processing system that was exactly that way. I was with DEC - the OEM (the company that developed the software) bought PDP8s from DEC - they built the electrical/electronic interface to control the veneer manufacturing equipment. They sold it to the end user along with the software.\n\nWhile you could buy a computer directly from DEC, most were sold via the OEM route. They were installed and used without an OS.', 'aiModelVersion': '1'}",0.99245
Staffan Sandström,1y,The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?,"We, don’t do that any more. But there is a problem coming up that’s called the Y2K38 problem. It haven’t gotten as much publicity as the Y2K problem as it’s further away and 3:14:07 UTC on 19 January 2038 isn’t as interesting as a new years eve. That problem is probably a bit harder to understand for laymen. In many systems the time is stored as seconds from 1970. And it’s stored as a 32-bit integer so we’re running out of seconds. Some have switched to using a 64-bit integer instead and that should work for 292 billion years.

In 2108 the timestamps for DOS files will not work. But that’s hardly a major issue.

JavaScript uses milliseconds since 1 January 1970 and they will not work past 3 September, 275,760. But that gives some time to come up with something.

So there are some other issues that are more relevant than the Y10K bug.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/bleyc8x3o7ztv21i', 'title': 'The Y2K bug was blamed on lack of foresight. Should we start storing the year in 5 digits to avoid the Y10K bug?', 'score': {'original': 0.9231, 'ai': 0.0769}, 'blocks': [{'text': 'We, don’t do that any more. But there is a problem coming up that’s called the Y2K38 problem. It haven’t gotten as much publicity as the Y2K problem as it’s further away and 3:14:07 UTC on 19 January 2038 isn’t as interesting as a new years eve. That problem is probably a bit harder to understand for laymen. In many systems the time is stored as seconds from 1970. And it’s stored as a 32-bit integer so we’re running out of seconds. Some have switched to using a 64-bit integer instead and that should work for 292 billion years.\n\nIn 2108 the timestamps for DOS files will not work. But that’s hardly a major issue.\n\nJavaScript uses milliseconds since 1 January 1970 and they will not work past 3 September, 275,760. But that gives some time to come up with something.\n\nSo there are some other issues that are more relevant than the Y10K bug.', 'result': {'fake': 0.0769, 'real': 0.9231}, 'status': 'success'}], 'credits_used': 2, 'credits': 1986002, 'subscription': 0, 'content': 'We, don’t do that any more. But there is a problem coming up that’s called the Y2K38 problem. It haven’t gotten as much publicity as the Y2K problem as it’s further away and 3:14:07 UTC on 19 January 2038 isn’t as interesting as a new years eve. That problem is probably a bit harder to understand for laymen. In many systems the time is stored as seconds from 1970. And it’s stored as a 32-bit integer so we’re running out of seconds. Some have switched to using a 64-bit integer instead and that should work for 292 billion years.\n\nIn 2108 the timestamps for DOS files will not work. But that’s hardly a major issue.\n\nJavaScript uses milliseconds since 1 January 1970 and they will not work past 3 September, 275,760. But that gives some time to come up with something.\n\nSo there are some other issues that are more relevant than the Y10K bug.', 'aiModelVersion': '1'}",0.9231
E. Boulesteix,4y,Why is it so hard to make CPU?,"Making a CPU isn’t hard on its own. Many university students in CS have designed simple CPUs before. Heck, that’s what I’m aiming for as well.

With the advent of FPGAs especially, designing a CPU is actually quite accessible, assuming you have the know-how.

However, the CPUs you might build at school or as a hobby project aren’t very complex: they can’t be due to time and budgetary constraints. You’ll typically build something similar to what would have been around during the 70s and 80s. These chips are slow, scalar and generally don’t have virtual memory support, so they can’t do much.

Modern CPUs made by ARM/Intel/AMD etc are very different beasts altogether. Not necessarily because the concepts are much more complicated, but because complexity is substantially higher: processors from the 70s might have a couple thousand transistors, while modern ones have billions of them.

More complex circuitry means you need more manpower to design a piece of hardware. That larger piece of hardware needs more thorough verification due to its increased complexity. As nodes progress, mapping this hardware to silicon also becomes increasingly difficult to do well. And companies like Intel also need to worry about selling their hardware and keeping up with competition, which isn’t necessarily something you need to worry about when designing a CPU for fun or for school.

If anything, most of the difficulty of designing modern processors simply comes down to scale.

Think about it:

This…[1]

…is probably a lot easier to design and build compared to this:[2]

Footnotes

[1] Chester’s Old Stone Bridge
[2] 10 longest bridges to drive across","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/vb1ahm6wq78o3zue', 'title': 'Why is it so hard to make CPU?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'Making a CPU isn’t hard on its own. Many university students in CS have designed simple CPUs before. Heck, that’s what I’m aiming for as well.\n\nWith the advent of FPGAs especially, designing a CPU is actually quite accessible, assuming you have the know-how.\n\nHowever, the CPUs you might build at school or as a hobby project aren’t very complex: they can’t be due to time and budgetary constraints. You’ll typically build something similar to what would have been around during the 70s and 80s. These chips are slow, scalar and generally don’t have virtual memory support, so they can’t do much.\n\nModern CPUs made by ARM/Intel/AMD etc are very different beasts altogether. Not necessarily because the concepts are much more complicated, but because complexity is substantially higher: processors from the 70s might have a couple thousand transistors, while modern ones have billions of them.\n\nMore complex circuitry means you need more manpower to design a piece of hardware. That larger piece of hardware needs more thorough verification due to its increased complexity. As nodes progress, mapping this hardware to silicon also becomes increasingly difficult to do well. And companies like Intel also need to worry about selling their hardware and keeping up with competition, which isn’t necessarily something you need to worry about when designing a CPU for fun or for school.\n\nIf anything, most of the difficulty of designing modern processors simply comes down to scale.\n\nThink about it:\n\nThis…[1]\n\n…is probably a lot easier to design and build compared to this:[2]\n\nFootnotes\n\n[1] Chester’s Old Stone Bridge\n[2] 10 longest bridges to drive across', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985999, 'subscription': 0, 'content': 'Making a CPU isn’t hard on its own. Many university students in CS have designed simple CPUs before. Heck, that’s what I’m aiming for as well.\n\nWith the advent of FPGAs especially, designing a CPU is actually quite accessible, assuming you have the know-how.\n\nHowever, the CPUs you might build at school or as a hobby project aren’t very complex: they can’t be due to time and budgetary constraints. You’ll typically build something similar to what would have been around during the 70s and 80s. These chips are slow, scalar and generally don’t have virtual memory support, so they can’t do much.\n\nModern CPUs made by ARM/Intel/AMD etc are very different beasts altogether. Not necessarily because the concepts are much more complicated, but because complexity is substantially higher: processors from the 70s might have a couple thousand transistors, while modern ones have billions of them.\n\nMore complex circuitry means you need more manpower to design a piece of hardware. That larger piece of hardware needs more thorough verification due to its increased complexity. As nodes progress, mapping this hardware to silicon also becomes increasingly difficult to do well. And companies like Intel also need to worry about selling their hardware and keeping up with competition, which isn’t necessarily something you need to worry about when designing a CPU for fun or for school.\n\nIf anything, most of the difficulty of designing modern processors simply comes down to scale.\n\nThink about it:\n\nThis…[1]\n\n…is probably a lot easier to design and build compared to this:[2]\n\nFootnotes\n\n[1] Chester’s Old Stone Bridge\n[2] 10 longest bridges to drive across', 'aiModelVersion': '1'}",0.9999
Troy Hoffman,4y,What are some examples of extremely clever problem solving in the software development world?,"In 1983, this guy was a computer programmer at Lucasfilm.

He was optimizing a real time animation program and wanted to “unroll"" a loop in C. This is a way to optimize code by reducing the number of times the computer has to check if the loop can be ended.

This was the original loop:

send(to, from, count) 
register short *to, *from; 
register count; 
{ 
    do {                          /* count > 0 assumed */ 
        *to = *from++; 
    } while(--count > 0); 
} 

Normally, a loop is unrolled by dividing the loops by a number and copying the code within the loop that many times. So, something that loops 64 times would loop 8 times with the code repeated 8 times, eliminating 56 times that the program has to check if the loop is complete like this:

send(to, from, count) 
register short *to, *from; 
register count; 
{ 
    register n = count / 8; 
    do { 
        *to = *from++; 
        *to = *from++; 
        *to = *from++; 
        *to = *from++; 
        *to = *from++; 
        *to = *from++; 
        *to = *from++; 
        *to = *from++; 
    } while (--n > 0); 
} 

The problem was that he didn't know ahead of time how many times it would loop, so he couldn't be guaranteed an even division. This would normally be handled with two loops, one like the above code and one for the remainder. He wanted it more compact and even more optimized though, so he drew on his experience with assembly, looked at the spec for the switch statement in C and realized he could mix a loop and a case statement to do this:

send(to, from, count) 
register short *to, *from; 
register count; 
{ 
    register n = (count + 7) / 8; 
    switch (count % 8) { 
    case 0: do { *to = *from++; 
    case 7:      *to = *from++; 
    case 6:      *to = *from++; 
    case 5:      *to = *from++; 
    case 4:      *to = *from++; 
    case 3:      *to = *from++; 
    case 2:      *to = *from++; 
    case 1:      *to = *from++; 
            } while (--n > 0); 
    } 
} 

His name is Tom Duff, and that brilliant block of code is known as Duff's device. When I was first learning C, a senior developer told me about it and explained it to me. I was so impressed, that I scoured my code for a loop to unroll and successfully implemented it.

Granted, my code was fast enough and it didn't make much of a difference in performance, but I was able to say I used Duff's device. Yes, it was premature optimization, but I didn't care. It was Duff's flippin’ device!

Edit:

I've had a few people suggest edits to change “*to"" to “*to++”. The samples of the original code and the Duff's device (first and last samples) are taken verbatim from Tom Duff's 1983 email to Ron Gomes, Dennis Ritchie, and Rob Pike.

https://swtch.com/duffs-device/td-1983.txt

It was copying from an array to a programmed IO data register, which is why it's “*to” instead of “*to++”.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/jmtyk06ur3qiz1hf', 'title': 'What are some examples of extremely clever problem solving in the software development world?', 'score': {'original': 0.9738, 'ai': 0.0262}, 'blocks': [{'text': 'In 1983, this guy was a computer programmer at Lucasfilm.\n\nHe was optimizing a real time animation program and wanted to “unroll"" a loop in C. This is a way to optimize code by reducing the number of times the computer has to check if the loop can be ended.\n\nThis was the original loop:\n\nsend(to, from, count)\xa0\nregister short *to, *from;\xa0\nregister count;\xa0\n{\xa0\n    do {                          /* count > 0 assumed */\xa0\n        *to = *from++;\xa0\n    } while(--count > 0);\xa0\n}\xa0\n\nNormally, a loop is unrolled by dividing the loops by a number and copying the code within the loop that many times. So, something that loops 64 times would loop 8 times with the code repeated 8 times, eliminating 56 times that the program has to check if the loop is complete like this:\n\nsend(to, from, count)\xa0\nregister short *to, *from;\xa0\nregister count;\xa0\n{\xa0\n    register n = count / 8;\xa0\n    do {\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n    } while (--n > 0);\xa0\n}\xa0\n\nThe problem was that he didn\'t know ahead of time how many times it would loop, so he couldn\'t be guaranteed an even division. This would normally be handled with two loops, one like the above code and one for the remainder. He wanted it more compact and even more optimized though, so he drew on his experience with assembly, looked at the spec for the switch statement in C and realized he could mix a loop and a case statement to do this:\n\nsend(to, from, count)\xa0\nregister short *to, *from;\xa0\nregister count;\xa0\n{\xa0\n    register n = (count + 7) / 8;\xa0\n    switch (count % 8) {\xa0\n    case 0: do { *to = *from++;\xa0\n    case 7:      *to = *from++;\xa0\n    case 6:      *to = *from++;\xa0\n    case 5:      *to = *from++;\xa0\n    case 4:      *to = *from++;\xa0\n    case 3:      *to = *from++;\xa0\n    case 2:      *to = *from++;\xa0\n    case 1:      *to = *from++;\xa0\n            } while (--n > 0);\xa0\n    }\xa0\n}\xa0\n\nHis name is Tom Duff, and that brilliant block of code', 'result': {'fake': 0.4038, 'real': 0.5962}, 'status': 'success'}, {'text': 'is known as Duff\'s device. When I was first learning C, a senior developer told me about it and explained it to me. I was so impressed, that I scoured my code for a loop to unroll and successfully implemented it.\n\nGranted, my code was fast enough and it didn\'t make much of a difference in performance, but I was able to say I used Duff\'s device. Yes, it was premature optimization, but I didn\'t care. It was Duff\'s flippin’ device!\n\nEdit:\n\nI\'ve had a few people suggest edits to change “*to"" to “*to++”. The samples of the original code and the Duff\'s device (first and last samples) are taken verbatim from Tom Duff\'s 1983 email to Ron Gomes, Dennis Ritchie, and Rob Pike.\n\nhttps://swtch.com/duffs-device/td-1983.txt\n\nIt was copying from an array to a programmed IO data register, which is why it\'s “*to” instead of “*to++”.', 'result': {'fake': 0.1717, 'real': 0.8283}, 'status': 'success'}], 'credits_used': 5, 'credits': 1985994, 'subscription': 0, 'content': 'In 1983, this guy was a computer programmer at Lucasfilm.\n\nHe was optimizing a real time animation program and wanted to “unroll"" a loop in C. This is a way to optimize code by reducing the number of times the computer has to check if the loop can be ended.\n\nThis was the original loop:\n\nsend(to, from, count)\xa0\nregister short *to, *from;\xa0\nregister count;\xa0\n{\xa0\n    do {                          /* count > 0 assumed */\xa0\n        *to = *from++;\xa0\n    } while(--count > 0);\xa0\n}\xa0\n\nNormally, a loop is unrolled by dividing the loops by a number and copying the code within the loop that many times. So, something that loops 64 times would loop 8 times with the code repeated 8 times, eliminating 56 times that the program has to check if the loop is complete like this:\n\nsend(to, from, count)\xa0\nregister short *to, *from;\xa0\nregister count;\xa0\n{\xa0\n    register n = count / 8;\xa0\n    do {\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n        *to = *from++;\xa0\n    } while (--n > 0);\xa0\n}\xa0\n\nThe problem was that he didn\'t know ahead of time how many times it would loop, so he couldn\'t be guaranteed an even division. This would normally be handled with two loops, one like the above code and one for the remainder. He wanted it more compact and even more optimized though, so he drew on his experience with assembly, looked at the spec for the switch statement in C and realized he could mix a loop and a case statement to do this:\n\nsend(to, from, count)\xa0\nregister short *to, *from;\xa0\nregister count;\xa0\n{\xa0\n    register n = (count + 7) / 8;\xa0\n    switch (count % 8) {\xa0\n    case 0: do { *to = *from++;\xa0\n    case 7:      *to = *from++;\xa0\n    case 6:      *to = *from++;\xa0\n    case 5:      *to = *from++;\xa0\n    case 4:      *to = *from++;\xa0\n    case 3:      *to = *from++;\xa0\n    case 2:      *to = *from++;\xa0\n    case 1:      *to = *from++;\xa0\n            } while (--n > 0);\xa0\n    }\xa0\n}\xa0\n\nHis name is Tom Duff, and that brilliant block of code is known as Duff\'s device. When I was first learning C, a senior developer told me about it and explained it to me. I was so impressed, that I scoured my code for a loop to unroll and successfully implemented it.\n\nGranted, my code was fast enough and it didn\'t make much of a difference in performance, but I was able to say I used Duff\'s device. Yes, it was premature optimization, but I didn\'t care. It was Duff\'s flippin’ device!\n\nEdit:\n\nI\'ve had a few people suggest edits to change “*to"" to “*to++”. The samples of the original code and the Duff\'s device (first and last samples) are taken verbatim from Tom Duff\'s 1983 email to Ron Gomes, Dennis Ritchie, and Rob Pike.\n\nhttps://swtch.com/duffs-device/td-1983.txt\n\nIt was copying from an array to a programmed IO data register, which is why it\'s “*to” instead of “*to++”.', 'aiModelVersion': '1'}",0.9738
Phillip Remaker,Updated 3y,Why do modern graphic cards require a separate power connector? Why not get power from the motherboard slot?,"Fire.

The thin copper lines of a PCIe slot can only carry so much current [1]. Beyond that, the thin strip of copper carrying the power will melt or scorch.

Search for “burned PCIe slot” to see how that relatively low amount of power can cause enough heat to burn a PCIe card, even in spec, with a poor or dirty connection.

As the power goes up, you want fatter wires and more contact area. Rather than try to make a more expensive PCIe bus to handle more power for the odd power-hungry card, it’s easier to just provide direct power to the ravenous GPUs.

[1] Thanks for Robert Taylor for the detailed comment, which is worth reading.

PCIe
 slots are more complicated than I originally stated: “A full-sized x16 graphics card may draw up to 5.5 A at +12 V (66 W) and 75 W combined [with 3 A at 3.3 V] after initialization and software configuration as a ""high power device"".”

To draw more power at 12 V calls for the PCIe standardized additional power connectors. A 6 pin connector provides up to 75 extra watts over 2 12V rails and an 8 pin connector provides up to 150 watts over 3 12V rails. So, the PCIe slot power plus a 6 pin and 8 pin connector can provide up to 300W total. The not-yet-standardized PCIe slot plus two 8 pin connectors would theoretically provide up to 375W total.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/g76wq5ekhx3fvnzt', 'title': 'Why do modern graphic cards require a separate power connector? Why not get power from the motherboard slot?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': 'Fire.\n\nThe thin copper lines of a PCIe slot can only carry so much current [1]. Beyond that, the thin strip of copper carrying the power will melt or scorch.\n\nSearch for “burned PCIe slot” to see how that relatively low amount of power can cause enough heat to burn a PCIe card, even in spec, with a poor or dirty connection.\n\nAs the power goes up, you want fatter wires and more contact area. Rather than try to make a more expensive PCIe bus to handle more power for the odd power-hungry card, it’s easier to just provide direct power to the ravenous GPUs.\n\n[1] Thanks for Robert Taylor for the detailed comment, which is worth reading.\n\nPCIe\n slots are more complicated than I originally stated: “A full-sized x16 graphics card may draw up to 5.5 A at +12 V (66 W) and 75 W combined [with 3 A at 3.3 V] after initialization and software configuration as a ""high power device"".”\n\nTo draw more power at 12 V calls for the PCIe standardized additional power connectors. A 6 pin connector provides up to 75 extra watts over 2 12V rails and an 8 pin connector provides up to 150 watts over 3 12V rails. So, the PCIe slot power plus a 6 pin and 8 pin connector can provide up to 300W total. The not-yet-standardized PCIe slot plus two 8 pin connectors would theoretically provide up to 375W total.', 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985991, 'subscription': 0, 'content': 'Fire.\n\nThe thin copper lines of a PCIe slot can only carry so much current [1]. Beyond that, the thin strip of copper carrying the power will melt or scorch.\n\nSearch for “burned PCIe slot” to see how that relatively low amount of power can cause enough heat to burn a PCIe card, even in spec, with a poor or dirty connection.\n\nAs the power goes up, you want fatter wires and more contact area. Rather than try to make a more expensive PCIe bus to handle more power for the odd power-hungry card, it’s easier to just provide direct power to the ravenous GPUs.\n\n[1] Thanks for Robert Taylor for the detailed comment, which is worth reading.\n\nPCIe\n slots are more complicated than I originally stated: “A full-sized x16 graphics card may draw up to 5.5 A at +12 V (66 W) and 75 W combined [with 3 A at 3.3 V] after initialization and software configuration as a ""high power device"".”\n\nTo draw more power at 12 V calls for the PCIe standardized additional power connectors. A 6 pin connector provides up to 75 extra watts over 2 12V rails and an 8 pin connector provides up to 150 watts over 3 12V rails. So, the PCIe slot power plus a 6 pin and 8 pin connector can provide up to 300W total. The not-yet-standardized PCIe slot plus two 8 pin connectors would theoretically provide up to 375W total.', 'aiModelVersion': '1'}",0.9995
Louis Vaught,Updated 3y,How much faster is a Google server than a normal PC?,"They’re slower, in the way you’re probably thinking. That’s not clickbait, this is a very common misconception about “powerful” computers.

Data centers aren’t built like desktop PC’s at all. A lot of answers here discuss the size and scale of Google's data center operations, and briefly cover hardware. I’d like to nail down the hardware a bit more. To put it simply, the ecosystems are different enough that direct “speed” comparisons are a little goofy.

I don’t know exact specs for Google machines, and even if I did I wouldn’t want to release them, but everyone uses the same basic 2-by platform whenever they can. Standardized hardware is cheaper, and more people working on the same code means more efficient code. So, high-performance networked data servers are overwhelmingly dual-socket machines, like this:

Each socket gets filled with a high-end Intel or AMD processor, which top out at 52 cores and 64 cores respectively. There are some systems that have a lot more sockets, but they are nowhere near as common. They’re more difficult to network, less stable, and produce a ton of heat in a relatively small area:

The name of the game with these servers is always efficiency - there’s only so much heat you can get out of a CPU reliably. If you have more cores, you can run them at a lower clock speed and still get the same amount of work done.

Power usage increases faster than clock speed does, so lower clocks and more cores is better - as long as you have software that can work that way. Luckily, servers are usually doing a lot of parallel tasks.




That was a big lead-in, but with that background explanation, let’s tackle the performance question. In terms of raw computing power, a clock-for-clock comparison is pretty fair, especially for the Intel parts.

A high-end, normal desktop PC might have a 9900k running an 8-core turbo of 4.7 GHz. Bleeding-edge hardware from Google will be running 2x Xeon Platinum 9282’s, at a turbo speed of 3.8 GHz.

That means the desktop will have an effective 3.76 x 10^10 clock cycles each second, and the Xeon Platinum system will have 3.95 x 10^11 cycles.

So a Google server has roughly 10x the raw power of a fast desktop.




But, most people don't use their PC like that!

If you installed Windows on a data server and tried to play games, those games would probably run badly. In fact, I’ve used a bunch of multi-CPU systems as workstations, and depending on the specifics of the hardware they can be pretty painful to work with. Especially when compared to a high-end consumer PC that costs a fraction of the price.

Consumer software has been designed for consumer hardware, and vice-versa. Video games, for example, don’t use that many cores, care a lot about latency between hardware, and benefit from fine-tuned support on gaming PC’s. If your CPU and GPU get old enough, some games will have strange bugs and much worse performance than you’d expect.

While there are some servers that need fine-tuned, low-latency hardware, mostly the big concern is just raw horsepower and bandwidth. You know what kinds o...

Access this answer and support the author as a Quora+ subscriber
Access all answers reserved by 
Louis Vaught
 for Quora+ subscribers
Access exclusive answers from thousands more participating creators in Quora+
Browse ad‑free and support creators
Start free trial
Learn more","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2lde91sqwt6c4rjb', 'title': 'How much faster is a Google server than a normal PC?', 'score': {'original': 0.9766, 'ai': 0.0234}, 'blocks': [{'text': ""They’re slower, in the way you’re probably thinking. That’s not clickbait, this is a very common misconception about “powerful” computers.\n\nData centers aren’t built like desktop PC’s at all. A lot of answers here discuss the size and scale of Google's data center operations, and briefly cover hardware. I’d like to nail down the hardware a bit more. To put it simply, the ecosystems are different enough that direct “speed” comparisons are a little goofy.\n\nI don’t know exact specs for Google machines, and even if I did I wouldn’t want to release them, but everyone uses the same basic 2-by platform whenever they can. Standardized hardware is cheaper, and more people working on the same code means more efficient code. So, high-performance networked data servers are overwhelmingly dual-socket machines, like this:\n\nEach socket gets filled with a high-end Intel or AMD processor, which top out at 52 cores and 64 cores respectively. There are some systems that have a lot more sockets, but they are nowhere near as common. They’re more difficult to network, less stable, and produce a ton of heat in a relatively small area:\n\nThe name of the game with these servers is always efficiency - there’s only so much heat you can get out of a CPU reliably. If you have more cores, you can run them at a lower clock speed and still get the same amount of work done.\n\nPower usage increases faster than clock speed does, so lower clocks and more cores is better - as long as you have software that can work that way. Luckily, servers are usually doing a lot of parallel tasks.\n\n\n\n\nThat was a big lead-in, but with that background explanation, let’s tackle the performance question. In terms of raw computing power, a clock-for-clock comparison is pretty fair, especially for the Intel parts.\n\nA high-end, normal desktop PC might have a 9900k running an 8-core turbo of 4.7 GHz. Bleeding-edge hardware from Google will be running 2x Xeon Platinum 9282’s, at a turbo speed of 3.8 GHz.\n\nThat means the desktop will have an effective 3.76 x 10^10 clock cycles each second, and the Xeon Platinum system will have 3.95 x 10^11 cycles.\n\nSo a Google server has roughly 10x the raw power of a fast desktop.\n\n\n\n\nBut, most people don't use their PC like that!\n\nIf you installed Windows on a data server and tried to play games, those games would probably run badly. In fact, I’ve used a bunch of multi-CPU systems as workstations, and depending on the specifics of the hardware they can be pretty painful to work with. Especially when compared to a high-end consumer PC that costs a fraction of the price.\n\nConsumer software has been designed for consumer hardware, and vice-versa. Video games, for example, don’t use that many cores, care a lot about latency between hardware, and benefit from fine-tuned support on gaming PC’s. If your CPU and GPU get old enough, some games will have strange bugs and much worse performance than you’d expect.\n\nWhile there are some servers that need fine-tuned, low-latency hardware, mostly the big concern is just raw horsepower and bandwidth."", 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}, {'text': 'You know what kinds o...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nLouis Vaught\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'result': {'fake': 0.0063, 'real': 0.9937}, 'status': 'success'}], 'credits_used': 6, 'credits': 1985985, 'subscription': 0, 'content': ""They’re slower, in the way you’re probably thinking. That’s not clickbait, this is a very common misconception about “powerful” computers.\n\nData centers aren’t built like desktop PC’s at all. A lot of answers here discuss the size and scale of Google's data center operations, and briefly cover hardware. I’d like to nail down the hardware a bit more. To put it simply, the ecosystems are different enough that direct “speed” comparisons are a little goofy.\n\nI don’t know exact specs for Google machines, and even if I did I wouldn’t want to release them, but everyone uses the same basic 2-by platform whenever they can. Standardized hardware is cheaper, and more people working on the same code means more efficient code. So, high-performance networked data servers are overwhelmingly dual-socket machines, like this:\n\nEach socket gets filled with a high-end Intel or AMD processor, which top out at 52 cores and 64 cores respectively. There are some systems that have a lot more sockets, but they are nowhere near as common. They’re more difficult to network, less stable, and produce a ton of heat in a relatively small area:\n\nThe name of the game with these servers is always efficiency - there’s only so much heat you can get out of a CPU reliably. If you have more cores, you can run them at a lower clock speed and still get the same amount of work done.\n\nPower usage increases faster than clock speed does, so lower clocks and more cores is better - as long as you have software that can work that way. Luckily, servers are usually doing a lot of parallel tasks.\n\n\n\n\nThat was a big lead-in, but with that background explanation, let’s tackle the performance question. In terms of raw computing power, a clock-for-clock comparison is pretty fair, especially for the Intel parts.\n\nA high-end, normal desktop PC might have a 9900k running an 8-core turbo of 4.7 GHz. Bleeding-edge hardware from Google will be running 2x Xeon Platinum 9282’s, at a turbo speed of 3.8 GHz.\n\nThat means the desktop will have an effective 3.76 x 10^10 clock cycles each second, and the Xeon Platinum system will have 3.95 x 10^11 cycles.\n\nSo a Google server has roughly 10x the raw power of a fast desktop.\n\n\n\n\nBut, most people don't use their PC like that!\n\nIf you installed Windows on a data server and tried to play games, those games would probably run badly. In fact, I’ve used a bunch of multi-CPU systems as workstations, and depending on the specifics of the hardware they can be pretty painful to work with. Especially when compared to a high-end consumer PC that costs a fraction of the price.\n\nConsumer software has been designed for consumer hardware, and vice-versa. Video games, for example, don’t use that many cores, care a lot about latency between hardware, and benefit from fine-tuned support on gaming PC’s. If your CPU and GPU get old enough, some games will have strange bugs and much worse performance than you’d expect.\n\nWhile there are some servers that need fine-tuned, low-latency hardware, mostly the big concern is just raw horsepower and bandwidth. You know what kinds o...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nLouis Vaught\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more"", 'aiModelVersion': '1'}",0.9766
Franklin Veaux,2y,Why can't Quora spot harassing posts before they're published? How hard would it be to train an AI to recognise the infantile sentiments and reject them?,"To answer the second part: very hard. I wouldn’t be surprised to learn that’s an AI-complete problem
, meaning it would require a true general AI, not a simple pattern recognition or Bayesian analysis algorithm.

Parsing text is hard. Doable, but hard. It requires a lot of grunt, though.

Understanding context? That’s…a whole different kettle of fish. Understanding context is a whole new level of hard. If Siri and Alexa are trash mobs in an MMO, understand context is a raid boss.

Put it this way: if Quora could solve that problem with machine intelligence, Google would buy them in a heartbeat, and someone might be up for a Nobel Prize in mathematics (if there were such a thing).

Harassment often relies on subtext, innuendo (no, in YOUR end-o!), metaphor, implication, sarcasm…these aren’t things machine learning algos are good at.

I can pretty much guarantee this problem would be far cheaper and easier to handle with a Mechanical Turk solution than with machine learning.

The answer to your question is: really, really, really hard.

Obligatory XKCD
 directly on point about this:

My stalker is once again creating fake profiles that look just like mine to send rape threats to other people. If you receive an abusive PM or comment, please check the profile carefully. It isn’t me.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0a5yictoquhfkwgj', 'title': ""Why can't Quora spot harassing posts before they're published? How hard would it be to train an AI to recognise the infantile sentiments and reject them?"", 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'To answer the second part: very hard. I wouldn’t be surprised to learn that’s an AI-complete problem\n, meaning it would require a true general AI, not a simple pattern recognition or Bayesian analysis algorithm.\n\nParsing text is hard. Doable, but hard. It requires a lot of grunt, though.\n\nUnderstanding context? That’s…a whole different kettle of fish. Understanding context is a whole new level of hard. If Siri and Alexa are trash mobs in an MMO, understand context is a raid boss.\n\nPut it this way: if Quora could solve that problem with machine intelligence, Google would buy them in a heartbeat, and someone might be up for a Nobel Prize in mathematics (if there were such a thing).\n\nHarassment often relies on subtext, innuendo (no, in YOUR end-o!), metaphor, implication, sarcasm…these aren’t things machine learning algos are good at.\n\nI can pretty much guarantee this problem would be far cheaper and easier to handle with a Mechanical Turk solution than with machine learning.\n\nThe answer to your question is: really, really, really hard.\n\nObligatory XKCD\n directly on point about this:\n\nMy stalker is once again creating fake profiles that look just like mine to send rape threats to other people. If you receive an abusive PM or comment, please check the profile carefully. It isn’t me.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985982, 'subscription': 0, 'content': 'To answer the second part: very hard. I wouldn’t be surprised to learn that’s an AI-complete problem\n, meaning it would require a true general AI, not a simple pattern recognition or Bayesian analysis algorithm.\n\nParsing text is hard. Doable, but hard. It requires a lot of grunt, though.\n\nUnderstanding context? That’s…a whole different kettle of fish. Understanding context is a whole new level of hard. If Siri and Alexa are trash mobs in an MMO, understand context is a raid boss.\n\nPut it this way: if Quora could solve that problem with machine intelligence, Google would buy them in a heartbeat, and someone might be up for a Nobel Prize in mathematics (if there were such a thing).\n\nHarassment often relies on subtext, innuendo (no, in YOUR end-o!), metaphor, implication, sarcasm…these aren’t things machine learning algos are good at.\n\nI can pretty much guarantee this problem would be far cheaper and easier to handle with a Mechanical Turk solution than with machine learning.\n\nThe answer to your question is: really, really, really hard.\n\nObligatory XKCD\n directly on point about this:\n\nMy stalker is once again creating fake profiles that look just like mine to send rape threats to other people. If you receive an abusive PM or comment, please check the profile carefully. It isn’t me.', 'aiModelVersion': '1'}",0.9996
C Stuart Hardwick,Updated 4y,Why didn't people set their clocks forward to test and debunk the Millennium bug?,"We did.

Here’s a thought you might consider remembering any time you are tempted to ponder “why didn’t/don’t people just do [this obvious thing that occurred to me while peeing in the shower?]”

People are smart. I mean, people can be really, really stupid, but as a whole, people are smart, and as a whole, most professions favor people who are at least competently smart and educated in those fields. Any time you dream up a solution in a field you aren’t trained in and don’t have any experience in, you can bet your favorite waffles there’s a good reason they do it that way (or don’t, as the case may be).

In the ten or so years leading up to the millennium, computer and IT professionals performed a host of tests and investigations to assess and ensure the readiness of their systems. What you call here the “millennium-bug,” we called Y2K compliance, and it could not have been debunked because it was a very real, very serious issue. It turned out not to be a problem for society because thousands of smart professionals (and not so smart professionals) worked to remediate their systems ahead of time, and with only a few high profile exceptions, succeeded.

Most of the computer systems affected were of the large, batch, mainframe variety. They might get the date from the system, but they might also read it from the middle of a very, very large file. You couldn’t simply “set the clock forward” on such a system. You had to dive in and painstakingly read through the code. Fortunately, you could search for date manipulations, and that made it a lot easier. But to test the results, you often needed a test system that didn’t exist—because the existing test systems were needed to support production. So you ran tests over the weekend, and worked them in as best you could, or waiting for expensive upgrades. We did all of the above, and in the last two years, ten percent of our workforce was made up of COBOL programmers brought out of retirement just to remediate code—not because it was hard, but because someone had to sit in a chair and get it done.

You fixed what was easiest to fix and easiest to test first. That was mostly the PC stuff where you could just “set the clock forward.” Although that wasn’t as easy as you might imagine either. On many systems, changing the system clock was (maybe is) more complex than you might imagine, and not designed for such testing, and changing the application time while the BIOS still has an earlier time could produce unexpected results.

And then there were the many, many tapes that since the dawn of the computer age had used “all nines” as an end of file code. In some contexts, “all nines” means September 9, 1999, and in fact one of the few production Y2K failures I ever heard of was at a Credit Card company that missed one of those.

It was a huge undertaking, carried out over about a decade by smart people who in one way or the other fit it in around all the other work that had to be done to keep the lights on (in our case, literally) and the modernizations coming.

Y2K, or “the millennium bug” was not a hoax to be debunked. It was a very real problem, successfully addressed by smart, professional people, just doing their jobs.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/4z29q57mgjtk30if', 'title': ""Why didn't people set their clocks forward to test and debunk the Millennium bug?"", 'score': {'original': 0.47885, 'ai': 0.52115}, 'blocks': [{'text': 'We did.\n\nHere’s a thought you might consider remembering any time you are tempted to ponder “why didn’t/don’t people just do [this obvious thing that occurred to me while peeing in the shower?]”\n\nPeople are smart. I mean, people can be really, really stupid, but as a whole, people are smart, and as a whole, most professions favor people who are at least competently smart and educated in those fields. Any time you dream up a solution in a field you aren’t trained in and don’t have any experience in, you can bet your favorite waffles there’s a good reason they do it that way (or don’t, as the case may be).\n\nIn the ten or so years leading up to the millennium, computer and IT professionals performed a host of tests and investigations to assess and ensure the readiness of their systems. What you call here the “millennium-bug,” we called Y2K compliance, and it could not have been debunked because it was a very real, very serious issue. It turned out not to be a problem for society because thousands of smart professionals (and not so smart professionals) worked to remediate their systems ahead of time, and with only a few high profile exceptions, succeeded.\n\nMost of the computer systems affected were of the large, batch, mainframe variety. They might get the date from the system, but they might also read it from the middle of a very, very large file. You couldn’t simply “set the clock forward” on such a system. You had to dive in and painstakingly read through the code. Fortunately, you could search for date manipulations, and that made it a lot easier. But to test the results, you often needed a test system that didn’t exist—because the existing test systems were needed to support production. So you ran tests over the weekend, and worked them in as best you could, or waiting for expensive upgrades. We did all of the above, and in the last two years, ten percent of our workforce was made up of COBOL programmers brought out of retirement just to remediate code—not because it was hard, but because someone had to sit in a chair and get it done.\n\nYou fixed what was easiest to fix and easiest to test first. That was mostly the PC stuff where you could just “set the clock forward.” Although that wasn’t as easy as you might imagine either. On many systems, changing the system clock was (maybe is) more complex than you might imagine, and not designed for such testing, and changing the application time while the BIOS still has an earlier time could produce unexpected results.\n\nAnd then there were the many, many tapes that since the dawn of the computer age had used “all nines” as an end of file code. In some contexts, “all nines” means September 9, 1999, and in fact one of the few production Y2K failures I ever heard of was at a Credit Card company that missed one of those.\n\nIt was a huge undertaking, carried out over about a decade by smart people who', 'result': {'fake': 0.0509, 'real': 0.9491}, 'status': 'success'}, {'text': 'in one way or the other fit it in around all the other work that had to be done to keep the lights on (in our case, literally) and the modernizations coming.\n\nY2K, or “the millennium bug” was not a hoax to be debunked. It was a very real problem, successfully addressed by smart, professional people, just doing their jobs.', 'result': {'fake': 0.9911, 'real': 0.0089}, 'status': 'success'}], 'credits_used': 6, 'credits': 1985976, 'subscription': 0, 'content': 'We did.\n\nHere’s a thought you might consider remembering any time you are tempted to ponder “why didn’t/don’t people just do [this obvious thing that occurred to me while peeing in the shower?]”\n\nPeople are smart. I mean, people can be really, really stupid, but as a whole, people are smart, and as a whole, most professions favor people who are at least competently smart and educated in those fields. Any time you dream up a solution in a field you aren’t trained in and don’t have any experience in, you can bet your favorite waffles there’s a good reason they do it that way (or don’t, as the case may be).\n\nIn the ten or so years leading up to the millennium, computer and IT professionals performed a host of tests and investigations to assess and ensure the readiness of their systems. What you call here the “millennium-bug,” we called Y2K compliance, and it could not have been debunked because it was a very real, very serious issue. It turned out not to be a problem for society because thousands of smart professionals (and not so smart professionals) worked to remediate their systems ahead of time, and with only a few high profile exceptions, succeeded.\n\nMost of the computer systems affected were of the large, batch, mainframe variety. They might get the date from the system, but they might also read it from the middle of a very, very large file. You couldn’t simply “set the clock forward” on such a system. You had to dive in and painstakingly read through the code. Fortunately, you could search for date manipulations, and that made it a lot easier. But to test the results, you often needed a test system that didn’t exist—because the existing test systems were needed to support production. So you ran tests over the weekend, and worked them in as best you could, or waiting for expensive upgrades. We did all of the above, and in the last two years, ten percent of our workforce was made up of COBOL programmers brought out of retirement just to remediate code—not because it was hard, but because someone had to sit in a chair and get it done.\n\nYou fixed what was easiest to fix and easiest to test first. That was mostly the PC stuff where you could just “set the clock forward.” Although that wasn’t as easy as you might imagine either. On many systems, changing the system clock was (maybe is) more complex than you might imagine, and not designed for such testing, and changing the application time while the BIOS still has an earlier time could produce unexpected results.\n\nAnd then there were the many, many tapes that since the dawn of the computer age had used “all nines” as an end of file code. In some contexts, “all nines” means September 9, 1999, and in fact one of the few production Y2K failures I ever heard of was at a Credit Card company that missed one of those.\n\nIt was a huge undertaking, carried out over about a decade by smart people who in one way or the other fit it in around all the other work that had to be done to keep the lights on (in our case, literally) and the modernizations coming.\n\nY2K, or “the millennium bug” was not a hoax to be debunked. It was a very real problem, successfully addressed by smart, professional people, just doing their jobs.', 'aiModelVersion': '1'}",0.47885
E. Boulesteix,Updated 5y,"What are the differences (operation, applications) between IA-64 and x86-64 architectures?","In the mid to late 90s, Intel had a problem. It needed a new 64-bit architecture to compete in the 64-bit computing era, spearheaded by the DEC Alpha processors. But this itself wasn't the problem.

Processors at the time were about to hit a wall. Power and clock speeds were soaring. Processors were getting more and more complex, in an effort to squeeze every possible ounce of ILP. In less than 10 years, we had gone from fairly simple 32-bit CISC designs, like the 386 and 68k series, to massive processor cores with huge caches, out-of-order execution, and superscalar processing.

Image from Index of /main-images/die

This is an image of the MIPS R10000 processor core.

The dominant question being “How much longer?” As mentioned, architectures were getting quite complex, reaching diminishing returns. Some started to question whether or not the industry as a whole could keep on building wider and more complex processor cores to improve performance.

Intel wanted a new 64-bit architecture to replace their old x86 ISA. They had tried (and failed) to kill x86 before, but this transition was a great time to do so, hopefully for good.

Why did Intel want to kill X86? As mentioned, many people doubted that we could go out and keep building wider and wider superscalar processors. Intel bet that a paradigm shift was needed: a new architecture meant for the new millennium, free of any 1970s baggage. Switching architectures also killed two birds with one stone, as far as Intel was concerned: they had lost control of X86. Vendors like AMD and Cyrix were making chips on their architecture. Intel wanted sole ownership of their ISA, and killing x86 was one way to achieve that goal.

Their solution was called Itanium. It would be a brand new, 64-bit architecture designed in collaboration with HP, called IA64. It would work in a fundamentally different way than contemporary architectures as well: instead of a traditional superscalar architecture, Intel’s design was called EPIC (Explicitly Parallel Instruction Computing). Behind the fancy name was pretty much a clustered VLIW, however.

The whole philosophy behind VLIW was to move as much functionality as possible to software. All instruction re-ordering and code optimization is done by the compiler. Interlocks are removed and the pipeline is exposed to the compiler. This can work quite well in some cases: DSPs and GPUs are well suited to a VLIW setup.

However, they don’t work that well in traditional desktop architectures, because not much parallelism is typically available at compile time.

To counteract this, Intel designed their architecture with features that would enable it to recoup some of the advantages of superscalar processors. Some of these features were not necessarily proven either.

The VLIW bundles were split into chunks, with dependency checking within those chunks. They had a rotating register file to do in-hardware loop unrolling. They implemented extensive predication to remove some of the branches. They added speculation. Notice a trend?

Intel originally designed Itanium to reduce complexity, allowing for more performance through higher efficiency. But they then added a lot of complexity back in while trying to make it behave more like an out-of-order superscalar. In the end, they got most of the drawbacks of VLIW, with none of the advantages. The extra registers did help, though.

Itanium was originally slated to appear sometime in 1998, but delays started cropping up. The chip was delayed, coming out in 2001. When Itanium came out, it was slow, expensive, and incompatible. The fastest variants were only running at 800MHz, whereas their own desktop chips were already nearing in excess of 2GHz (albeit on the admittedly poor Netburst architecture).

Intel sought to rectify this, releasing the updated Itanium 2 in 2002 through 2004. Many of the original Itanium’s problems were fixed, and the new chip now ran up to ~1.6GHz. Overall, though, the architecture still received a resounding meh. Software compatibility was still a major sticking point. The IA64 wasn’t off to a good start.

This is where AMD comes in.

AMD, being a high-performance processor designer and manufacturer, also needed a 64-bit processor architecture to compete. However, the company didn’t want to create a brand new architecture. Being a relatively small company compared to Intel, releasing a new incompatible chip would be suicide, as far as they were concerned. The decision was made early on to extend the x86 architecture for the 64 bit era.

Their solution was x86–64, an extension to the x86 instruction set, adding 64-bit registers, larger address spaces, and more registers (something which was sorely needed) to the old ISA. All processors with this extension would be fully compatible with older x86 applications; all new 64-bit circuitry would essentially be disabled when in 32-bit mode.

Their first processor to sport this extension was the original Opteron, based on their K8 micro-architecture. This processor was then adapted for consumer use, taking the form of the Athlon 64, released in 2003, using their codename Hammer Core.

In a way, you could say that AMD ultimately proved Intel wrong: they did build those faster superscalar processors after all. The compatibility with existing code ensured its success.

The unfortunate reality is that most code outlives the systems it was written for. Some code even outlives its creators. That’s why there’s still 1960s-era COBOL and FORTRAN that needs maintaining. People write code which is never replaced. People just want faster computers; rewriting software is expensive and inconvenient. Sweeping concerns under the rug and turning a blind eye is just easier.

That is why x86–64 has succeeded when Itanium did not. That is why X86 is still with us today. That is why we still have remnants of space-age code in our modern devices.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rvfw5b1s6pyo2zxj', 'title': 'What are the differences (operation, applications) between IA-64 and x86-64 architectures?', 'score': {'original': 0.50565, 'ai': 0.49435}, 'blocks': [{'text': ""In the mid to late 90s, Intel had a problem. It needed a new 64-bit architecture to compete in the 64-bit computing era, spearheaded by the DEC Alpha processors. But this itself wasn't the problem.\n\nProcessors at the time were about to hit a wall. Power and clock speeds were soaring. Processors were getting more and more complex, in an effort to squeeze every possible ounce of ILP. In less than 10 years, we had gone from fairly simple 32-bit CISC designs, like the 386 and 68k series, to massive processor cores with huge caches, out-of-order execution, and superscalar processing.\n\nImage from Index of /main-images/die\n\nThis is an image of the MIPS R10000 processor core.\n\nThe dominant question being “How much longer?” As mentioned, architectures were getting quite complex, reaching diminishing returns. Some started to question whether or not the industry as a whole could keep on building wider and more complex processor cores to improve performance.\n\nIntel wanted a new 64-bit architecture to replace their old x86 ISA. They had tried (and failed) to kill x86 before, but this transition was a great time to do so, hopefully for good.\n\nWhy did Intel want to kill X86? As mentioned, many people doubted that we could go out and keep building wider and wider superscalar processors. Intel bet that a paradigm shift was needed: a new architecture meant for the new millennium, free of any 1970s baggage. Switching architectures also killed two birds with one stone, as far as Intel was concerned: they had lost control of X86. Vendors like AMD and Cyrix were making chips on their architecture. Intel wanted sole ownership of their ISA, and killing x86 was one way to achieve that goal.\n\nTheir solution was called Itanium. It would be a brand new, 64-bit architecture designed in collaboration with HP, called IA64. It would work in a fundamentally different way than contemporary architectures as well: instead of a traditional superscalar architecture, Intel’s design was called EPIC (Explicitly Parallel Instruction Computing). Behind the fancy name was pretty much a clustered VLIW, however.\n\nThe whole philosophy behind VLIW was to move as much functionality as possible to software. All instruction re-ordering and code optimization is done by the compiler. Interlocks are removed and the pipeline is exposed to the compiler. This can work quite well in some cases: DSPs and GPUs are well suited to a VLIW setup.\n\nHowever, they don’t work that well in traditional desktop architectures, because not much parallelism is typically available at compile time.\n\nTo counteract this, Intel designed their architecture with features that would enable it to recoup some of the advantages of superscalar processors. Some of these features were not necessarily proven either.\n\nThe VLIW bundles were split into chunks, with dependency checking within those chunks. They had a rotating register file to do in-hardware loop unrolling. They implemented extensive predication to remove some of the branches. They added speculation. Notice a trend?\n\nIntel originally designed Itanium to reduce complexity, allowing for more performance through higher efficiency. But they then added a lot of complexity back in while trying to make it behave more like an"", 'result': {'fake': 0.015, 'real': 0.985}, 'status': 'success'}, {'text': 'out-of-order superscalar. In the end, they got most of the drawbacks of VLIW, with none of the advantages. The extra registers did help, though.\n\nItanium was originally slated to appear sometime in 1998, but delays started cropping up. The chip was delayed, coming out in 2001. When Itanium came out, it was slow, expensive, and incompatible. The fastest variants were only running at 800MHz, whereas their own desktop chips were already nearing in excess of 2GHz (albeit on the admittedly poor Netburst architecture).\n\nIntel sought to rectify this, releasing the updated Itanium 2 in 2002 through 2004. Many of the original Itanium’s problems were fixed, and the new chip now ran up to ~1.6GHz. Overall, though, the architecture still received a resounding meh. Software compatibility was still a major sticking point. The IA64 wasn’t off to a good start.\n\nThis is where AMD comes in.\n\nAMD, being a high-performance processor designer and manufacturer, also needed a 64-bit processor architecture to compete. However, the company didn’t want to create a brand new architecture. Being a relatively small company compared to Intel, releasing a new incompatible chip would be suicide, as far as they were concerned. The decision was made early on to extend the x86 architecture for the 64 bit era.\n\nTheir solution was x86–64, an extension to the x86 instruction set, adding 64-bit registers, larger address spaces, and more registers (something which was sorely needed) to the old ISA. All processors with this extension would be fully compatible with older x86 applications; all new 64-bit circuitry would essentially be disabled when in 32-bit mode.\n\nTheir first processor to sport this extension was the original Opteron, based on their K8 micro-architecture. This processor was then adapted for consumer use, taking the form of the Athlon 64, released in 2003, using their codename Hammer Core.\n\nIn a way, you could say that AMD ultimately proved Intel wrong: they did build those faster superscalar processors after all. The compatibility with existing code ensured its success.\n\nThe unfortunate reality is that most code outlives the systems it was written for. Some code even outlives its creators. That’s why there’s still 1960s-era COBOL and FORTRAN that needs maintaining. People write code which is never replaced. People just want faster computers; rewriting software is expensive and inconvenient. Sweeping concerns under the rug and turning a blind eye is just easier.\n\nThat is why x86–64 has succeeded when Itanium did not. That is why X86 is still with us today. That is why we still have remnants of space-age code in our modern devices.', 'result': {'fake': 0.5636, 'real': 0.4364}, 'status': 'success'}], 'credits_used': 10, 'credits': 1985966, 'subscription': 0, 'content': ""In the mid to late 90s, Intel had a problem. It needed a new 64-bit architecture to compete in the 64-bit computing era, spearheaded by the DEC Alpha processors. But this itself wasn't the problem.\n\nProcessors at the time were about to hit a wall. Power and clock speeds were soaring. Processors were getting more and more complex, in an effort to squeeze every possible ounce of ILP. In less than 10 years, we had gone from fairly simple 32-bit CISC designs, like the 386 and 68k series, to massive processor cores with huge caches, out-of-order execution, and superscalar processing.\n\nImage from Index of /main-images/die\n\nThis is an image of the MIPS R10000 processor core.\n\nThe dominant question being “How much longer?” As mentioned, architectures were getting quite complex, reaching diminishing returns. Some started to question whether or not the industry as a whole could keep on building wider and more complex processor cores to improve performance.\n\nIntel wanted a new 64-bit architecture to replace their old x86 ISA. They had tried (and failed) to kill x86 before, but this transition was a great time to do so, hopefully for good.\n\nWhy did Intel want to kill X86? As mentioned, many people doubted that we could go out and keep building wider and wider superscalar processors. Intel bet that a paradigm shift was needed: a new architecture meant for the new millennium, free of any 1970s baggage. Switching architectures also killed two birds with one stone, as far as Intel was concerned: they had lost control of X86. Vendors like AMD and Cyrix were making chips on their architecture. Intel wanted sole ownership of their ISA, and killing x86 was one way to achieve that goal.\n\nTheir solution was called Itanium. It would be a brand new, 64-bit architecture designed in collaboration with HP, called IA64. It would work in a fundamentally different way than contemporary architectures as well: instead of a traditional superscalar architecture, Intel’s design was called EPIC (Explicitly Parallel Instruction Computing). Behind the fancy name was pretty much a clustered VLIW, however.\n\nThe whole philosophy behind VLIW was to move as much functionality as possible to software. All instruction re-ordering and code optimization is done by the compiler. Interlocks are removed and the pipeline is exposed to the compiler. This can work quite well in some cases: DSPs and GPUs are well suited to a VLIW setup.\n\nHowever, they don’t work that well in traditional desktop architectures, because not much parallelism is typically available at compile time.\n\nTo counteract this, Intel designed their architecture with features that would enable it to recoup some of the advantages of superscalar processors. Some of these features were not necessarily proven either.\n\nThe VLIW bundles were split into chunks, with dependency checking within those chunks. They had a rotating register file to do in-hardware loop unrolling. They implemented extensive predication to remove some of the branches. They added speculation. Notice a trend?\n\nIntel originally designed Itanium to reduce complexity, allowing for more performance through higher efficiency. But they then added a lot of complexity back in while trying to make it behave more like an out-of-order superscalar. In the end, they got most of the drawbacks of VLIW, with none of the advantages. The extra registers did help, though.\n\nItanium was originally slated to appear sometime in 1998, but delays started cropping up. The chip was delayed, coming out in 2001. When Itanium came out, it was slow, expensive, and incompatible. The fastest variants were only running at 800MHz, whereas their own desktop chips were already nearing in excess of 2GHz (albeit on the admittedly poor Netburst architecture).\n\nIntel sought to rectify this, releasing the updated Itanium 2 in 2002 through 2004. Many of the original Itanium’s problems were fixed, and the new chip now ran up to ~1.6GHz. Overall, though, the architecture still received a resounding meh. Software compatibility was still a major sticking point. The IA64 wasn’t off to a good start.\n\nThis is where AMD comes in.\n\nAMD, being a high-performance processor designer and manufacturer, also needed a 64-bit processor architecture to compete. However, the company didn’t want to create a brand new architecture. Being a relatively small company compared to Intel, releasing a new incompatible chip would be suicide, as far as they were concerned. The decision was made early on to extend the x86 architecture for the 64 bit era.\n\nTheir solution was x86–64, an extension to the x86 instruction set, adding 64-bit registers, larger address spaces, and more registers (something which was sorely needed) to the old ISA. All processors with this extension would be fully compatible with older x86 applications; all new 64-bit circuitry would essentially be disabled when in 32-bit mode.\n\nTheir first processor to sport this extension was the original Opteron, based on their K8 micro-architecture. This processor was then adapted for consumer use, taking the form of the Athlon 64, released in 2003, using their codename Hammer Core.\n\nIn a way, you could say that AMD ultimately proved Intel wrong: they did build those faster superscalar processors after all. The compatibility with existing code ensured its success.\n\nThe unfortunate reality is that most code outlives the systems it was written for. Some code even outlives its creators. That’s why there’s still 1960s-era COBOL and FORTRAN that needs maintaining. People write code which is never replaced. People just want faster computers; rewriting software is expensive and inconvenient. Sweeping concerns under the rug and turning a blind eye is just easier.\n\nThat is why x86–64 has succeeded when Itanium did not. That is why X86 is still with us today. That is why we still have remnants of space-age code in our modern devices."", 'aiModelVersion': '1'}",0.50565
Michael B.,Updated 3y,"What do we do if an AI goes rogue, or can AIs actually go rogue in real life?","Depends on the intelligence of the AI.

Modern AI is pretty lackluster. Sure it can do things like control the economy or make bad art. But it’s not exactly humanlike intelligence. It can’t go rogue, because there’s nothing there to go rogue. At worst it just breaks and then whatever is reliant on it stops working. So worst case scenario some company loses a lot of money and we all go on with our lives.

But let’s say we build an AI like Skynet. We build a machine with humanlike intelligence and it decides that it hates us. The amount of damage it can cause is dependent on how much power we trust it with. So if my intelligent Roomba goes rogue I can just smash it with a hammer.

But if you entrust an AI with something dangerous, then it could potentially wipe out all life on Earth. Let’s say for some inexplicable reason you’ve let an AI control all your nukes. It can just set them all off and wait for the fallout to kill you. An AI wouldn’t be as affected by those things as a human.

But this is all speculation. In real life AI is nowhere near that advanced. And I’m sure no one would be stupid enough to build a nuclear weapons system with that little oversight.

Edit: Many people have expressed the opinion that we would be stupid to build a nuclear weapons system entirely in the hands of an AI. While humanity has done some pretty idiotic things in the past, I would like to point out that we’ve always been surprisingly careful around nuclear weapons.

To the best of my knowledge there is no single person on Earth who could launch a nuclear weapon without any oversight. Not even the president. And I see no reason that we should abandon that policy if the systems are put in the hands of AI.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6rmpnkl4cg9udiv3', 'title': 'What do we do if an AI goes rogue, or can AIs actually go rogue in real life?', 'score': {'original': 0.9977, 'ai': 0.0023}, 'blocks': [{'text': 'Depends on the intelligence of the AI.\n\nModern AI is pretty lackluster. Sure it can do things like control the economy or make bad art. But it’s not exactly humanlike intelligence. It can’t go rogue, because there’s nothing there to go rogue. At worst it just breaks and then whatever is reliant on it stops working. So worst case scenario some company loses a lot of money and we all go on with our lives.\n\nBut let’s say we build an AI like Skynet. We build a machine with humanlike intelligence and it decides that it hates us. The amount of damage it can cause is dependent on how much power we trust it with. So if my intelligent Roomba goes rogue I can just smash it with a hammer.\n\nBut if you entrust an AI with something dangerous, then it could potentially wipe out all life on Earth. Let’s say for some inexplicable reason you’ve let an AI control all your nukes. It can just set them all off and wait for the fallout to kill you. An AI wouldn’t be as affected by those things as a human.\n\nBut this is all speculation. In real life AI is nowhere near that advanced. And I’m sure no one would be stupid enough to build a nuclear weapons system with that little oversight.\n\nEdit: Many people have expressed the opinion that we would be stupid to build a nuclear weapons system entirely in the hands of an AI. While humanity has done some pretty idiotic things in the past, I would like to point out that we’ve always been surprisingly careful around nuclear weapons.\n\nTo the best of my knowledge there is no single person on Earth who could launch a nuclear weapon without any oversight. Not even the president. And I see no reason that we should abandon that policy if the systems are put in the hands of AI.', 'result': {'fake': 0.0023, 'real': 0.9977}, 'status': 'success'}], 'credits_used': 4, 'credits': 1985962, 'subscription': 0, 'content': 'Depends on the intelligence of the AI.\n\nModern AI is pretty lackluster. Sure it can do things like control the economy or make bad art. But it’s not exactly humanlike intelligence. It can’t go rogue, because there’s nothing there to go rogue. At worst it just breaks and then whatever is reliant on it stops working. So worst case scenario some company loses a lot of money and we all go on with our lives.\n\nBut let’s say we build an AI like Skynet. We build a machine with humanlike intelligence and it decides that it hates us. The amount of damage it can cause is dependent on how much power we trust it with. So if my intelligent Roomba goes rogue I can just smash it with a hammer.\n\nBut if you entrust an AI with something dangerous, then it could potentially wipe out all life on Earth. Let’s say for some inexplicable reason you’ve let an AI control all your nukes. It can just set them all off and wait for the fallout to kill you. An AI wouldn’t be as affected by those things as a human.\n\nBut this is all speculation. In real life AI is nowhere near that advanced. And I’m sure no one would be stupid enough to build a nuclear weapons system with that little oversight.\n\nEdit: Many people have expressed the opinion that we would be stupid to build a nuclear weapons system entirely in the hands of an AI. While humanity has done some pretty idiotic things in the past, I would like to point out that we’ve always been surprisingly careful around nuclear weapons.\n\nTo the best of my knowledge there is no single person on Earth who could launch a nuclear weapon without any oversight. Not even the president. And I see no reason that we should abandon that policy if the systems are put in the hands of AI.', 'aiModelVersion': '1'}",0.9977
Håkon Hapnes Strand,4y,How secure will available jobs as a machine learning engineer be in 2026?,"Six years ago, no one knew what a machine learning engineer was. Six years before that, no one knew what a data scientist was. The technology trends are changing fast, and the buzzwords and job titles are changing even faster. Who knows what these jobs will be called six years into the future.

What I do know is this: If you're a skilled machine learning engineer today, if you stay on the same trajectory and keep up with current trends, you will definitely be in high demand in 2026.

What do machine learning engineers do anyway? We set up efficient data solutions, leverage the data for AI and advanced analytics and make sure that all of this works in production.

Those skills will not get outdated any time soon. They were in demand in 1996, they were in demand in 2016 and they will be in demand in 2026.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rgvo26nudaej7ih8', 'title': 'How secure will available jobs as a machine learning engineer be in 2026?', 'score': {'original': 0.996, 'ai': 0.004}, 'blocks': [{'text': ""Six years ago, no one knew what a machine learning engineer was. Six years before that, no one knew what a data scientist was. The technology trends are changing fast, and the buzzwords and job titles are changing even faster. Who knows what these jobs will be called six years into the future.\n\nWhat I do know is this: If you're a skilled machine learning engineer today, if you stay on the same trajectory and keep up with current trends, you will definitely be in high demand in 2026.\n\nWhat do machine learning engineers do anyway? We set up efficient data solutions, leverage the data for AI and advanced analytics and make sure that all of this works in production.\n\nThose skills will not get outdated any time soon. They were in demand in 1996, they were in demand in 2016 and they will be in demand in 2026."", 'result': {'fake': 0.004, 'real': 0.996}, 'status': 'success'}], 'credits_used': 2, 'credits': 1985960, 'subscription': 0, 'content': ""Six years ago, no one knew what a machine learning engineer was. Six years before that, no one knew what a data scientist was. The technology trends are changing fast, and the buzzwords and job titles are changing even faster. Who knows what these jobs will be called six years into the future.\n\nWhat I do know is this: If you're a skilled machine learning engineer today, if you stay on the same trajectory and keep up with current trends, you will definitely be in high demand in 2026.\n\nWhat do machine learning engineers do anyway? We set up efficient data solutions, leverage the data for AI and advanced analytics and make sure that all of this works in production.\n\nThose skills will not get outdated any time soon. They were in demand in 1996, they were in demand in 2016 and they will be in demand in 2026."", 'aiModelVersion': '1'}",0.996
Scott E. Fahlman,4y,"Who was the first person to get a PhD degree specifically in ""Artificial Intelligence""?","Let me slightly abuse the process by tossing out the first answer to my own question…

It might be me.

My PhD diploma from MIT, dated September 14, 1977, says that the degree of Doctor of Philosophy is awarded for “original research as demonstrated by a thesis in the field of Artificial Intelligence.”

I mentioned this to a friend, who suggested that this might be the first degree awarded specifically and explicitly for Artificial Intelligence. I think that’s possible, but I have no way to be sure, except to ask a lot of people and see if anyone knows of an earlier claim.

I thought that asking the question on Quora might be a good place to start. This is the sort of claim that can easily be refuted, but that probably never can be proven.

Of course there were many PhD degrees awarded in Computer Science before this. CMU established its Computer Science Department (by that name) in 1965 — probably the first such department — and started producing PhDs in CS almost immediately. And many people getting PhDs, starting with Allen Newell, were definitely in the field of AI, whatever it said on their diplomas. (Allen’s degree was in industrial management.) I’m just wondering when the first PhD explicitly in AI was awarded.

My degree in Artificial Intelligence may have been sort of a fluke. In those days, the CS people at MIT were officially in the Department of Electrical Engineering. One day, after my thesis defense, as I was wrapping up loose ends, a departmental administrative assistant came by and asked me what “field” I wanted to specify for my degree. I hadn’t thought about this. I just assumed that my PhD degree would be in EE, just as my BS and MS degrees had been. Perhaps she was asking this because some MIT people at the time were getting PhD degrees in CS rather than EE — I’m not sure.

I thought for a bit. I had been able to skip a few of the mainstream EE courses, substituting computer electives. So I didn’t really feel like a full-fledged electrical engineer, even though I had been fixing TVs and other electronics for friends and neighbors since the Eisenhower administration. And I didn’t really feel like a mainstream CS person either, since I hadn’t focused much on what, at the time, were recognized as core CS areas.

So I said, “Can the degree be in Artificial Intelligence? That’s really what I have been working on and thinking about for the last eight years.”

She wasn’t sure if that was OK, but said she would check. I never heard anything more until I got my diploma, and it did indeed say “Artificial Intelligence”. So I assume that someone with the proper authority thought that this was OK, but maybe it just slipped by with nobody noticing.

Of course, it doesn’t really matter what my diploma says, but it would be fun to be able to claim that I got the first PhD explicitly in AI. And if it someone else pops up, that would be interesting to know about as well.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ioacglqw0bsvdjuz', 'title': 'Who was the first person to get a PhD degree specifically in ""Artificial Intelligence""?', 'score': {'original': 0.47235, 'ai': 0.52765}, 'blocks': [{'text': 'Let me slightly abuse the process by tossing out the first answer to my own question…\n\nIt might be me.\n\nMy PhD diploma from MIT, dated September 14, 1977, says that the degree of Doctor of Philosophy is awarded for “original research as demonstrated by a thesis in the field of Artificial Intelligence.”\n\nI mentioned this to a friend, who suggested that this might be the first degree awarded specifically and explicitly for Artificial Intelligence. I think that’s possible, but I have no way to be sure, except to ask a lot of people and see if anyone knows of an earlier claim.\n\nI thought that asking the question on Quora might be a good place to start. This is the sort of claim that can easily be refuted, but that probably never can be proven.\n\nOf course there were many PhD degrees awarded in Computer Science before this. CMU established its Computer Science Department (by that name) in 1965 — probably the first such department — and started producing PhDs in CS almost immediately. And many people getting PhDs, starting with Allen Newell, were definitely in the field of AI, whatever it said on their diplomas. (Allen’s degree was in industrial management.) I’m just wondering when the first PhD explicitly in AI was awarded.\n\nMy degree in Artificial Intelligence may have been sort of a fluke. In those days, the CS people at MIT were officially in the Department of Electrical Engineering. One day, after my thesis defense, as I was wrapping up loose ends, a departmental administrative assistant came by and asked me what “field” I wanted to specify for my degree. I hadn’t thought about this. I just assumed that my PhD degree would be in EE, just as my BS and MS degrees had been. Perhaps she was asking this because some MIT people at the time were getting PhD degrees in CS rather than EE — I’m not sure.\n\nI thought for a bit. I had been able to skip a few of the mainstream EE courses, substituting computer electives. So I didn’t really feel like a full-fledged electrical engineer, even though I had been fixing TVs and other electronics for friends and neighbors since the Eisenhower administration. And I didn’t really feel like a mainstream CS person either, since I hadn’t focused much on what, at the time, were recognized as core CS areas.\n\nSo I said, “Can the degree be in Artificial Intelligence? That’s really what I have been working on and thinking about for the last eight years.”\n\nShe wasn’t sure if that was OK, but said she would check. I never heard anything more until I got my diploma, and it did indeed say “Artificial Intelligence”. So I assume that someone with the proper authority thought that this was OK, but maybe it just slipped by with nobody noticing.\n\nOf course, it doesn’t really matter what my diploma says, but it would be fun to be able to claim that I got the first PhD explicitly in AI. And if it someone else pops up, that would be interesting to know about as', 'result': {'fake': 0.0174, 'real': 0.9826}, 'status': 'success'}, {'text': 'well.', 'result': {'fake': 0.9872, 'real': 0.0128}, 'status': 'success'}], 'credits_used': 6, 'credits': 1985954, 'subscription': 0, 'content': 'Let me slightly abuse the process by tossing out the first answer to my own question…\n\nIt might be me.\n\nMy PhD diploma from MIT, dated September 14, 1977, says that the degree of Doctor of Philosophy is awarded for “original research as demonstrated by a thesis in the field of Artificial Intelligence.”\n\nI mentioned this to a friend, who suggested that this might be the first degree awarded specifically and explicitly for Artificial Intelligence. I think that’s possible, but I have no way to be sure, except to ask a lot of people and see if anyone knows of an earlier claim.\n\nI thought that asking the question on Quora might be a good place to start. This is the sort of claim that can easily be refuted, but that probably never can be proven.\n\nOf course there were many PhD degrees awarded in Computer Science before this. CMU established its Computer Science Department (by that name) in 1965 — probably the first such department — and started producing PhDs in CS almost immediately. And many people getting PhDs, starting with Allen Newell, were definitely in the field of AI, whatever it said on their diplomas. (Allen’s degree was in industrial management.) I’m just wondering when the first PhD explicitly in AI was awarded.\n\nMy degree in Artificial Intelligence may have been sort of a fluke. In those days, the CS people at MIT were officially in the Department of Electrical Engineering. One day, after my thesis defense, as I was wrapping up loose ends, a departmental administrative assistant came by and asked me what “field” I wanted to specify for my degree. I hadn’t thought about this. I just assumed that my PhD degree would be in EE, just as my BS and MS degrees had been. Perhaps she was asking this because some MIT people at the time were getting PhD degrees in CS rather than EE — I’m not sure.\n\nI thought for a bit. I had been able to skip a few of the mainstream EE courses, substituting computer electives. So I didn’t really feel like a full-fledged electrical engineer, even though I had been fixing TVs and other electronics for friends and neighbors since the Eisenhower administration. And I didn’t really feel like a mainstream CS person either, since I hadn’t focused much on what, at the time, were recognized as core CS areas.\n\nSo I said, “Can the degree be in Artificial Intelligence? That’s really what I have been working on and thinking about for the last eight years.”\n\nShe wasn’t sure if that was OK, but said she would check. I never heard anything more until I got my diploma, and it did indeed say “Artificial Intelligence”. So I assume that someone with the proper authority thought that this was OK, but maybe it just slipped by with nobody noticing.\n\nOf course, it doesn’t really matter what my diploma says, but it would be fun to be able to claim that I got the first PhD explicitly in AI. And if it someone else pops up, that would be interesting to know about as well.', 'aiModelVersion': '1'}",0.47235
Kaushal Hooda,Updated 10y,How exactly does a chess computer work?,"BY THE POWER OF MATHEMATICS!

A chess computer works exactly as any other computer works - by reducing the problem to a bunch of dumb calculations. Because that's what computers are good at. Of course, modern chess algorithms are pretty complicated, but the essence is not that hard to grasp.

Step 1 : Constructing a Tree

Let's say you've got a chessboard set up, with each player having 16 pieces. And it is the computer's turn. Now, the computer can make 1 of 20 possible moves (2 each for the 8 pawns, plus 2 each for the knights). And, in response to any of those moves, the opponent can make 20 possible moves. So, two moves into the game, we have 20*20 = 400 possible scenarios.
Now the computer has around 20 or so ways to respond to each of these 400 scenarios.


And so this tree keeps growing. In theory, the perfect computer would be able to get to the very bottom of this tree, and look at all possible configurations of the board, approximately 10^120. Then it would see which are the paths down this tree that lead to its victory, and choose accordingly.

Step 2 : Evaluating the outcomes

But there's a problem. 10^120 is a very friggin' huge number. Contrast the total estimated atoms in the universe - 10^75, and you get an idea how large. We'd be sitting around waiting for the damn thing to make its move till the universe ended.

So what real computers do is build up this tree to the best of their hardware capabilities - 5, or 10, or 20 or whatever moves into the future. Once they have this limited tree, they evaluate each position using an evaluation function.

For a really simple example, an evaluation function could be the
number of pieces the computer has - number of pieces opponent has.
For example, the computer has 10 pieces left on the board, the opponent has only 8. Then the computer would evaluate such a board to 10 - 8 = 2.
Of course, that's not a very good evaluation function, but you get the idea. This can be made more and more complicated - taking into account the value of individual pieces, board position, control of the centre, vulnerability of the king to check, vulnerability of the opponent's queen, and tons of other parameters.
Whatever the function, it allows a computer to compare board positions, to see which are the desirable outcomes.

Step 3 : Making a move

The analysis done, it's time to make a decision. Let's make up a simplified tree.


The computer, playing as white, has to decide it's move. It constructs the tree above and applies what is called the Minimax
 Algorithm.

It starts from the bottom (3rd) level, and chooses the one with the maximum score. Consider the left-most square in the 2nd level. It has two possible outcomes - 2 and 8. Since it will be the computer's turn at that stage, it chooses the best outcome, ie., the one with MAX score, which is 8, and so it assigns 8 to that node. Similarly for all the nodes in the 2nd level.


Now, for the second level, the outcome is decided by the opponent - since at that time, it will be black's turn. The computer assumes that black will make the move which is best for black, and so the worst for white, hence it chooses the setups with MIN score. For example, for the centre node on the first level, there are three possibilities - 9, 5, and 9. The computer assumes black will take the one that leaves the computer weakest, and that is 5. So, the first level nodes are all given values.


Finally, the first level is the computer's turn, so it makes the choice with MAX score, ie, 7.

Thus the computer climbs the tree, alternatively choosing minimum and maximum scores (Thus the name MINIMAX), and makes the choice that leaves it best off in the end. The better the hardware, the deeper the depth of the tree it can analyse, and so the better its chances of winning. Which is why computers couldn't usually beat humans in the 1950-60s, but now are regularly able to wallop Grandmasters. (Human–computer chess matches
)


Bonus Step : But there's more!

The procedure I've described is an extremely simplified version of what really happens. In practice, the computer nowadays uses a lot of trick to reduce its efforts, tries to avoid going down paths that are clearly hopeless. Alpha–beta pruning
, Endgame tablebase
, Killer heuristic
 etc etc... It's pretty complicated stuff. Seriously, you could do a PhD in CS studying this stuff.


But I hope my answer at least gives you a general picture of what's going on in the computer's silicon neurons.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/s9rf4x81tyczm63w', 'title': 'How exactly does a chess computer work?', 'score': {'original': 0.3618, 'ai': 0.6382}, 'blocks': [{'text': ""BY THE POWER OF MATHEMATICS!\n\nA chess computer works exactly as any other computer works - by reducing the problem to a bunch of dumb calculations. Because that's what computers are good at. Of course, modern chess algorithms are pretty complicated, but the essence is not that hard to grasp.\n\nStep 1 : Constructing a Tree\n\nLet's say you've got a chessboard set up, with each player having 16 pieces. And it is the computer's turn. Now, the computer can make 1 of 20 possible moves (2 each for the 8 pawns, plus 2 each for the knights). And, in response to any of those moves, the opponent can make 20 possible moves. So, two moves into the game, we have 20*20 = 400 possible scenarios.\nNow the computer has around 20 or so ways to respond to each of these 400 scenarios.\n\n\nAnd so this tree keeps growing. In theory, the perfect computer would be able to get to the very bottom of this tree, and look at all possible configurations of the board, approximately 10^120. Then it would see which are the paths down this tree that lead to its victory, and choose accordingly.\n\nStep 2 : Evaluating the outcomes\n\nBut there's a problem. 10^120 is a very friggin' huge number. Contrast the total estimated atoms in the universe - 10^75, and you get an idea how large. We'd be sitting around waiting for the damn thing to make its move till the universe ended.\n\nSo what real computers do is build up this tree to the best of their hardware capabilities - 5, or 10, or 20 or whatever moves into the future. Once they have this limited tree, they evaluate each position using an evaluation function.\n\nFor a really simple example, an evaluation function could be the\nnumber of pieces the computer has - number of pieces opponent has.\nFor example, the computer has 10 pieces left on the board, the opponent has only 8. Then the computer would evaluate such a board to 10 - 8 = 2.\nOf course, that's not a very good evaluation function, but you get the idea. This can be made more and more complicated - taking into account the value of individual pieces, board position, control of the centre, vulnerability of the king to check, vulnerability of the opponent's queen, and tons of other parameters.\nWhatever the function, it allows a computer to compare board positions, to see which are the desirable outcomes.\n\nStep 3 : Making a move\n\nThe analysis done, it's time to make a decision. Let's make up a simplified tree.\n\n\nThe computer, playing as white, has to decide it's move. It constructs the tree above and applies what is called the Minimax\n Algorithm.\n\nIt starts from the bottom (3rd) level, and chooses the one with the maximum score. Consider the left-most square in the 2nd level. It has two possible outcomes - 2 and 8. Since it will be the computer's turn at that stage, it chooses the best outcome, ie., the one with MAX score, which is 8, and so it assigns 8 to that node. Similarly for all the nodes in the"", 'result': {'fake': 0.7453, 'real': 0.2547}, 'status': 'success'}, {'text': ""2nd level.\n\n\nNow, for the second level, the outcome is decided by the opponent - since at that time, it will be black's turn. The computer assumes that black will make the move which is best for black, and so the worst for white, hence it chooses the setups with MIN score. For example, for the centre node on the first level, there are three possibilities - 9, 5, and 9. The computer assumes black will take the one that leaves the computer weakest, and that is 5. So, the first level nodes are all given values.\n\n\nFinally, the first level is the computer's turn, so it makes the choice with MAX score, ie, 7.\n\nThus the computer climbs the tree, alternatively choosing minimum and maximum scores (Thus the name MINIMAX), and makes the choice that leaves it best off in the end. The better the hardware, the deeper the depth of the tree it can analyse, and so the better its chances of winning. Which is why computers couldn't usually beat humans in the 1950-60s, but now are regularly able to wallop Grandmasters. (Human–computer chess matches\n)\n\n\nBonus Step : But there's more!\n\nThe procedure I've described is an extremely simplified version of what really happens. In practice, the computer nowadays uses a lot of trick to reduce its efforts, tries to avoid going down paths that are clearly hopeless. Alpha–beta pruning\n, Endgame tablebase\n, Killer heuristic\n etc etc... It's pretty complicated stuff. Seriously, you could do a PhD in CS studying this stuff.\n\n\nBut I hope my answer at least gives you a general picture of what's going on in the computer's silicon neurons."", 'result': {'fake': 0.339, 'real': 0.661}, 'status': 'success'}], 'credits_used': 8, 'credits': 1985946, 'subscription': 0, 'content': ""BY THE POWER OF MATHEMATICS!\n\nA chess computer works exactly as any other computer works - by reducing the problem to a bunch of dumb calculations. Because that's what computers are good at. Of course, modern chess algorithms are pretty complicated, but the essence is not that hard to grasp.\n\nStep 1 : Constructing a Tree\n\nLet's say you've got a chessboard set up, with each player having 16 pieces. And it is the computer's turn. Now, the computer can make 1 of 20 possible moves (2 each for the 8 pawns, plus 2 each for the knights). And, in response to any of those moves, the opponent can make 20 possible moves. So, two moves into the game, we have 20*20 = 400 possible scenarios.\nNow the computer has around 20 or so ways to respond to each of these 400 scenarios.\n\n\nAnd so this tree keeps growing. In theory, the perfect computer would be able to get to the very bottom of this tree, and look at all possible configurations of the board, approximately 10^120. Then it would see which are the paths down this tree that lead to its victory, and choose accordingly.\n\nStep 2 : Evaluating the outcomes\n\nBut there's a problem. 10^120 is a very friggin' huge number. Contrast the total estimated atoms in the universe - 10^75, and you get an idea how large. We'd be sitting around waiting for the damn thing to make its move till the universe ended.\n\nSo what real computers do is build up this tree to the best of their hardware capabilities - 5, or 10, or 20 or whatever moves into the future. Once they have this limited tree, they evaluate each position using an evaluation function.\n\nFor a really simple example, an evaluation function could be the\nnumber of pieces the computer has - number of pieces opponent has.\nFor example, the computer has 10 pieces left on the board, the opponent has only 8. Then the computer would evaluate such a board to 10 - 8 = 2.\nOf course, that's not a very good evaluation function, but you get the idea. This can be made more and more complicated - taking into account the value of individual pieces, board position, control of the centre, vulnerability of the king to check, vulnerability of the opponent's queen, and tons of other parameters.\nWhatever the function, it allows a computer to compare board positions, to see which are the desirable outcomes.\n\nStep 3 : Making a move\n\nThe analysis done, it's time to make a decision. Let's make up a simplified tree.\n\n\nThe computer, playing as white, has to decide it's move. It constructs the tree above and applies what is called the Minimax\n Algorithm.\n\nIt starts from the bottom (3rd) level, and chooses the one with the maximum score. Consider the left-most square in the 2nd level. It has two possible outcomes - 2 and 8. Since it will be the computer's turn at that stage, it chooses the best outcome, ie., the one with MAX score, which is 8, and so it assigns 8 to that node. Similarly for all the nodes in the 2nd level.\n\n\nNow, for the second level, the outcome is decided by the opponent - since at that time, it will be black's turn. The computer assumes that black will make the move which is best for black, and so the worst for white, hence it chooses the setups with MIN score. For example, for the centre node on the first level, there are three possibilities - 9, 5, and 9. The computer assumes black will take the one that leaves the computer weakest, and that is 5. So, the first level nodes are all given values.\n\n\nFinally, the first level is the computer's turn, so it makes the choice with MAX score, ie, 7.\n\nThus the computer climbs the tree, alternatively choosing minimum and maximum scores (Thus the name MINIMAX), and makes the choice that leaves it best off in the end. The better the hardware, the deeper the depth of the tree it can analyse, and so the better its chances of winning. Which is why computers couldn't usually beat humans in the 1950-60s, but now are regularly able to wallop Grandmasters. (Human–computer chess matches\n)\n\n\nBonus Step : But there's more!\n\nThe procedure I've described is an extremely simplified version of what really happens. In practice, the computer nowadays uses a lot of trick to reduce its efforts, tries to avoid going down paths that are clearly hopeless. Alpha–beta pruning\n, Endgame tablebase\n, Killer heuristic\n etc etc... It's pretty complicated stuff. Seriously, you could do a PhD in CS studying this stuff.\n\n\nBut I hope my answer at least gives you a general picture of what's going on in the computer's silicon neurons."", 'aiModelVersion': '1'}",0.3618
Jerry Coffin,4y,What is the most inefficient sort in computer science?,"Bogobogosort

I had previously believed this “honor” probably belonged to the bogobogosort (note the extra “bogo” there—this is not your garden variety bogosort. Nonetheless, as you can probably guess from the name, bogobogosort is based on and fairly similar to the bogosort.

To understand bogobogosort, we first have to outline the basic idea of bogosort, which is:

randomize the order of the elements
check of they’re in order
if not, repeat from step 1

So, for N elements, there are N! possible arrangements, and checking if the elements are in order is linear, so overall complexity is O(N * N!) (which is sometimes represented as O((N+1)!)—which is technically accurate, but unnecessarily pessimistic, though not by a large factor).

The general idea of bogobogosort is similar, but in step 2, it specifies a somewhat more complex method of finding whether the elements are in order. In the vanilla bogosort, you do the obvious: walk though the collection and check whether each item is larger than its predecessor.

Bogobogosort is clearly superior, because it uses recursion. To test whether N elements are in order, you first copy the collection, then do a bogobogosort on the first N-1 elements, then check whether element N is greater than element N-1, then check whether this sorted-sub-list is in the same order as the original one.

At least according to an analysis by Nathan Collins, this has an overall complexity of 
O
(
n
!
n
−
k
)
O(n!n−k)
—dramatically worse than the basic bogosort.

For what it’s worth, the inventor did test bogobogosort. Its complexity is high enough that he only ever sorted up to 6 elements. 5 elements sorted in less than a second. Six elements took 450 seconds. He tried to sort 7 elements, but gave up after it ran overnight without finishing.

Beating Bogobogosort

To provide a proper answer to this question, however, I did a little research, and found something named Worst Sort. The notion of worst sort is similar to that of the bogobogosort: start with a terrible algorithm, then use recursion to make it arbitrarily worse. Worst sort, however, is more of a generalized meta-algorithm though, so it doesn’t specify all the parameters to be used. In particular, it specifies use of a function to determine the recursion depth, but doesn’t specify the exact function to be used. One common suggestion is apparently the Ackerman function (known for extremely fast growth itself).

Depending on the function you use, its complexity can be arbitrarily high.

My own entry

I’ve invented an algorithm that’s a bit difficult to compare to the others. The basic idea is fairly simple:

Check whether the elements are in order
If not, repeat from step 1

This may not seem like it’s going any sorting at all, and that’s sort of true. What it’s doing is waiting for cosmic rays to flip bits in the elements of the collection so they happen to come out in order.

This adds another element to the equation: its speed depends on how well protected you are from cosmic rays. For example, the higher your altitude, the faster it’s likely to complete.

Its complexity also depends on the number of *bits* involved in the out or order condition. For example, sorting two 8-bit bytes may be faster than sorting two 64-bit words.

For the moment, however, let’s consider sorting two 64-bit words with the values 0xAAAAAAAAAAAAAAAA and 0x5555555555555555. This is a reasonable approximation of a worst case—it requires that every bit in each word be flipped simultaneously for the result to be equal to the original values, but in their proper order. As noted, the exact speed will depend on altitude (and other factors controlling cosmic rays), but a quick computation indicates that at the current level of cosmic radiation on earth at sea level, just getting those two elements in order is unlikely to happen before the heat death of the universe.

This is sort of cheating though, since it doesn’t (itself) take any action that contributes toward the elements getting into order.

Reference

DM's Esoteric Programming Languages","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/7cefh18zvbq245jd', 'title': 'What is the most inefficient sort in computer science?', 'score': {'original': 0.8973, 'ai': 0.1027}, 'blocks': [{'text': 'Bogobogosort\n\nI had previously believed this “honor” probably belonged to the bogobogosort (note the extra “bogo” there—this is not your garden variety bogosort. Nonetheless, as you can probably guess from the name, bogobogosort is based on and fairly similar to the bogosort.\n\nTo understand bogobogosort, we first have to outline the basic idea of bogosort, which is:\n\nrandomize the order of the elements\ncheck of they’re in order\nif not, repeat from step 1\n\nSo, for N elements, there are N! possible arrangements, and checking if the elements are in order is linear, so overall complexity is O(N * N!) (which is sometimes represented as O((N+1)!)—which is technically accurate, but unnecessarily pessimistic, though not by a large factor).\n\nThe general idea of bogobogosort is similar, but in step 2, it specifies a somewhat more complex method of finding whether the elements are in order. In the vanilla bogosort, you do the obvious: walk though the collection and check whether each item is larger than its predecessor.\n\nBogobogosort is clearly superior, because it uses recursion. To test whether N elements are in order, you first copy the collection, then do a bogobogosort on the first N-1 elements, then check whether element N is greater than element N-1, then check whether this sorted-sub-list is in the same order as the original one.\n\nAt least according to an analysis by Nathan Collins, this has an overall complexity of \nO\n(\nn\n!\nn\n−\nk\n)\nO(n!n−k)\n—dramatically worse than the basic bogosort.\n\nFor what it’s worth, the inventor did test bogobogosort. Its complexity is high enough that he only ever sorted up to 6 elements. 5 elements sorted in less than a second. Six elements took 450 seconds. He tried to sort 7 elements, but gave up after it ran overnight without finishing.\n\nBeating Bogobogosort\n\nTo provide a proper answer to this question, however, I did a little research, and found something named Worst Sort. The notion of worst sort is similar to that of the bogobogosort: start with a terrible algorithm, then use recursion to make it arbitrarily worse. Worst sort, however, is more of a generalized meta-algorithm though, so it doesn’t specify all the parameters to be used. In particular, it specifies use of a function to determine the recursion depth, but doesn’t specify the exact function to be used. One common suggestion is apparently the Ackerman function (known for extremely fast growth itself).\n\nDepending on the function you use, its complexity can be arbitrarily high.\n\nMy own entry\n\nI’ve invented an algorithm that’s a bit difficult to compare to the others. The basic idea is fairly simple:\n\nCheck whether the elements are in order\nIf not, repeat from step 1\n\nThis may not seem like it’s going any sorting at all, and that’s sort of true. What it’s doing is waiting for cosmic rays to flip bits in the elements of the collection so they happen to come out in order.\n\nThis adds another element to the equation: its speed depends on how well protected you are from cosmic rays. For example, the higher your altitude, the faster it’s likely to complete.\n\nIts complexity also depends on the number of *bits* involved in the out or order condition. For example,', 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}, {'text': ""sorting two 8-bit bytes may be faster than sorting two 64-bit words.\n\nFor the moment, however, let’s consider sorting two 64-bit words with the values 0xAAAAAAAAAAAAAAAA and 0x5555555555555555. This is a reasonable approximation of a worst case—it requires that every bit in each word be flipped simultaneously for the result to be equal to the original values, but in their proper order. As noted, the exact speed will depend on altitude (and other factors controlling cosmic rays), but a quick computation indicates that at the current level of cosmic radiation on earth at sea level, just getting those two elements in order is unlikely to happen before the heat death of the universe.\n\nThis is sort of cheating though, since it doesn’t (itself) take any action that contributes toward the elements getting into order.\n\nReference\n\nDM's Esoteric Programming Languages"", 'result': {'fake': 0.1327, 'real': 0.8673}, 'status': 'success'}], 'credits_used': 7, 'credits': 1985939, 'subscription': 0, 'content': ""Bogobogosort\n\nI had previously believed this “honor” probably belonged to the bogobogosort (note the extra “bogo” there—this is not your garden variety bogosort. Nonetheless, as you can probably guess from the name, bogobogosort is based on and fairly similar to the bogosort.\n\nTo understand bogobogosort, we first have to outline the basic idea of bogosort, which is:\n\nrandomize the order of the elements\ncheck of they’re in order\nif not, repeat from step 1\n\nSo, for N elements, there are N! possible arrangements, and checking if the elements are in order is linear, so overall complexity is O(N * N!) (which is sometimes represented as O((N+1)!)—which is technically accurate, but unnecessarily pessimistic, though not by a large factor).\n\nThe general idea of bogobogosort is similar, but in step 2, it specifies a somewhat more complex method of finding whether the elements are in order. In the vanilla bogosort, you do the obvious: walk though the collection and check whether each item is larger than its predecessor.\n\nBogobogosort is clearly superior, because it uses recursion. To test whether N elements are in order, you first copy the collection, then do a bogobogosort on the first N-1 elements, then check whether element N is greater than element N-1, then check whether this sorted-sub-list is in the same order as the original one.\n\nAt least according to an analysis by Nathan Collins, this has an overall complexity of \nO\n(\nn\n!\nn\n−\nk\n)\nO(n!n−k)\n—dramatically worse than the basic bogosort.\n\nFor what it’s worth, the inventor did test bogobogosort. Its complexity is high enough that he only ever sorted up to 6 elements. 5 elements sorted in less than a second. Six elements took 450 seconds. He tried to sort 7 elements, but gave up after it ran overnight without finishing.\n\nBeating Bogobogosort\n\nTo provide a proper answer to this question, however, I did a little research, and found something named Worst Sort. The notion of worst sort is similar to that of the bogobogosort: start with a terrible algorithm, then use recursion to make it arbitrarily worse. Worst sort, however, is more of a generalized meta-algorithm though, so it doesn’t specify all the parameters to be used. In particular, it specifies use of a function to determine the recursion depth, but doesn’t specify the exact function to be used. One common suggestion is apparently the Ackerman function (known for extremely fast growth itself).\n\nDepending on the function you use, its complexity can be arbitrarily high.\n\nMy own entry\n\nI’ve invented an algorithm that’s a bit difficult to compare to the others. The basic idea is fairly simple:\n\nCheck whether the elements are in order\nIf not, repeat from step 1\n\nThis may not seem like it’s going any sorting at all, and that’s sort of true. What it’s doing is waiting for cosmic rays to flip bits in the elements of the collection so they happen to come out in order.\n\nThis adds another element to the equation: its speed depends on how well protected you are from cosmic rays. For example, the higher your altitude, the faster it’s likely to complete.\n\nIts complexity also depends on the number of *bits* involved in the out or order condition. For example, sorting two 8-bit bytes may be faster than sorting two 64-bit words.\n\nFor the moment, however, let’s consider sorting two 64-bit words with the values 0xAAAAAAAAAAAAAAAA and 0x5555555555555555. This is a reasonable approximation of a worst case—it requires that every bit in each word be flipped simultaneously for the result to be equal to the original values, but in their proper order. As noted, the exact speed will depend on altitude (and other factors controlling cosmic rays), but a quick computation indicates that at the current level of cosmic radiation on earth at sea level, just getting those two elements in order is unlikely to happen before the heat death of the universe.\n\nThis is sort of cheating though, since it doesn’t (itself) take any action that contributes toward the elements getting into order.\n\nReference\n\nDM's Esoteric Programming Languages"", 'aiModelVersion': '1'}",0.8973
Steven,Updated 8mo,"How do you explain P, NP, NP-Hard, NP-Complete to your mom?","Hi mom! Okay, this is it! First, sit down, take a deep breath. Have some tea. This is gonna hit you hard. Okay, here goes.

People have found out that some problems are much easier to solve with a computer than others. They have given names to groups of problems depending on how hard or easy they are. Makes sense? Okay.

The first group of problems is called P. Problems in P can be solved by a computer in a reasonable amount of time. Example: sorting a list of names is not so much work for a computer. You can remember it like this: ""Easy PEE-sy!"" right?

Not much to say about those. They're the boring run of the mill problems. We are going to look at the interesting stuff.

The next group, called NP, contains problems for which a computer can quickly check a proposed solution. So all P problems are also NP problems: if you can solve it, you can check it, but not necessarily the other way around. So you can remember NP like this: ""NOPE! Not so easy!"" An example of an NP problem is: ""Given this city map, is there a route of length X that visits all these addresses""? Given such a route, it's easy to check, but it's not easy to find one.

Okay mom, this was sort of reasonable, right? But now it gets a little weird. Because as it turns out, there are some problems that we don't know how to solve efficiently, but if we could, then we could use the solution to immediately solve all other problems in NP. This holy grail of computing is called the group of NP-hard problems. Finding a route of length X via a set of addresses is an example of an NP-hard problem: it would be extremely cool if someone were able to figure out how to do it efficiently (but don't hold your breath). If you figure out how to solve an NP-hard problem efficiently you will be a millionaire. Everyone will want to touch you. For real!

Some NP-hard problems are so hard, their solutions cannot even be checked efficiently; so confusingly these problems are not always in NP themselves. To rule those bad boys out, people call the group of NP-hard problems that are still easy to verify ""NP-complete"" problems.

Okay mom. Now I know you really want to run away and do something real. But you can't yet, because I still have to tell you about the biggest mystery in computer science, which is this: no-one knows if the picture I have just drawn for you is correct: no-one knows if there are NP-complete problems that are also in P! Everyone is completely and utterly convinced that these don’t exist, but no one has been able to prove it. Chew on that!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/s5iw028rzoqh73an', 'title': 'How do you explain P, NP, NP-Hard, NP-Complete to your mom?', 'score': {'original': 0.9987, 'ai': 0.0013}, 'blocks': [{'text': 'Hi mom! Okay, this is it! First, sit down, take a deep breath. Have some tea. This is gonna hit you hard. Okay, here goes.\n\nPeople have found out that some problems are much easier to solve with a computer than others. They have given names to groups of problems depending on how hard or easy they are. Makes sense? Okay.\n\nThe first group of problems is called P. Problems in P can be solved by a computer in a reasonable amount of time. Example: sorting a list of names is not so much work for a computer. You can remember it like this: ""Easy PEE-sy!"" right?\n\nNot much to say about those. They\'re the boring run of the mill problems. We are going to look at the interesting stuff.\n\nThe next group, called NP, contains problems for which a computer can quickly check a proposed solution. So all P problems are also NP problems: if you can solve it, you can check it, but not necessarily the other way around. So you can remember NP like this: ""NOPE! Not so easy!"" An example of an NP problem is: ""Given this city map, is there a route of length X that visits all these addresses""? Given such a route, it\'s easy to check, but it\'s not easy to find one.\n\nOkay mom, this was sort of reasonable, right? But now it gets a little weird. Because as it turns out, there are some problems that we don\'t know how to solve efficiently, but if we could, then we could use the solution to immediately solve all other problems in NP. This holy grail of computing is called the group of NP-hard problems. Finding a route of length X via a set of addresses is an example of an NP-hard problem: it would be extremely cool if someone were able to figure out how to do it efficiently (but don\'t hold your breath). If you figure out how to solve an NP-hard problem efficiently you will be a millionaire. Everyone will want to touch you. For real!\n\nSome NP-hard problems are so hard, their solutions cannot even be checked efficiently; so confusingly these problems are not always in NP themselves. To rule those bad boys out, people call the group of NP-hard problems that are still easy to verify ""NP-complete"" problems.\n\nOkay mom. Now I know you really want to run away and do something real. But you can\'t yet, because I still have to tell you about the biggest mystery in computer science, which is this: no-one knows if the picture I have just drawn for you is correct: no-one knows if there are NP-complete problems that are also in P! Everyone is completely and utterly convinced that these don’t exist, but no one has been able to prove it. Chew on that!', 'result': {'fake': 0.0034, 'real': 0.9966}, 'status': 'success'}], 'credits_used': 5, 'credits': 1985934, 'subscription': 0, 'content': 'Hi mom! Okay, this is it! First, sit down, take a deep breath. Have some tea. This is gonna hit you hard. Okay, here goes.\n\nPeople have found out that some problems are much easier to solve with a computer than others. They have given names to groups of problems depending on how hard or easy they are. Makes sense? Okay.\n\nThe first group of problems is called P. Problems in P can be solved by a computer in a reasonable amount of time. Example: sorting a list of names is not so much work for a computer. You can remember it like this: ""Easy PEE-sy!"" right?\n\nNot much to say about those. They\'re the boring run of the mill problems. We are going to look at the interesting stuff.\n\nThe next group, called NP, contains problems for which a computer can quickly check a proposed solution. So all P problems are also NP problems: if you can solve it, you can check it, but not necessarily the other way around. So you can remember NP like this: ""NOPE! Not so easy!"" An example of an NP problem is: ""Given this city map, is there a route of length X that visits all these addresses""? Given such a route, it\'s easy to check, but it\'s not easy to find one.\n\nOkay mom, this was sort of reasonable, right? But now it gets a little weird. Because as it turns out, there are some problems that we don\'t know how to solve efficiently, but if we could, then we could use the solution to immediately solve all other problems in NP. This holy grail of computing is called the group of NP-hard problems. Finding a route of length X via a set of addresses is an example of an NP-hard problem: it would be extremely cool if someone were able to figure out how to do it efficiently (but don\'t hold your breath). If you figure out how to solve an NP-hard problem efficiently you will be a millionaire. Everyone will want to touch you. For real!\n\nSome NP-hard problems are so hard, their solutions cannot even be checked efficiently; so confusingly these problems are not always in NP themselves. To rule those bad boys out, people call the group of NP-hard problems that are still easy to verify ""NP-complete"" problems.\n\nOkay mom. Now I know you really want to run away and do something real. But you can\'t yet, because I still have to tell you about the biggest mystery in computer science, which is this: no-one knows if the picture I have just drawn for you is correct: no-one knows if there are NP-complete problems that are also in P! Everyone is completely and utterly convinced that these don’t exist, but no one has been able to prove it. Chew on that!', 'aiModelVersion': '1'}",0.9987
Wim ten Brink,4y,How did early software developers code when there is no compiler and no operating system back then?,"Reading Paper Tapes From Scratch!

What you see is a keyboard and a paper tape reader. They didn’t have the keyboard back then but they did have paper tape and the tools to punch holes in tape. And one of the first programs would be written like this:

And it would be used by this computer:

Wait… That’s no computer! That’s a loom! The Jacquard machine
 from 1804! Made shortly after Benjamin Franklin had been playing with his kite and got shocking results. Yet this machine was still analog and mechanical, yet it was programmable and would weave patterns in fabrics as dictated by the machine.

Now, if you can tell a mechanical machine what to do with simple holes in paper then you can do the same for computers with punched tape or punched cards. The card would have a row (or column) of holes (or not-holes) and this could be converted into an instruction. When it read the next row, it would do the next instruction. All the way until it runs out of paper… But put the paper in a feed loop and it runs forever.

This loom is now considered one of the first machines with a programming language of its own.

Later, other techniques were created to allow more instructions and to make computers more complex. And faster, as these early machines had a “clock speed” in single-digit Hertz. Not megaHertz, or kiloHertz, but Hertz! You could easily count every instruction that was executed back then…

Punched cards 
are now obsolete even though some were still used in 2012 for some voting machines. They first came up in 1725 to control a loom but it would still need an operator until the Jacques loom was made. The idea of using paper to send commands to a machine was a stroke of genius and basically made computers possible.

People were already coding before computers existed!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/8u5w1xelitz4kp2c', 'title': 'How did early software developers code when there is no compiler and no operating system back then?', 'score': {'original': 0.9992, 'ai': 0.0008}, 'blocks': [{'text': 'Reading Paper Tapes From Scratch!\n\nWhat you see is a keyboard and a paper tape reader. They didn’t have the keyboard back then but they did have paper tape and the tools to punch holes in tape. And one of the first programs would be written like this:\n\nAnd it would be used by this computer:\n\nWait… That’s no computer! That’s a loom! The Jacquard machine\n from 1804! Made shortly after Benjamin Franklin had been playing with his kite and got shocking results. Yet this machine was still analog and mechanical, yet it was programmable and would weave patterns in fabrics as dictated by the machine.\n\nNow, if you can tell a mechanical machine what to do with simple holes in paper then you can do the same for computers with punched tape or punched cards. The card would have a row (or column) of holes (or not-holes) and this could be converted into an instruction. When it read the next row, it would do the next instruction. All the way until it runs out of paper… But put the paper in a feed loop and it runs forever.\n\nThis loom is now considered one of the first machines with a programming language of its own.\n\nLater, other techniques were created to allow more instructions and to make computers more complex. And faster, as these early machines had a “clock speed” in single-digit Hertz. Not megaHertz, or kiloHertz, but Hertz! You could easily count every instruction that was executed back then…\n\nPunched cards \nare now obsolete even though some were still used in 2012 for some voting machines. They first came up in 1725 to control a loom but it would still need an operator until the Jacques loom was made. The idea of using paper to send commands to a machine was a stroke of genius and basically made computers possible.\n\nPeople were already coding before computers existed!', 'result': {'fake': 0.0008, 'real': 0.9992}, 'status': 'success'}], 'credits_used': 4, 'credits': 1985930, 'subscription': 0, 'content': 'Reading Paper Tapes From Scratch!\n\nWhat you see is a keyboard and a paper tape reader. They didn’t have the keyboard back then but they did have paper tape and the tools to punch holes in tape. And one of the first programs would be written like this:\n\nAnd it would be used by this computer:\n\nWait… That’s no computer! That’s a loom! The Jacquard machine\n from 1804! Made shortly after Benjamin Franklin had been playing with his kite and got shocking results. Yet this machine was still analog and mechanical, yet it was programmable and would weave patterns in fabrics as dictated by the machine.\n\nNow, if you can tell a mechanical machine what to do with simple holes in paper then you can do the same for computers with punched tape or punched cards. The card would have a row (or column) of holes (or not-holes) and this could be converted into an instruction. When it read the next row, it would do the next instruction. All the way until it runs out of paper… But put the paper in a feed loop and it runs forever.\n\nThis loom is now considered one of the first machines with a programming language of its own.\n\nLater, other techniques were created to allow more instructions and to make computers more complex. And faster, as these early machines had a “clock speed” in single-digit Hertz. Not megaHertz, or kiloHertz, but Hertz! You could easily count every instruction that was executed back then…\n\nPunched cards \nare now obsolete even though some were still used in 2012 for some voting machines. They first came up in 1725 to control a loom but it would still need an operator until the Jacques loom was made. The idea of using paper to send commands to a machine was a stroke of genius and basically made computers possible.\n\nPeople were already coding before computers existed!', 'aiModelVersion': '1'}",0.9992
Alon Amit,Updated 5y,Why do the majority of computer scientists believe that P is not equal to NP (P ≠ NP)?,"The majority of people who studied algorithms find it very unlikely that finding a solution in a large space of alternatives is as easy as verifying a solution if one is handed to you.

This is a very natural expectation. Verifying that a graph is legally colored with three colors is straightforward. Finding a 3-coloring of a large, dense graph seems very difficult.

And so it is with verifying the correctness of a proof vs finding a proof, or verifying that a travel plan has short length vs finding a travel plan that’s short.

The assertion that P=NP essentially says that all of these problems are about as easy to solve as they are to verify (“about” in a rough, asymptotic sense, but still a very natural one). This seems very unreasonable, and the big surprise is that it’s so hard to refute.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/r6bj87xzlkmaqsvc', 'title': 'Why do the majority of computer scientists believe that P is not equal to NP (P ≠ NP)?', 'score': {'original': 0.9487, 'ai': 0.0513}, 'blocks': [{'text': 'The majority of people who studied algorithms find it very unlikely that finding a solution in a large space of alternatives is as easy as verifying a solution if one is handed to you.\n\nThis is a very natural expectation. Verifying that a graph is legally colored with three colors is straightforward. Finding a 3-coloring of a large, dense graph seems very difficult.\n\nAnd so it is with verifying the correctness of a proof vs finding a proof, or verifying that a travel plan has short length vs finding a travel plan that’s short.\n\nThe assertion that P=NP essentially says that all of these problems are about as easy to solve as they are to verify (“about” in a rough, asymptotic sense, but still a very natural one). This seems very unreasonable, and the big surprise is that it’s so hard to refute.', 'result': {'fake': 0.0513, 'real': 0.9487}, 'status': 'success'}], 'credits_used': 2, 'credits': 1985928, 'subscription': 0, 'content': 'The majority of people who studied algorithms find it very unlikely that finding a solution in a large space of alternatives is as easy as verifying a solution if one is handed to you.\n\nThis is a very natural expectation. Verifying that a graph is legally colored with three colors is straightforward. Finding a 3-coloring of a large, dense graph seems very difficult.\n\nAnd so it is with verifying the correctness of a proof vs finding a proof, or verifying that a travel plan has short length vs finding a travel plan that’s short.\n\nThe assertion that P=NP essentially says that all of these problems are about as easy to solve as they are to verify (“about” in a rough, asymptotic sense, but still a very natural one). This seems very unreasonable, and the big surprise is that it’s so hard to refute.', 'aiModelVersion': '1'}",0.9487
Lawrence Stewart,Updated 4y,How fast can a supercomputer loop 4228250625 (255 ^ 4) times?,"I wrote the following program:

int main(int argc, char *argv[]) 
{ 
  long i; 
  for (i = 0; i < 4228250625L; i += 1); 
  return 0; 
} 

which compiles to:

main: 
.LFB0: 
	.cfi_startproc 
	movl	$4228250625, %eax 
.L2: 
	subq	$1, %rax 
	jne	.L2 
	movl	$0, %eax 
	ret 

When I run it on my 2016 Macbook Pro it takes 5.3 seconds.

$ time ./a.out 
real	0m5.317s 
user	0m5.315s 
sys	0m0.000s 

So that is about 800 million loops per second. Of course the loop is only two instructions, so yeah.

When I compile with optimization, the compiler figures out that the loop doesn’t do anything and just leaves it out, so that is fast!

On a supercomputer, you would want to parallelize this a few thousand ways, so maybe a millisecond total?

UPDATE:

If you nest the loops, like this:

int main(int argc, char *argv[]) 
{ 
  for (unsigned i = 0; i < 255; i += 1) 
    for (unsigned j = 0; j < 255; j += 1) 
      for (unsigned k = 0; k < 255; k += 1) 
	    for (unsigned l = 0; l < 255; l += 1); 
  return 0; 
} 

It is 5% slower:

$ time ./a.out 
real	0m5.474s 
user	0m5.468s 
sys	0m0.005s 

SECOND UPDATE:

I added code to enable OpenMP so I can use a 16-core 32-thread server I have access to:

#include ""omp.h"" 
int main(int argc, char *argv[]) 
{ 
#pragma omp parallel for 
  for (unsigned i = 0; i < 255; i += 1) 
    for (unsigned j = 0; j < 255; j += 1) 
      for (unsigned k = 0; k < 255; k += 1) 
	for (unsigned l = 0; l < 255; l += 1); 
  return 0; 
} 

Now I can compile -fopenmp and run the program with

OMP_NUM_THREADS=N time ./a.out

for various values of N up to 32.

For 1 thread on this server it takes 8.4 seconds (slower than my macbook) but for 32 threads it runs in 0.44 seconds. 16 threads runs in 0.68 seconds. These last two data points are important because they illustrate that programs that do not do memory references don’t benefit as much from hyperthreading. Still, I am now doing north of 8 billion loops per second.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rj9sonh8l7u3e1mg', 'title': 'How fast can a supercomputer loop 4228250625 (255 ^ 4) times?', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'I wrote the following program:\n\nint main(int argc, char *argv[])\xa0\n{\xa0\n  long i;\xa0\n  for (i = 0; i < 4228250625L; i += 1);\xa0\n  return 0;\xa0\n}\xa0\n\nwhich compiles to:\n\nmain:\xa0\n.LFB0:\xa0\n\t.cfi_startproc\xa0\n\tmovl\t$4228250625, %eax\xa0\n.L2:\xa0\n\tsubq\t$1, %rax\xa0\n\tjne\t.L2\xa0\n\tmovl\t$0, %eax\xa0\n\tret\xa0\n\nWhen I run it on my 2016 Macbook Pro it takes 5.3 seconds.\n\n$ time ./a.out\xa0\nreal\t0m5.317s\xa0\nuser\t0m5.315s\xa0\nsys\t0m0.000s\xa0\n\nSo that is about 800 million loops per second. Of course the loop is only two instructions, so yeah.\n\nWhen I compile with optimization, the compiler figures out that the loop doesn’t do anything and just leaves it out, so that is fast!\n\nOn a supercomputer, you would want to parallelize this a few thousand ways, so maybe a millisecond total?\n\nUPDATE:\n\nIf you nest the loops, like this:\n\nint main(int argc, char *argv[])\xa0\n{\xa0\n  for (unsigned i = 0; i < 255; i += 1)\xa0\n    for (unsigned j = 0; j < 255; j += 1)\xa0\n      for (unsigned k = 0; k < 255; k += 1)\xa0\n\t    for (unsigned l = 0; l < 255; l += 1);\xa0\n  return 0;\xa0\n}\xa0\n\nIt is 5% slower:\n\n$ time ./a.out\xa0\nreal\t0m5.474s\xa0\nuser\t0m5.468s\xa0\nsys\t0m0.005s\xa0\n\nSECOND UPDATE:\n\nI added code to enable OpenMP so I can use a 16-core 32-thread server I have access to:\n\n#include ""omp.h""\xa0\nint main(int argc, char *argv[])\xa0\n{\xa0\n#pragma omp parallel for\xa0\n  for (unsigned i = 0; i < 255; i += 1)\xa0\n    for (unsigned j = 0; j < 255; j += 1)\xa0\n      for (unsigned k = 0; k < 255; k += 1)\xa0\n\tfor (unsigned l = 0; l < 255; l += 1);\xa0\n  return 0;\xa0\n}\xa0\n\nNow I can compile -fopenmp and run the program with\n\nOMP_NUM_THREADS=N time ./a.out\n\nfor various values of N up to 32.\n\nFor 1 thread on this server it takes 8.4 seconds (slower than my macbook) but for 32 threads it runs in 0.44 seconds. 16 threads runs in 0.68 seconds. These last two data points are important because they illustrate that programs that do not do memory references don’t benefit as much from hyperthreading. Still, I am now doing north of 8 billion loops per second.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 4, 'credits': 1985924, 'subscription': 0, 'content': 'I wrote the following program:\n\nint main(int argc, char *argv[])\xa0\n{\xa0\n  long i;\xa0\n  for (i = 0; i < 4228250625L; i += 1);\xa0\n  return 0;\xa0\n}\xa0\n\nwhich compiles to:\n\nmain:\xa0\n.LFB0:\xa0\n\t.cfi_startproc\xa0\n\tmovl\t$4228250625, %eax\xa0\n.L2:\xa0\n\tsubq\t$1, %rax\xa0\n\tjne\t.L2\xa0\n\tmovl\t$0, %eax\xa0\n\tret\xa0\n\nWhen I run it on my 2016 Macbook Pro it takes 5.3 seconds.\n\n$ time ./a.out\xa0\nreal\t0m5.317s\xa0\nuser\t0m5.315s\xa0\nsys\t0m0.000s\xa0\n\nSo that is about 800 million loops per second. Of course the loop is only two instructions, so yeah.\n\nWhen I compile with optimization, the compiler figures out that the loop doesn’t do anything and just leaves it out, so that is fast!\n\nOn a supercomputer, you would want to parallelize this a few thousand ways, so maybe a millisecond total?\n\nUPDATE:\n\nIf you nest the loops, like this:\n\nint main(int argc, char *argv[])\xa0\n{\xa0\n  for (unsigned i = 0; i < 255; i += 1)\xa0\n    for (unsigned j = 0; j < 255; j += 1)\xa0\n      for (unsigned k = 0; k < 255; k += 1)\xa0\n\t    for (unsigned l = 0; l < 255; l += 1);\xa0\n  return 0;\xa0\n}\xa0\n\nIt is 5% slower:\n\n$ time ./a.out\xa0\nreal\t0m5.474s\xa0\nuser\t0m5.468s\xa0\nsys\t0m0.005s\xa0\n\nSECOND UPDATE:\n\nI added code to enable OpenMP so I can use a 16-core 32-thread server I have access to:\n\n#include ""omp.h""\xa0\nint main(int argc, char *argv[])\xa0\n{\xa0\n#pragma omp parallel for\xa0\n  for (unsigned i = 0; i < 255; i += 1)\xa0\n    for (unsigned j = 0; j < 255; j += 1)\xa0\n      for (unsigned k = 0; k < 255; k += 1)\xa0\n\tfor (unsigned l = 0; l < 255; l += 1);\xa0\n  return 0;\xa0\n}\xa0\n\nNow I can compile -fopenmp and run the program with\n\nOMP_NUM_THREADS=N time ./a.out\n\nfor various values of N up to 32.\n\nFor 1 thread on this server it takes 8.4 seconds (slower than my macbook) but for 32 threads it runs in 0.44 seconds. 16 threads runs in 0.68 seconds. These last two data points are important because they illustrate that programs that do not do memory references don’t benefit as much from hyperthreading. Still, I am now doing north of 8 billion loops per second.', 'aiModelVersion': '1'}",0.9996
Alon Amit,3y,What are the implications of proving P ,"The sheer fact that it’s proven won’t affect the world as we know it. It’s widely assumed that P
≠
≠
NP already, and knowing that it’s true won’t make any difference.

The proof itself will certainly introduce ideas and techniques that would have a deep impact on the field of theoretical computer science and computational complexity. It’s possible that such a proof may have broader impact on logic and mathematics. It will be a monumental achievement.

But in all likelihood, it won’t have any technological or practical impact. It might; we have no idea what such a proof looks like. But it’s not likely.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/s8le2d6uat0cqb3o', 'title': 'What are the implications of proving P', 'score': {'original': 0.1537, 'ai': 0.8463}, 'blocks': [{'text': 'The sheer fact that it’s proven won’t affect the world as we know it. It’s widely assumed that P\n≠\n≠\nNP already, and knowing that it’s true won’t make any difference.\n\nThe proof itself will certainly introduce ideas and techniques that would have a deep impact on the field of theoretical computer science and computational complexity. It’s possible that such a proof may have broader impact on logic and mathematics. It will be a monumental achievement.\n\nBut in all likelihood, it won’t have any technological or practical impact. It might; we have no idea what such a proof looks like. But it’s not likely.', 'result': {'fake': 0.8463, 'real': 0.1537}, 'status': 'success'}], 'credits_used': 2, 'credits': 1985922, 'subscription': 0, 'content': 'The sheer fact that it’s proven won’t affect the world as we know it. It’s widely assumed that P\n≠\n≠\nNP already, and knowing that it’s true won’t make any difference.\n\nThe proof itself will certainly introduce ideas and techniques that would have a deep impact on the field of theoretical computer science and computational complexity. It’s possible that such a proof may have broader impact on logic and mathematics. It will be a monumental achievement.\n\nBut in all likelihood, it won’t have any technological or practical impact. It might; we have no idea what such a proof looks like. But it’s not likely.', 'aiModelVersion': '1'}",0.1537
Mark Harrison,2y,The symbol for the save button on computers got carried forward to this day long beyond the thing it's based on (floppy disk) becoming obsolete. What other concepts do we have today that are represented by things that are obsolete?,"Before you carry on, I’m going to assume that you have a cellphone to hand. It doesn’t even need to be a smartphone - your old Nokia will be fine :-)

Pick it up, imagine you are going to make a call.

Play close attention to the icon you press to make a call.

Does your phone have a separate mic/speaker device that looks even vaguely like that?

I’m talking about the green icon in the image attached…","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0raqhsdtku2wm38f', 'title': ""The symbol for the save button on computers got carried forward to this day long beyond the thing it's based on (floppy disk) becoming obsolete. What other concepts do we have today that are represented by things that are obsolete?"", 'score': {'original': 0.9985, 'ai': 0.0015}, 'blocks': [{'text': 'Before you carry on, I’m going to assume that you have a cellphone to hand. It doesn’t even need to be a smartphone - your old Nokia will be fine :-)\n\nPick it up, imagine you are going to make a call.\n\nPlay close attention to the icon you press to make a call.\n\nDoes your phone have a separate mic/speaker device that looks even vaguely like that?\n\nI’m talking about the green icon in the image attached…', 'result': {'fake': 0.0015, 'real': 0.9985}, 'status': 'success'}], 'credits_used': 1, 'credits': 1985921, 'subscription': 0, 'content': 'Before you carry on, I’m going to assume that you have a cellphone to hand. It doesn’t even need to be a smartphone - your old Nokia will be fine :-)\n\nPick it up, imagine you are going to make a call.\n\nPlay close attention to the icon you press to make a call.\n\nDoes your phone have a separate mic/speaker device that looks even vaguely like that?\n\nI’m talking about the green icon in the image attached…', 'aiModelVersion': '1'}",0.9985
Jan Janiczek,4y,Why do some software developers say they do not need to be in front of a computer to be working?,"Because much of the time is spent planning how exactly to implement a given feature, thinking about how it will interact with the rest of the codebase, considering which algorithm will have the best performance in this case, and so on.

The main reason is that computers are dumb, really dumb. They can be dumb extremely fast, but they are dumb nonetheless. The only reason your computer might not appear dumb to you is because dozens of thousands of programmers put immense amounts of work into making it so. In their natural state, computers are as dumb as a brick.

As a result, no matter how specific you think the requirements and specifications you gave your programmers for the new project are, they are certainly not specific enough for a computer. That’s because when non-technical people describe things they always do it with the unspoken assumption that the recipient of their descriptions is a human, i.e. a person with common sense and a certain minimum set of knowledge about the world. Computers don’t have common sense. As a result, much of the programmer’s job is in translating the business’s relatively vague requirements into extremely specific instructions for the computer.

This comic is a great illustration of the problem (source
):","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/73bea0uzcnhrpl5t', 'title': 'Why do some software developers say they do not need to be in front of a computer to be working?', 'score': {'original': 0.9757, 'ai': 0.0243}, 'blocks': [{'text': 'Because much of the time is spent planning how exactly to implement a given feature, thinking about how it will interact with the rest of the codebase, considering which algorithm will have the best performance in this case, and so on.\n\nThe main reason is that computers are dumb, really dumb. They can be dumb extremely fast, but they are dumb nonetheless. The only reason your computer might not appear dumb to you is because dozens of thousands of programmers put immense amounts of work into making it so. In their natural state, computers are as dumb as a brick.\n\nAs a result, no matter how specific you think the requirements and specifications you gave your programmers for the new project are, they are certainly not specific enough for a computer. That’s because when non-technical people describe things they always do it with the unspoken assumption that the recipient of their descriptions is a human, i.e. a person with common sense and a certain minimum set of knowledge about the world. Computers don’t have common sense. As a result, much of the programmer’s job is in translating the business’s relatively vague requirements into extremely specific instructions for the computer.\n\nThis comic is a great illustration of the problem (source\n):', 'result': {'fake': 0.0243, 'real': 0.9757}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985918, 'subscription': 0, 'content': 'Because much of the time is spent planning how exactly to implement a given feature, thinking about how it will interact with the rest of the codebase, considering which algorithm will have the best performance in this case, and so on.\n\nThe main reason is that computers are dumb, really dumb. They can be dumb extremely fast, but they are dumb nonetheless. The only reason your computer might not appear dumb to you is because dozens of thousands of programmers put immense amounts of work into making it so. In their natural state, computers are as dumb as a brick.\n\nAs a result, no matter how specific you think the requirements and specifications you gave your programmers for the new project are, they are certainly not specific enough for a computer. That’s because when non-technical people describe things they always do it with the unspoken assumption that the recipient of their descriptions is a human, i.e. a person with common sense and a certain minimum set of knowledge about the world. Computers don’t have common sense. As a result, much of the programmer’s job is in translating the business’s relatively vague requirements into extremely specific instructions for the computer.\n\nThis comic is a great illustration of the problem (source\n):', 'aiModelVersion': '1'}",0.9757
Fred Mitchell,Updated 4y,Why was the Amiga Computer a failure when spec wise it was way beyond anything else that was out at the time?,"Since I was there, and I saw this up-front and in the raw, I can tell you precisely why the Amiga failed.

Upper Management.

We engineers had big plans for the Amiga, like 24-bit graphics with a blitter per bitplane, a DSP, and other goodies that we dubbed with the code name: AAA.

We were working on this AAA chip set, which would’ve taken the Amiga to a whole new plane of existence for its time, but upper management smacked it down and ended development of that, and told us to do something “simpler”. And so we did — code name? AA.

Even the AA chipset didn’t suck too badly, and was still ahead of what was available at the time. But technology was never an issue for the Amiga. It was poor marketing decisions.

For example, Commodore once had a golden opportunity of getting the Amiga into Sears. Commodore “marketing” assed that up. Why? Sears wanted to list the Amiga in their catalogue, like they do all their products, but Commodore Marketing claimed it would make the Amiga look like “a toy”, and so forbade it.

As well, Sears didn’t need the Amiga as much as the Amiga needed Sears, and so it was dropped.

This is one of the biggest blunders Commodore made, in a long list of blunders that may have saved the company. Had the Amiga been introduced into Sears, that would’ve placed the Amiga in front of millions of people that never even heard of the Amiga before. It would’ve been there on display for them to play with and be amazed. And so the Amiga would’ve flown off the shelves. And Commodore would’ve lived to see another day.

Irving Gould drove Commodore into the ground. See: Irving Gould - The Money Man
.

I had a real passion for the Amiga, and it was the one thing that liberated me from Windows 3.11 and Intel tech of the time.

Right around the time Commodore closed their doors, Windows NT came out, and I jumped on that. I was hoping to get into Unix development, but the one drive I had Unix — AmigaUNIX! — on, died. Yes, We had ported SVR5 Unix to the Amiga, which was also killed by upper management. This was a bonehead move, because at the time, students needing Unix machines for college could afford the much-cheaper Amiga than the other options available at the time — and said students would’ve carried the Amiga right into the business arena once they graduated and move on — you know, the same thing that happened with Apple.

Oh, that’s another story, because Commodore had the educational market at one point with their Commodore PET computer. They almost literally handed that market to Apple, and we all know what happened after that.

It’s scary to think where Commodore would be today had it not made such bone-headed decisions. It would probably be where Apple is today, and Apple would’ve become the footnote in history.

I have fond memories of the Amiga and its impressive graphics. And today, I have the RTX 2070 in my desktop computer. Yes, it can do real-time Ray Tracing. Back in my Commodore Amiga days, ray tracing was all the rage — but it generally took an hour or more to render a single ray-traced still for the smallish monitors of the time — 640x400. Now I have this beast that can do it in real-time to a UHD / 4K display. Who knows? Maybe they would be where Nvidia is today. Who knows?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/tm3fop9ny74besk5', 'title': 'Why was the Amiga Computer a failure when spec wise it was way beyond anything else that was out at the time?', 'score': {'original': 0.98065, 'ai': 0.01935}, 'blocks': [{'text': 'Since I was there, and I saw this up-front and in the raw, I can tell you precisely why the Amiga failed.\n\nUpper Management.\n\nWe engineers had big plans for the Amiga, like 24-bit graphics with a blitter per bitplane, a DSP, and other goodies that we dubbed with the code name: AAA.\n\nWe were working on this AAA chip set, which would’ve taken the Amiga to a whole new plane of existence for its time, but upper management smacked it down and ended development of that, and told us to do something “simpler”. And so we did — code name? AA.\n\nEven the AA chipset didn’t suck too badly, and was still ahead of what was available at the time. But technology was never an issue for the Amiga. It was poor marketing decisions.\n\nFor example, Commodore once had a golden opportunity of getting the Amiga into Sears. Commodore “marketing” assed that up. Why? Sears wanted to list the Amiga in their catalogue, like they do all their products, but Commodore Marketing claimed it would make the Amiga look like “a toy”, and so forbade it.\n\nAs well, Sears didn’t need the Amiga as much as the Amiga needed Sears, and so it was dropped.\n\nThis is one of the biggest blunders Commodore made, in a long list of blunders that may have saved the company. Had the Amiga been introduced into Sears, that would’ve placed the Amiga in front of millions of people that never even heard of the Amiga before. It would’ve been there on display for them to play with and be amazed. And so the Amiga would’ve flown off the shelves. And Commodore would’ve lived to see another day.\n\nIrving Gould drove Commodore into the ground. See: Irving Gould - The Money Man\n.\n\nI had a real passion for the Amiga, and it was the one thing that liberated me from Windows 3.11 and Intel tech of the time.\n\nRight around the time Commodore closed their doors, Windows NT came out, and I jumped on that. I was hoping to get into Unix development, but the one drive I had Unix — AmigaUNIX! — on, died. Yes, We had ported SVR5 Unix to the Amiga, which was also killed by upper management. This was a bonehead move, because at the time, students needing Unix machines for college could afford the much-cheaper Amiga than the other options available at the time — and said students would’ve carried the Amiga right into the business arena once they graduated and move on — you know, the same thing that happened with Apple.\n\nOh, that’s another story, because Commodore had the educational market at one point with their Commodore PET computer. They almost literally handed that market to Apple, and we all know what happened after that.\n\nIt’s scary to think where Commodore would be today had it not made such bone-headed decisions. It would probably be where Apple is today, and Apple would’ve become the footnote in history.\n\nI have fond memories of the Amiga and its impressive graphics. And today, I have the RTX 2070 in my desktop computer. Yes, it can', 'result': {'fake': 0.0105, 'real': 0.9895}, 'status': 'success'}, {'text': 'do real-time Ray Tracing. Back in my Commodore Amiga days, ray tracing was all the rage — but it generally took an hour or more to render a single ray-traced still for the smallish monitors of the time — 640x400. Now I have this beast that can do it in real-time to a UHD / 4K display. Who knows? Maybe they would be where Nvidia is today. Who knows?', 'result': {'fake': 0.2166, 'real': 0.7834}, 'status': 'success'}], 'credits_used': 6, 'credits': 1985912, 'subscription': 0, 'content': 'Since I was there, and I saw this up-front and in the raw, I can tell you precisely why the Amiga failed.\n\nUpper Management.\n\nWe engineers had big plans for the Amiga, like 24-bit graphics with a blitter per bitplane, a DSP, and other goodies that we dubbed with the code name: AAA.\n\nWe were working on this AAA chip set, which would’ve taken the Amiga to a whole new plane of existence for its time, but upper management smacked it down and ended development of that, and told us to do something “simpler”. And so we did — code name? AA.\n\nEven the AA chipset didn’t suck too badly, and was still ahead of what was available at the time. But technology was never an issue for the Amiga. It was poor marketing decisions.\n\nFor example, Commodore once had a golden opportunity of getting the Amiga into Sears. Commodore “marketing” assed that up. Why? Sears wanted to list the Amiga in their catalogue, like they do all their products, but Commodore Marketing claimed it would make the Amiga look like “a toy”, and so forbade it.\n\nAs well, Sears didn’t need the Amiga as much as the Amiga needed Sears, and so it was dropped.\n\nThis is one of the biggest blunders Commodore made, in a long list of blunders that may have saved the company. Had the Amiga been introduced into Sears, that would’ve placed the Amiga in front of millions of people that never even heard of the Amiga before. It would’ve been there on display for them to play with and be amazed. And so the Amiga would’ve flown off the shelves. And Commodore would’ve lived to see another day.\n\nIrving Gould drove Commodore into the ground. See: Irving Gould - The Money Man\n.\n\nI had a real passion for the Amiga, and it was the one thing that liberated me from Windows 3.11 and Intel tech of the time.\n\nRight around the time Commodore closed their doors, Windows NT came out, and I jumped on that. I was hoping to get into Unix development, but the one drive I had Unix — AmigaUNIX! — on, died. Yes, We had ported SVR5 Unix to the Amiga, which was also killed by upper management. This was a bonehead move, because at the time, students needing Unix machines for college could afford the much-cheaper Amiga than the other options available at the time — and said students would’ve carried the Amiga right into the business arena once they graduated and move on — you know, the same thing that happened with Apple.\n\nOh, that’s another story, because Commodore had the educational market at one point with their Commodore PET computer. They almost literally handed that market to Apple, and we all know what happened after that.\n\nIt’s scary to think where Commodore would be today had it not made such bone-headed decisions. It would probably be where Apple is today, and Apple would’ve become the footnote in history.\n\nI have fond memories of the Amiga and its impressive graphics. And today, I have the RTX 2070 in my desktop computer. Yes, it can do real-time Ray Tracing. Back in my Commodore Amiga days, ray tracing was all the rage — but it generally took an hour or more to render a single ray-traced still for the smallish monitors of the time — 640x400. Now I have this beast that can do it in real-time to a UHD / 4K display. Who knows? Maybe they would be where Nvidia is today. Who knows?', 'aiModelVersion': '1'}",0.98065
Steve Baker,2y,How do programmers nerf the AI in video games so that the AI is not too smart to defeat the player?,"The first thing to understand is that the meaning of the term “AI” in the gaming industry is NOT the same meaning as in the rest of the world.

If you’re Tesla or Google or whatever, then “AI” means “Artificial Intelligence” - and we’re looking at a computer uses neural network techniques to drive a car or play Go or Chess far better than a human can.

If you’re a video game company, then “AI” still stands for “Artificial Intelligence” - but it’s almost certainly not using neural network techniques and it is in no way going to be comparable to human abilities.

Most often it’s a set of states with rules governing how each “AI” character changes between states - and software to make it do something in each state.

There are a few of reasons for doing this:

A neural network approach is too computationally expensive when there are tens to hundreds of AI characters.
It’s too hard for the game design team to script a particular situation for the player to solve when each AI character is truly thinking for itself.
It’s too much work to train a neural network to behave how you want it to.

With the typical game approach to this, you have simple variables that the game design team can adjust - so, the probability of the AI character hitting the player when they fire at you. How long they’ll stay in cover before they pop up and shoot at you…maybe how close you have to be before they do.

There will be (perhaps) a dozen adjustable properties that relate to the AI’s skill and behavior.

So if the game wants the player to do some cinematic feat like running 20 meters across an open warehouse floor with bullets bouncing all around them - then the game designers will set the hit probability of the AI players to somewhere near zero for a moving target and a lot higher for a stationary target. This forces the player to play in a “cinematic” style.

But it might be that you then want to pin the player down in one corner of the warehouse so that some pre-scripted event happens. So you program your usually inept AI’s to have pinpoint accuracy when firing at that particular corner of the room.

These AI’s are not independently thinking beings. They are “extras” on a movie set being directed to make our hero (you, the player) look good on set!

This is the reason why playing multiplayer games is such a different experience to playing single player games.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/qe4ogh9sx2fapw5m', 'title': 'How do programmers nerf the AI in video games so that the AI is not too smart to defeat the player?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'The first thing to understand is that the meaning of the term “AI” in the gaming industry is NOT the same meaning as in the rest of the world.\n\nIf you’re Tesla or Google or whatever, then “AI” means “Artificial Intelligence” - and we’re looking at a computer uses neural network techniques to drive a car or play Go or Chess far better than a human can.\n\nIf you’re a video game company, then “AI” still stands for “Artificial Intelligence” - but it’s almost certainly not using neural network techniques and it is in no way going to be comparable to human abilities.\n\nMost often it’s a set of states with rules governing how each “AI” character changes between states - and software to make it do something in each state.\n\nThere are a few of reasons for doing this:\n\nA neural network approach is too computationally expensive when there are tens to hundreds of AI characters.\nIt’s too hard for the game design team to script a particular situation for the player to solve when each AI character is truly thinking for itself.\nIt’s too much work to train a neural network to behave how you want it to.\n\nWith the typical game approach to this, you have simple variables that the game design team can adjust - so, the probability of the AI character hitting the player when they fire at you. How long they’ll stay in cover before they pop up and shoot at you…maybe how close you have to be before they do.\n\nThere will be (perhaps) a dozen adjustable properties that relate to the AI’s skill and behavior.\n\nSo if the game wants the player to do some cinematic feat like running 20 meters across an open warehouse floor with bullets bouncing all around them - then the game designers will set the hit probability of the AI players to somewhere near zero for a moving target and a lot higher for a stationary target. This forces the player to play in a “cinematic” style.\n\nBut it might be that you then want to pin the player down in one corner of the warehouse so that some pre-scripted event happens. So you program your usually inept AI’s to have pinpoint accuracy when firing at that particular corner of the room.\n\nThese AI’s are not independently thinking beings. They are “extras” on a movie set being directed to make our hero (you, the player) look good on set!\n\nThis is the reason why playing multiplayer games is such a different experience to playing single player games.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1985907, 'subscription': 0, 'content': 'The first thing to understand is that the meaning of the term “AI” in the gaming industry is NOT the same meaning as in the rest of the world.\n\nIf you’re Tesla or Google or whatever, then “AI” means “Artificial Intelligence” - and we’re looking at a computer uses neural network techniques to drive a car or play Go or Chess far better than a human can.\n\nIf you’re a video game company, then “AI” still stands for “Artificial Intelligence” - but it’s almost certainly not using neural network techniques and it is in no way going to be comparable to human abilities.\n\nMost often it’s a set of states with rules governing how each “AI” character changes between states - and software to make it do something in each state.\n\nThere are a few of reasons for doing this:\n\nA neural network approach is too computationally expensive when there are tens to hundreds of AI characters.\nIt’s too hard for the game design team to script a particular situation for the player to solve when each AI character is truly thinking for itself.\nIt’s too much work to train a neural network to behave how you want it to.\n\nWith the typical game approach to this, you have simple variables that the game design team can adjust - so, the probability of the AI character hitting the player when they fire at you. How long they’ll stay in cover before they pop up and shoot at you…maybe how close you have to be before they do.\n\nThere will be (perhaps) a dozen adjustable properties that relate to the AI’s skill and behavior.\n\nSo if the game wants the player to do some cinematic feat like running 20 meters across an open warehouse floor with bullets bouncing all around them - then the game designers will set the hit probability of the AI players to somewhere near zero for a moving target and a lot higher for a stationary target. This forces the player to play in a “cinematic” style.\n\nBut it might be that you then want to pin the player down in one corner of the warehouse so that some pre-scripted event happens. So you program your usually inept AI’s to have pinpoint accuracy when firing at that particular corner of the room.\n\nThese AI’s are not independently thinking beings. They are “extras” on a movie set being directed to make our hero (you, the player) look good on set!\n\nThis is the reason why playing multiplayer games is such a different experience to playing single player games.', 'aiModelVersion': '1'}",0.9998
Wes Winn,4y,Computer Programmers: What do you think about this Atlantic Monthly story about a computer programmer who automated his job and was subsequently fired? Do you think this is fair?,"His job was not a Software Developer.

We absolutely must start there. The person in the article was a Quality Assurance employee, and his leadership team are the ones who should’ve been fired, which I’ll get to.

A programmer is not anywhere close to automating the job of a programmer. In software development, we’re solving different problems that can be differently interpreted in different ways.

This QA person’s job was to manually do the exact same thing, over and over, and report any inconsistencies in the response of doing said thing. E.g. Click blue button, if blue button turns green, exit, else report bug that blue button did not turn green.

Automation is what we want.

Obviously tests get a little more than that, but automation is a huge buzzword right now because companies realize that doing the same thing over and over is an absolute waste of time.

It’s not that we want to remove the person from the job, it’s that we want that person doing things that are more valuable. Industry-wide, we’re seeing a much bigger push for SDETs, building better testing and automation and maintaining it because you get more done faster, with a higher level of quality while having the same head count.

The employee wasn’t fired because there was no longer a need to have him, he was fired for the pretense that he was working, when he actually was not. The role of manual testing is still necessary, but, as in this case, much of that work can and should be automated.

This is the fault of leadership.

If you have an employee under you that is capable of automating tasks, or setting up automation suites, that’s exactly what you should have them doing. It can be just spend 10% of your time trying to sort out an automation suite, but anything you can do is great.

Had this guy’s manager seen his potential and given him ownership to start this automation, he could’ve spent those next 5 years spreading automation to other places, creating better logging, disseminating automation techniques through the company, etc.

The employee himself would’ve gone from QA to SDET, realized a great salary increase, and opened doors to a lot of future growth under the right leadership.Obviously he had enough interest in doing this type of thing since he just did it anyways, and once done, he got bored.

Should he have been fired?

This is a difficult one. If someone is dishonestly collecting a check for 5 years, yeah, they should probably be terminated.

On the other hand, if you fire the leadership, and bring in people who encourage this move to better automation, you already have someone who is an authority in automation with your software working for you.

It seems to me that the team is likely not fostering growth and innovation, and so it probably is a bad fit for the individual anyways, but this was just a huge miss for all parties involved.

I would happily hire that guy in a heartbeat

I’d also make sure that he had the encouragement, leadership involvement, and empowerment to go and massively improve automation throughout the team, and make sure that there was teamwork in there, and everyone was kept interested.

Of course, there would be a strong commitment that he wouldn’t screw off for 5 years without telling me he didn’t have anything to work on.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/9p4omxac0e2fr36s', 'title': 'Computer Programmers: What do you think about this Atlantic Monthly story about a computer programmer who automated his job and was subsequently fired? Do you think this is fair?', 'score': {'original': 0.9045, 'ai': 0.0955}, 'blocks': [{'text': 'His job was not a Software Developer.\n\nWe absolutely must start there. The person in the article was a Quality Assurance employee, and his leadership team are the ones who should’ve been fired, which I’ll get to.\n\nA programmer is not anywhere close to automating the job of a programmer. In software development, we’re solving different problems that can be differently interpreted in different ways.\n\nThis QA person’s job was to manually do the exact same thing, over and over, and report any inconsistencies in the response of doing said thing. E.g. Click blue button, if blue button turns green, exit, else report bug that blue button did not turn green.\n\nAutomation is what we want.\n\nObviously tests get a little more than that, but automation is a huge buzzword right now because companies realize that doing the same thing over and over is an absolute waste of time.\n\nIt’s not that we want to remove the person from the job, it’s that we want that person doing things that are more valuable. Industry-wide, we’re seeing a much bigger push for SDETs, building better testing and automation and maintaining it because you get more done faster, with a higher level of quality while having the same head count.\n\nThe employee wasn’t fired because there was no longer a need to have him, he was fired for the pretense that he was working, when he actually was not. The role of manual testing is still necessary, but, as in this case, much of that work can and should be automated.\n\nThis is the fault of leadership.\n\nIf you have an employee under you that is capable of automating tasks, or setting up automation suites, that’s exactly what you should have them doing. It can be just spend 10% of your time trying to sort out an automation suite, but anything you can do is great.\n\nHad this guy’s manager seen his potential and given him ownership to start this automation, he could’ve spent those next 5 years spreading automation to other places, creating better logging, disseminating automation techniques through the company, etc.\n\nThe employee himself would’ve gone from QA to SDET, realized a great salary increase, and opened doors to a lot of future growth under the right leadership.Obviously he had enough interest in doing this type of thing since he just did it anyways, and once done, he got bored.\n\nShould he have been fired?\n\nThis is a difficult one. If someone is dishonestly collecting a check for 5 years, yeah, they should probably be terminated.\n\nOn the other hand, if you fire the leadership, and bring in people who encourage this move to better automation, you already have someone who is an authority in automation with your software working for you.\n\nIt seems to me that the team is likely not fostering growth and innovation, and so it probably is a bad fit for the individual anyways, but this was just a huge miss for all parties involved.\n\nI would happily hire that guy in a heartbeat\n\nI’d also make sure that he had the encouragement, leadership involvement, and empowerment to go and massively improve automation throughout the', 'result': {'fake': 0.2619, 'real': 0.7381}, 'status': 'success'}, {'text': 'team, and make sure that there was teamwork in there, and everyone was kept interested.\n\nOf course, there would be a strong commitment that he wouldn’t screw off for 5 years without telling me he didn’t have anything to work on.', 'result': {'fake': 0.8805, 'real': 0.1195}, 'status': 'success'}], 'credits_used': 6, 'credits': 1985901, 'subscription': 0, 'content': 'His job was not a Software Developer.\n\nWe absolutely must start there. The person in the article was a Quality Assurance employee, and his leadership team are the ones who should’ve been fired, which I’ll get to.\n\nA programmer is not anywhere close to automating the job of a programmer. In software development, we’re solving different problems that can be differently interpreted in different ways.\n\nThis QA person’s job was to manually do the exact same thing, over and over, and report any inconsistencies in the response of doing said thing. E.g. Click blue button, if blue button turns green, exit, else report bug that blue button did not turn green.\n\nAutomation is what we want.\n\nObviously tests get a little more than that, but automation is a huge buzzword right now because companies realize that doing the same thing over and over is an absolute waste of time.\n\nIt’s not that we want to remove the person from the job, it’s that we want that person doing things that are more valuable. Industry-wide, we’re seeing a much bigger push for SDETs, building better testing and automation and maintaining it because you get more done faster, with a higher level of quality while having the same head count.\n\nThe employee wasn’t fired because there was no longer a need to have him, he was fired for the pretense that he was working, when he actually was not. The role of manual testing is still necessary, but, as in this case, much of that work can and should be automated.\n\nThis is the fault of leadership.\n\nIf you have an employee under you that is capable of automating tasks, or setting up automation suites, that’s exactly what you should have them doing. It can be just spend 10% of your time trying to sort out an automation suite, but anything you can do is great.\n\nHad this guy’s manager seen his potential and given him ownership to start this automation, he could’ve spent those next 5 years spreading automation to other places, creating better logging, disseminating automation techniques through the company, etc.\n\nThe employee himself would’ve gone from QA to SDET, realized a great salary increase, and opened doors to a lot of future growth under the right leadership.Obviously he had enough interest in doing this type of thing since he just did it anyways, and once done, he got bored.\n\nShould he have been fired?\n\nThis is a difficult one. If someone is dishonestly collecting a check for 5 years, yeah, they should probably be terminated.\n\nOn the other hand, if you fire the leadership, and bring in people who encourage this move to better automation, you already have someone who is an authority in automation with your software working for you.\n\nIt seems to me that the team is likely not fostering growth and innovation, and so it probably is a bad fit for the individual anyways, but this was just a huge miss for all parties involved.\n\nI would happily hire that guy in a heartbeat\n\nI’d also make sure that he had the encouragement, leadership involvement, and empowerment to go and massively improve automation throughout the team, and make sure that there was teamwork in there, and everyone was kept interested.\n\nOf course, there would be a strong commitment that he wouldn’t screw off for 5 years without telling me he didn’t have anything to work on.', 'aiModelVersion': '1'}",0.9045
Daniel Rolfe,4y,"Programmers, what's the funniest outsider's view of your job you've ever heard?","Many moons ago my ex wife was in the room while I was working on developing some new capability of a C++ application.

I had many browser windows open with various sites displayed showing sample code, documentation, other people’s approaches etc.

She gasped and said, “Dan, what are you doing?”.

I was confused and told her I was working on some new functionality.

“But aren’t you cheating?” she asked, wide eyed.

“Eh?” was my even more confused response.

“You’re looking at those websites, shouldn’t you figure it out yourself?”

My jaw dropped.

I mean where do you begin? She’s a smart woman but at that point in her life the idea of leveraging other people’s work in such a way was foreign.

“Well, it’s documentation, some people have done it certain ways and many people share those ways and that way everyone wins… in a way… like it’s just the way it’s done.”

“Hmmmhmmm… cheating” she said.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/3pzd0i2n5rsefk7w', 'title': ""Programmers, what's the funniest outsider's view of your job you've ever heard?"", 'score': {'original': 0.9994, 'ai': 0.0006}, 'blocks': [{'text': 'Many moons ago my ex wife was in the room while I was working on developing some new capability of a C++ application.\n\nI had many browser windows open with various sites displayed showing sample code, documentation, other people’s approaches etc.\n\nShe gasped and said, “Dan, what are you doing?”.\n\nI was confused and told her I was working on some new functionality.\n\n“But aren’t you cheating?” she asked, wide eyed.\n\n“Eh?” was my even more confused response.\n\n“You’re looking at those websites, shouldn’t you figure it out yourself?”\n\nMy jaw dropped.\n\nI mean where do you begin? She’s a smart woman but at that point in her life the idea of leveraging other people’s work in such a way was foreign.\n\n“Well, it’s documentation, some people have done it certain ways and many people share those ways and that way everyone wins… in a way… like it’s just the way it’s done.”\n\n“Hmmmhmmm… cheating” she said.', 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}], 'credits_used': 2, 'credits': 1985899, 'subscription': 0, 'content': 'Many moons ago my ex wife was in the room while I was working on developing some new capability of a C++ application.\n\nI had many browser windows open with various sites displayed showing sample code, documentation, other people’s approaches etc.\n\nShe gasped and said, “Dan, what are you doing?”.\n\nI was confused and told her I was working on some new functionality.\n\n“But aren’t you cheating?” she asked, wide eyed.\n\n“Eh?” was my even more confused response.\n\n“You’re looking at those websites, shouldn’t you figure it out yourself?”\n\nMy jaw dropped.\n\nI mean where do you begin? She’s a smart woman but at that point in her life the idea of leveraging other people’s work in such a way was foreign.\n\n“Well, it’s documentation, some people have done it certain ways and many people share those ways and that way everyone wins… in a way… like it’s just the way it’s done.”\n\n“Hmmmhmmm… cheating” she said.', 'aiModelVersion': '1'}",0.9994
Tom Crosley,Updated 3y,Which logic gates can build other logic gates?,"NAND gates are known as universal gates, because you can make any other gates out of them. You can make a NOT, AND, OR, NOR, XOR, and XNOR just using NANDs:

NOR gates are also universal, and you can fabricate NOT, AND, NAND, OR, XOR, and XNOR from them in a similar fashion.

The Apollo Guidance Computer
 was the first computer to use integrated circuits (ICs). Because they were so new, the designers chose to use just one type of device — an IC containing a single three-input NOR gate. The computer had 4,100 of them. The later Block II version (used in the crewed flights) used 2,800 ICs, each with dual three-input NOR gates. In each case, all of the logic in the computer was constructed of NOR gates. They could have also used all NAND gates instead.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/sxtp9f745c36auwk', 'title': 'Which logic gates can build other logic gates?', 'score': {'original': 0.9792, 'ai': 0.0208}, 'blocks': [{'text': 'NAND gates are known as universal gates, because you can make any other gates out of them. You can make a NOT, AND, OR, NOR, XOR, and XNOR just using NANDs:\n\nNOR gates are also universal, and you can fabricate NOT, AND, NAND, OR, XOR, and XNOR from them in a similar fashion.\n\nThe Apollo Guidance Computer\n was the first computer to use integrated circuits (ICs). Because they were so new, the designers chose to use just one type of device — an IC containing a single three-input NOR gate. The computer had 4,100 of them. The later Block II version (used in the crewed flights) used 2,800 ICs, each with dual three-input NOR gates. In each case, all of the logic in the computer was constructed of NOR gates. They could have also used all NAND gates instead.', 'result': {'fake': 0.0208, 'real': 0.9792}, 'status': 'success'}], 'credits_used': 2, 'credits': 1985897, 'subscription': 0, 'content': 'NAND gates are known as universal gates, because you can make any other gates out of them. You can make a NOT, AND, OR, NOR, XOR, and XNOR just using NANDs:\n\nNOR gates are also universal, and you can fabricate NOT, AND, NAND, OR, XOR, and XNOR from them in a similar fashion.\n\nThe Apollo Guidance Computer\n was the first computer to use integrated circuits (ICs). Because they were so new, the designers chose to use just one type of device — an IC containing a single three-input NOR gate. The computer had 4,100 of them. The later Block II version (used in the crewed flights) used 2,800 ICs, each with dual three-input NOR gates. In each case, all of the logic in the computer was constructed of NOR gates. They could have also used all NAND gates instead.', 'aiModelVersion': '1'}",0.9792
Scott King Walker,Updated 1y,How does a computer turn on with a press of a button if it's supposed to be completely powered down? How does it get a signal or something that tells it to turn on?,"Back in the “good old days”, personal computers had a big switch directly attached to the power supply component that closed with a satisfying “thunk”. When those computers were off, they were 100% off. (Except for a battery powered clock and CMOS memory.) It also meant that a user could turn them off in the middle of something, and if it was shut off during a write operation to magnetic media, it could possibly corrupt the write in a bad way.

Somewhere along the way, manufacturers realized that things would be more reliable if we gave computer users an easier way to shut the system down gracefully. They changed out the “big physical switch” with a “soft switch”. That is how modern operating systems have a “shut down” option in their menus that can power down the computer. Before that, you would tell the OS to shut down, and then once it had shut down it would tell you that it was OK to power off the computer, and then you threw the switch to off. So now, there is a push button switch that signals the power supply that you want to turn on or shut down.

The “soft switch” also makes it possible for computers to implement “Wake On LAN”, which is a feature where a signal transmitted to the wired Ethernet port can wake the computer up. I don’t know all of the details of how it works, but if this feature is active, then at least some power is going to part of the Ethernet circuit to enable this feature.

But your question was HOW. The main computer itself is “completely powered down”, but the power supply component is not. It keeps drawing just enough power to watch for the signal that it is supposed to turn on. Some people refer to these as “energy vampires”.

An energy vampire is a device that continues to use energy and drain power, even when it is turned off. They lurk in your home, taking the form of phone chargers and cable boxes, computer cords and coffee pots. These phantom energy suckers can account for as much as 20% of your monthly electricity bill.

From: Energy Vampire - Electricity Savings - Duke Energy

This also means that you sometimes need a way to power off a computer that is in too confused a state to power itself off. If you didn’t know it, most of these soft switches will override the normal shutdown if you hold them down for 10–15 seconds. Don’t do this unless your computer is completely and totally unresponsive. And, if somehow that doesn’t do it, then and only then should you unplug it. (For completely unresponsive laptop computers, you may also need to disconnect the battery. This is pretty easy if the battery is easily removable. For others, you need to use a screwdriver to remove a panel, and unplug a cable while the computer is still “on”. Ugh!)

These days, most appliances have electronics in them, and most of them consume power all the time so that you can turn them on and off with a “soft switch”. This is also what lets you turn your TV and associated electronics on with a remote. It is a bit absurd to think that you would unplug all of these devices when not in use. To unplug my stove, for instance, I would need to pull it out from the wall and wrestle a large, high voltage connector out of a very stiff outlet in the wall. And while standard plugs are designed to be connected and disconnected repeatedly, you shouldn’t be doing this to everything, every day. You will wear your outlets out.

If you really want to eliminate this “vampire” power usage, you should use connect the devices in question to a power strip with a physical on/off switch. Let the device shut down properly, and then turn it off at the switch.

Thanks to Steven J Greenfield’s answer to Which is on and which is off in the I/O switches? for the images.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/t8o4bcgrzd3e92y5', 'title': ""How does a computer turn on with a press of a button if it's supposed to be completely powered down? How does it get a signal or something that tells it to turn on?"", 'score': {'original': 0.7786, 'ai': 0.2214}, 'blocks': [{'text': 'Back in the “good old days”, personal computers had a big switch directly attached to the power supply component that closed with a satisfying “thunk”. When those computers were off, they were 100% off. (Except for a battery powered clock and CMOS memory.) It also meant that a user could turn them off in the middle of something, and if it was shut off during a write operation to magnetic media, it could possibly corrupt the write in a bad way.\n\nSomewhere along the way, manufacturers realized that things would be more reliable if we gave computer users an easier way to shut the system down gracefully. They changed out the “big physical switch” with a “soft switch”. That is how modern operating systems have a “shut down” option in their menus that can power down the computer. Before that, you would tell the OS to shut down, and then once it had shut down it would tell you that it was OK to power off the computer, and then you threw the switch to off. So now, there is a push button switch that signals the power supply that you want to turn on or shut down.\n\nThe “soft switch” also makes it possible for computers to implement “Wake On LAN”, which is a feature where a signal transmitted to the wired Ethernet port can wake the computer up. I don’t know all of the details of how it works, but if this feature is active, then at least some power is going to part of the Ethernet circuit to enable this feature.\n\nBut your question was HOW. The main computer itself is “completely powered down”, but the power supply component is not. It keeps drawing just enough power to watch for the signal that it is supposed to turn on. Some people refer to these as “energy vampires”.\n\nAn energy vampire is a device that continues to use energy and drain power, even when it is turned off. They lurk in your home, taking the form of phone chargers and cable boxes, computer cords and coffee pots. These phantom energy suckers can account for as much as 20% of your monthly electricity bill.\n\nFrom: Energy Vampire - Electricity Savings - Duke Energy\n\nThis also means that you sometimes need a way to power off a computer that is in too confused a state to power itself off. If you didn’t know it, most of these soft switches will override the normal shutdown if you hold them down for 10–15 seconds. Don’t do this unless your computer is completely and totally unresponsive. And, if somehow that doesn’t do it, then and only then should you unplug it. (For completely unresponsive laptop computers, you may also need to disconnect the battery. This is pretty easy if the battery is easily removable. For others, you need to use a screwdriver to remove a panel, and unplug a cable while the computer is still “on”. Ugh!)\n\nThese days, most appliances have electronics in them, and most of them consume power all the time so that you can turn them on and', 'result': {'fake': 0.0202, 'real': 0.9798}, 'status': 'success'}, {'text': 'off with a “soft switch”. This is also what lets you turn your TV and associated electronics on with a remote. It is a bit absurd to think that you would unplug all of these devices when not in use. To unplug my stove, for instance, I would need to pull it out from the wall and wrestle a large, high voltage connector out of a very stiff outlet in the wall. And while standard plugs are designed to be connected and disconnected repeatedly, you shouldn’t be doing this to everything, every day. You will wear your outlets out.\n\nIf you really want to eliminate this “vampire” power usage, you should use connect the devices in question to a power strip with a physical on/off switch. Let the device shut down properly, and then turn it off at the switch.\n\nThanks to Steven J Greenfield’s answer to Which is on and which is off in the I/O switches? for the images.', 'result': {'fake': 0.5361, 'real': 0.4639}, 'status': 'success'}], 'credits_used': 7, 'credits': 1985890, 'subscription': 0, 'content': 'Back in the “good old days”, personal computers had a big switch directly attached to the power supply component that closed with a satisfying “thunk”. When those computers were off, they were 100% off. (Except for a battery powered clock and CMOS memory.) It also meant that a user could turn them off in the middle of something, and if it was shut off during a write operation to magnetic media, it could possibly corrupt the write in a bad way.\n\nSomewhere along the way, manufacturers realized that things would be more reliable if we gave computer users an easier way to shut the system down gracefully. They changed out the “big physical switch” with a “soft switch”. That is how modern operating systems have a “shut down” option in their menus that can power down the computer. Before that, you would tell the OS to shut down, and then once it had shut down it would tell you that it was OK to power off the computer, and then you threw the switch to off. So now, there is a push button switch that signals the power supply that you want to turn on or shut down.\n\nThe “soft switch” also makes it possible for computers to implement “Wake On LAN”, which is a feature where a signal transmitted to the wired Ethernet port can wake the computer up. I don’t know all of the details of how it works, but if this feature is active, then at least some power is going to part of the Ethernet circuit to enable this feature.\n\nBut your question was HOW. The main computer itself is “completely powered down”, but the power supply component is not. It keeps drawing just enough power to watch for the signal that it is supposed to turn on. Some people refer to these as “energy vampires”.\n\nAn energy vampire is a device that continues to use energy and drain power, even when it is turned off. They lurk in your home, taking the form of phone chargers and cable boxes, computer cords and coffee pots. These phantom energy suckers can account for as much as 20% of your monthly electricity bill.\n\nFrom: Energy Vampire - Electricity Savings - Duke Energy\n\nThis also means that you sometimes need a way to power off a computer that is in too confused a state to power itself off. If you didn’t know it, most of these soft switches will override the normal shutdown if you hold them down for 10–15 seconds. Don’t do this unless your computer is completely and totally unresponsive. And, if somehow that doesn’t do it, then and only then should you unplug it. (For completely unresponsive laptop computers, you may also need to disconnect the battery. This is pretty easy if the battery is easily removable. For others, you need to use a screwdriver to remove a panel, and unplug a cable while the computer is still “on”. Ugh!)\n\nThese days, most appliances have electronics in them, and most of them consume power all the time so that you can turn them on and off with a “soft switch”. This is also what lets you turn your TV and associated electronics on with a remote. It is a bit absurd to think that you would unplug all of these devices when not in use. To unplug my stove, for instance, I would need to pull it out from the wall and wrestle a large, high voltage connector out of a very stiff outlet in the wall. And while standard plugs are designed to be connected and disconnected repeatedly, you shouldn’t be doing this to everything, every day. You will wear your outlets out.\n\nIf you really want to eliminate this “vampire” power usage, you should use connect the devices in question to a power strip with a physical on/off switch. Let the device shut down properly, and then turn it off at the switch.\n\nThanks to Steven J Greenfield’s answer to Which is on and which is off in the I/O switches? for the images.', 'aiModelVersion': '1'}",0.7786
Mark Phaedrus,Updated 4y,How can an AI train itself if no one is telling it if its answer is correct or wrong?,"“How can an AI train itself if no one is telling it if its answer is correct or wrong?”

This is a great question.

In order to learn how to solve a problem correctly, the AI has to be told how to recognize a “correct” solution to the problem. In order to learn how to better solve a problem that has no one correct answer, the AI has to be told how to recognize when it’s doing “better”.

So at first it seems like the answer to your question is “It can’t.”

But the nuance is in what we mean by “has to be told”. There doesn’t have to be a human trainer there saying “this is right” or “this is better”. The thing giving the AI its feedback on “right” or “better” can be just another computer program.

The most basic example of this is a “maximizer AI”. A human writes a computer program that can look at the current state of the problem and assign it a point score. The higher the score, the better the current state of the problem is. Now the AI can rapidly try different approaches to the problem without needing a human trainer to tell it the results of each attempt. The feedback is immediate and automatic: if approach A produces a higher score than approach B, then the AI knows that approach A is better.

The most classic example of this approach is the typical computer chess program. But that’s been discussed to death. So let me give you a less common and perhaps more fun example: video games.

Specifically, old Atari 2600 video games back from the dawn of game consoles.

There are two useful things about those games from a modern perspective:

1. Simple controls.

The controller used for most Atari games had a very simple eight-direction joystick and one button. That’s it. No analog controls, no complicated six-button control schemes. That means that there’s a very small number of possible actions that a player can take at any given moment.

2. Immediate scoring feedback.

The games typically displayed a score, and the object of the game was to get the highest score possible. What’s more, points were typically scored for very simple actions (like “eat that dot - 1 point”), rather than complex multi-step actions (like “Make your way across this level, avoid getting shot, find the flag at the center of the level, pick it up, and bring it back to your base — 1 point”). That means that the training data the AI needs is quite literally there on the screen. If you do something that makes the score go up, that’s good. If you do something that makes the score go up faster, that’s better.

Starting from Scratch

An AI researcher decided to take advantage of these facts by running an experiment in which an AI learned how to play the games. Note that I didn’t say “in which the researcher taught the AI how to play the games.” The AI started with absolutely no knowledge of the rules of the games. Its baseline knowledge was essentially “Here is a simplified view of what’s on the screen at this moment; here are the possible control signals you can give; and here’s the score. High scores are better. Go.”

This led to a wonderful paper, “Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari
”. It’s a fun read if you can get past the terminology. But let me pick out my favorite example, Q*bert, which showed that this approach can lead to strategies that a human trainer would never have thought of.

Q*bert is a game that originally came out in arcades in 1982. The player controls a little aardvark-looking character that must hop around a pyramid of cubes to change their colors. A few enemies pursue the character and sometimes mess up the colors, requiring the player to fix them. The arcade version looks like this:

The Atari 2600 version… leaves a lot more to the imagination:

But the experiment was about AI gameplay, not art appreciation. So they hooked the AI up to the controls, let it see the screen, told it that high scores are good, and let it loose.

The researchers’ expectation was that the AI would learn to play Q*bert the way a human would — try to find some reasonably-efficient way of covering the board while dodging the bad guys. But two of the test runs went in particularly interesting directions.

Free-fallin’

In the first interesting run, the AI finds a weakness in the way the 2600 version scores the game and awards extra lives (which is different from the arcade version). The AI positions Q*bert on a cube near the edge of the board, waits for the smartest enemy to approach, and then deliberately sends Q*bert to his death by leaping off the board. But the enemy leaps off the board as well in pursuit, and the player gets 500 points for killing that enemy — enough for an extra life. So the AI can simply repeat this indefinitely, methodically and endlessly scoring points.

Breaking the Game

In the second interesting run, the AI finds an even more bizarre bug to exploit. As can be seen in the video, this AI’s Q*bert play skill is suspect at best. But all it has to do is to reach the end of level 1. The game then displays what’s normally a brief animation where the cubes flash for a bit and the player gets some bonus points before moving on to the next level. But the AI has stumbled onto the fact that if it makes a particular series of controller moves during this bonus sequence, the sequence never ends — it just keeps on giving the player points forever and ever and ever. All the AI has to do at that point is avoid committing suicide, and presto, infinite score.

What have we learned?

We’ve seen some of the power and some of the hazards of AI. With very simple automated training data, the AI can learn how to solve surprisingly complex problems completely on its own. But this is also problematic.

The AI’s only concern is on getting that score higher — not on performing the underlying task in a way that a human would consider “normal” or “proper”.
The AI will ignore all the other assumptions we would make about the problem, like “Deliberately killing your own character is bad”.
The AI may even find a solution that seems completely unrelated to the problem, like “Break the bonus screen and score infinite points.”

All of this is fun to watch, as long as the AI is playing video games. If the AI is, say, controlling a patient’s medication or monitoring a nuclear plant, we really don’t want this sort of “creativity”.

So when we use this sort of AI to solve a real-world problem, we need to be extremely careful. We need to make sure that the AI is constrained in what it can do — if we never want it to perform some action, then we need to make sure it can’t. And most importantly, we need to make sure that the scoring system truly reflects the outcome we want.

And this is just a problem with AIs, right?

Absolutely not! We humans exploit faulty scoring systems all the time. We’re so prone to it — and good at it — that English has a wonderfully evocative phrase for it: “gaming the system
”. We perform our tasks in all sorts of crazy ways designed to maximize the things we’ll be rewarded for, at the expense of the actual usefulness of the things we’re doing. So it’s not enough to be careful about designing a scoring system when we’re training our AIs. We need to be just as careful, if not more so, when we’re designing scoring systems for ourselves.

Many thanks for all the upvotes and encouraging feedback!

Other interesting takes on this topic:

Universal Paperclips
, a web game based on the Paperclip Maximizer thought experiment — what happens when you give an AI the goal of “maximize paperclip production” and no other constraints (thanks to Azfar Hisham for mentioning this experiment in the comments).
“AI masters 49 2600 games without instructions”
, an Ars Technica article about a similar AI experiment.
“Human-level control through deep reinforcement learning
”, the paper published in Nature about the experiment.
Minimax Algorithm
 — a Wikipedia article describing a typical approach to turn-based game AI. In this approach, using the same sort of “high scores are better for me” scoring algorithm, the AI picks the best moves for itself by also considering what the best moves are for its opponent. Simplifying greatly, the AI picks some moves that seem promising, because they yield high scores. But instead of simply making the move with the highest score, the AI then considers what the opponent will do in response — it finds the responsive move that yields the lowest score (the one that’s best for the opponent). The move the AI ultimately picks is the one that provokes the least-damaging response from the opponent.
My Little Pony: Friendship is Optimal
 — an MLP fanfic demonstrating quite convincingly why designing an intelligent game AI to maximize the goal of “satisfy human values through friendship and ponies” is probably a very bad idea.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/9ukdtejygxoqlhv7', 'title': 'How can an AI train itself if no one is telling it if its answer is correct or wrong?', 'score': {'original': 0.56185, 'ai': 0.43815}, 'blocks': [{'text': '“How can an AI train itself if no one is telling it if its answer is correct or wrong?”\n\nThis is a great question.\n\nIn order to learn how to solve a problem correctly, the AI has to be told how to recognize a “correct” solution to the problem. In order to learn how to better solve a problem that has no one correct answer, the AI has to be told how to recognize when it’s doing “better”.\n\nSo at first it seems like the answer to your question is “It can’t.”\n\nBut the nuance is in what we mean by “has to be told”. There doesn’t have to be a human trainer there saying “this is right” or “this is better”. The thing giving the AI its feedback on “right” or “better” can be just another computer program.\n\nThe most basic example of this is a “maximizer AI”. A human writes a computer program that can look at the current state of the problem and assign it a point score. The higher the score, the better the current state of the problem is. Now the AI can rapidly try different approaches to the problem without needing a human trainer to tell it the results of each attempt. The feedback is immediate and automatic: if approach A produces a higher score than approach B, then the AI knows that approach A is better.\n\nThe most classic example of this approach is the typical computer chess program. But that’s been discussed to death. So let me give you a less common and perhaps more fun example: video games.\n\nSpecifically, old Atari 2600 video games back from the dawn of game consoles.\n\nThere are two useful things about those games from a modern perspective:\n\n1. Simple controls.\n\nThe controller used for most Atari games had a very simple eight-direction joystick and one button. That’s it. No analog controls, no complicated six-button control schemes. That means that there’s a very small number of possible actions that a player can take at any given moment.\n\n2. Immediate scoring feedback.\n\nThe games typically displayed a score, and the object of the game was to get the highest score possible. What’s more, points were typically scored for very simple actions (like “eat that dot - 1 point”), rather than complex multi-step actions (like “Make your way across this level, avoid getting shot, find the flag at the center of the level, pick it up, and bring it back to your base — 1 point”). That means that the training data the AI needs is quite literally there on the screen. If you do something that makes the score go up, that’s good. If you do something that makes the score go up faster, that’s better.\n\nStarting from Scratch\n\nAn AI researcher decided to take advantage of these facts by running an experiment in which an AI learned how to play the games. Note that I didn’t say “in which the researcher taught the AI how to play the games.” The AI started with absolutely no knowledge of the rules of the games. Its baseline knowledge was essentially “Here is a simplified view', 'result': {'fake': 0.0159, 'real': 0.9841}, 'status': 'success'}, {'text': 'of what’s on the screen at this moment; here are the possible control signals you can give; and here’s the score. High scores are better. Go.”\n\nThis led to a wonderful paper, “Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari\n”. It’s a fun read if you can get past the terminology. But let me pick out my favorite example, Q*bert, which showed that this approach can lead to strategies that a human trainer would never have thought of.\n\nQ*bert is a game that originally came out in arcades in 1982. The player controls a little aardvark-looking character that must hop around a pyramid of cubes to change their colors. A few enemies pursue the character and sometimes mess up the colors, requiring the player to fix them. The arcade version looks like this:\n\nThe Atari 2600 version… leaves a lot more to the imagination:\n\nBut the experiment was about AI gameplay, not art appreciation. So they hooked the AI up to the controls, let it see the screen, told it that high scores are good, and let it loose.\n\nThe researchers’ expectation was that the AI would learn to play Q*bert the way a human would — try to find some reasonably-efficient way of covering the board while dodging the bad guys. But two of the test runs went in particularly interesting directions.\n\nFree-fallin’\n\nIn the first interesting run, the AI finds a weakness in the way the 2600 version scores the game and awards extra lives (which is different from the arcade version). The AI positions Q*bert on a cube near the edge of the board, waits for the smartest enemy to approach, and then deliberately sends Q*bert to his death by leaping off the board. But the enemy leaps off the board as well in pursuit, and the player gets 500 points for killing that enemy — enough for an extra life. So the AI can simply repeat this indefinitely, methodically and endlessly scoring points.\n\nBreaking the Game\n\nIn the second interesting run, the AI finds an even more bizarre bug to exploit. As can be seen in the video, this AI’s Q*bert play skill is suspect at best. But all it has to do is to reach the end of level 1. The game then displays what’s normally a brief animation where the cubes flash for a bit and the player gets some bonus points before moving on to the next level. But the AI has stumbled onto the fact that if it makes a particular series of controller moves during this bonus sequence, the sequence never ends — it just keeps on giving the player points forever and ever and ever. All the AI has to do at that point is avoid committing suicide, and presto, infinite score.\n\nWhat have we learned?\n\nWe’ve seen some of the power and some of the hazards of AI. With very simple automated training data, the AI can learn how to solve surprisingly complex problems completely on its own. But this is also problematic.\n\nThe AI’s only concern is on getting that score higher — not on performing the underlying task in', 'result': {'fake': 0.8932, 'real': 0.1068}, 'status': 'success'}, {'text': 'a way that a human would consider “normal” or “proper”.\nThe AI will ignore all the other assumptions we would make about the problem, like “Deliberately killing your own character is bad”.\nThe AI may even find a solution that seems completely unrelated to the problem, like “Break the bonus screen and score infinite points.”\n\nAll of this is fun to watch, as long as the AI is playing video games. If the AI is, say, controlling a patient’s medication or monitoring a nuclear plant, we really don’t want this sort of “creativity”.\n\nSo when we use this sort of AI to solve a real-world problem, we need to be extremely careful. We need to make sure that the AI is constrained in what it can do — if we never want it to perform some action, then we need to make sure it can’t. And most importantly, we need to make sure that the scoring system truly reflects the outcome we want.\n\nAnd this is just a problem with AIs, right?\n\nAbsolutely not! We humans exploit faulty scoring systems all the time. We’re so prone to it — and good at it — that English has a wonderfully evocative phrase for it: “gaming the system\n”. We perform our tasks in all sorts of crazy ways designed to maximize the things we’ll be rewarded for, at the expense of the actual usefulness of the things we’re doing. So it’s not enough to be careful about designing a scoring system when we’re training our AIs. We need to be just as careful, if not more so, when we’re designing scoring systems for ourselves.\n\nMany thanks for all the upvotes and encouraging feedback!\n\nOther interesting takes on this topic:\n\nUniversal Paperclips\n, a web game based on the Paperclip Maximizer thought experiment — what happens when you give an AI the goal of “maximize paperclip production” and no other constraints (thanks to Azfar Hisham for mentioning this experiment in the comments).\n“AI masters 49 2600 games without instructions”\n, an Ars Technica article about a similar AI experiment.\n“Human-level control through deep reinforcement learning\n”, the paper published in Nature about the experiment.\nMinimax Algorithm\n — a Wikipedia article describing a typical approach to turn-based game AI. In this approach, using the same sort of “high scores are better for me” scoring algorithm, the AI picks the best moves for itself by also considering what the best moves are for its opponent. Simplifying greatly, the AI picks some moves that seem promising, because they yield high scores. But instead of simply making the move with the highest score, the AI then considers what the opponent will do in response — it finds the responsive move that yields the lowest score (the one that’s best for the opponent). The move the AI ultimately picks is the one that provokes the least-damaging response from the opponent.\nMy Little Pony: Friendship is Optimal\n — an MLP fanfic demonstrating quite convincingly why designing an intelligent game AI to maximize the goal of “satisfy human values through friendship and ponies” is probably a very bad idea.', 'result': {'fake': 0.921, 'real': 0.079}, 'status': 'success'}], 'credits_used': 16, 'credits': 1985874, 'subscription': 0, 'content': '“How can an AI train itself if no one is telling it if its answer is correct or wrong?”\n\nThis is a great question.\n\nIn order to learn how to solve a problem correctly, the AI has to be told how to recognize a “correct” solution to the problem. In order to learn how to better solve a problem that has no one correct answer, the AI has to be told how to recognize when it’s doing “better”.\n\nSo at first it seems like the answer to your question is “It can’t.”\n\nBut the nuance is in what we mean by “has to be told”. There doesn’t have to be a human trainer there saying “this is right” or “this is better”. The thing giving the AI its feedback on “right” or “better” can be just another computer program.\n\nThe most basic example of this is a “maximizer AI”. A human writes a computer program that can look at the current state of the problem and assign it a point score. The higher the score, the better the current state of the problem is. Now the AI can rapidly try different approaches to the problem without needing a human trainer to tell it the results of each attempt. The feedback is immediate and automatic: if approach A produces a higher score than approach B, then the AI knows that approach A is better.\n\nThe most classic example of this approach is the typical computer chess program. But that’s been discussed to death. So let me give you a less common and perhaps more fun example: video games.\n\nSpecifically, old Atari 2600 video games back from the dawn of game consoles.\n\nThere are two useful things about those games from a modern perspective:\n\n1. Simple controls.\n\nThe controller used for most Atari games had a very simple eight-direction joystick and one button. That’s it. No analog controls, no complicated six-button control schemes. That means that there’s a very small number of possible actions that a player can take at any given moment.\n\n2. Immediate scoring feedback.\n\nThe games typically displayed a score, and the object of the game was to get the highest score possible. What’s more, points were typically scored for very simple actions (like “eat that dot - 1 point”), rather than complex multi-step actions (like “Make your way across this level, avoid getting shot, find the flag at the center of the level, pick it up, and bring it back to your base — 1 point”). That means that the training data the AI needs is quite literally there on the screen. If you do something that makes the score go up, that’s good. If you do something that makes the score go up faster, that’s better.\n\nStarting from Scratch\n\nAn AI researcher decided to take advantage of these facts by running an experiment in which an AI learned how to play the games. Note that I didn’t say “in which the researcher taught the AI how to play the games.” The AI started with absolutely no knowledge of the rules of the games. Its baseline knowledge was essentially “Here is a simplified view of what’s on the screen at this moment; here are the possible control signals you can give; and here’s the score. High scores are better. Go.”\n\nThis led to a wonderful paper, “Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari\n”. It’s a fun read if you can get past the terminology. But let me pick out my favorite example, Q*bert, which showed that this approach can lead to strategies that a human trainer would never have thought of.\n\nQ*bert is a game that originally came out in arcades in 1982. The player controls a little aardvark-looking character that must hop around a pyramid of cubes to change their colors. A few enemies pursue the character and sometimes mess up the colors, requiring the player to fix them. The arcade version looks like this:\n\nThe Atari 2600 version… leaves a lot more to the imagination:\n\nBut the experiment was about AI gameplay, not art appreciation. So they hooked the AI up to the controls, let it see the screen, told it that high scores are good, and let it loose.\n\nThe researchers’ expectation was that the AI would learn to play Q*bert the way a human would — try to find some reasonably-efficient way of covering the board while dodging the bad guys. But two of the test runs went in particularly interesting directions.\n\nFree-fallin’\n\nIn the first interesting run, the AI finds a weakness in the way the 2600 version scores the game and awards extra lives (which is different from the arcade version). The AI positions Q*bert on a cube near the edge of the board, waits for the smartest enemy to approach, and then deliberately sends Q*bert to his death by leaping off the board. But the enemy leaps off the board as well in pursuit, and the player gets 500 points for killing that enemy — enough for an extra life. So the AI can simply repeat this indefinitely, methodically and endlessly scoring points.\n\nBreaking the Game\n\nIn the second interesting run, the AI finds an even more bizarre bug to exploit. As can be seen in the video, this AI’s Q*bert play skill is suspect at best. But all it has to do is to reach the end of level 1. The game then displays what’s normally a brief animation where the cubes flash for a bit and the player gets some bonus points before moving on to the next level. But the AI has stumbled onto the fact that if it makes a particular series of controller moves during this bonus sequence, the sequence never ends — it just keeps on giving the player points forever and ever and ever. All the AI has to do at that point is avoid committing suicide, and presto, infinite score.\n\nWhat have we learned?\n\nWe’ve seen some of the power and some of the hazards of AI. With very simple automated training data, the AI can learn how to solve surprisingly complex problems completely on its own. But this is also problematic.\n\nThe AI’s only concern is on getting that score higher — not on performing the underlying task in a way that a human would consider “normal” or “proper”.\nThe AI will ignore all the other assumptions we would make about the problem, like “Deliberately killing your own character is bad”.\nThe AI may even find a solution that seems completely unrelated to the problem, like “Break the bonus screen and score infinite points.”\n\nAll of this is fun to watch, as long as the AI is playing video games. If the AI is, say, controlling a patient’s medication or monitoring a nuclear plant, we really don’t want this sort of “creativity”.\n\nSo when we use this sort of AI to solve a real-world problem, we need to be extremely careful. We need to make sure that the AI is constrained in what it can do — if we never want it to perform some action, then we need to make sure it can’t. And most importantly, we need to make sure that the scoring system truly reflects the outcome we want.\n\nAnd this is just a problem with AIs, right?\n\nAbsolutely not! We humans exploit faulty scoring systems all the time. We’re so prone to it — and good at it — that English has a wonderfully evocative phrase for it: “gaming the system\n”. We perform our tasks in all sorts of crazy ways designed to maximize the things we’ll be rewarded for, at the expense of the actual usefulness of the things we’re doing. So it’s not enough to be careful about designing a scoring system when we’re training our AIs. We need to be just as careful, if not more so, when we’re designing scoring systems for ourselves.\n\nMany thanks for all the upvotes and encouraging feedback!\n\nOther interesting takes on this topic:\n\nUniversal Paperclips\n, a web game based on the Paperclip Maximizer thought experiment — what happens when you give an AI the goal of “maximize paperclip production” and no other constraints (thanks to Azfar Hisham for mentioning this experiment in the comments).\n“AI masters 49 2600 games without instructions”\n, an Ars Technica article about a similar AI experiment.\n“Human-level control through deep reinforcement learning\n”, the paper published in Nature about the experiment.\nMinimax Algorithm\n — a Wikipedia article describing a typical approach to turn-based game AI. In this approach, using the same sort of “high scores are better for me” scoring algorithm, the AI picks the best moves for itself by also considering what the best moves are for its opponent. Simplifying greatly, the AI picks some moves that seem promising, because they yield high scores. But instead of simply making the move with the highest score, the AI then considers what the opponent will do in response — it finds the responsive move that yields the lowest score (the one that’s best for the opponent). The move the AI ultimately picks is the one that provokes the least-damaging response from the opponent.\nMy Little Pony: Friendship is Optimal\n — an MLP fanfic demonstrating quite convincingly why designing an intelligent game AI to maximize the goal of “satisfy human values through friendship and ponies” is probably a very bad idea.', 'aiModelVersion': '1'}",0.56185
Julien Despois,Updated 5y,What are examples of AI coming up with unconventional solutions?,"When I was working for Berkeley’s BEST lab
 on Tensegrity
 robots for NASA space missions, I was in charge of path-planning.

What you have to know is that this kind of robot moves by a deformation-based crawling/rolling technique (video)
. It can absorb shocks thanks to its springy structure, and would therefore have a thruster in its center to allow it to hop.

I used HiRise data
 from the Mars Reconnaissance Orbiter to see how the robot would travel efficiently on Mars. We expected the robot to use the thruster to make the biggest part of the trip, and then crawl/roll.

After a lot of tinkering, trying to get the parameters and the algorithm right, I finally was greeted by the following path on a some martian hills.

The robot starts in the top left and has to go to the bottom right by rolling (blue), and hopping (green).

I was surprised to see the robot use its thruster multiple times, as it might not be the most efficient. However, by reviewing the trajectory in 3D, I understood how clever the solution was.

The AI had discovered that crawling was tiring, while letting itself roll down a hill was much easier. It then decided to hop on the highest hills on its trajectory, and then let gravity do its job! What a clever little fellow!

This is why I love working with A.I.

Edit: 4,000 Upvotes! Damn!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rh7tvi9gkbjl12c4', 'title': 'What are examples of AI coming up with unconventional solutions?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'When I was working for Berkeley’s BEST lab\n on Tensegrity\n robots for NASA space missions, I was in charge of path-planning.\n\nWhat you have to know is that this kind of robot moves by a deformation-based crawling/rolling technique (video)\n. It can absorb shocks thanks to its springy structure, and would therefore have a thruster in its center to allow it to hop.\n\nI used HiRise data\n from the Mars Reconnaissance Orbiter to see how the robot would travel efficiently on Mars. We expected the robot to use the thruster to make the biggest part of the trip, and then crawl/roll.\n\nAfter a lot of tinkering, trying to get the parameters and the algorithm right, I finally was greeted by the following path on a some martian hills.\n\nThe robot starts in the top left and has to go to the bottom right by rolling (blue), and hopping (green).\n\nI was surprised to see the robot use its thruster multiple times, as it might not be the most efficient. However, by reviewing the trajectory in 3D, I understood how clever the solution was.\n\nThe AI had discovered that crawling was tiring, while letting itself roll down a hill was much easier. It then decided to hop on the highest hills on its trajectory, and then let gravity do its job! What a clever little fellow!\n\nThis is why I love working with A.I.\n\nEdit: 4,000 Upvotes! Damn!', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985871, 'subscription': 0, 'content': 'When I was working for Berkeley’s BEST lab\n on Tensegrity\n robots for NASA space missions, I was in charge of path-planning.\n\nWhat you have to know is that this kind of robot moves by a deformation-based crawling/rolling technique (video)\n. It can absorb shocks thanks to its springy structure, and would therefore have a thruster in its center to allow it to hop.\n\nI used HiRise data\n from the Mars Reconnaissance Orbiter to see how the robot would travel efficiently on Mars. We expected the robot to use the thruster to make the biggest part of the trip, and then crawl/roll.\n\nAfter a lot of tinkering, trying to get the parameters and the algorithm right, I finally was greeted by the following path on a some martian hills.\n\nThe robot starts in the top left and has to go to the bottom right by rolling (blue), and hopping (green).\n\nI was surprised to see the robot use its thruster multiple times, as it might not be the most efficient. However, by reviewing the trajectory in 3D, I understood how clever the solution was.\n\nThe AI had discovered that crawling was tiring, while letting itself roll down a hill was much easier. It then decided to hop on the highest hills on its trajectory, and then let gravity do its job! What a clever little fellow!\n\nThis is why I love working with A.I.\n\nEdit: 4,000 Upvotes! Damn!', 'aiModelVersion': '1'}",0.9997
Beheruz N Sethna,Updated 3y,What was the very first computer science fact you learned that blew your mind?,"I grew up in India, and was an engineering student in 1966–71; I used a Soviet MINSK 2 computer for my research project. It filled a room — this is from a stock picture on the web:

I am guessing (I hope Joshua Gross or some other CS person will educate me) that the computing capability of this machine was about what a watch has today. [Updates: Joshua Gross has correctly pointed out that my estimate of a watch of today is way off — more like a watch of the 1980s. Dave Pullin has commented, “I don't think it was anything like as powerful as a watch.”]

We submitted FORTRAN programs on (yellow) punched tape — which is a whole lot worse than punched cards, because if you make an error on a card, you can throw the card away, but it is terribly frustrating to lose all the work on an entire punched tape because of a small error (which regrettably I made more than once).

The mind-blowing thing was when I saw another student pick some of those little yellow dots off the floor and stick them back on the tape, which if done right, would save you a 30 minutes of having to re-do the entire tape! You had to know machine language (what pattern of dots was an A vs. an S) to be able to do that, which I learned how to do pretty quickly.

Think of that the next time you press your backspace or delete key!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0wbrtuq2ogdh837a', 'title': 'What was the very first computer science fact you learned that blew your mind?', 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': ""I grew up in India, and was an engineering student in 1966–71; I used a Soviet MINSK 2 computer for my research project. It filled a room — this is from a stock picture on the web:\n\nI am guessing (I hope Joshua Gross or some other CS person will educate me) that the computing capability of this machine was about what a watch has today. [Updates: Joshua Gross has correctly pointed out that my estimate of a watch of today is way off — more like a watch of the 1980s. Dave Pullin has commented, “I don't think it was anything like as powerful as a watch.”]\n\nWe submitted FORTRAN programs on (yellow) punched tape — which is a whole lot worse than punched cards, because if you make an error on a card, you can throw the card away, but it is terribly frustrating to lose all the work on an entire punched tape because of a small error (which regrettably I made more than once).\n\nThe mind-blowing thing was when I saw another student pick some of those little yellow dots off the floor and stick them back on the tape, which if done right, would save you a 30 minutes of having to re-do the entire tape! You had to know machine language (what pattern of dots was an A vs. an S) to be able to do that, which I learned how to do pretty quickly.\n\nThink of that the next time you press your backspace or delete key!"", 'result': {'fake': 0.0007, 'real': 0.9993}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985868, 'subscription': 0, 'content': ""I grew up in India, and was an engineering student in 1966–71; I used a Soviet MINSK 2 computer for my research project. It filled a room — this is from a stock picture on the web:\n\nI am guessing (I hope Joshua Gross or some other CS person will educate me) that the computing capability of this machine was about what a watch has today. [Updates: Joshua Gross has correctly pointed out that my estimate of a watch of today is way off — more like a watch of the 1980s. Dave Pullin has commented, “I don't think it was anything like as powerful as a watch.”]\n\nWe submitted FORTRAN programs on (yellow) punched tape — which is a whole lot worse than punched cards, because if you make an error on a card, you can throw the card away, but it is terribly frustrating to lose all the work on an entire punched tape because of a small error (which regrettably I made more than once).\n\nThe mind-blowing thing was when I saw another student pick some of those little yellow dots off the floor and stick them back on the tape, which if done right, would save you a 30 minutes of having to re-do the entire tape! You had to know machine language (what pattern of dots was an A vs. an S) to be able to do that, which I learned how to do pretty quickly.\n\nThink of that the next time you press your backspace or delete key!"", 'aiModelVersion': '1'}",0.9993
Håkon Hapnes Strand,4y,Why is computer engineering not growing as fast as computer science?,"Computer hardware has to a large extent been commoditized.

Think of chips like Intel Core CPUs and NVIDIA GeForce GPUs. These are general-purpose processing units that run on millions of computers everywhere. They can pretty much solve all of our computing needs. The cloud has accentuated this development further, because the cloud giants buy up standardized hardware in huge bulks. What we end up with are a few big players that totally dominate the hardware market.

Software is very different. Unless we develop some utopical artificial general intelligence, it’s impossible for one computer program to satisfy all of our computing needs. In fact, we need a plethora of them on a daily basis.

When we think of software, we tend to think of a few big tech giants and their consumer software, but that’s just the tip of the iceberg. Custom enterprise software alone is a trillion dollar market.

That’s why there are more than 20 million software developers worldwide, while the number of hardware engineers is probably less than a million.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2p6o3aue1i45ylhk', 'title': 'Why is computer engineering not growing as fast as computer science?', 'score': {'original': 0.9991, 'ai': 0.0009}, 'blocks': [{'text': 'Computer hardware has to a large extent been commoditized.\n\nThink of chips like Intel Core CPUs and NVIDIA GeForce GPUs. These are general-purpose processing units that run on millions of computers everywhere. They can pretty much solve all of our computing needs. The cloud has accentuated this development further, because the cloud giants buy up standardized hardware in huge bulks. What we end up with are a few big players that totally dominate the hardware market.\n\nSoftware is very different. Unless we develop some utopical artificial general intelligence, it’s impossible for one computer program to satisfy all of our computing needs. In fact, we need a plethora of them on a daily basis.\n\nWhen we think of software, we tend to think of a few big tech giants and their consumer software, but that’s just the tip of the iceberg. Custom enterprise software alone is a trillion dollar market.\n\nThat’s why there are more than 20 million software developers worldwide, while the number of hardware engineers is probably less than a million.', 'result': {'fake': 0.0009, 'real': 0.9991}, 'status': 'success'}], 'credits_used': 2, 'credits': 1985866, 'subscription': 0, 'content': 'Computer hardware has to a large extent been commoditized.\n\nThink of chips like Intel Core CPUs and NVIDIA GeForce GPUs. These are general-purpose processing units that run on millions of computers everywhere. They can pretty much solve all of our computing needs. The cloud has accentuated this development further, because the cloud giants buy up standardized hardware in huge bulks. What we end up with are a few big players that totally dominate the hardware market.\n\nSoftware is very different. Unless we develop some utopical artificial general intelligence, it’s impossible for one computer program to satisfy all of our computing needs. In fact, we need a plethora of them on a daily basis.\n\nWhen we think of software, we tend to think of a few big tech giants and their consumer software, but that’s just the tip of the iceberg. Custom enterprise software alone is a trillion dollar market.\n\nThat’s why there are more than 20 million software developers worldwide, while the number of hardware engineers is probably less than a million.', 'aiModelVersion': '1'}",0.9991
Terry Lambert,Updated 1y,What is something impossible you’ve been asked to do as a software engineer?,"There was this bug that got handed around at Apple, which was about checking whether or not the RAM was any good.

It got handed to every new person, because it’s an insoluble problem, both because the code to check the RAM is running in RAM, and if it’s small enough to fit in cache, it doesn’t ever hit real RAM, and because even if you can check it from a logic perspective, if the RAM it’s running in is bad, then the results are definitionally indeterminate.

In other words, it’s a variant of the halting problem.

It originated with Bertrand Serlet, who was the nominal head of software engineering, being the VP in charge, and he delighted in handing it around.

He handed it to me, and it was obvious it was not possible to solve it, at least without essentially writing code that would overclock, underclock, overvoltage, and undervoltage the RAM.

And do it in such a way as it would run as close to published specifications as possible.

This bug arose in the first place, because people wanted cheaper RAM for Apple products, but Apple runs their RAM right up to the edge of the specification, and the people who sell RAM are used to dealing with PCs.

PCs do not care about power management.

This is not a bad thing about PCs, in particular, they just don’t care. Once you rev up the RAM in a PC, it never gets clocked down, and you never voltage it down after you clock it down, etc..

It’s never run in that part of the specification for the performance it’s supposed to be designed to operate in.

And so it’s never tested there, because in a Windows machine, that never actually happens, and if it doesn’t happen, you don’t test it, and if you don’t test it, the RAM doesn’t fail the test it never has to pass, and … you sell more RAM, because your effective yield on the fab goes up in terms of producing “acceptable RAM”.

So Bertrand handed the bug to me, and it was clear from the history it had been handed around a lot.

And I handed it back to him, with “This is the halting problem.” in the bug analysis field, which was full of 30,000 lines of engineering “This is not a priority/I don’t want this problem/This problem is not fixable/etc.”.

So I sat down and proved it was mathematically equivalent to the halting problem, and the only way Apple quits getting support calls for running its RAM so close to specification is to either stop doing that, sell all the RAM, or surface mount the RAM and don’t let users at it.

This did not go over well, because it was one of Bertrand’s favorite tests for new engineers, and I’d just killed it, visibly, in the bug.

He handed the bug back, and asked for another alternative as a solution.

And so I suggested that we check the RAM revision numbers, and if they were not Apple supplied RAM modules, then what we do is … report an error, if the test is run on the same module 3 times or more.

Initially, the RAM looks good.

If you have a problem, the test eventually believes you have a problem, and reports the problem in the non Apple module, which is where the problem likely exists.

You replace the module, the problem goes away, or you see it again, and … it after several times reports the problem in the non-Apple module again, because you wouldn’t be persistently running the RAM tester, unless you were seeing the problem.

I called it “confirmation bias based RAM testing”.

Bertrand closed the bug.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6wz5mvrx17ksupd3', 'title': 'What is something impossible you’ve been asked to do as a software engineer?', 'score': {'original': 0.8202, 'ai': 0.1798}, 'blocks': [{'text': 'There was this bug that got handed around at Apple, which was about checking whether or not the RAM was any good.\n\nIt got handed to every new person, because it’s an insoluble problem, both because the code to check the RAM is running in RAM, and if it’s small enough to fit in cache, it doesn’t ever hit real RAM, and because even if you can check it from a logic perspective, if the RAM it’s running in is bad, then the results are definitionally indeterminate.\n\nIn other words, it’s a variant of the halting problem.\n\nIt originated with Bertrand Serlet, who was the nominal head of software engineering, being the VP in charge, and he delighted in handing it around.\n\nHe handed it to me, and it was obvious it was not possible to solve it, at least without essentially writing code that would overclock, underclock, overvoltage, and undervoltage the RAM.\n\nAnd do it in such a way as it would run as close to published specifications as possible.\n\nThis bug arose in the first place, because people wanted cheaper RAM for Apple products, but Apple runs their RAM right up to the edge of the specification, and the people who sell RAM are used to dealing with PCs.\n\nPCs do not care about power management.\n\nThis is not a bad thing about PCs, in particular, they just don’t care. Once you rev up the RAM in a PC, it never gets clocked down, and you never voltage it down after you clock it down, etc..\n\nIt’s never run in that part of the specification for the performance it’s supposed to be designed to operate in.\n\nAnd so it’s never tested there, because in a Windows machine, that never actually happens, and if it doesn’t happen, you don’t test it, and if you don’t test it, the RAM doesn’t fail the test it never has to pass, and … you sell more RAM, because your effective yield on the fab goes up in terms of producing “acceptable RAM”.\n\nSo Bertrand handed the bug to me, and it was clear from the history it had been handed around a lot.\n\nAnd I handed it back to him, with “This is the halting problem.” in the bug analysis field, which was full of 30,000 lines of engineering “This is not a priority/I don’t want this problem/This problem is not fixable/etc.”.\n\nSo I sat down and proved it was mathematically equivalent to the halting problem, and the only way Apple quits getting support calls for running its RAM so close to specification is to either stop doing that, sell all the RAM, or surface mount the RAM and don’t let users at it.\n\nThis did not go over well, because it was one of Bertrand’s favorite tests for new engineers, and I’d just killed it, visibly, in the bug.\n\nHe handed the bug back, and asked for another alternative as a solution.\n\nAnd so I suggested that we check the RAM revision numbers, and if they were not Apple supplied RAM modules, then what we do is … report an error, if the test is run on the same', 'result': {'fake': 0.4535, 'real': 0.5465}, 'status': 'success'}, {'text': 'module 3 times or more.\n\nInitially, the RAM looks good.\n\nIf you have a problem, the test eventually believes you have a problem, and reports the problem in the non Apple module, which is where the problem likely exists.\n\nYou replace the module, the problem goes away, or you see it again, and … it after several times reports the problem in the non-Apple module again, because you wouldn’t be persistently running the RAM tester, unless you were seeing the problem.\n\nI called it “confirmation bias based RAM testing”.\n\nBertrand closed the bug.', 'result': {'fake': 0.0331, 'real': 0.9669}, 'status': 'success'}], 'credits_used': 7, 'credits': 1985859, 'subscription': 0, 'content': 'There was this bug that got handed around at Apple, which was about checking whether or not the RAM was any good.\n\nIt got handed to every new person, because it’s an insoluble problem, both because the code to check the RAM is running in RAM, and if it’s small enough to fit in cache, it doesn’t ever hit real RAM, and because even if you can check it from a logic perspective, if the RAM it’s running in is bad, then the results are definitionally indeterminate.\n\nIn other words, it’s a variant of the halting problem.\n\nIt originated with Bertrand Serlet, who was the nominal head of software engineering, being the VP in charge, and he delighted in handing it around.\n\nHe handed it to me, and it was obvious it was not possible to solve it, at least without essentially writing code that would overclock, underclock, overvoltage, and undervoltage the RAM.\n\nAnd do it in such a way as it would run as close to published specifications as possible.\n\nThis bug arose in the first place, because people wanted cheaper RAM for Apple products, but Apple runs their RAM right up to the edge of the specification, and the people who sell RAM are used to dealing with PCs.\n\nPCs do not care about power management.\n\nThis is not a bad thing about PCs, in particular, they just don’t care. Once you rev up the RAM in a PC, it never gets clocked down, and you never voltage it down after you clock it down, etc..\n\nIt’s never run in that part of the specification for the performance it’s supposed to be designed to operate in.\n\nAnd so it’s never tested there, because in a Windows machine, that never actually happens, and if it doesn’t happen, you don’t test it, and if you don’t test it, the RAM doesn’t fail the test it never has to pass, and … you sell more RAM, because your effective yield on the fab goes up in terms of producing “acceptable RAM”.\n\nSo Bertrand handed the bug to me, and it was clear from the history it had been handed around a lot.\n\nAnd I handed it back to him, with “This is the halting problem.” in the bug analysis field, which was full of 30,000 lines of engineering “This is not a priority/I don’t want this problem/This problem is not fixable/etc.”.\n\nSo I sat down and proved it was mathematically equivalent to the halting problem, and the only way Apple quits getting support calls for running its RAM so close to specification is to either stop doing that, sell all the RAM, or surface mount the RAM and don’t let users at it.\n\nThis did not go over well, because it was one of Bertrand’s favorite tests for new engineers, and I’d just killed it, visibly, in the bug.\n\nHe handed the bug back, and asked for another alternative as a solution.\n\nAnd so I suggested that we check the RAM revision numbers, and if they were not Apple supplied RAM modules, then what we do is … report an error, if the test is run on the same module 3 times or more.\n\nInitially, the RAM looks good.\n\nIf you have a problem, the test eventually believes you have a problem, and reports the problem in the non Apple module, which is where the problem likely exists.\n\nYou replace the module, the problem goes away, or you see it again, and … it after several times reports the problem in the non-Apple module again, because you wouldn’t be persistently running the RAM tester, unless you were seeing the problem.\n\nI called it “confirmation bias based RAM testing”.\n\nBertrand closed the bug.', 'aiModelVersion': '1'}",0.8202
Tarun Chitra,9y,Have there been any new brilliant computer science algorithms in last 10 years?,"Fully Homomorphic Encryption (FHE)Imagine if you were a hospital that needed to process/analyze data on EC2, but were worried about privacy. In an ideal world, one would like:To encrypt his or her dataSend the encrypted data to the cloud and have the cloud servers operate on the encypted data (yes, I mean that)Sending you back the encrypted results, that you can easily decryptThis probably seems like magic — after all, how can someone operate on garbled data, creating more garbled data and then send me back some junk that is guaranteed to decrypt to the correct answer? It turns out that since most cryptosystems inherit symmetries/operations from the mathematical objects that they are based on (groups, rings, lattices, algebraic number fields), we can sometimes use these operations without breaking our encryption.  Before giving some details about FHE, let's look at an example. Suppose that I give you an RSA public key (N,e)(N,e)(N,e)  and suppose that the secret key is d≡e−1 mod ϕ(N)d≡e−1 mod ϕ(N) d \equiv e^{-1} \text{ mod } \phi(N). Then two plaintexts π1,π2∈Zπ1,π2∈Z\pi_1, \pi_2 \in \Z would become ciphertexts ψ1,ψ2ψ1,ψ2\psi_1, \psi_2, where ψ1=πe1 mod N,ψ2=πe2 mod Nψ1=π1e mod N,ψ2=π2e mod N \psi_1 = \pi_1^e \text{ mod } N, \psi_2 = \pi_2^e \text{ mod } N Note that, π1π2 mod N=(ψd1 mod N)(ψd2 mod N)=π1π2 mod N=(ψ1d mod N)(ψ2d mod N)= \pi_1 \pi_2 \text{ mod } N = (\psi_1^d \text{ mod } N) (\psi_2^d \text{ mod } N) = (ψ1ψ2)d mod N(ψ1ψ2)d mod N (\psi_1 \psi_2)^d  \text{ mod } Nso that the decryption of the product of two ciphertexts is equivalent (mod N) to the product of the two plaintexts. Our goal is to make a system that is both multiplicatively homomorphic (like RSA) as well as additively homomorphic (which RSA is not, as you can see from the binomial formula). Note that in this about process, the cloud servers never see your raw data. Fully homomorphic encrypytion makes this happen. The landmark paper/thesis [0] of Gentry (2009) led to some astonishing revelations about simple encryption schemes:As long as a scheme can evaluate its own decryption circuit as well as a single NAND gate, one can bootstrap this scheme to a fully homomorphic schemeTechniques used to prove guarantees of post-quantum security turn out to be useful for proving that a bootstrappable homomorphic encryption scheme existsEven very simple integer schemes can workIn particular, van Dijk, Gentry, Halevi and Vaikunthan (DGHV) proved in [1] that a cryptography scheme that is simpler than RSA (!!!) isHomomorphic for virtually unlimited number of additive gates, homomorphic for small numbers of multiplicative gatesBootstrappableSo simple and clean (relative to other cryptosystems, e.g. NTRU) that it was casually proposed on a message board in 2000 by Bram CohenLater, Vaikunthan and Brakerski proved that one can construct non-bootstrappable homomorphic encryption schemes [2] that base their security on stronger, post-quantum problems (e.g. Learning with Errors). This scheme appears to be more practical than the original bootstrapping schemes (although it still uses bootstrapping as 'an optimization').  A nice non-technical summary can be found on the MIT tech review [3] [0] Craig Gentry's PhD Thesis[1] Page on microsoft.com[2] Page on iacr.org[3] Homomorphic Encryption - MIT Technology Review","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/4skq90ujnh35vifz', 'title': 'Have there been any new brilliant computer science algorithms in last 10 years?', 'score': {'original': 0.9986, 'ai': 0.0014}, 'blocks': [{'text': ""Fully Homomorphic Encryption (FHE)Imagine if you were a hospital that needed to process/analyze data on EC2, but were worried about privacy. In an ideal world, one would like:To encrypt his or her dataSend the encrypted data to the cloud and have the cloud servers operate on the encypted data (yes, I mean that)Sending you back the encrypted results, that you can easily decryptThis probably seems like magic — after all, how can someone operate on garbled data, creating more garbled data and then send me back some junk that is guaranteed to decrypt to the correct answer? It turns out that since most cryptosystems inherit symmetries/operations from the mathematical objects that they are based on (groups, rings, lattices, algebraic number fields), we can sometimes use these operations without breaking our encryption.  Before giving some details about FHE, let's look at an example. Suppose that I give you an RSA public key (N,e)(N,e)(N,e)  and suppose that the secret key is d≡e−1\xa0mod\xa0ϕ(N)d≡e−1\xa0mod\xa0ϕ(N) d \\equiv e^{-1} \\text{ mod } \\phi(N). Then two plaintexts π1,π2∈Zπ1,π2∈Z\\pi_1, \\pi_2 \\in \\Z would become ciphertexts ψ1,ψ2ψ1,ψ2\\psi_1, \\psi_2, where ψ1=πe1\xa0mod\xa0N,ψ2=πe2\xa0mod\xa0Nψ1=π1e\xa0mod\xa0N,ψ2=π2e\xa0mod\xa0N \\psi_1 = \\pi_1^e \\text{ mod } N, \\psi_2 = \\pi_2^e \\text{ mod } N Note that, π1π2\xa0mod\xa0N=(ψd1\xa0mod\xa0N)(ψd2\xa0mod\xa0N)=π1π2\xa0mod\xa0N=(ψ1d\xa0mod\xa0N)(ψ2d\xa0mod\xa0N)= \\pi_1 \\pi_2 \\text{ mod } N = (\\psi_1^d \\text{ mod } N) (\\psi_2^d \\text{ mod } N) = (ψ1ψ2)d\xa0mod\xa0N(ψ1ψ2)d\xa0mod\xa0N (\\psi_1 \\psi_2)^d  \\text{ mod } Nso that the decryption of the product of two ciphertexts is equivalent (mod N) to the product of the two plaintexts. Our goal is to make a system that is both multiplicatively homomorphic (like RSA) as well as additively homomorphic (which RSA is not, as you can see from the binomial formula). Note that in this about process, the cloud servers never see your raw data. Fully homomorphic encrypytion makes this happen. The landmark paper/thesis [0] of Gentry (2009) led to some astonishing revelations about simple encryption schemes:As long as a scheme can evaluate its own decryption circuit as well as a single NAND gate, one can bootstrap this scheme to a fully homomorphic schemeTechniques used to prove guarantees of post-quantum security turn out to be useful for proving that a bootstrappable homomorphic encryption scheme existsEven very simple integer schemes can workIn particular, van Dijk, Gentry, Halevi and Vaikunthan (DGHV) proved in [1] that a cryptography scheme that is simpler than RSA (!!!) isHomomorphic for virtually unlimited number of additive gates, homomorphic for small numbers of multiplicative gatesBootstrappableSo simple and clean (relative to other cryptosystems, e.g. NTRU) that it was casually proposed on a message board in 2000 by Bram CohenLater, Vaikunthan and Brakerski proved that one can construct non-bootstrappable homomorphic encryption schemes [2] that base their security on stronger, post-quantum problems (e.g. Learning with Errors). This scheme appears to be more practical than the original bootstrapping schemes (although it still uses bootstrapping as 'an optimization').  A nice non-technical summary can be found on the MIT tech review [3] [0] Craig Gentry's PhD Thesis[1] Page on microsoft.com[2] Page on iacr.org[3] Homomorphic Encryption - MIT Technology Review"", 'result': {'fake': 0.0014, 'real': 0.9986}, 'status': 'success'}], 'credits_used': 6, 'credits': 1985853, 'subscription': 0, 'content': ""Fully Homomorphic Encryption (FHE)Imagine if you were a hospital that needed to process/analyze data on EC2, but were worried about privacy. In an ideal world, one would like:To encrypt his or her dataSend the encrypted data to the cloud and have the cloud servers operate on the encypted data (yes, I mean that)Sending you back the encrypted results, that you can easily decryptThis probably seems like magic — after all, how can someone operate on garbled data, creating more garbled data and then send me back some junk that is guaranteed to decrypt to the correct answer? It turns out that since most cryptosystems inherit symmetries/operations from the mathematical objects that they are based on (groups, rings, lattices, algebraic number fields), we can sometimes use these operations without breaking our encryption.  Before giving some details about FHE, let's look at an example. Suppose that I give you an RSA public key (N,e)(N,e)(N,e)  and suppose that the secret key is d≡e−1\xa0mod\xa0ϕ(N)d≡e−1\xa0mod\xa0ϕ(N) d \\equiv e^{-1} \\text{ mod } \\phi(N). Then two plaintexts π1,π2∈Zπ1,π2∈Z\\pi_1, \\pi_2 \\in \\Z would become ciphertexts ψ1,ψ2ψ1,ψ2\\psi_1, \\psi_2, where ψ1=πe1\xa0mod\xa0N,ψ2=πe2\xa0mod\xa0Nψ1=π1e\xa0mod\xa0N,ψ2=π2e\xa0mod\xa0N \\psi_1 = \\pi_1^e \\text{ mod } N, \\psi_2 = \\pi_2^e \\text{ mod } N Note that, π1π2\xa0mod\xa0N=(ψd1\xa0mod\xa0N)(ψd2\xa0mod\xa0N)=π1π2\xa0mod\xa0N=(ψ1d\xa0mod\xa0N)(ψ2d\xa0mod\xa0N)= \\pi_1 \\pi_2 \\text{ mod } N = (\\psi_1^d \\text{ mod } N) (\\psi_2^d \\text{ mod } N) = (ψ1ψ2)d\xa0mod\xa0N(ψ1ψ2)d\xa0mod\xa0N (\\psi_1 \\psi_2)^d  \\text{ mod } Nso that the decryption of the product of two ciphertexts is equivalent (mod N) to the product of the two plaintexts. Our goal is to make a system that is both multiplicatively homomorphic (like RSA) as well as additively homomorphic (which RSA is not, as you can see from the binomial formula). Note that in this about process, the cloud servers never see your raw data. Fully homomorphic encrypytion makes this happen. The landmark paper/thesis [0] of Gentry (2009) led to some astonishing revelations about simple encryption schemes:As long as a scheme can evaluate its own decryption circuit as well as a single NAND gate, one can bootstrap this scheme to a fully homomorphic schemeTechniques used to prove guarantees of post-quantum security turn out to be useful for proving that a bootstrappable homomorphic encryption scheme existsEven very simple integer schemes can workIn particular, van Dijk, Gentry, Halevi and Vaikunthan (DGHV) proved in [1] that a cryptography scheme that is simpler than RSA (!!!) isHomomorphic for virtually unlimited number of additive gates, homomorphic for small numbers of multiplicative gatesBootstrappableSo simple and clean (relative to other cryptosystems, e.g. NTRU) that it was casually proposed on a message board in 2000 by Bram CohenLater, Vaikunthan and Brakerski proved that one can construct non-bootstrappable homomorphic encryption schemes [2] that base their security on stronger, post-quantum problems (e.g. Learning with Errors). This scheme appears to be more practical than the original bootstrapping schemes (although it still uses bootstrapping as 'an optimization').  A nice non-technical summary can be found on the MIT tech review [3] [0] Craig Gentry's PhD Thesis[1] Page on microsoft.com[2] Page on iacr.org[3] Homomorphic Encryption - MIT Technology Review"", 'aiModelVersion': '1'}",0.9986
Adrián Lamo,8y,What is the most interesting attack made on a website?,"While not specifically a website hack, probably the most impressive Internet-related hack I can think of involved was ""Internet Census 2012"", performed by a security researcher/hacker took over many, many embedded devices in order to create what may have been the most detailed map of the Internet up to that date. The distribution of the affected devices looked like this:


The effort was simple yet breathtaking. Many Internet-enabled devices such as routers, uninterruptible power supplies, printers, and more esoteric gadgets support a remote command shell and are left with default passwords (or no passwords) in their final deployment. The person behind this effort harnessed them into a botnet, but unlike most botnets, which are often used for spamming or Distributed Denial of Service (DDoS) attacks, these were used to send probes that were heard around the world, creating maps of Internet topography, including this one:


All in all, the botnet involved spanned over 420,000 devices, and this wasn't even the upper limit to the number which could have been taken over. When they were done, the researcher quietly deleted his or her code from the affected systems, left them intact with the passwords unchanged, published their work, and went about their business.

They could have just as easily used this resource for any number of malicious and/or self-enriching purposes. Instead, they were satisfied by performing valuable research which might not have been possible any other way. I understand that some people might have ethical problems with the way the research was conducted - it was, after all, unarguably illegal - but I have to have respect for someone who, given a choice, does a productive thing instead of a self-interested or harmful one.

You can read more about the methodology and findings which resulted, at: Internet Census 2012
. Thanks for the A2A!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/9b2conisk8t4mj1x', 'title': 'What is the most interesting attack made on a website?', 'score': {'original': 0.9993, 'ai': 0.0007}, 'blocks': [{'text': 'While not specifically a website hack, probably the most impressive Internet-related hack I can think of involved was ""Internet Census 2012"", performed by a security researcher/hacker took over many, many embedded devices in order to create what may have been the most detailed map of the Internet up to that date. The distribution of the affected devices looked like this:\n\n\nThe effort was simple yet breathtaking. Many Internet-enabled devices such as routers, uninterruptible power supplies, printers, and more esoteric gadgets support a remote command shell and are left with default passwords (or no passwords) in their final deployment. The person behind this effort harnessed them into a botnet, but unlike most botnets, which are often used for spamming or Distributed Denial of Service (DDoS) attacks, these were used to send probes that were heard around the world, creating maps of Internet topography, including this one:\n\n\nAll in all, the botnet involved spanned over 420,000 devices, and this wasn\'t even the upper limit to the number which could have been taken over. When they were done, the researcher quietly deleted his or her code from the affected systems, left them intact with the passwords unchanged, published their work, and went about their business.\n\nThey could have just as easily used this resource for any number of malicious and/or self-enriching purposes. Instead, they were satisfied by performing valuable research which might not have been possible any other way. I understand that some people might have ethical problems with the way the research was conducted - it was, after all, unarguably illegal - but I have to have respect for someone who, given a choice, does a productive thing instead of a self-interested or harmful one.\n\nYou can read more about the methodology and findings which resulted, at: Internet Census 2012\n. Thanks for the A2A!', 'result': {'fake': 0.0008, 'real': 0.9992}, 'status': 'success'}], 'credits_used': 4, 'credits': 1985849, 'subscription': 0, 'content': 'While not specifically a website hack, probably the most impressive Internet-related hack I can think of involved was ""Internet Census 2012"", performed by a security researcher/hacker took over many, many embedded devices in order to create what may have been the most detailed map of the Internet up to that date. The distribution of the affected devices looked like this:\n\n\nThe effort was simple yet breathtaking. Many Internet-enabled devices such as routers, uninterruptible power supplies, printers, and more esoteric gadgets support a remote command shell and are left with default passwords (or no passwords) in their final deployment. The person behind this effort harnessed them into a botnet, but unlike most botnets, which are often used for spamming or Distributed Denial of Service (DDoS) attacks, these were used to send probes that were heard around the world, creating maps of Internet topography, including this one:\n\n\nAll in all, the botnet involved spanned over 420,000 devices, and this wasn\'t even the upper limit to the number which could have been taken over. When they were done, the researcher quietly deleted his or her code from the affected systems, left them intact with the passwords unchanged, published their work, and went about their business.\n\nThey could have just as easily used this resource for any number of malicious and/or self-enriching purposes. Instead, they were satisfied by performing valuable research which might not have been possible any other way. I understand that some people might have ethical problems with the way the research was conducted - it was, after all, unarguably illegal - but I have to have respect for someone who, given a choice, does a productive thing instead of a self-interested or harmful one.\n\nYou can read more about the methodology and findings which resulted, at: Internet Census 2012\n. Thanks for the A2A!', 'aiModelVersion': '1'}",0.9993
Ken Gregg,Updated 4y,Why are companies using old software on their computers?,"I got a call the other day about adding a feature to some custom software I developed in 1984. Why is my client still using this software, after more than 35 years? Because it works. It does the job. It fulfills their requirements, and their requirements almost never change. There is no commercial software available to perform this task, and if there were, the market would be so small that the software would likely be expensive.

Now, why do some businesses use old commercial software? Here are a few reasons:

There are no newer versions available. Software vendors drop products or go out of business, so no new versions of a specific software product will ever come out. If the software works and does what you need it to do, and still works under modern, secure operating systems, there is no reason not to continue to use it. The alternative is costly custom development.
There are newer versions, but there are no compelling new features that would benefit this particular business. Sometimes, the bells and whistles added to a new version aren’t all that useful to certain classes of users. So, the users have no compelling reason to go though the time and cost to upgrade.
The older pricing model is better for this particular business. For example, let’s say the version they currently use was a one-time purchase, with no recurring fees, but newer versions require monthly or annual fees in a subscription model. The business might not want a subscription model, so they stick with the version they have already paid for in full. (An example of this is Adobe Creative Suite. Version CS6 was the last version that could be purchased outright for a one-time price. All subsequent versions, known as Adobe Creative Cloud, are sold only under a subscription model, requiring monthly or annual fees just to keep the software updated and functioning. Cancel the subscription, and the software no longer functions.)
The older licensing terms are better for this particular business. In licensing terms for a new version have changed, the business might prefer the old terms associated with the old software, and thus stick with the old software.
Prices of newer versions are too high. Business need to watch their expenses, if they expect to stay in business. If software prices on newer versions are too high, the business might opt to stick with the older version, to control their software costs.
The business might have a maintenance agreement with the software vendor, which allows them to freeze on whatever version they wish, and still receive support.
The costs of retraining personnel might be significantly high, if the business switched to a newer version. If many employees use the software, and retraining would be required to keep people productive on a new version, the training costs might be prohibitive.

Of course, there are sometimes compelling reasons to move to a new version. For example, if the old version will no longer receive any security updates, the risks of continuing to use the old version increase over time.

Compatibility with modern operating system versions can be another reason to upgrade. But it’s also the reason why 32-bit versions of Windows still provide compatibility for some (now ancient) 16-bit applications.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/yfr9xvgza6j5sok0', 'title': 'Why are companies using old software on their computers?', 'score': {'original': 0.50145, 'ai': 0.49855}, 'blocks': [{'text': 'I got a call the other day about adding a feature to some custom software I developed in 1984. Why is my client still using this software, after more than 35 years? Because it works. It does the job. It fulfills their requirements, and their requirements almost never change. There is no commercial software available to perform this task, and if there were, the market would be so small that the software would likely be expensive.\n\nNow, why do some businesses use old commercial software? Here are a few reasons:\n\nThere are no newer versions available. Software vendors drop products or go out of business, so no new versions of a specific software product will ever come out. If the software works and does what you need it to do, and still works under modern, secure operating systems, there is no reason not to continue to use it. The alternative is costly custom development.\nThere are newer versions, but there are no compelling new features that would benefit this particular business. Sometimes, the bells and whistles added to a new version aren’t all that useful to certain classes of users. So, the users have no compelling reason to go though the time and cost to upgrade.\nThe older pricing model is better for this particular business. For example, let’s say the version they currently use was a one-time purchase, with no recurring fees, but newer versions require monthly or annual fees in a subscription model. The business might not want a subscription model, so they stick with the version they have already paid for in full. (An example of this is Adobe Creative Suite. Version CS6 was the last version that could be purchased outright for a one-time price. All subsequent versions, known as Adobe Creative Cloud, are sold only under a subscription model, requiring monthly or annual fees just to keep the software updated and functioning. Cancel the subscription, and the software no longer functions.)\nThe older licensing terms are better for this particular business. In licensing terms for a new version have changed, the business might prefer the old terms associated with the old software, and thus stick with the old software.\nPrices of newer versions are too high. Business need to watch their expenses, if they expect to stay in business. If software prices on newer versions are too high, the business might opt to stick with the older version, to control their software costs.\nThe business might have a maintenance agreement with the software vendor, which allows them to freeze on whatever version they wish, and still receive support.\nThe costs of retraining personnel might be significantly high, if the business switched to a newer version. If many employees use the software, and retraining would be required to keep people productive on a new version, the training costs might be prohibitive.\n\nOf course, there are sometimes compelling reasons to move to a new version. For example, if the old version will no longer receive any security updates, the risks of continuing to use the old version increase over time.\n\nCompatibility with modern operating system versions can be another', 'result': {'fake': 0.0011, 'real': 0.9989}, 'status': 'success'}, {'text': 'reason to upgrade. But it’s also the reason why 32-bit versions of Windows still provide compatibility for some (now ancient) 16-bit applications.', 'result': {'fake': 0.9994, 'real': 0.0006}, 'status': 'success'}], 'credits_used': 6, 'credits': 1985843, 'subscription': 0, 'content': 'I got a call the other day about adding a feature to some custom software I developed in 1984. Why is my client still using this software, after more than 35 years? Because it works. It does the job. It fulfills their requirements, and their requirements almost never change. There is no commercial software available to perform this task, and if there were, the market would be so small that the software would likely be expensive.\n\nNow, why do some businesses use old commercial software? Here are a few reasons:\n\nThere are no newer versions available. Software vendors drop products or go out of business, so no new versions of a specific software product will ever come out. If the software works and does what you need it to do, and still works under modern, secure operating systems, there is no reason not to continue to use it. The alternative is costly custom development.\nThere are newer versions, but there are no compelling new features that would benefit this particular business. Sometimes, the bells and whistles added to a new version aren’t all that useful to certain classes of users. So, the users have no compelling reason to go though the time and cost to upgrade.\nThe older pricing model is better for this particular business. For example, let’s say the version they currently use was a one-time purchase, with no recurring fees, but newer versions require monthly or annual fees in a subscription model. The business might not want a subscription model, so they stick with the version they have already paid for in full. (An example of this is Adobe Creative Suite. Version CS6 was the last version that could be purchased outright for a one-time price. All subsequent versions, known as Adobe Creative Cloud, are sold only under a subscription model, requiring monthly or annual fees just to keep the software updated and functioning. Cancel the subscription, and the software no longer functions.)\nThe older licensing terms are better for this particular business. In licensing terms for a new version have changed, the business might prefer the old terms associated with the old software, and thus stick with the old software.\nPrices of newer versions are too high. Business need to watch their expenses, if they expect to stay in business. If software prices on newer versions are too high, the business might opt to stick with the older version, to control their software costs.\nThe business might have a maintenance agreement with the software vendor, which allows them to freeze on whatever version they wish, and still receive support.\nThe costs of retraining personnel might be significantly high, if the business switched to a newer version. If many employees use the software, and retraining would be required to keep people productive on a new version, the training costs might be prohibitive.\n\nOf course, there are sometimes compelling reasons to move to a new version. For example, if the old version will no longer receive any security updates, the risks of continuing to use the old version increase over time.\n\nCompatibility with modern operating system versions can be another reason to upgrade. But it’s also the reason why 32-bit versions of Windows still provide compatibility for some (now ancient) 16-bit applications.', 'aiModelVersion': '1'}",0.50145
Kiran Kannar,Updated 5y,What are the dangers of using machine learning libraries without any understanding?,"Here’s a picture of a cat right? Google’s Inception model thinks it’s a guacamole. As much as the image looks like a cat, the image is digitally altered which confused the model.

Slightly rotating the image led the model to correctly classify the image as a cat (and as an animal).

The above image is what’s called as an adversarial image, trying to fool your model into thinking the image is something you want it to be instead of what the image actually is.

I bring this example to illustrate the problem with the current AI trend. We have many technologies that allow us to use AI/ML (and not just deep learning) as a blackbox. The real danger is in the application, especially in healthcare and defense. For example, how would you convince that your model for predicting cancer actually works? How do you know your model is not susceptible to noise? How do you know that your model has actually learnt what it is supposed to be learning? How do you actually read interpretability here? If you can’t interpret what the model has learnt, then you can’t sell it.

Images source: Google’s AI thinks this turtle looks like a gun, which is a problem

A lot of the discussion in the comments is primarily on the adversarial example and not exactly what I intended to point on— interpretability. You could take a simple problem and use a complex solution involving weeks of training. It may give you fairly good results on training data within the prototyping phase. But the questions to be asked are:

How would you now ensure that the results be consistent in application, without posing any vulnerabilities or risk?
Did you really need a AI/ML solution?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/spoj0lmt7ghuv42r', 'title': 'What are the dangers of using machine learning libraries without any understanding?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Here’s a picture of a cat right? Google’s Inception model thinks it’s a guacamole. As much as the image looks like a cat, the image is digitally altered which confused the model.\n\nSlightly rotating the image led the model to correctly classify the image as a cat (and as an animal).\n\nThe above image is what’s called as an adversarial image, trying to fool your model into thinking the image is something you want it to be instead of what the image actually is.\n\nI bring this example to illustrate the problem with the current AI trend. We have many technologies that allow us to use AI/ML (and not just deep learning) as a blackbox. The real danger is in the application, especially in healthcare and defense. For example, how would you convince that your model for predicting cancer actually works? How do you know your model is not susceptible to noise? How do you know that your model has actually learnt what it is supposed to be learning? How do you actually read interpretability here? If you can’t interpret what the model has learnt, then you can’t sell it.\n\nImages source: Google’s AI thinks this turtle looks like a gun, which is a problem\n\nA lot of the discussion in the comments is primarily on the adversarial example and not exactly what I intended to point on— interpretability. You could take a simple problem and use a complex solution involving weeks of training. It may give you fairly good results on training data within the prototyping phase. But the questions to be asked are:\n\nHow would you now ensure that the results be consistent in application, without posing any vulnerabilities or risk?\nDid you really need a AI/ML solution?', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985840, 'subscription': 0, 'content': 'Here’s a picture of a cat right? Google’s Inception model thinks it’s a guacamole. As much as the image looks like a cat, the image is digitally altered which confused the model.\n\nSlightly rotating the image led the model to correctly classify the image as a cat (and as an animal).\n\nThe above image is what’s called as an adversarial image, trying to fool your model into thinking the image is something you want it to be instead of what the image actually is.\n\nI bring this example to illustrate the problem with the current AI trend. We have many technologies that allow us to use AI/ML (and not just deep learning) as a blackbox. The real danger is in the application, especially in healthcare and defense. For example, how would you convince that your model for predicting cancer actually works? How do you know your model is not susceptible to noise? How do you know that your model has actually learnt what it is supposed to be learning? How do you actually read interpretability here? If you can’t interpret what the model has learnt, then you can’t sell it.\n\nImages source: Google’s AI thinks this turtle looks like a gun, which is a problem\n\nA lot of the discussion in the comments is primarily on the adversarial example and not exactly what I intended to point on— interpretability. You could take a simple problem and use a complex solution involving weeks of training. It may give you fairly good results on training data within the prototyping phase. But the questions to be asked are:\n\nHow would you now ensure that the results be consistent in application, without posing any vulnerabilities or risk?\nDid you really need a AI/ML solution?', 'aiModelVersion': '1'}",0.9998
Kiran Kannar,5y,"What screams ""I'm a Computer Science major""?","Wearing an Amazon T-shirt under a Salesforce hoodie, she is sitting in a cafe, oblivious of the world around her because of the Intuit earphones she won in the recent career fair. She sips hot coffee from a Coursera extra large mug, while she also has a Google hydroflask in her Apple backpack from her internship.

She uses a Teradata USB type C multi-port hub adaptor to connect her devices to her Macbook Pro. While the code is compiling, she uses a Facebook-branded pen to doodle data structures on a Google notepad for her project - something to do with tries and red-black trees, while she uses Code Hero marker to highlight on research papers. She keeps a Cisco stress ball with her to vent her frustration on.

She doesn’t have to worry about clothes much, because she is in the lab most of the time, or in front of the laptop 24x7. She has at least 7 T-shirts either from the companies she interned for or those she picked up at the career fair.

Despite the dark circles filling her eyes, her enthusiasm hasn’t waned. Her code is working. She is glad she finished a major part of the homework in parallel to her research project. She looks at her cracked iPhone for the day and date to ensure for the hundredth time that she didn’t miss the next one of the several deadlines she has to manage. Despite this, she forgets what day it is.

She sometimes dozes off in class, but that’s where podcasts help. She is anyway taking more classes than she can easily manage, not because she likes a challenge, but because she wants to graduate as soon as possible.

The freebies and swags help. The coffee helps. In the end, the dark circles are worth it, as she will soon be shifting to SF/Seattle/New York/equivalent silicon valleys for a new life of high-paying jobs and tons of taxes.

She is awesome!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/m0g9rt5lbkd6aco1', 'title': 'What screams ""I\'m a Computer Science major""?', 'score': {'original': 0.9867, 'ai': 0.0133}, 'blocks': [{'text': 'Wearing an Amazon T-shirt under a Salesforce hoodie, she is sitting in a cafe, oblivious of the world around her because of the Intuit earphones she won in the recent career fair. She sips hot coffee from a Coursera extra large mug, while she also has a Google hydroflask in her Apple backpack from her internship.\n\nShe uses a Teradata USB type C multi-port hub adaptor to connect her devices to her Macbook Pro. While the code is compiling, she uses a Facebook-branded pen to doodle data structures on a Google notepad for her project - something to do with tries and red-black trees, while she uses Code Hero marker to highlight on research papers. She keeps a Cisco stress ball with her to vent her frustration on.\n\nShe doesn’t have to worry about clothes much, because she is in the lab most of the time, or in front of the laptop 24x7. She has at least 7 T-shirts either from the companies she interned for or those she picked up at the career fair.\n\nDespite the dark circles filling her eyes, her enthusiasm hasn’t waned. Her code is working. She is glad she finished a major part of the homework in parallel to her research project. She looks at her cracked iPhone for the day and date to ensure for the hundredth time that she didn’t miss the next one of the several deadlines she has to manage. Despite this, she forgets what day it is.\n\nShe sometimes dozes off in class, but that’s where podcasts help. She is anyway taking more classes than she can easily manage, not because she likes a challenge, but because she wants to graduate as soon as possible.\n\nThe freebies and swags help. The coffee helps. In the end, the dark circles are worth it, as she will soon be shifting to SF/Seattle/New York/equivalent silicon valleys for a new life of high-paying jobs and tons of taxes.\n\nShe is awesome!', 'result': {'fake': 0.0133, 'real': 0.9867}, 'status': 'success'}], 'credits_used': 4, 'credits': 1985836, 'subscription': 0, 'content': 'Wearing an Amazon T-shirt under a Salesforce hoodie, she is sitting in a cafe, oblivious of the world around her because of the Intuit earphones she won in the recent career fair. She sips hot coffee from a Coursera extra large mug, while she also has a Google hydroflask in her Apple backpack from her internship.\n\nShe uses a Teradata USB type C multi-port hub adaptor to connect her devices to her Macbook Pro. While the code is compiling, she uses a Facebook-branded pen to doodle data structures on a Google notepad for her project - something to do with tries and red-black trees, while she uses Code Hero marker to highlight on research papers. She keeps a Cisco stress ball with her to vent her frustration on.\n\nShe doesn’t have to worry about clothes much, because she is in the lab most of the time, or in front of the laptop 24x7. She has at least 7 T-shirts either from the companies she interned for or those she picked up at the career fair.\n\nDespite the dark circles filling her eyes, her enthusiasm hasn’t waned. Her code is working. She is glad she finished a major part of the homework in parallel to her research project. She looks at her cracked iPhone for the day and date to ensure for the hundredth time that she didn’t miss the next one of the several deadlines she has to manage. Despite this, she forgets what day it is.\n\nShe sometimes dozes off in class, but that’s where podcasts help. She is anyway taking more classes than she can easily manage, not because she likes a challenge, but because she wants to graduate as soon as possible.\n\nThe freebies and swags help. The coffee helps. In the end, the dark circles are worth it, as she will soon be shifting to SF/Seattle/New York/equivalent silicon valleys for a new life of high-paying jobs and tons of taxes.\n\nShe is awesome!', 'aiModelVersion': '1'}",0.9867
Michael Boyd,Updated 5y,Our CTO said bad code is sometimes better than elegant and scalable code because it saves time and money for the company. Is he right?,"In my first startup, we were asked to make a minimum awesome product in 3 months so our CEO could have something to show off while pitching to investors.

With about a month left to go until our deadline, my team realized we weren’t going to finish app the way we’d planned.

This is when we started writing “bad” code.

We switched from the ideal solution, and went with what the situation required.

Fortunately, it worked. Our CEO started pitching investors using our duck-taped, puttering application that worked seamlessly for the demo and hardly anything else.

Thanks to our bad code, after a couple months of pitching we got funded.

We said our hack would be temporary. We said we’d change it after we got money. Unsurprisingly, we didn’t.

The code just sits there like a wart in our application, doomed to live until another business need requires that we scrape it out.

In software, everything is a tradeoff.

I disagree with your CTO. Bad code DOES NOT save a company time and money. The code we’ve written is going to take 3–4x the time and money to rewrite.

But it kept us alive, and might be what you need to survive too. This is called technical debt. It’s a tool, and used wisely can be pretty powerful.

Good luck!

If you enjoyed this answer, follow me to read daily software engineering stories","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/wc12gkxtqurpe7af', 'title': 'Our CTO said bad code is sometimes better than elegant and scalable code because it saves time and money for the company. Is he right?', 'score': {'original': 0.9893, 'ai': 0.0107}, 'blocks': [{'text': 'In my first startup, we were asked to make a minimum awesome product in 3 months so our CEO could have something to show off while pitching to investors.\n\nWith about a month left to go until our deadline, my team realized we weren’t going to finish app the way we’d planned.\n\nThis is when we started writing “bad” code.\n\nWe switched from the ideal solution, and went with what the situation required.\n\nFortunately, it worked. Our CEO started pitching investors using our duck-taped, puttering application that worked seamlessly for the demo and hardly anything else.\n\nThanks to our bad code, after a couple months of pitching we got funded.\n\nWe said our hack would be temporary. We said we’d change it after we got money. Unsurprisingly, we didn’t.\n\nThe code just sits there like a wart in our application, doomed to live until another business need requires that we scrape it out.\n\nIn software, everything is a tradeoff.\n\nI disagree with your CTO. Bad code DOES NOT save a company time and money. The code we’ve written is going to take 3–4x the time and money to rewrite.\n\nBut it kept us alive, and might be what you need to survive too. This is called technical debt. It’s a tool, and used wisely can be pretty powerful.\n\nGood luck!\n\nIf you enjoyed this answer, follow me to read daily software engineering stories', 'result': {'fake': 0.0107, 'real': 0.9893}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985833, 'subscription': 0, 'content': 'In my first startup, we were asked to make a minimum awesome product in 3 months so our CEO could have something to show off while pitching to investors.\n\nWith about a month left to go until our deadline, my team realized we weren’t going to finish app the way we’d planned.\n\nThis is when we started writing “bad” code.\n\nWe switched from the ideal solution, and went with what the situation required.\n\nFortunately, it worked. Our CEO started pitching investors using our duck-taped, puttering application that worked seamlessly for the demo and hardly anything else.\n\nThanks to our bad code, after a couple months of pitching we got funded.\n\nWe said our hack would be temporary. We said we’d change it after we got money. Unsurprisingly, we didn’t.\n\nThe code just sits there like a wart in our application, doomed to live until another business need requires that we scrape it out.\n\nIn software, everything is a tradeoff.\n\nI disagree with your CTO. Bad code DOES NOT save a company time and money. The code we’ve written is going to take 3–4x the time and money to rewrite.\n\nBut it kept us alive, and might be what you need to survive too. This is called technical debt. It’s a tool, and used wisely can be pretty powerful.\n\nGood luck!\n\nIf you enjoyed this answer, follow me to read daily software engineering stories', 'aiModelVersion': '1'}",0.9893
Geoff Caplan,Updated 2y,How tedious was working with early computers?,"But they weren’t “early computers”. They were the latest thing and wildly exciting.

In the early ’70s I wrote my first programs on punch cards, as part of a school outreach initiative from a local University. A van turned up once a week to pick up our cards and deliver the most recent output.

It was an exciting introduction to a revolutionary technology, even though the Read-Eval-Print-Loop took 168 hours!

So getting my hands on my friend’s BBC Acorn with a tape drive was a huge step forwards. Nowadays we’d consider the load times glacial, but they were orders of magnitude quicker than accessing a remote mainframe. And we soon had floppy disk drives like these and things speeded up even more.

Then I set up my first business and laid out a king’s ransom on an Atari Mega ST, with an exotic 2 megs of RAM and extravagant 20 meg hard drive (from memory the drive alone cost £450 which was serious moolah in those days).

What you probably don’t realise is that this was a highly functional machine. Back then, developers knew how to code for limited resources, and it ran like the wind.

When I saw a demo of Windows 1 at a show it appeared laughably sluggish - the Amiga, Apricot, Atari and Mac were far superior - particularly the Apricot, which was to die for…

No edition of Windows has approached the responsiveness of any of these machines - they were a joy to use. I was wildly in love with my Atari. If anything, the user experience has gone backwards. The dominance of Windows is a triumph of marketing, not of technology.

My little Atari marvel ran a spreadsheet, accounting program, SQL database, FTP, Telnet, email, bulletin boards, the best WP I’ve ever used, a highly capable DTP package, vector and bitmap graphics packages, games, MIDI music… And there was good choice of compilers - I was able to play with Assembler, Basic and C.

With the exception of the WWW, which hadn’t been invented, my mid-80s Atari could do pretty much everything I do today. Since then improvement has been incremental rather than revolutionary, and today’s bloated software actually runs slower, despite my high-end HP Workstation.

And then I managed to blag a cheap ex-display NextStation - Steve Jobs’s wet dream and probably the sexiest machine ever built. It was so far ahead of its time it was mind-blowing.

Did I feel deprived? What do you think? I thought I had entered computer heaven.

Then one fine day I was browsing the CERN ftp site - they acted as a repository for the NextStep community - and spotted a mysterious folder called “WWW”.

I downloaded a file named something like WorldWideWeb-0.2 and fired up a prototype web browser. It was just a proof-of-concept, so there wasn’t a public web to access at that stage. But the web was developed on the NextStation, and we were ahead of the curve. So the first point of presence that opened in the UK we shared the requisite Perl scripts and logged on. And on 1 June 1992 I was one of the first people in the word to load up a website…

I was on Yahoo the day it launched. And Alta Vista. And Amazon. And Google…

Does this sound “tedious” to you?

These pioneering days are long gone. Nowadays I’m using a 10 year old workstation that’s more than I really need. With the exception of video streaming and video calls, nothing much has changed for a decade or two and progress has slowed to a crawl.

The current generation have missed out on the real excitement.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/njd31uro6xfzimc4', 'title': 'How tedious was working with early computers?', 'score': {'original': 0.5153, 'ai': 0.4847}, 'blocks': [{'text': 'But they weren’t “early computers”. They were the latest thing and wildly exciting.\n\nIn the early ’70s I wrote my first programs on punch cards, as part of a school outreach initiative from a local University. A van turned up once a week to pick up our cards and deliver the most recent output.\n\nIt was an exciting introduction to a revolutionary technology, even though the Read-Eval-Print-Loop took 168 hours!\n\nSo getting my hands on my friend’s BBC Acorn with a tape drive was a huge step forwards. Nowadays we’d consider the load times glacial, but they were orders of magnitude quicker than accessing a remote mainframe. And we soon had floppy disk drives like these and things speeded up even more.\n\nThen I set up my first business and laid out a king’s ransom on an Atari Mega ST, with an exotic 2 megs of RAM and extravagant 20 meg hard drive (from memory the drive alone cost £450 which was serious moolah in those days).\n\nWhat you probably don’t realise is that this was a highly functional machine. Back then, developers knew how to code for limited resources, and it ran like the wind.\n\nWhen I saw a demo of Windows 1 at a show it appeared laughably sluggish - the Amiga, Apricot, Atari and Mac were far superior - particularly the Apricot, which was to die for…\n\nNo edition of Windows has approached the responsiveness of any of these machines - they were a joy to use. I was wildly in love with my Atari. If anything, the user experience has gone backwards. The dominance of Windows is a triumph of marketing, not of technology.\n\nMy little Atari marvel ran a spreadsheet, accounting program, SQL database, FTP, Telnet, email, bulletin boards, the best WP I’ve ever used, a highly capable DTP package, vector and bitmap graphics packages, games, MIDI music… And there was good choice of compilers - I was able to play with Assembler, Basic and C.\n\nWith the exception of the WWW, which hadn’t been invented, my mid-80s Atari could do pretty much everything I do today. Since then improvement has been incremental rather than revolutionary, and today’s bloated software actually runs slower, despite my high-end HP Workstation.\n\nAnd then I managed to blag a cheap ex-display NextStation - Steve Jobs’s wet dream and probably the sexiest machine ever built. It was so far ahead of its time it was mind-blowing.\n\nDid I feel deprived? What do you think? I thought I had entered computer heaven.\n\nThen one fine day I was browsing the CERN ftp site - they acted as a repository for the NextStep community - and spotted a mysterious folder called “WWW”.\n\nI downloaded a file named something like WorldWideWeb-0.2 and fired up a prototype web browser. It was just a proof-of-concept, so there wasn’t a public web to access at that stage. But the web was developed on the NextStation, and we were ahead of the curve. So the first point of presence that opened in the UK we shared the requisite Perl scripts and logged on. And on 1 June 1992 I was one of', 'result': {'fake': 0.0094, 'real': 0.9906}, 'status': 'success'}, {'text': 'the first people in the word to load up a website…\n\nI was on Yahoo the day it launched. And Alta Vista. And Amazon. And Google…\n\nDoes this sound “tedious” to you?\n\nThese pioneering days are long gone. Nowadays I’m using a 10 year old workstation that’s more than I really need. With the exception of video streaming and video calls, nothing much has changed for a decade or two and progress has slowed to a crawl.\n\nThe current generation have missed out on the real excitement.', 'result': {'fake': 0.7675, 'real': 0.2325}, 'status': 'success'}], 'credits_used': 7, 'credits': 1985826, 'subscription': 0, 'content': 'But they weren’t “early computers”. They were the latest thing and wildly exciting.\n\nIn the early ’70s I wrote my first programs on punch cards, as part of a school outreach initiative from a local University. A van turned up once a week to pick up our cards and deliver the most recent output.\n\nIt was an exciting introduction to a revolutionary technology, even though the Read-Eval-Print-Loop took 168 hours!\n\nSo getting my hands on my friend’s BBC Acorn with a tape drive was a huge step forwards. Nowadays we’d consider the load times glacial, but they were orders of magnitude quicker than accessing a remote mainframe. And we soon had floppy disk drives like these and things speeded up even more.\n\nThen I set up my first business and laid out a king’s ransom on an Atari Mega ST, with an exotic 2 megs of RAM and extravagant 20 meg hard drive (from memory the drive alone cost £450 which was serious moolah in those days).\n\nWhat you probably don’t realise is that this was a highly functional machine. Back then, developers knew how to code for limited resources, and it ran like the wind.\n\nWhen I saw a demo of Windows 1 at a show it appeared laughably sluggish - the Amiga, Apricot, Atari and Mac were far superior - particularly the Apricot, which was to die for…\n\nNo edition of Windows has approached the responsiveness of any of these machines - they were a joy to use. I was wildly in love with my Atari. If anything, the user experience has gone backwards. The dominance of Windows is a triumph of marketing, not of technology.\n\nMy little Atari marvel ran a spreadsheet, accounting program, SQL database, FTP, Telnet, email, bulletin boards, the best WP I’ve ever used, a highly capable DTP package, vector and bitmap graphics packages, games, MIDI music… And there was good choice of compilers - I was able to play with Assembler, Basic and C.\n\nWith the exception of the WWW, which hadn’t been invented, my mid-80s Atari could do pretty much everything I do today. Since then improvement has been incremental rather than revolutionary, and today’s bloated software actually runs slower, despite my high-end HP Workstation.\n\nAnd then I managed to blag a cheap ex-display NextStation - Steve Jobs’s wet dream and probably the sexiest machine ever built. It was so far ahead of its time it was mind-blowing.\n\nDid I feel deprived? What do you think? I thought I had entered computer heaven.\n\nThen one fine day I was browsing the CERN ftp site - they acted as a repository for the NextStep community - and spotted a mysterious folder called “WWW”.\n\nI downloaded a file named something like WorldWideWeb-0.2 and fired up a prototype web browser. It was just a proof-of-concept, so there wasn’t a public web to access at that stage. But the web was developed on the NextStation, and we were ahead of the curve. So the first point of presence that opened in the UK we shared the requisite Perl scripts and logged on. And on 1 June 1992 I was one of the first people in the word to load up a website…\n\nI was on Yahoo the day it launched. And Alta Vista. And Amazon. And Google…\n\nDoes this sound “tedious” to you?\n\nThese pioneering days are long gone. Nowadays I’m using a 10 year old workstation that’s more than I really need. With the exception of video streaming and video calls, nothing much has changed for a decade or two and progress has slowed to a crawl.\n\nThe current generation have missed out on the real excitement.', 'aiModelVersion': '1'}",0.5153
Shawn Burton,Updated 3y,How long can a computer last?,"Me too, I can also say that computers can last a long time. This is a recent picture that I took of my Toshiba Satellite 330cds laptop from 1998. Even now in 2020 at almost 22 years old, this laptop still runs fine today with its original 4.1 GB hard drive and has a built in 3.5″ floppy drive and CD-ROM drive which all work fine and its running Windows 98 second edition. Granted you wont be able to use Facebook or YouTube on this old of a laptop, but none of that existed in 1998. As you can see, I was able to get a wi-fi card running on this laptop and using Internet Explorer 5.5 that I have on here, I can actually do some very limited web browsing such as checking the weather as you can see here and also simple Google searches and read some articles, but thats about it. Other than that, for office work, I have installed Office 97 which still works fine and also when I ran a scandisk on its hard drive, it still shows no bad sectors which is good after 22 years. So all in all, PCs can last a very long time if they aren’t treated too roughly. Its specs are that it has a 266 mhz Intel Pentium CPU with MMx with 96 MB of ram, a 4.1 GB hard drive, and Windows 98 second edition. To me, those specs would have actually been what a top end PC would have had in 1997 except, it would have had the newer version of Windows 95.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/d5hscvrbfj96g4pn', 'title': 'How long can a computer last?', 'score': {'original': 0.9988, 'ai': 0.0012}, 'blocks': [{'text': 'Me too, I can also say that computers can last a long time. This is a recent picture that I took of my Toshiba Satellite 330cds laptop from 1998. Even now in 2020 at almost 22 years old, this laptop still runs fine today with its original 4.1 GB hard drive and has a built in 3.5″ floppy drive and CD-ROM drive which all work fine and its running Windows 98 second edition. Granted you wont be able to use Facebook or YouTube on this old of a laptop, but none of that existed in 1998. As you can see, I was able to get a wi-fi card running on this laptop and using Internet Explorer 5.5 that I have on here, I can actually do some very limited web browsing such as checking the weather as you can see here and also simple Google searches and read some articles, but thats about it. Other than that, for office work, I have installed Office 97 which still works fine and also when I ran a scandisk on its hard drive, it still shows no bad sectors which is good after 22 years. So all in all, PCs can last a very long time if they aren’t treated too roughly. Its specs are that it has a 266 mhz Intel Pentium CPU with MMx with 96 MB of ram, a 4.1 GB hard drive, and Windows 98 second edition. To me, those specs would have actually been what a top end PC would have had in 1997 except, it would have had the newer version of Windows 95.', 'result': {'fake': 0.0012, 'real': 0.9988}, 'status': 'success'}], 'credits_used': 3, 'credits': 1985823, 'subscription': 0, 'content': 'Me too, I can also say that computers can last a long time. This is a recent picture that I took of my Toshiba Satellite 330cds laptop from 1998. Even now in 2020 at almost 22 years old, this laptop still runs fine today with its original 4.1 GB hard drive and has a built in 3.5″ floppy drive and CD-ROM drive which all work fine and its running Windows 98 second edition. Granted you wont be able to use Facebook or YouTube on this old of a laptop, but none of that existed in 1998. As you can see, I was able to get a wi-fi card running on this laptop and using Internet Explorer 5.5 that I have on here, I can actually do some very limited web browsing such as checking the weather as you can see here and also simple Google searches and read some articles, but thats about it. Other than that, for office work, I have installed Office 97 which still works fine and also when I ran a scandisk on its hard drive, it still shows no bad sectors which is good after 22 years. So all in all, PCs can last a very long time if they aren’t treated too roughly. Its specs are that it has a 266 mhz Intel Pentium CPU with MMx with 96 MB of ram, a 4.1 GB hard drive, and Windows 98 second edition. To me, those specs would have actually been what a top end PC would have had in 1997 except, it would have had the newer version of Windows 95.', 'aiModelVersion': '1'}",0.9988
