Authors,Time,Questions,Answers,api_response,originality_score
Matthew Bates,2y,Are you worried about the possible effects of artificial intelligence?,"Not so much for me, since I plan on not being around when the worst of it comes to pass, but for my kids and my students. By “worst of it,” I mean the lack of low-skill or even medium-skill career paths.

I often think about what jobs could be performed by a machine, if only we had slightly more intelligent machines. Driving will be the next big shoe to drop, once automated driving is perfected. Food and beverage prep will also be largely automated in my lifetime.

Currently, two of the best career paths for non-college graduates are delivery driver and barista. Those doors are closing rapidly, and none are opening to take their place.

Here is a painful truth that I’ve seen firsthand as a teacher: not every student is college material. In fact, I’d argue that the majority of students aren’t college material. The majority of students aren’t trade school material, either. There is a nontrivial percentage of people in the country whose only job skill is the ability to (usually) show up to work on time and follow some basic directions. And machines are very good at taking those types of jobs.

Think about this: Starbucks has almost 350,000 employees. Around here, they start at just under $16 an hour. It’s a great way for reliable, friendly people to make some money. The average Starbucks has around 85 employees.

Then this comes along:

A fully-automated Starbucks kiosk. Imagine if Starbucks could get that 85-employees-per-site average down to just five or so, just to make sure the machine was running properly. Starbucks’ owners (I am one… I have a dozen shares) would be thrilled! So would a lot of customers, who’d rather not talk to human anyway.

But that would be one less career path for the reliable, motivated, friendly, but otherwise unskilled crowd.

Improvements in A.I. are just going to make inequalities worse. The bell curve is turning into an “M” curve, and the people on the left hump of the “M” won’t sit back and accept their fate easily. Nor should they. There needs to be viable paths into the middle and upper classes for people of all ability levels. You should be able to work your way out of poverty, but A.I. makes that more difficult with every passing advancement.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0m7x5vatdprg9w8j', 'title': 'Are you worried about the possible effects of artificial intelligence?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'Not so much for me, since I plan on not being around when the worst of it comes to pass, but for my kids and my students. By “worst of it,” I mean the lack of low-skill or even medium-skill career paths.\n\nI often think about what jobs could be performed by a machine, if only we had slightly more intelligent machines. Driving will be the next big shoe to drop, once automated driving is perfected. Food and beverage prep will also be largely automated in my lifetime.\n\nCurrently, two of the best career paths for non-college graduates are delivery driver and barista. Those doors are closing rapidly, and none are opening to take their place.\n\nHere is a painful truth that I’ve seen firsthand as a teacher: not every student is college material. In fact, I’d argue that the majority of students aren’t college material. The majority of students aren’t trade school material, either. There is a nontrivial percentage of people in the country whose only job skill is the ability to (usually) show up to work on time and follow some basic directions. And machines are very good at taking those types of jobs.\n\nThink about this: Starbucks has almost 350,000 employees. Around here, they start at just under $16 an hour. It’s a great way for reliable, friendly people to make some money. The average Starbucks has around 85 employees.\n\nThen this comes along:\n\nA fully-automated Starbucks kiosk. Imagine if Starbucks could get that 85-employees-per-site average down to just five or so, just to make sure the machine was running properly. Starbucks’ owners (I am one… I have a dozen shares) would be thrilled! So would a lot of customers, who’d rather not talk to human anyway.\n\nBut that would be one less career path for the reliable, motivated, friendly, but otherwise unskilled crowd.\n\nImprovements in A.I. are just going to make inequalities worse. The bell curve is turning into an “M” curve, and the people on the left hump of the “M” won’t sit back and accept their fate easily. Nor should they. There needs to be viable paths into the middle and upper classes for people of all ability levels. You should be able to work your way out of poverty, but A.I. makes that more difficult with every passing advancement.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 4, 'credits': 1983015, 'subscription': 0, 'content': 'Not so much for me, since I plan on not being around when the worst of it comes to pass, but for my kids and my students. By “worst of it,” I mean the lack of low-skill or even medium-skill career paths.\n\nI often think about what jobs could be performed by a machine, if only we had slightly more intelligent machines. Driving will be the next big shoe to drop, once automated driving is perfected. Food and beverage prep will also be largely automated in my lifetime.\n\nCurrently, two of the best career paths for non-college graduates are delivery driver and barista. Those doors are closing rapidly, and none are opening to take their place.\n\nHere is a painful truth that I’ve seen firsthand as a teacher: not every student is college material. In fact, I’d argue that the majority of students aren’t college material. The majority of students aren’t trade school material, either. There is a nontrivial percentage of people in the country whose only job skill is the ability to (usually) show up to work on time and follow some basic directions. And machines are very good at taking those types of jobs.\n\nThink about this: Starbucks has almost 350,000 employees. Around here, they start at just under $16 an hour. It’s a great way for reliable, friendly people to make some money. The average Starbucks has around 85 employees.\n\nThen this comes along:\n\nA fully-automated Starbucks kiosk. Imagine if Starbucks could get that 85-employees-per-site average down to just five or so, just to make sure the machine was running properly. Starbucks’ owners (I am one… I have a dozen shares) would be thrilled! So would a lot of customers, who’d rather not talk to human anyway.\n\nBut that would be one less career path for the reliable, motivated, friendly, but otherwise unskilled crowd.\n\nImprovements in A.I. are just going to make inequalities worse. The bell curve is turning into an “M” curve, and the people on the left hump of the “M” won’t sit back and accept their fate easily. Nor should they. There needs to be viable paths into the middle and upper classes for people of all ability levels. You should be able to work your way out of poverty, but A.I. makes that more difficult with every passing advancement.', 'aiModelVersion': '1'}",0.9997
Imtiaz Mohammad,4y,How can you set yourself apart when everyone is doing machine learning or data science in 2019?,"Great question.

There are some excellent videos on YouTube by 3Blue1Brown on Machine Learning and other areas. Study the four images below and see if you observe something.

There are 4 chapters dedicated to Deep Learning. At this point

First chapter has 4M views
Second chapter has 1.5M views
Third chapter has 1M views
Fourth chapter has 0.6M views

Clearly, as the content got deeper into algorithms (chapters 2 & 3) and mathematics (chapter 4), most viewers simply disappeared.

Most students of hard sciences are like that. They scratch the surface and move on. They offer

no competition to the small set of dedicated students.
no value to the employers.

If you want to stand apart, have the perseverance to drill deeper into the hard math that enables Machine Learning.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/6f0upst1mlqnixyj', 'title': 'How can you set yourself apart when everyone is doing machine learning or data science in 2019?', 'score': {'original': 0.9976, 'ai': 0.0024}, 'blocks': [{'text': 'Great question.\n\nThere are some excellent videos on YouTube by 3Blue1Brown on Machine Learning and other areas. Study the four images below and see if you observe something.\n\nThere are 4 chapters dedicated to Deep Learning. At this point\n\nFirst chapter has 4M views\nSecond chapter has 1.5M views\nThird chapter has 1M views\nFourth chapter has 0.6M views\n\nClearly, as the content got deeper into algorithms (chapters 2 & 3) and mathematics (chapter 4), most viewers simply disappeared.\n\nMost students of hard sciences are like that. They scratch the surface and move on. They offer\n\nno competition to the small set of dedicated students.\nno value to the employers.\n\nIf you want to stand apart, have the perseverance to drill deeper into the hard math that enables Machine Learning.', 'result': {'fake': 0.0022, 'real': 0.9978}, 'status': 'success'}], 'credits_used': 2, 'credits': 1983013, 'subscription': 0, 'content': 'Great question.\n\nThere are some excellent videos on YouTube by 3Blue1Brown on Machine Learning and other areas. Study the four images below and see if you observe something.\n\nThere are 4 chapters dedicated to Deep Learning. At this point\n\nFirst chapter has 4M views\nSecond chapter has 1.5M views\nThird chapter has 1M views\nFourth chapter has 0.6M views\n\nClearly, as the content got deeper into algorithms (chapters 2 & 3) and mathematics (chapter 4), most viewers simply disappeared.\n\nMost students of hard sciences are like that. They scratch the surface and move on. They offer\n\nno competition to the small set of dedicated students.\nno value to the employers.\n\nIf you want to stand apart, have the perseverance to drill deeper into the hard math that enables Machine Learning.', 'aiModelVersion': '1'}",0.9976
Alan Mellor,5y,Why is there a sudden craze of programmers with little math background jumping onto machine learning which requires a much different skill set than traditional programming?,"Fashion. The promise of high pay. Misunderstanding what development is.

We all start programming thinking about mastering a language, maybe using a design pattern only clever people know.

Then our first real work shows us a whole new vista.

The language is the easy bit. The problem domain is hard.

I’ve programmed industrial controllers. I can now talk about factories, and PID control, and PLCs and acceleration of fragile goods.

I worked in PC games. I can talk about rigid body dynamics, matrix normalisation, quaternions. A bit.

I worked in marketing automation. I can talk about sales funnels, double opt in, transactional emails, drip feeds.

I worked in mobile games. I can talk about level design. Of one way systems to force player flow. Of stepped reward systems.

Do you see that we have to learn about the business we code for?

Code is literally nothing. Language nothing. Tech stack nothing. Nobody gives a monkeys, we can all do that.

To write a real app, you have to understand why it will succeed. What problem it solves. How it relates to the real world. Understand the domain, in other words.

But when you start out, you don’t know that.

So newbs today see ML is trendy and think I know, I will ‘master Python’. ML uses Python, so that will make me good at ML.

They are wrong. But I don’t blame them - they are new.

In time, they will learn that the maths in this field is important, more than the code.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/jvkxazlqr627fwdg', 'title': 'Why is there a sudden craze of programmers with little math background jumping onto machine learning which requires a much different skill set than traditional programming?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Fashion. The promise of high pay. Misunderstanding what development is.\n\nWe all start programming thinking about mastering a language, maybe using a design pattern only clever people know.\n\nThen our first real work shows us a whole new vista.\n\nThe language is the easy bit. The problem domain is hard.\n\nI’ve programmed industrial controllers. I can now talk about factories, and PID control, and PLCs and acceleration of fragile goods.\n\nI worked in PC games. I can talk about rigid body dynamics, matrix normalisation, quaternions. A bit.\n\nI worked in marketing automation. I can talk about sales funnels, double opt in, transactional emails, drip feeds.\n\nI worked in mobile games. I can talk about level design. Of one way systems to force player flow. Of stepped reward systems.\n\nDo you see that we have to learn about the business we code for?\n\nCode is literally nothing. Language nothing. Tech stack nothing. Nobody gives a monkeys, we can all do that.\n\nTo write a real app, you have to understand why it will succeed. What problem it solves. How it relates to the real world. Understand the domain, in other words.\n\nBut when you start out, you don’t know that.\n\nSo newbs today see ML is trendy and think I know, I will ‘master Python’. ML uses Python, so that will make me good at ML.\n\nThey are wrong. But I don’t blame them - they are new.\n\nIn time, they will learn that the maths in this field is important, more than the code.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1983010, 'subscription': 0, 'content': 'Fashion. The promise of high pay. Misunderstanding what development is.\n\nWe all start programming thinking about mastering a language, maybe using a design pattern only clever people know.\n\nThen our first real work shows us a whole new vista.\n\nThe language is the easy bit. The problem domain is hard.\n\nI’ve programmed industrial controllers. I can now talk about factories, and PID control, and PLCs and acceleration of fragile goods.\n\nI worked in PC games. I can talk about rigid body dynamics, matrix normalisation, quaternions. A bit.\n\nI worked in marketing automation. I can talk about sales funnels, double opt in, transactional emails, drip feeds.\n\nI worked in mobile games. I can talk about level design. Of one way systems to force player flow. Of stepped reward systems.\n\nDo you see that we have to learn about the business we code for?\n\nCode is literally nothing. Language nothing. Tech stack nothing. Nobody gives a monkeys, we can all do that.\n\nTo write a real app, you have to understand why it will succeed. What problem it solves. How it relates to the real world. Understand the domain, in other words.\n\nBut when you start out, you don’t know that.\n\nSo newbs today see ML is trendy and think I know, I will ‘master Python’. ML uses Python, so that will make me good at ML.\n\nThey are wrong. But I don’t blame them - they are new.\n\nIn time, they will learn that the maths in this field is important, more than the code.', 'aiModelVersion': '1'}",0.9998
Jean-Marie Valheur,2y,Who is your favorite historical figure and why?,"I’ve always been both impressed and terrified at the same time by Roman general, consul and dictator Lucius Cornelius Sulla. His rise to power and the boldness of his decisions set the stage for the end of the Roman republic and would later inspire Julius Caesar in his crossing of the Rubicon. A man whose epitaph read only: “No better friend, no worse enemy”.

Sulla was an odd character — as a young boy, his father died, his mother abandoned him and he was left alone in the care of a rich but distant stepmother. In the end, he had to fend entirely for himself. So he disappeared into the lower classes of Roman life. The shady, criminal, debauched places. Some say he made his money as a gigolo. Others say he was a pimp.

Fact remains, Sulla was popular. A great lover with little to no moral qualms, his best friend were actors — then considered the lowest of the low! — crossdressers, prostitutes and swindlers. He was reportedly exceptionally handsome and well-endowed, his face strikingly beautiful with the palest of skins and a thick mop of golden-red hair. In his early twenties, he managed to inherit money from both his stepmother and a rich courtesan, rumor had it he seduced both to secure the fortune. He then entered the political arena.

Sulla allied himself with the famous Roman general Gaius Marius. He started a military campaign, joined the senate and marched with Marius to Gaul. He fought the barbarians for years. And managed to make a name for himself. When one day he achieved the highest and loftiest of positions any Roman could ever reach, that of consul, he was over the moon!

As an aside, here’s a little picture I found of someone reconstructing Marius and Sulla from their busts and descriptions. Old Sulla vaguely… reminds me of someone, hmmm. Anyway, Sulla becomes consul, he’s in his early fifties, and he gets the command against a worthy adversary, Mithridates of Pontus. He deserves it, too — he has managed to squish the Italian rebels that threatened the empire just months earlier. Marius, meanwhile, cannot handle that he is no longer the top dog. So he stages a coup in the senate through bribes and calling in favors, thus cheating his former officer of his rightful command.

Sulla is furious! How dare that old man steal his glory and attempt to gain the command for his own selfish means? He makes a powerful, and daring decision… he marches on to Rome, at the head of his five legions. Marches on, calls on to the senate and has his command reinstated. Forces Marius to flee with his tail between his legs and rides off on a glorious Pontic adventure into the sunset…

As Sulla is out, Marius and his allies once more return to Rome, granting Marius an unheard of seventh consulship that makes him into an eternal legend. By now the old general seems to have gone mad and he butchers many of Sulla’s allies, and pretty much everyone he holds a grudge against. He then dies of a stroke at the height of his power, only a few weeks into office. Soon, Sulla returns to the Italian peninsula, having kicked some serious Pontic ass, now once again marching onto Rome at the head of his armies. He meets what remains of the Marian forces outside the gates and decisively defeats them in battle.

Now he is the undisputed champion of Rome, Sulla once again marches in. Declares himself dictator of the city and takes full control. He massacres his enemies, and all those who betrayed or belittled him in spite of his decades of service to Rome. Nails their heads to the rostra, on the walls of the city and near its gates.

Once, people snickered at Lucius Cornelius Sulla. Now, soon, they snicker once again, as he starts wandering around the city like a sad old man, back crooked, without bodyguards protecting him. People assume the massacres might have been orchestrated by those next to him, maybe the now-old general was unaware? People began to let down their guard around him. He hears a lot of things he wasn’t meant to hear. He plays Rome and its citizens like a fiddle, revealing himself to still be strong, capable and decisive after a few weeks of gathering intel.

He acts with terrifying swiftness. Restores old laws, abolishes new ones. Limits the powers of the senate to take away a consul’s rightful command, reforms laws, tries, and succeeds, in rebuilding the economy, and manages to appoint people who are actually competent for the most part. After just two years in power, Sulla decided to retire from public life, surrendering his powers to the senate and retreating to his Italian villa. He left Rome with a full treasury and a lot of ruthlessly ambitious people without a head.

In old age, Sulla wrote his memoirs, and lived a debased and scandalous life, once more surrounded by the very people he started his early years with — pimps, actors, whores and swindlers. He shared a bedroom with his very young, very pregnant fifth wife and the Greek actor Metrobius, both of them his lovers, making no secret of the fact. He died shortly after finishing his memoirs. Rome’s biggest badboy made Mark Antony seem like a choirboy.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ujxnd7a09wf1vqpk', 'title': 'Who is your favorite historical figure and why?', 'score': {'original': 0.77, 'ai': 0.23}, 'blocks': [{'text': 'I’ve always been both impressed and terrified at the same time by Roman general, consul and dictator Lucius Cornelius Sulla. His rise to power and the boldness of his decisions set the stage for the end of the Roman republic and would later inspire Julius Caesar in his crossing of the Rubicon. A man whose epitaph read only: “No better friend, no worse enemy”.\n\nSulla was an odd character — as a young boy, his father died, his mother abandoned him and he was left alone in the care of a rich but distant stepmother. In the end, he had to fend entirely for himself. So he disappeared into the lower classes of Roman life. The shady, criminal, debauched places. Some say he made his money as a gigolo. Others say he was a pimp.\n\nFact remains, Sulla was popular. A great lover with little to no moral qualms, his best friend were actors — then considered the lowest of the low! — crossdressers, prostitutes and swindlers. He was reportedly exceptionally handsome and well-endowed, his face strikingly beautiful with the palest of skins and a thick mop of golden-red hair. In his early twenties, he managed to inherit money from both his stepmother and a rich courtesan, rumor had it he seduced both to secure the fortune. He then entered the political arena.\n\nSulla allied himself with the famous Roman general Gaius Marius. He started a military campaign, joined the senate and marched with Marius to Gaul. He fought the barbarians for years. And managed to make a name for himself. When one day he achieved the highest and loftiest of positions any Roman could ever reach, that of consul, he was over the moon!\n\nAs an aside, here’s a little picture I found of someone reconstructing Marius and Sulla from their busts and descriptions. Old Sulla vaguely… reminds me of someone, hmmm. Anyway, Sulla becomes consul, he’s in his early fifties, and he gets the command against a worthy adversary, Mithridates of Pontus. He deserves it, too — he has managed to squish the Italian rebels that threatened the empire just months earlier. Marius, meanwhile, cannot handle that he is no longer the top dog. So he stages a coup in the senate through bribes and calling in favors, thus cheating his former officer of his rightful command.\n\nSulla is furious! How dare that old man steal his glory and attempt to gain the command for his own selfish means? He makes a powerful, and daring decision… he marches on to Rome, at the head of his five legions. Marches on, calls on to the senate and has his command reinstated. Forces Marius to flee with his tail between his legs and rides off on a glorious Pontic adventure into the sunset…\n\nAs Sulla is out, Marius and his allies once more return to Rome, granting Marius an unheard of seventh consulship that makes him into an eternal legend. By now the old general seems to have gone mad and he butchers many of Sulla’s allies, and pretty much everyone he holds a grudge against. He then', 'result': {'fake': 0.0082, 'real': 0.9918}, 'status': 'success'}, {'text': 'dies of a stroke at the height of his power, only a few weeks into office. Soon, Sulla returns to the Italian peninsula, having kicked some serious Pontic ass, now once again marching onto Rome at the head of his armies. He meets what remains of the Marian forces outside the gates and decisively defeats them in battle.\n\nNow he is the undisputed champion of Rome, Sulla once again marches in. Declares himself dictator of the city and takes full control. He massacres his enemies, and all those who betrayed or belittled him in spite of his decades of service to Rome. Nails their heads to the rostra, on the walls of the city and near its gates.\n\nOnce, people snickered at Lucius Cornelius Sulla. Now, soon, they snicker once again, as he starts wandering around the city like a sad old man, back crooked, without bodyguards protecting him. People assume the massacres might have been orchestrated by those next to him, maybe the now-old general was unaware? People began to let down their guard around him. He hears a lot of things he wasn’t meant to hear. He plays Rome and its citizens like a fiddle, revealing himself to still be strong, capable and decisive after a few weeks of gathering intel.\n\nHe acts with terrifying swiftness. Restores old laws, abolishes new ones. Limits the powers of the senate to take away a consul’s rightful command, reforms laws, tries, and succeeds, in rebuilding the economy, and manages to appoint people who are actually competent for the most part. After just two years in power, Sulla decided to retire from public life, surrendering his powers to the senate and retreating to his Italian villa. He left Rome with a full treasury and a lot of ruthlessly ambitious people without a head.\n\nIn old age, Sulla wrote his memoirs, and lived a debased and scandalous life, once more surrounded by the very people he started his early years with — pimps, actors, whores and swindlers. He shared a bedroom with his very young, very pregnant fifth wife and the Greek actor Metrobius, both of them his lovers, making no secret of the fact. He died shortly after finishing his memoirs. Rome’s biggest badboy made Mark Antony seem like a choirboy.', 'result': {'fake': 0.0454, 'real': 0.9546}, 'status': 'success'}], 'credits_used': 9, 'credits': 1983001, 'subscription': 0, 'content': 'I’ve always been both impressed and terrified at the same time by Roman general, consul and dictator Lucius Cornelius Sulla. His rise to power and the boldness of his decisions set the stage for the end of the Roman republic and would later inspire Julius Caesar in his crossing of the Rubicon. A man whose epitaph read only: “No better friend, no worse enemy”.\n\nSulla was an odd character — as a young boy, his father died, his mother abandoned him and he was left alone in the care of a rich but distant stepmother. In the end, he had to fend entirely for himself. So he disappeared into the lower classes of Roman life. The shady, criminal, debauched places. Some say he made his money as a gigolo. Others say he was a pimp.\n\nFact remains, Sulla was popular. A great lover with little to no moral qualms, his best friend were actors — then considered the lowest of the low! — crossdressers, prostitutes and swindlers. He was reportedly exceptionally handsome and well-endowed, his face strikingly beautiful with the palest of skins and a thick mop of golden-red hair. In his early twenties, he managed to inherit money from both his stepmother and a rich courtesan, rumor had it he seduced both to secure the fortune. He then entered the political arena.\n\nSulla allied himself with the famous Roman general Gaius Marius. He started a military campaign, joined the senate and marched with Marius to Gaul. He fought the barbarians for years. And managed to make a name for himself. When one day he achieved the highest and loftiest of positions any Roman could ever reach, that of consul, he was over the moon!\n\nAs an aside, here’s a little picture I found of someone reconstructing Marius and Sulla from their busts and descriptions. Old Sulla vaguely… reminds me of someone, hmmm. Anyway, Sulla becomes consul, he’s in his early fifties, and he gets the command against a worthy adversary, Mithridates of Pontus. He deserves it, too — he has managed to squish the Italian rebels that threatened the empire just months earlier. Marius, meanwhile, cannot handle that he is no longer the top dog. So he stages a coup in the senate through bribes and calling in favors, thus cheating his former officer of his rightful command.\n\nSulla is furious! How dare that old man steal his glory and attempt to gain the command for his own selfish means? He makes a powerful, and daring decision… he marches on to Rome, at the head of his five legions. Marches on, calls on to the senate and has his command reinstated. Forces Marius to flee with his tail between his legs and rides off on a glorious Pontic adventure into the sunset…\n\nAs Sulla is out, Marius and his allies once more return to Rome, granting Marius an unheard of seventh consulship that makes him into an eternal legend. By now the old general seems to have gone mad and he butchers many of Sulla’s allies, and pretty much everyone he holds a grudge against. He then dies of a stroke at the height of his power, only a few weeks into office. Soon, Sulla returns to the Italian peninsula, having kicked some serious Pontic ass, now once again marching onto Rome at the head of his armies. He meets what remains of the Marian forces outside the gates and decisively defeats them in battle.\n\nNow he is the undisputed champion of Rome, Sulla once again marches in. Declares himself dictator of the city and takes full control. He massacres his enemies, and all those who betrayed or belittled him in spite of his decades of service to Rome. Nails their heads to the rostra, on the walls of the city and near its gates.\n\nOnce, people snickered at Lucius Cornelius Sulla. Now, soon, they snicker once again, as he starts wandering around the city like a sad old man, back crooked, without bodyguards protecting him. People assume the massacres might have been orchestrated by those next to him, maybe the now-old general was unaware? People began to let down their guard around him. He hears a lot of things he wasn’t meant to hear. He plays Rome and its citizens like a fiddle, revealing himself to still be strong, capable and decisive after a few weeks of gathering intel.\n\nHe acts with terrifying swiftness. Restores old laws, abolishes new ones. Limits the powers of the senate to take away a consul’s rightful command, reforms laws, tries, and succeeds, in rebuilding the economy, and manages to appoint people who are actually competent for the most part. After just two years in power, Sulla decided to retire from public life, surrendering his powers to the senate and retreating to his Italian villa. He left Rome with a full treasury and a lot of ruthlessly ambitious people without a head.\n\nIn old age, Sulla wrote his memoirs, and lived a debased and scandalous life, once more surrounded by the very people he started his early years with — pimps, actors, whores and swindlers. He shared a bedroom with his very young, very pregnant fifth wife and the Greek actor Metrobius, both of them his lovers, making no secret of the fact. He died shortly after finishing his memoirs. Rome’s biggest badboy made Mark Antony seem like a choirboy.', 'aiModelVersion': '1'}",0.77
Håkon Hapnes Strand,2y,How are machine learning projects managed?,"In one word: Wrongly.

Machine learning projects differ from other types of software development projects in that they deal with nondeterminism.

Many machine learning projects are led by project managers with backgrounds in traditional software projects where everything is deterministic and cozy. They tend to apply the same approaches to machine learning projects, and that’s where things can go wrong.

The thing is, machine learning models are programmed implicitly, not explicitly, and we don’t know how they will perform until we’ve tried.

Let’s say you want to develop an e-commerce site that supports 10 million monthly users and has 99.9% uptime. We know this is doable. It’s just a matter of time and money.

Now let’s say you want to develop a machine learning model that predicts what your users want to buy with 90% accuracy. That may or may not be possible.

In the first example (the website), if your developers fail to build a working prototype of the site even when given enough time and resources, it means they did a poor job. In the second example (the ML model), if your data scientists fail to come up with an accurate enough model, it may just be that what they were asked to do turned out to be essentially impossible. But you couldn’t know that beforehand.

Machine learning projects are basically R&D projects, and they need more focus on process and less on outcomes than your typical software development project.

The deliverables in the early stages are feasibility studies and experiments. Only when the results are very promising can the focus shift over to making an actual product out of it.

This requires a shift in mindset on the part of the project manager, and that’s why I say that these types of projects are typically wrongly managed. The worst you can do to a data scientist is give them very detailed tasks and timebox them. But that is often what happens in the name of agile.

In an ideal world, you start out with a feasibility study, potentially followed by a proof of concept and finally a product. There should be a continuing emphasis on experimentation. You need to celebrate when you are able to perform experiments that lead to some result, even if the result is unfavorable and the concept turns out to be infeasible. At least that’s how I see it.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/rqp8zjdvtb7xh93k', 'title': 'How are machine learning projects managed?', 'score': {'original': 0.9984, 'ai': 0.0016}, 'blocks': [{'text': 'In one word: Wrongly.\n\nMachine learning projects differ from other types of software development projects in that they deal with nondeterminism.\n\nMany machine learning projects are led by project managers with backgrounds in traditional software projects where everything is deterministic and cozy. They tend to apply the same approaches to machine learning projects, and that’s where things can go wrong.\n\nThe thing is, machine learning models are programmed implicitly, not explicitly, and we don’t know how they will perform until we’ve tried.\n\nLet’s say you want to develop an e-commerce site that supports 10 million monthly users and has 99.9% uptime. We know this is doable. It’s just a matter of time and money.\n\nNow let’s say you want to develop a machine learning model that predicts what your users want to buy with 90% accuracy. That may or may not be possible.\n\nIn the first example (the website), if your developers fail to build a working prototype of the site even when given enough time and resources, it means they did a poor job. In the second example (the ML model), if your data scientists fail to come up with an accurate enough model, it may just be that what they were asked to do turned out to be essentially impossible. But you couldn’t know that beforehand.\n\nMachine learning projects are basically R&D projects, and they need more focus on process and less on outcomes than your typical software development project.\n\nThe deliverables in the early stages are feasibility studies and experiments. Only when the results are very promising can the focus shift over to making an actual product out of it.\n\nThis requires a shift in mindset on the part of the project manager, and that’s why I say that these types of projects are typically wrongly managed. The worst you can do to a data scientist is give them very detailed tasks and timebox them. But that is often what happens in the name of agile.\n\nIn an ideal world, you start out with a feasibility study, potentially followed by a proof of concept and finally a product. There should be a continuing emphasis on experimentation. You need to celebrate when you are able to perform experiments that lead to some result, even if the result is unfavorable and the concept turns out to be infeasible. At least that’s how I see it.', 'result': {'fake': 0.0023, 'real': 0.9977}, 'status': 'success'}], 'credits_used': 5, 'credits': 1982996, 'subscription': 0, 'content': 'In one word: Wrongly.\n\nMachine learning projects differ from other types of software development projects in that they deal with nondeterminism.\n\nMany machine learning projects are led by project managers with backgrounds in traditional software projects where everything is deterministic and cozy. They tend to apply the same approaches to machine learning projects, and that’s where things can go wrong.\n\nThe thing is, machine learning models are programmed implicitly, not explicitly, and we don’t know how they will perform until we’ve tried.\n\nLet’s say you want to develop an e-commerce site that supports 10 million monthly users and has 99.9% uptime. We know this is doable. It’s just a matter of time and money.\n\nNow let’s say you want to develop a machine learning model that predicts what your users want to buy with 90% accuracy. That may or may not be possible.\n\nIn the first example (the website), if your developers fail to build a working prototype of the site even when given enough time and resources, it means they did a poor job. In the second example (the ML model), if your data scientists fail to come up with an accurate enough model, it may just be that what they were asked to do turned out to be essentially impossible. But you couldn’t know that beforehand.\n\nMachine learning projects are basically R&D projects, and they need more focus on process and less on outcomes than your typical software development project.\n\nThe deliverables in the early stages are feasibility studies and experiments. Only when the results are very promising can the focus shift over to making an actual product out of it.\n\nThis requires a shift in mindset on the part of the project manager, and that’s why I say that these types of projects are typically wrongly managed. The worst you can do to a data scientist is give them very detailed tasks and timebox them. But that is often what happens in the name of agile.\n\nIn an ideal world, you start out with a feasibility study, potentially followed by a proof of concept and finally a product. There should be a continuing emphasis on experimentation. You need to celebrate when you are able to perform experiments that lead to some result, even if the result is unfavorable and the concept turns out to be infeasible. At least that’s how I see it.', 'aiModelVersion': '1'}",0.9984
S M,6y,What is a neural network in layman’s terms?,"Let’s detect sarcasm. Very simple problem, right? (I just went meta.) Okay. Let’s look at a couple of sarcastic product reviews. Intuitively, if a review has a positive sentiment but a low rating, then it’s probably sarcastic. Examples:“I was tired of getting hit on by beautiful women. After I bought this jacket, problem solved!” (Rating: 0.5/5)“Great burrito, now actually try cooking the beans.” (Rating: 1/5)You may have noticed that the sentiment of the reviews are positive (“problem solved”, “great”), but the ratings are low. That seems like a sign of sarcasm. Now that we suspect there is some relationship between {sentiment, rating} and {sarcasm}, we list down some data points: Sentiment (+1 for positive, 0 for neutral, -1 for negative), Rating (0 to 5), Sarcasm (1 for Yes, 0 for No) (Sentiment, Rating, Sarcasm)(1, 0.5, 1)(1, 1, 1)(1, 5, 0)(-1, 4, 1)(-1, 1, 0)... and a few thousand more. So, to find out the actual relationship, we want to work on sentiment and rating values to somehow get the value of sarcasm. We will use layers as steps to move from inputs to output. Let’s look at the first example (1, 0.5, 1):Each line in that network has a weight. We will use those weights to calculate the values in the circles in the hidden layer and the output layer (which we hope will be ‘1’). Initially we assign weights randomly:Now we have our initial stupid neural network. Let’s see what the output will be. At each circle (aka “neuron”) in the hidden and output layer, we multiply their inputs with the corresponding weights and sum up the results. Hidden Layer 1st Neuron = (1∗0.2)+(0.5∗0.4)=0.4(1∗0.2)+(0.5∗0.4)=0.4(1 * 0.2) + (0.5 * 0.4) = 0.4  Hidden Layer 2nd Neuron =(1∗0.3)+(0.5∗0.6)=0.6(1∗0.3)+(0.5∗0.6)=0.6 (1 * 0.3) + (0.5 * 0.6) = 0.6  Hidden Layer 3rd Neuron =(1∗0.4)+(0.5∗0.7)=0.75(1∗0.4)+(0.5∗0.7)=0.75 (1 * 0.4) + (0.5 * 0.7) = 0.75 Also, we want the output (Sarcasm) to be a number between 0 and 1 (because nothing else makes sense). We do this by using a magic function on the output layer, that reduces any given number to a number between 0 and 1. Any function we use at any neuron is called the activation function and in this case, we use the sigmoid function on the output layer. Final Layer = (0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4 * 0.3) + (0.6 * 0.4) + (0.75 * 0.5) = 0.735 Output = sigmoid(0.735)sigmoid(0.735)sigmoid(0.735) =0.324=0.324= 0.324 So, we have an output 0.324. But we were expecting 1! So, what do we do? We change the weights slightly to nudge the output towards the correct value. We do this using a method called Back propagation, which is explained in this blog. We repeat this thousands of times covering all the training data, changing the weights slightly every time. Eventually, we’ll get the ‘right’ weights which will best predict sarcasm, given sentiment and rating. That’s it! Most applications of neural networks that you see, are variations of the above neural network with changes in:The structure of inputs and outputs (duh).The number of hidden layers/neurons.How the neurons are connected.The training process.The activation function.… and some other hyper parameters. And in case you haven’t noticed, logistic regression is just a one-layer neural network. Whaaaaaa More importantly, can we all take a moment here to appreciate how perfectly circular those circles in my diagrams are? :)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/65srg9cjq8dkvayb', 'title': 'What is a neural network in layman’s terms?', 'score': {'original': 0.9752, 'ai': 0.0248}, 'blocks': [{'text': 'Let’s detect sarcasm. Very simple problem, right? (I just went meta.) Okay. Let’s look at a couple of sarcastic product reviews. Intuitively, if a review has a positive sentiment but a low rating, then it’s probably sarcastic. Examples:“I was tired of getting hit on by beautiful women. After I bought this jacket, problem solved!” (Rating: 0.5/5)“Great burrito, now actually try cooking the beans.” (Rating: 1/5)You may have noticed that the sentiment of the reviews are positive (“problem solved”, “great”), but the ratings are low. That seems like a sign of sarcasm. Now that we suspect there is some relationship between {sentiment, rating} and {sarcasm}, we list down some data points: Sentiment (+1 for positive, 0 for neutral, -1 for negative), Rating (0 to 5), Sarcasm (1 for Yes, 0 for No) (Sentiment, Rating, Sarcasm)(1, 0.5, 1)(1, 1, 1)(1, 5, 0)(-1, 4, 1)(-1, 1, 0)... and a few thousand more. So, to find out the actual relationship, we want to work on sentiment and rating values to somehow get the value of sarcasm. We will use layers as steps to move from inputs to output. Let’s look at the first example (1, 0.5, 1):Each line in that network has a weight. We will use those weights to calculate the values in the circles in the hidden layer and the output layer (which we hope will be ‘1’). Initially we assign weights randomly:Now we have our initial stupid neural network. Let’s see what the output will be. At each circle (aka “neuron”) in the hidden and output layer, we multiply their inputs with the corresponding weights and sum up the results. Hidden Layer 1st Neuron = (1∗0.2)+(0.5∗0.4)=0.4(1∗0.2)+(0.5∗0.4)=0.4(1 * 0.2) + (0.5 * 0.4) = 0.4  Hidden Layer 2nd Neuron =(1∗0.3)+(0.5∗0.6)=0.6(1∗0.3)+(0.5∗0.6)=0.6 (1 * 0.3) + (0.5 * 0.6) = 0.6  Hidden Layer 3rd Neuron =(1∗0.4)+(0.5∗0.7)=0.75(1∗0.4)+(0.5∗0.7)=0.75 (1 * 0.4) + (0.5 * 0.7) = 0.75 Also, we want the output (Sarcasm) to be a number between 0 and 1 (because nothing else makes sense). We do this by using a magic function on the output layer, that reduces any given number to a number between 0 and 1. Any function we use at any neuron is called the activation function and in this case, we use the sigmoid function on the output layer. Final Layer = (0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4 * 0.3) + (0.6 * 0.4) + (0.75 * 0.5) = 0.735 Output = sigmoid(0.735)sigmoid(0.735)sigmoid(0.735) =0.324=0.324= 0.324 So, we have an output 0.324. But we were expecting 1! So, what do we do? We change the weights slightly to nudge the output towards the correct value. We do this using a method called Back propagation, which is explained in this blog. We repeat this thousands of times covering all the training data, changing the weights slightly every time. Eventually, we’ll get the ‘right’ weights which will best predict sarcasm, given sentiment and rating. That’s it! Most applications of neural networks that you see, are variations of the above neural network with changes in:The structure of inputs and outputs (duh).The number of hidden layers/neurons.How the neurons are connected.The', 'result': {'fake': 0.0326, 'real': 0.9674}, 'status': 'success'}, {'text': 'training process.The activation function.… and some other hyper parameters. And in case you haven’t noticed, logistic regression is just a one-layer neural network. Whaaaaaa More importantly, can we all take a moment here to appreciate how perfectly circular those circles in my diagrams are? :)', 'result': {'fake': 0.0116, 'real': 0.9884}, 'status': 'success'}], 'credits_used': 6, 'credits': 1982990, 'subscription': 0, 'content': 'Let’s detect sarcasm. Very simple problem, right? (I just went meta.) Okay. Let’s look at a couple of sarcastic product reviews. Intuitively, if a review has a positive sentiment but a low rating, then it’s probably sarcastic. Examples:“I was tired of getting hit on by beautiful women. After I bought this jacket, problem solved!” (Rating: 0.5/5)“Great burrito, now actually try cooking the beans.” (Rating: 1/5)You may have noticed that the sentiment of the reviews are positive (“problem solved”, “great”), but the ratings are low. That seems like a sign of sarcasm. Now that we suspect there is some relationship between {sentiment, rating} and {sarcasm}, we list down some data points: Sentiment (+1 for positive, 0 for neutral, -1 for negative), Rating (0 to 5), Sarcasm (1 for Yes, 0 for No) (Sentiment, Rating, Sarcasm)(1, 0.5, 1)(1, 1, 1)(1, 5, 0)(-1, 4, 1)(-1, 1, 0)... and a few thousand more. So, to find out the actual relationship, we want to work on sentiment and rating values to somehow get the value of sarcasm. We will use layers as steps to move from inputs to output. Let’s look at the first example (1, 0.5, 1):Each line in that network has a weight. We will use those weights to calculate the values in the circles in the hidden layer and the output layer (which we hope will be ‘1’). Initially we assign weights randomly:Now we have our initial stupid neural network. Let’s see what the output will be. At each circle (aka “neuron”) in the hidden and output layer, we multiply their inputs with the corresponding weights and sum up the results. Hidden Layer 1st Neuron = (1∗0.2)+(0.5∗0.4)=0.4(1∗0.2)+(0.5∗0.4)=0.4(1 * 0.2) + (0.5 * 0.4) = 0.4  Hidden Layer 2nd Neuron =(1∗0.3)+(0.5∗0.6)=0.6(1∗0.3)+(0.5∗0.6)=0.6 (1 * 0.3) + (0.5 * 0.6) = 0.6  Hidden Layer 3rd Neuron =(1∗0.4)+(0.5∗0.7)=0.75(1∗0.4)+(0.5∗0.7)=0.75 (1 * 0.4) + (0.5 * 0.7) = 0.75 Also, we want the output (Sarcasm) to be a number between 0 and 1 (because nothing else makes sense). We do this by using a magic function on the output layer, that reduces any given number to a number between 0 and 1. Any function we use at any neuron is called the activation function and in this case, we use the sigmoid function on the output layer. Final Layer = (0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4∗0.3)+(0.6∗0.4)+(0.75∗0.5)=0.735(0.4 * 0.3) + (0.6 * 0.4) + (0.75 * 0.5) = 0.735 Output = sigmoid(0.735)sigmoid(0.735)sigmoid(0.735) =0.324=0.324= 0.324 So, we have an output 0.324. But we were expecting 1! So, what do we do? We change the weights slightly to nudge the output towards the correct value. We do this using a method called Back propagation, which is explained in this blog. We repeat this thousands of times covering all the training data, changing the weights slightly every time. Eventually, we’ll get the ‘right’ weights which will best predict sarcasm, given sentiment and rating. That’s it! Most applications of neural networks that you see, are variations of the above neural network with changes in:The structure of inputs and outputs (duh).The number of hidden layers/neurons.How the neurons are connected.The training process.The activation function.… and some other hyper parameters. And in case you haven’t noticed, logistic regression is just a one-layer neural network. Whaaaaaa More importantly, can we all take a moment here to appreciate how perfectly circular those circles in my diagrams are? :)', 'aiModelVersion': '1'}",0.9752
Louis Stevens,8mo,"Google's 'Bard' chatbot will compete with ChatGPT, what is special?","Google's Bard chatbot is a unique product that sets itself apart in the competitive space of chatbots.

It provides an intuitive conversational interface that is able to learn directly from user interactions, which sets it apart from other chatbot options.

Additionally, the chatbot uses natural language processing to understand and respond appropriately to a variety of conversations.

As a user, this means that the conversation is more engaging and less frustrating since the chatbot is able to understand the context of the conversations.

All in all, Bard offers a unique take on the world of chatbots that is sure to stand out amongst the competition.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/o6vh7rxztql1ypwe', 'title': ""Google's 'Bard' chatbot will compete with ChatGPT, what is special?"", 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""Google's Bard chatbot is a unique product that sets itself apart in the competitive space of chatbots.\n\nIt provides an intuitive conversational interface that is able to learn directly from user interactions, which sets it apart from other chatbot options.\n\nAdditionally, the chatbot uses natural language processing to understand and respond appropriately to a variety of conversations.\n\nAs a user, this means that the conversation is more engaging and less frustrating since the chatbot is able to understand the context of the conversations.\n\nAll in all, Bard offers a unique take on the world of chatbots that is sure to stand out amongst the competition."", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982988, 'subscription': 0, 'content': ""Google's Bard chatbot is a unique product that sets itself apart in the competitive space of chatbots.\n\nIt provides an intuitive conversational interface that is able to learn directly from user interactions, which sets it apart from other chatbot options.\n\nAdditionally, the chatbot uses natural language processing to understand and respond appropriately to a variety of conversations.\n\nAs a user, this means that the conversation is more engaging and less frustrating since the chatbot is able to understand the context of the conversations.\n\nAll in all, Bard offers a unique take on the world of chatbots that is sure to stand out amongst the competition."", 'aiModelVersion': '1'}",0.0
Dr Jo,Dec 12,"What is the significance of the Turing test in the field of artificial intelligence, and how does it contribute to our understanding of machine intelligence?","Hi Kabir

None—and it doesn’t. It’s a hindrance. Let me explain.

Alan Turing was a bloody genius. Put aside the contribution he made to British code-breaking during World War II; he had a remarkable grasp what could be done with the technology of the time—but more than this, he had a beautiful and clear vision of the principles of computing.

If you’ve played with building Turing machines, for example, then you’ll immediately grok my meaning. Genius! Superficially, the Turing test is also rather appealing.

First, the Turing Test

A machine ‘passes the Turing test’ if a human observer is unable to distinguish between a real human and the ‘intelligent’ machine, after a substantial interaction with both.
∗
∗
 In his 1950 paper on Computing Machinery and Intelligence,[1] Turing limited this interaction to text alone. There’s some disagreement about the precise formulation of the Test, [2] but the principle seems simple, doesn’t it? Not so fast.

The Turing test is ‘Unscience’

Let’s start in the 1930s, when the Logical Positivists were all the rage. They saw science as meticulous building of scientific constructs on a stable foundation. There are ground truths that you can assume, and move on. Heck, some people still think like this.

Unfortunately for them, life doesn’t work this way. You can have a model that works—that passes every test—and then some bugger comes along with a test that reveals foundational flaws in your model. This doesn’t necessarily mean that you have to abandon using your model for many or even most things, but it does suggest that you need to tread cautiously. Your foundation isn’t actually foundational at all. It’s more like winter ice—and in some places, the ice may be thin.

In fact, the more complex the thing you’re testing, the less assured you should be that even ‘comprehensive’ testing has validated your “model”. And what could be more complex than ‘intelligence’? In other words, however you reasonably define intelligence, it would be unscientific to attempt to prove that it passes some test, give it your stamp of approval, and move on.

By the 1950s, most philosophers had moved on—as had not a few scientists. It seems that either Turing didn’t get the memo, or that he didn’t see the implications of the failure of Logical Positivism for things like testing models and indeed devices.

But before we dip into the philosophy, let’s look at a practical example.

Practically

Look at the image above. It was generated by Leonardo|DiffusionXL with the prompt “A photograph of a robot shaking hands with Alan Turing, in black and white. The hands of both are clearly visible.”

I think it’s not a bad representation—although perhaps the Turing has a slight flavour of Benedict Cumberbatch,
†
†
 and his right hand is a bit weird; the robot is pretty stereotyped (although not quite Rossum’s Universal Robots :)

But I chose that image from among four. The other three were along these lines…

Not only did the generator stuff up Turing’s right eye—look at the hands, too. Eugh! Hands are something that current image generators get almost right more by luck than anything else. (I won’t use the term ‘judgement’ here, for reasons explored later).

If you look at the first image, you might be so smitten as to believe that image generation has come of age. Wow! But a better test of fidelity is surely where the AI under-performs. This is at present all-too-easy to identify. As an aside, here are a few things that you might easily imagine, but I find the current crop of image generators struggle with:

“A photograph of a horse riding an astronaut on the Moon”
“A hippopotamus with eight legs”
“A cat hanging by its claws from a giant tongue. The cat is on the left, the tongue is on the right.”

Try these. It’s fun to explore the boundaries, isn’t it?

Don’t get me wrong

The above handshake is still pretty impressive—and things will improve. All I’m doing here is illustrating the point that I’ve already made: finding demonstrations that assert the presence of a given capability is daft, particularly in the field of AI. But this practical application of the thinking that refutes Logical Positivism has two other, major implications. Let’s look at them briefly.

Data ‘science’ mostly isn’t

I’m afraid I tend to bang on a bit about Science. But looking through multiple recent posts on Quora that discuss ‘Science’, I can’t help but feel that there are a lot of crypto-logical positivists still hidden among us. So let me once again give my definition of Science, so that you can shoot at it.
‡
‡
 (Karl Popper worked most of this out in the 1930s, in his book Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwissenschaft).

Science starts with a problem. We propose an explanatory model that not only describes the problem, but is predictive. This prediction doesn’t have to be perfect, just better than random chance, and ideally better than other competing models.
We then test our model. We test the logic—’cos we often stuff up the internal workings; once we’re happy with this, we then go out in reality and test the model to see how it really performs. Most models fail easily.
If a model succeeds—hooray—we provisionally accept it as ‘true’. But we continue back to step (1) again, testing and refining and trying to break or supplant our model. This is good Science. There should be a Darwinian elimination of less favourable models.

Contrast this with a lot of modern approaches to ‘data science’. This often involves garnering vast amounts of data, running it through some sort of black box, pulling out features and then trying to use the outputs in reality. Can you see the huge problems with this approach? Yes, there are many, but the biggest is often missed completely—that this too is Unscience. The meta-model is wrong!

Spotting a winner

Our second implication of the oracular “let’s spot a winner” approach is even more disturbing. Strangely, it was also worked out in the 1930s, by an American called Walter Shewhart. Busy times, the 1930s. Shewhart was struggling with quality control problems on assembly lines. The story is complex, and I won’t delve deeply into it, but the bottom line can be illustrated quite easily by an analogy.

Let’s say you’re running a breakfast diner, and you have a problem with burnt toast. One approach is to deploy a complex system of toast evaluation, with burnt toast sent back—or perhaps you might employ a dedicated toast-scraper. More sensible is not to burn the toast in the first place!

Get the process right. The analogy is obvious: if you’re making a product, especially a complex product, rather than extensively testing it at the end of the assembly line to be “sure that it works” (Hint: based on evaluation, you can’t be sure), design the quality you desire into the product from the start. This can and indeed should be iterative—just like good science.

(Poe/StableDiffusion: “A photograph of an eight-legged hippopotamus, walking on winter ice.” Note not just the inability to count, but the shape of the legs. It’s not just human extremities these bots struggle with).

Applying this to AI

If you listen to real experts on AI (like Geoffrey Hinton) and AI companies’ frontmen (yep, mostly male and charming, like Sam Altman), you’ll often get the assurance that the bot is “really thinking”, just like us. For example, Sam has said that we too are ‘stochastic parrots’,[3] and Hinton has even more recently claimed that LLMs “understand and have empathy”.[4]

Which is bullshit. Surely, you can produce isolated examples that seem to demonstrate this—but this is Unscience. These models have vast domains of incompetence. It’s still trivially easy to show how incompetent these models are. But even if this demonstration becomes challenging, we still can’t assert competence.

More to the point, what are these domains? I’ve already hinted at this. Yes, they pretty much can’t count (although some have attached extra, prosthetic limbs to enable this); but more fundamentally, they have no meta-cognition. They struggle to generalise from one domain to another, and importantly, cannot reliably do either causal inference or counterfactual reasoning. And despite this, Hinton is also claiming that—effectively—they have a limbic system!

These are not just things you can graft on—the maths behind causality and counterfactuals was only worked out in the 1990s, it’s subtle, and it’s unreasonable to assume that the required functionality will somehow just emerge. Solid models of emotion? Even more challenging.

Sure, you can scrape the toast—pick up the ‘defective patterns of behaviour’, and put on extra prosthetics to try to make your LLM ‘behave’. But designing in the right functionality—well, that’s the charm. We’re simply not there yet. As Gary Marcus has pointed out repeatedly, it appears unlikely that current models will ever get there, simply by extension.

And even then, you shouldn’t rely on a demonstration of ‘intelligent performance’. That’s the most unconvincing thing ever.

It’s just Unscience.

My 2c, Dr Jo.

∗
∗
 I’m not going to explore silly and pointless digressions like Searle’s Chinese Room argument, ‘qualia’, dualism, and other philosophical claptrap.
§
§
 Let’s also lay aside whether the evaluator should be a computer, how long you should bash on for, and how smart the benchmark human being should be.

†
†
 I have significant prosopagnosia, so don’t take my word for this.

‡
‡
 Metacognitively <grin>

§
§
 (Waits for explosion from philosophers who haven’t quite caught up).

Footnotes

[1] Computing Machinery and Intelligence - Wikipedia
[2] Turing test - Wikipedia
[3] Dr Jo's answer to What is a ""stochastic parrot""?
[4] Geoffrey Hinton: Large Language Models in Medicine. They Understand and Have Empathy","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/o7btkuyv5i24n1ej', 'title': 'What is the significance of the Turing test in the field of artificial intelligence, and how does it contribute to our understanding of machine intelligence?', 'score': {'original': 0.788, 'ai': 0.212}, 'blocks': [{'text': 'Hi Kabir\n\nNone—and it doesn’t. It’s a hindrance. Let me explain.\n\nAlan Turing was a bloody genius. Put aside the contribution he made to British code-breaking during World War II; he had a remarkable grasp what could be done with the technology of the time—but more than this, he had a beautiful and clear vision of the principles of computing.\n\nIf you’ve played with building Turing machines, for example, then you’ll immediately grok my meaning. Genius! Superficially, the Turing test is also rather appealing.\n\nFirst, the Turing Test\n\nA machine ‘passes the Turing test’ if a human observer is unable to distinguish between a real human and the ‘intelligent’ machine, after a substantial interaction with both.\n∗\n∗\n In his 1950 paper on Computing Machinery and Intelligence,[1] Turing limited this interaction to text alone. There’s some disagreement about the precise formulation of the Test, [2] but the principle seems simple, doesn’t it? Not so fast.\n\nThe Turing test is ‘Unscience’\n\nLet’s start in the 1930s, when the Logical Positivists were all the rage. They saw science as meticulous building of scientific constructs on a stable foundation. There are ground truths that you can assume, and move on. Heck, some people still think like this.\n\nUnfortunately for them, life doesn’t work this way. You can have a model that works—that passes every test—and then some bugger comes along with a test that reveals foundational flaws in your model. This doesn’t necessarily mean that you have to abandon using your model for many or even most things, but it does suggest that you need to tread cautiously. Your foundation isn’t actually foundational at all. It’s more like winter ice—and in some places, the ice may be thin.\n\nIn fact, the more complex the thing you’re testing, the less assured you should be that even ‘comprehensive’ testing has validated your “model”. And what could be more complex than ‘intelligence’? In other words, however you reasonably define intelligence, it would be unscientific to attempt to prove that it passes some test, give it your stamp of approval, and move on.\n\nBy the 1950s, most philosophers had moved on—as had not a few scientists. It seems that either Turing didn’t get the memo, or that he didn’t see the implications of the failure of Logical Positivism for things like testing models and indeed devices.\n\nBut before we dip into the philosophy, let’s look at a practical example.\n\nPractically\n\nLook at the image above. It was generated by Leonardo|DiffusionXL with the prompt “A photograph of a robot shaking hands with Alan Turing, in black and white. The hands of both are clearly visible.”\n\nI think it’s not a bad representation—although perhaps the Turing has a slight flavour of Benedict Cumberbatch,\n†\n†\n and his right hand is a bit weird; the robot is pretty stereotyped (although not quite Rossum’s Universal Robots :)\n\nBut I chose that image from among four. The other three were along these lines…\n\nNot only did the generator stuff up Turing’s right eye—look at the hands, too. Eugh! Hands are something that current image generators get almost right more by luck than anything else. (I won’t use the term ‘judgement’ here, for reasons explored', 'result': {'fake': 0.0051, 'real': 0.9949}, 'status': 'success'}, {'text': 'later).\n\nIf you look at the first image, you might be so smitten as to believe that image generation has come of age. Wow! But a better test of fidelity is surely where the AI under-performs. This is at present all-too-easy to identify. As an aside, here are a few things that you might easily imagine, but I find the current crop of image generators struggle with:\n\n“A photograph of a horse riding an astronaut on the Moon”\n“A hippopotamus with eight legs”\n“A cat hanging by its claws from a giant tongue. The cat is on the left, the tongue is on the right.”\n\nTry these. It’s fun to explore the boundaries, isn’t it?\n\nDon’t get me wrong\n\nThe above handshake is still pretty impressive—and things will improve. All I’m doing here is illustrating the point that I’ve already made: finding demonstrations that assert the presence of a given capability is daft, particularly in the field of AI. But this practical application of the thinking that refutes Logical Positivism has two other, major implications. Let’s look at them briefly.\n\nData ‘science’ mostly isn’t\n\nI’m afraid I tend to bang on a bit about Science. But looking through multiple recent posts on Quora that discuss ‘Science’, I can’t help but feel that there are a lot of crypto-logical positivists still hidden among us. So let me once again give my definition of Science, so that you can shoot at it.\n‡\n‡\n (Karl Popper worked most of this out in the 1930s, in his book Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwissenschaft).\n\nScience starts with a problem. We propose an explanatory model that not only describes the problem, but is predictive. This prediction doesn’t have to be perfect, just better than random chance, and ideally better than other competing models.\nWe then test our model. We test the logic—’cos we often stuff up the internal workings; once we’re happy with this, we then go out in reality and test the model to see how it really performs. Most models fail easily.\nIf a model succeeds—hooray—we provisionally accept it as ‘true’. But we continue back to step (1) again, testing and refining and trying to break or supplant our model. This is good Science. There should be a Darwinian elimination of less favourable models.\n\nContrast this with a lot of modern approaches to ‘data science’. This often involves garnering vast amounts of data, running it through some sort of black box, pulling out features and then trying to use the outputs in reality. Can you see the huge problems with this approach? Yes, there are many, but the biggest is often missed completely—that this too is Unscience. The meta-model is wrong!\n\nSpotting a winner\n\nOur second implication of the oracular “let’s spot a winner” approach is even more disturbing. Strangely, it was also worked out in the 1930s, by an American called Walter Shewhart. Busy times, the 1930s. Shewhart was struggling with quality control problems on assembly lines. The story is complex, and I won’t delve deeply into it, but the bottom line can be illustrated quite easily by an analogy.\n\nLet’s say you’re running a breakfast diner, and you', 'result': {'fake': 0.1802, 'real': 0.8198}, 'status': 'success'}, {'text': 'have a problem with burnt toast. One approach is to deploy a complex system of toast evaluation, with burnt toast sent back—or perhaps you might employ a dedicated toast-scraper. More sensible is not to burn the toast in the first place!\n\nGet the process right. The analogy is obvious: if you’re making a product, especially a complex product, rather than extensively testing it at the end of the assembly line to be “sure that it works” (Hint: based on evaluation, you can’t be sure), design the quality you desire into the product from the start. This can and indeed should be iterative—just like good science.\n\n(Poe/StableDiffusion: “A photograph of an eight-legged hippopotamus, walking on winter ice.” Note not just the inability to count, but the shape of the legs. It’s not just human extremities these bots struggle with).\n\nApplying this to AI\n\nIf you listen to real experts on AI (like Geoffrey Hinton) and AI companies’ frontmen (yep, mostly male and charming, like Sam Altman), you’ll often get the assurance that the bot is “really thinking”, just like us. For example, Sam has said that we too are ‘stochastic parrots’,[3] and Hinton has even more recently claimed that LLMs “understand and have empathy”.[4]\n\nWhich is bullshit. Surely, you can produce isolated examples that seem to demonstrate this—but this is Unscience. These models have vast domains of incompetence. It’s still trivially easy to show how incompetent these models are. But even if this demonstration becomes challenging, we still can’t assert competence.\n\nMore to the point, what are these domains? I’ve already hinted at this. Yes, they pretty much can’t count (although some have attached extra, prosthetic limbs to enable this); but more fundamentally, they have no meta-cognition. They struggle to generalise from one domain to another, and importantly, cannot reliably do either causal inference or counterfactual reasoning. And despite this, Hinton is also claiming that—effectively—they have a limbic system!\n\nThese are not just things you can graft on—the maths behind causality and counterfactuals was only worked out in the 1990s, it’s subtle, and it’s unreasonable to assume that the required functionality will somehow just emerge. Solid models of emotion? Even more challenging.\n\nSure, you can scrape the toast—pick up the ‘defective patterns of behaviour’, and put on extra prosthetics to try to make your LLM ‘behave’. But designing in the right functionality—well, that’s the charm. We’re simply not there yet. As Gary Marcus has pointed out repeatedly, it appears unlikely that current models will ever get there, simply by extension.\n\nAnd even then, you shouldn’t rely on a demonstration of ‘intelligent performance’. That’s the most unconvincing thing ever.\n\nIt’s just Unscience.\n\nMy 2c, Dr Jo.\n\n∗\n∗\n I’m not going to explore silly and pointless digressions like Searle’s Chinese Room argument, ‘qualia’, dualism, and other philosophical claptrap.\n§\n§\n Let’s also lay aside whether the evaluator should be a computer, how long you should bash on for, and how smart the benchmark human being should be.\n\n†\n†\n I have significant prosopagnosia, so don’t take my word for this.\n\n‡\n‡\n Metacognitively <grin>\n\n§\n§\n (Waits for explosion from philosophers who haven’t quite caught up).\n\nFootnotes\n\n[1] Computing Machinery and Intelligence - Wikipedia\n[2] Turing test -', 'result': {'fake': 0.0591, 'real': 0.9409}, 'status': 'success'}, {'text': 'Wikipedia\n[3] Dr Jo\'s answer to What is a ""stochastic parrot""?\n[4] Geoffrey Hinton: Large Language Models in Medicine. They Understand and Have Empathy', 'result': {'fake': 0.2667, 'real': 0.7333}, 'status': 'success'}], 'credits_used': 17, 'credits': 1982971, 'subscription': 0, 'content': 'Hi Kabir\n\nNone—and it doesn’t. It’s a hindrance. Let me explain.\n\nAlan Turing was a bloody genius. Put aside the contribution he made to British code-breaking during World War II; he had a remarkable grasp what could be done with the technology of the time—but more than this, he had a beautiful and clear vision of the principles of computing.\n\nIf you’ve played with building Turing machines, for example, then you’ll immediately grok my meaning. Genius! Superficially, the Turing test is also rather appealing.\n\nFirst, the Turing Test\n\nA machine ‘passes the Turing test’ if a human observer is unable to distinguish between a real human and the ‘intelligent’ machine, after a substantial interaction with both.\n∗\n∗\n In his 1950 paper on Computing Machinery and Intelligence,[1] Turing limited this interaction to text alone. There’s some disagreement about the precise formulation of the Test, [2] but the principle seems simple, doesn’t it? Not so fast.\n\nThe Turing test is ‘Unscience’\n\nLet’s start in the 1930s, when the Logical Positivists were all the rage. They saw science as meticulous building of scientific constructs on a stable foundation. There are ground truths that you can assume, and move on. Heck, some people still think like this.\n\nUnfortunately for them, life doesn’t work this way. You can have a model that works—that passes every test—and then some bugger comes along with a test that reveals foundational flaws in your model. This doesn’t necessarily mean that you have to abandon using your model for many or even most things, but it does suggest that you need to tread cautiously. Your foundation isn’t actually foundational at all. It’s more like winter ice—and in some places, the ice may be thin.\n\nIn fact, the more complex the thing you’re testing, the less assured you should be that even ‘comprehensive’ testing has validated your “model”. And what could be more complex than ‘intelligence’? In other words, however you reasonably define intelligence, it would be unscientific to attempt to prove that it passes some test, give it your stamp of approval, and move on.\n\nBy the 1950s, most philosophers had moved on—as had not a few scientists. It seems that either Turing didn’t get the memo, or that he didn’t see the implications of the failure of Logical Positivism for things like testing models and indeed devices.\n\nBut before we dip into the philosophy, let’s look at a practical example.\n\nPractically\n\nLook at the image above. It was generated by Leonardo|DiffusionXL with the prompt “A photograph of a robot shaking hands with Alan Turing, in black and white. The hands of both are clearly visible.”\n\nI think it’s not a bad representation—although perhaps the Turing has a slight flavour of Benedict Cumberbatch,\n†\n†\n and his right hand is a bit weird; the robot is pretty stereotyped (although not quite Rossum’s Universal Robots :)\n\nBut I chose that image from among four. The other three were along these lines…\n\nNot only did the generator stuff up Turing’s right eye—look at the hands, too. Eugh! Hands are something that current image generators get almost right more by luck than anything else. (I won’t use the term ‘judgement’ here, for reasons explored later).\n\nIf you look at the first image, you might be so smitten as to believe that image generation has come of age. Wow! But a better test of fidelity is surely where the AI under-performs. This is at present all-too-easy to identify. As an aside, here are a few things that you might easily imagine, but I find the current crop of image generators struggle with:\n\n“A photograph of a horse riding an astronaut on the Moon”\n“A hippopotamus with eight legs”\n“A cat hanging by its claws from a giant tongue. The cat is on the left, the tongue is on the right.”\n\nTry these. It’s fun to explore the boundaries, isn’t it?\n\nDon’t get me wrong\n\nThe above handshake is still pretty impressive—and things will improve. All I’m doing here is illustrating the point that I’ve already made: finding demonstrations that assert the presence of a given capability is daft, particularly in the field of AI. But this practical application of the thinking that refutes Logical Positivism has two other, major implications. Let’s look at them briefly.\n\nData ‘science’ mostly isn’t\n\nI’m afraid I tend to bang on a bit about Science. But looking through multiple recent posts on Quora that discuss ‘Science’, I can’t help but feel that there are a lot of crypto-logical positivists still hidden among us. So let me once again give my definition of Science, so that you can shoot at it.\n‡\n‡\n (Karl Popper worked most of this out in the 1930s, in his book Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwissenschaft).\n\nScience starts with a problem. We propose an explanatory model that not only describes the problem, but is predictive. This prediction doesn’t have to be perfect, just better than random chance, and ideally better than other competing models.\nWe then test our model. We test the logic—’cos we often stuff up the internal workings; once we’re happy with this, we then go out in reality and test the model to see how it really performs. Most models fail easily.\nIf a model succeeds—hooray—we provisionally accept it as ‘true’. But we continue back to step (1) again, testing and refining and trying to break or supplant our model. This is good Science. There should be a Darwinian elimination of less favourable models.\n\nContrast this with a lot of modern approaches to ‘data science’. This often involves garnering vast amounts of data, running it through some sort of black box, pulling out features and then trying to use the outputs in reality. Can you see the huge problems with this approach? Yes, there are many, but the biggest is often missed completely—that this too is Unscience. The meta-model is wrong!\n\nSpotting a winner\n\nOur second implication of the oracular “let’s spot a winner” approach is even more disturbing. Strangely, it was also worked out in the 1930s, by an American called Walter Shewhart. Busy times, the 1930s. Shewhart was struggling with quality control problems on assembly lines. The story is complex, and I won’t delve deeply into it, but the bottom line can be illustrated quite easily by an analogy.\n\nLet’s say you’re running a breakfast diner, and you have a problem with burnt toast. One approach is to deploy a complex system of toast evaluation, with burnt toast sent back—or perhaps you might employ a dedicated toast-scraper. More sensible is not to burn the toast in the first place!\n\nGet the process right. The analogy is obvious: if you’re making a product, especially a complex product, rather than extensively testing it at the end of the assembly line to be “sure that it works” (Hint: based on evaluation, you can’t be sure), design the quality you desire into the product from the start. This can and indeed should be iterative—just like good science.\n\n(Poe/StableDiffusion: “A photograph of an eight-legged hippopotamus, walking on winter ice.” Note not just the inability to count, but the shape of the legs. It’s not just human extremities these bots struggle with).\n\nApplying this to AI\n\nIf you listen to real experts on AI (like Geoffrey Hinton) and AI companies’ frontmen (yep, mostly male and charming, like Sam Altman), you’ll often get the assurance that the bot is “really thinking”, just like us. For example, Sam has said that we too are ‘stochastic parrots’,[3] and Hinton has even more recently claimed that LLMs “understand and have empathy”.[4]\n\nWhich is bullshit. Surely, you can produce isolated examples that seem to demonstrate this—but this is Unscience. These models have vast domains of incompetence. It’s still trivially easy to show how incompetent these models are. But even if this demonstration becomes challenging, we still can’t assert competence.\n\nMore to the point, what are these domains? I’ve already hinted at this. Yes, they pretty much can’t count (although some have attached extra, prosthetic limbs to enable this); but more fundamentally, they have no meta-cognition. They struggle to generalise from one domain to another, and importantly, cannot reliably do either causal inference or counterfactual reasoning. And despite this, Hinton is also claiming that—effectively—they have a limbic system!\n\nThese are not just things you can graft on—the maths behind causality and counterfactuals was only worked out in the 1990s, it’s subtle, and it’s unreasonable to assume that the required functionality will somehow just emerge. Solid models of emotion? Even more challenging.\n\nSure, you can scrape the toast—pick up the ‘defective patterns of behaviour’, and put on extra prosthetics to try to make your LLM ‘behave’. But designing in the right functionality—well, that’s the charm. We’re simply not there yet. As Gary Marcus has pointed out repeatedly, it appears unlikely that current models will ever get there, simply by extension.\n\nAnd even then, you shouldn’t rely on a demonstration of ‘intelligent performance’. That’s the most unconvincing thing ever.\n\nIt’s just Unscience.\n\nMy 2c, Dr Jo.\n\n∗\n∗\n I’m not going to explore silly and pointless digressions like Searle’s Chinese Room argument, ‘qualia’, dualism, and other philosophical claptrap.\n§\n§\n Let’s also lay aside whether the evaluator should be a computer, how long you should bash on for, and how smart the benchmark human being should be.\n\n†\n†\n I have significant prosopagnosia, so don’t take my word for this.\n\n‡\n‡\n Metacognitively <grin>\n\n§\n§\n (Waits for explosion from philosophers who haven’t quite caught up).\n\nFootnotes\n\n[1] Computing Machinery and Intelligence - Wikipedia\n[2] Turing test - Wikipedia\n[3] Dr Jo\'s answer to What is a ""stochastic parrot""?\n[4] Geoffrey Hinton: Large Language Models in Medicine. They Understand and Have Empathy', 'aiModelVersion': '1'}",0.788
Franklin Veaux,2y,Does being able to talk to a robot mean it thinks?,"It…depends.

A philosopher named John Searle invented the idea of the “Chinese room,” which argues that a person in a room who does not speak, read, or understand Chinese could give an illusion of understanding simply by taking Chinese notes, consulting a book that’s essentially just a giant lookup table, then responding however the book tells him to respond in Chinese. Ergo, you can talk to him in Chinese, but there’s no comprehension or understanding.

This turns out to be a completely bullshit argument, but then again, the same John Searle also claims to have invented a magic free-energy machine and an antigravity machine, so consider the source.

John Searle is philosophically opposed to the idea of strong AI; he believes there’s something mystical about consciousness and awareness of self that can never, ever be reproduced in a machine. He can’t defend that idea, so he approaches it from the other direction: giving a reason to believe a machine that seems to be self-aware actually isn’t.

The thing is, people who do natural language processing for a living can explain to you—often in excruciating detail and with lots of hilarious examples—that language generation and response cannot be done with a simple, or even a complex, set of rules and look-up tables. You can mimic natural language comprehension only in limited domains for limited amounts of time that way.

Language is big. Language is complicated. Language is reflexive and self-modifying and the awareness of self seems bound up in the awareness of language to such an extent I don’t think you can so glibly dismiss it as Searle does without one hell of a convincing argument, which so far I have yet to see.

That guy in the room? Would not fool a native, aware Chinese speaker for longer than the first few exchanges. A lot of companies have thrown a lot of money and a lot of comp sci Ph.D.s at the problem, and as soon as you stray from fairly narrow conversational subjects, even sophisticated AI chatbots backed by tremendous computational horsepower tend to bomb the Turing Test.

But let’s assume that a machine can pass the Turing Test. Let’s assume, for the sake of argument, you accept it’s possible for a blind, non-sentient lookup table to mimic natural language conversation flawlessly.

When faced with a bot that seems sentient, but you have no way to prove whether it’s sentient or just a blind lookup table, any reasonable set of ethics—consequentialism, utilitarianism, hell even just harm reduction—all would say that the ethical thing to do is to treat it as a self-aware person.

If you treat it as a self-aware person and it’s just a thing, you actually don’t lose anything. You haven’t committed an ethical violation.

If, on the other hand, you treat it as a thing and it’s a self-aware person, you’ve committed a gross ethical violation. And if you treat it as a thing and it’s a self-aware person, simply because you believe biological life has some sort of magic essence that you can’t identify or explain but it makes you think all non-biological things are always things, then you’ve committed a gross ethical violation for no reason other than you’re a bigot.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/n379zilkcrwugfs5', 'title': 'Does being able to talk to a robot mean it thinks?', 'score': {'original': 0.47725, 'ai': 0.52275}, 'blocks': [{'text': 'It…depends.\n\nA philosopher named John Searle invented the idea of the “Chinese room,” which argues that a person in a room who does not speak, read, or understand Chinese could give an illusion of understanding simply by taking Chinese notes, consulting a book that’s essentially just a giant lookup table, then responding however the book tells him to respond in Chinese. Ergo, you can talk to him in Chinese, but there’s no comprehension or understanding.\n\nThis turns out to be a completely bullshit argument, but then again, the same John Searle also claims to have invented a magic free-energy machine and an antigravity machine, so consider the source.\n\nJohn Searle is philosophically opposed to the idea of strong AI; he believes there’s something mystical about consciousness and awareness of self that can never, ever be reproduced in a machine. He can’t defend that idea, so he approaches it from the other direction: giving a reason to believe a machine that seems to be self-aware actually isn’t.\n\nThe thing is, people who do natural language processing for a living can explain to you—often in excruciating detail and with lots of hilarious examples—that language generation and response cannot be done with a simple, or even a complex, set of rules and look-up tables. You can mimic natural language comprehension only in limited domains for limited amounts of time that way.\n\nLanguage is big. Language is complicated. Language is reflexive and self-modifying and the awareness of self seems bound up in the awareness of language to such an extent I don’t think you can so glibly dismiss it as Searle does without one hell of a convincing argument, which so far I have yet to see.\n\nThat guy in the room? Would not fool a native, aware Chinese speaker for longer than the first few exchanges. A lot of companies have thrown a lot of money and a lot of comp sci Ph.D.s at the problem, and as soon as you stray from fairly narrow conversational subjects, even sophisticated AI chatbots backed by tremendous computational horsepower tend to bomb the Turing Test.\n\nBut let’s assume that a machine can pass the Turing Test. Let’s assume, for the sake of argument, you accept it’s possible for a blind, non-sentient lookup table to mimic natural language conversation flawlessly.\n\nWhen faced with a bot that seems sentient, but you have no way to prove whether it’s sentient or just a blind lookup table, any reasonable set of ethics—consequentialism, utilitarianism, hell even just harm reduction—all would say that the ethical thing to do is to treat it as a self-aware person.\n\nIf you treat it as a self-aware person and it’s just a thing, you actually don’t lose anything. You haven’t committed an ethical violation.\n\nIf, on the other hand, you treat it as a thing and it’s a self-aware person, you’ve committed a gross ethical violation. And if you treat it as a thing and it’s a self-aware person, simply because you believe biological life has some sort of magic essence that you can’t identify or explain but it makes you think all non-biological things are always things,', 'result': {'fake': 0.0127, 'real': 0.9873}, 'status': 'success'}, {'text': 'then you’ve committed a gross ethical violation for no reason other than you’re a bigot.', 'result': {'fake': 0.9997, 'real': 0.0003}, 'status': 'success'}], 'credits_used': 6, 'credits': 1982965, 'subscription': 0, 'content': 'It…depends.\n\nA philosopher named John Searle invented the idea of the “Chinese room,” which argues that a person in a room who does not speak, read, or understand Chinese could give an illusion of understanding simply by taking Chinese notes, consulting a book that’s essentially just a giant lookup table, then responding however the book tells him to respond in Chinese. Ergo, you can talk to him in Chinese, but there’s no comprehension or understanding.\n\nThis turns out to be a completely bullshit argument, but then again, the same John Searle also claims to have invented a magic free-energy machine and an antigravity machine, so consider the source.\n\nJohn Searle is philosophically opposed to the idea of strong AI; he believes there’s something mystical about consciousness and awareness of self that can never, ever be reproduced in a machine. He can’t defend that idea, so he approaches it from the other direction: giving a reason to believe a machine that seems to be self-aware actually isn’t.\n\nThe thing is, people who do natural language processing for a living can explain to you—often in excruciating detail and with lots of hilarious examples—that language generation and response cannot be done with a simple, or even a complex, set of rules and look-up tables. You can mimic natural language comprehension only in limited domains for limited amounts of time that way.\n\nLanguage is big. Language is complicated. Language is reflexive and self-modifying and the awareness of self seems bound up in the awareness of language to such an extent I don’t think you can so glibly dismiss it as Searle does without one hell of a convincing argument, which so far I have yet to see.\n\nThat guy in the room? Would not fool a native, aware Chinese speaker for longer than the first few exchanges. A lot of companies have thrown a lot of money and a lot of comp sci Ph.D.s at the problem, and as soon as you stray from fairly narrow conversational subjects, even sophisticated AI chatbots backed by tremendous computational horsepower tend to bomb the Turing Test.\n\nBut let’s assume that a machine can pass the Turing Test. Let’s assume, for the sake of argument, you accept it’s possible for a blind, non-sentient lookup table to mimic natural language conversation flawlessly.\n\nWhen faced with a bot that seems sentient, but you have no way to prove whether it’s sentient or just a blind lookup table, any reasonable set of ethics—consequentialism, utilitarianism, hell even just harm reduction—all would say that the ethical thing to do is to treat it as a self-aware person.\n\nIf you treat it as a self-aware person and it’s just a thing, you actually don’t lose anything. You haven’t committed an ethical violation.\n\nIf, on the other hand, you treat it as a thing and it’s a self-aware person, you’ve committed a gross ethical violation. And if you treat it as a thing and it’s a self-aware person, simply because you believe biological life has some sort of magic essence that you can’t identify or explain but it makes you think all non-biological things are always things, then you’ve committed a gross ethical violation for no reason other than you’re a bigot.', 'aiModelVersion': '1'}",0.47725
Fred Feinberg,6y,What do you think of Noam Chomsky's view on modern AI?,"[Disclosure: I took a syntax class with Chomsky in grad school and regard him as one of the few living researchers to whom the term “genius” fully applies. I’m also primarily a statistician, which may be apparent in my reaction.]

TL;DR: Chomsky and the AI people are simply talking past one another. He is both “correct” and short-sighted.

Longer version:

Chomsky is critiquing what was referred to in the article as “new systems approaches to understanding the brain.” Let’s unpack that, and distinguish three (to me) rather different goals:

Understanding how the human brain does what it does.
Understanding some general notion of “intelligence”.
Creating a machine-based system that can reproduce what human intelligence does.

The problem with this entire debate is that these three themes (among others) are continually interwoven in discussions without clear distinctions among their assumptions and goalposts for success.

The basic issue comes up in Turing Tests and more pointedly in Searles’ Chinese room
. Simply put: if we can create a system that reproduces another system so well that we cannot tell them apart, does studying the first system tell us anything critical about the second?

Chomsky’s answer, which is hard to argue fully against, is No. Norvig, who is answering a different question, is also correct in his domain, and offers a qualified Yes. Their specific battle ground is language. Norvig and many other researchers have had smashing success using computationally intensive methods to deal with real-time natural language processing. Despite these successes, it’s still in its infancy: the sorts of errors it makes are very different from the sorts of errors humans make, and stem nearly entirely from conceptual misunderstandings. Doug Lenat has spent years building CYC (e.g., An AI that spent 30 years learning some common sense is ready for work
), which attempts to bridge that gap by providing full contextual information for computers attempting to use natural language and conceptual reasoning.

To take a simple example, the word “home”. It comes up in baseball, it comes up in sales of houses, it comes up in 50 other ways; and, more elusively, when we say “metaphorical” things such as “it was, you know, like going home.” Humans know what it FEELS like — the “qualia” — of returning home. Computers don’t. The chief questions, to me, are as follows:

Can computers “fully feel” what its like to “go home”? [After all, human brains can. Are human brains somehow noncorporeal? If so: wooooooo.]
Can computers convincingly replicate in any way you’d like that they’ve appreciated the concept of “going home”?

My sense of Chomsky’s position is that, even if computers could do (2), they not only are not doing (1), but studying a system that can do (2) tells us nothing about the actually human system of doing (1).

I fully agree with Chomsky on this. If there were a neural net system that could take in all the sensations of a child and train itself and for all intents and purposes reproduce everything a child says and does, merely examining the “parameters” of such a system would tell us little about how a child actually goes about dealing with the world, reasoning, feeling, seeing, experiencing, etc. No sane machine learning specialist would confuse a computer implementation of a task with how humans accomplish it.

But Chomsky may be selling computers short. Right now, one obvious glitch in computer reasoning is that it’s simply too good: computers have essentially limitless memory, and can perform computations essentially infinitely beyond humans. A system that fully reproduced the range of human behavior — including errors, boundedness, and everything else, as a natural consequence of its internal state and not merely by “hobbling” (e.g., no fast division, please) — could be “queried” in ways no single human could, and might potentially provide some insight into the Inner Workings of our brain’s fault-tolerant, resilient, evolved, interoperating systems.

Chomsky seems to think this could “never” happen. My sense is the jury is still out.

[And I’d add that this answer is a ridiculously compressed micro-overview of a topic that has occupied philosophers, computer scientists, and linguists for generations. No one is going to nail the topic or the complexity of Chomsky’s views in a few sentences.]","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/x0w6k37u9amzph8o', 'title': ""What do you think of Noam Chomsky's view on modern AI?"", 'score': {'original': 0.98575, 'ai': 0.01425}, 'blocks': [{'text': '[Disclosure: I took a syntax class with Chomsky in grad school and regard him as one of the few living researchers to whom the term “genius” fully applies. I’m also primarily a statistician, which may be apparent in my reaction.]\n\nTL;DR: Chomsky and the AI people are simply talking past one another. He is both “correct” and short-sighted.\n\nLonger version:\n\nChomsky is critiquing what was referred to in the article as “new systems approaches to understanding the brain.” Let’s unpack that, and distinguish three (to me) rather different goals:\n\nUnderstanding how the human brain does what it does.\nUnderstanding some general notion of “intelligence”.\nCreating a machine-based system that can reproduce what human intelligence does.\n\nThe problem with this entire debate is that these three themes (among others) are continually interwoven in discussions without clear distinctions among their assumptions and goalposts for success.\n\nThe basic issue comes up in Turing Tests and more pointedly in Searles’ Chinese room\n. Simply put: if we can create a system that reproduces another system so well that we cannot tell them apart, does studying the first system tell us anything critical about the second?\n\nChomsky’s answer, which is hard to argue fully against, is No. Norvig, who is answering a different question, is also correct in his domain, and offers a qualified Yes. Their specific battle ground is language. Norvig and many other researchers have had smashing success using computationally intensive methods to deal with real-time natural language processing. Despite these successes, it’s still in its infancy: the sorts of errors it makes are very different from the sorts of errors humans make, and stem nearly entirely from conceptual misunderstandings. Doug Lenat has spent years building CYC (e.g., An AI that spent 30 years learning some common sense is ready for work\n), which attempts to bridge that gap by providing full contextual information for computers attempting to use natural language and conceptual reasoning.\n\nTo take a simple example, the word “home”. It comes up in baseball, it comes up in sales of houses, it comes up in 50 other ways; and, more elusively, when we say “metaphorical” things such as “it was, you know, like going home.” Humans know what it FEELS like — the “qualia” — of returning home. Computers don’t. The chief questions, to me, are as follows:\n\nCan computers “fully feel” what its like to “go home”? [After all, human brains can. Are human brains somehow noncorporeal? If so: wooooooo.]\nCan computers convincingly replicate in any way you’d like that they’ve appreciated the concept of “going home”?\n\nMy sense of Chomsky’s position is that, even if computers could do (2), they not only are not doing (1), but studying a system that can do (2) tells us nothing about the actually human system of doing (1).\n\nI fully agree with Chomsky on this. If there were a neural net system that could take in all the sensations of a child and train itself and for all intents and purposes reproduce everything a child says and does, merely examining the “parameters” of such a system would tell us little about how a child actually goes about dealing with', 'result': {'fake': 0.0253, 'real': 0.9747}, 'status': 'success'}, {'text': 'the world, reasoning, feeling, seeing, experiencing, etc. No sane machine learning specialist would confuse a computer implementation of a task with how humans accomplish it.\n\nBut Chomsky may be selling computers short. Right now, one obvious glitch in computer reasoning is that it’s simply too good: computers have essentially limitless memory, and can perform computations essentially infinitely beyond humans. A system that fully reproduced the range of human behavior — including errors, boundedness, and everything else, as a natural consequence of its internal state and not merely by “hobbling” (e.g., no fast division, please) — could be “queried” in ways no single human could, and might potentially provide some insight into the Inner Workings of our brain’s fault-tolerant, resilient, evolved, interoperating systems.\n\nChomsky seems to think this could “never” happen. My sense is the jury is still out.\n\n[And I’d add that this answer is a ridiculously compressed micro-overview of a topic that has occupied philosophers, computer scientists, and linguists for generations. No one is going to nail the topic or the complexity of Chomsky’s views in a few sentences.]', 'result': {'fake': 0.7758, 'real': 0.2242}, 'status': 'success'}], 'credits_used': 8, 'credits': 1982957, 'subscription': 0, 'content': '[Disclosure: I took a syntax class with Chomsky in grad school and regard him as one of the few living researchers to whom the term “genius” fully applies. I’m also primarily a statistician, which may be apparent in my reaction.]\n\nTL;DR: Chomsky and the AI people are simply talking past one another. He is both “correct” and short-sighted.\n\nLonger version:\n\nChomsky is critiquing what was referred to in the article as “new systems approaches to understanding the brain.” Let’s unpack that, and distinguish three (to me) rather different goals:\n\nUnderstanding how the human brain does what it does.\nUnderstanding some general notion of “intelligence”.\nCreating a machine-based system that can reproduce what human intelligence does.\n\nThe problem with this entire debate is that these three themes (among others) are continually interwoven in discussions without clear distinctions among their assumptions and goalposts for success.\n\nThe basic issue comes up in Turing Tests and more pointedly in Searles’ Chinese room\n. Simply put: if we can create a system that reproduces another system so well that we cannot tell them apart, does studying the first system tell us anything critical about the second?\n\nChomsky’s answer, which is hard to argue fully against, is No. Norvig, who is answering a different question, is also correct in his domain, and offers a qualified Yes. Their specific battle ground is language. Norvig and many other researchers have had smashing success using computationally intensive methods to deal with real-time natural language processing. Despite these successes, it’s still in its infancy: the sorts of errors it makes are very different from the sorts of errors humans make, and stem nearly entirely from conceptual misunderstandings. Doug Lenat has spent years building CYC (e.g., An AI that spent 30 years learning some common sense is ready for work\n), which attempts to bridge that gap by providing full contextual information for computers attempting to use natural language and conceptual reasoning.\n\nTo take a simple example, the word “home”. It comes up in baseball, it comes up in sales of houses, it comes up in 50 other ways; and, more elusively, when we say “metaphorical” things such as “it was, you know, like going home.” Humans know what it FEELS like — the “qualia” — of returning home. Computers don’t. The chief questions, to me, are as follows:\n\nCan computers “fully feel” what its like to “go home”? [After all, human brains can. Are human brains somehow noncorporeal? If so: wooooooo.]\nCan computers convincingly replicate in any way you’d like that they’ve appreciated the concept of “going home”?\n\nMy sense of Chomsky’s position is that, even if computers could do (2), they not only are not doing (1), but studying a system that can do (2) tells us nothing about the actually human system of doing (1).\n\nI fully agree with Chomsky on this. If there were a neural net system that could take in all the sensations of a child and train itself and for all intents and purposes reproduce everything a child says and does, merely examining the “parameters” of such a system would tell us little about how a child actually goes about dealing with the world, reasoning, feeling, seeing, experiencing, etc. No sane machine learning specialist would confuse a computer implementation of a task with how humans accomplish it.\n\nBut Chomsky may be selling computers short. Right now, one obvious glitch in computer reasoning is that it’s simply too good: computers have essentially limitless memory, and can perform computations essentially infinitely beyond humans. A system that fully reproduced the range of human behavior — including errors, boundedness, and everything else, as a natural consequence of its internal state and not merely by “hobbling” (e.g., no fast division, please) — could be “queried” in ways no single human could, and might potentially provide some insight into the Inner Workings of our brain’s fault-tolerant, resilient, evolved, interoperating systems.\n\nChomsky seems to think this could “never” happen. My sense is the jury is still out.\n\n[And I’d add that this answer is a ridiculously compressed micro-overview of a topic that has occupied philosophers, computer scientists, and linguists for generations. No one is going to nail the topic or the complexity of Chomsky’s views in a few sentences.]', 'aiModelVersion': '1'}",0.98575
Reza Borhani,4y,What is mirror descent and how is it different from gradient descent?,"To understand mirror descent it's helpful to first look at gradient descent from a proximal perspective. What's that you ask? I'll explain. In machine learning contexts gradient descent is often introduced like this: given a cost function ggg, a steplength αα\alpha, and the current input point wkwk\mathbf{w}^{k} (this is the vector of all model parameters at the kkkth step of learning), we update the parameter vector and move to a new point wk+1wk+1\mathbf{w}^{k+1}, computed as wk+1=wk−α∇g(wk)wk+1=wk−α∇g(wk)\mathbf{w}^{k+1}=\mathbf{w}^{k}-\alpha\nabla g\left(\mathbf{w}^{k}\right) Simple enough: at each point you descend in the direction of (negative) gradient. However there's a different perspective on gradient decent - which I personally like a lot - which goes as follows: Forget for a moment you know about gradient descent, and imagine you want to minimize ggg starting at point wkwk\mathbf{w}^{k}.Now, the function ggg itself can be super-complicated (think of a multi-layer neural network). So let's be lazy and replace ggg with its first order Taylor series approximation (the blue tangent line) and minimize it instead: after all, it is just a line (or hyperplane in higher dimensions). How hard can it be to minimize a simple function like that? Not difficult at all! There's one problem though: hyperplanes are (generally) unbounded meaning that minimizeg(wk)+[∇g(wk)]T(w−wk)minimizeg(wk)+[∇g(wk)]T(w−wk)\text{minimize}\qquad g\left(\mathbf{w}^{k}\right)+\left[\nabla g\left(\mathbf{w}^{k}\right)\right]^{T}\left(\mathbf{w}-\mathbf{w}^{k}\right) will return −∞−∞-\infty and that's not useful! So let's make a compromise and adjust the objective function above by adding the term 12α∥∥w−wk∥∥22=12α(w−wk)T(w−wk)12α‖w−wk‖22=12α(w−wk)T(w−wk)\frac{1}{2\alpha}\left\Vert \mathbf{w}-\mathbf{w}^{k}\right\Vert _{2}^{2}=\frac{1}{2\alpha}\left(\mathbf{w}-\mathbf{w}^{k}\right)^{T}\left(\mathbf{w}-\mathbf{w}^{k}\right) It's a quadratic centered at wkwk\mathbf{w}^{k}, guaranteeing that the new objective function is now bounded from below (i.e., it now has a minimum). But why does it make sense to add this particular term? Notice, the addition of 12α∥∥w−wk∥∥2212α‖w−wk‖22\frac{1}{2\alpha}\left\Vert \mathbf{w}-\mathbf{w}^{k}\right\Vert _{2}^{2} essentially penalizes the new objective harshly (quadratically) if you step too far away from wkwk\mathbf{w}^{k}, or in other words, encourages you to stay in a proximity of wkwk\mathbf{w}^{k}, which makes sense for two reasons: First, you'd hope that your algorithm eventually converges, so as it runs you expect the updates to become closer and closer to one another, and eventually converge. Secondly, the validity of our Taylor series approximation deteriorates as we move away from the center point wkwk\mathbf{w}^{k}. In other words, the closer you are to wkwk\mathbf{w}^{k}, the better the approximation provided by the Taylor series. Okay, I said all of this to bring us to this point to minimizeg(wk)+[∇g(wk)]T(w−wk)+12α∥∥w−wk∥∥22minimizeg(wk)+[∇g(wk)]T(w−wk)+12α‖w−wk‖22\text{minimize}\qquad g\left(\mathbf{w}^{k}\right)+\left[\nabla g\left(\mathbf{w}^{k}\right)\right]^{T}\left(\mathbf{w}-\mathbf{w}^{k}\right)+\frac{1}{2\alpha}\left\Vert \mathbf{w}-\mathbf{w}^{k}\right\Vert _{2}^{2} If you're excited to see a new algorithm emerge, I'm afraid it won't happen. Turns out what minimizes the objective above is, our old buddy, gradient descent. That is, w=wk−α∇g(wk)w=wk−α∇g(wk)\mathbf{w}=\mathbf{w}^{k}-\alpha\nabla g\left(\mathbf{w}^{k}\right) is the solution to the last minimization problem. You can verify it yourself by taking the gradient and set it to zero, or by completing the square - whichever you're more comfortable with. So, lets wrap this up. We just saw that gradient descent can be derived using a first order Taylor series approximation (the blue line) augmented by addition of the proximity term 12α∥∥w−wk∥∥2212α‖w−wk‖22\frac{1}{2\alpha}\left\Vert \mathbf{w}-\mathbf{w}^{k}\right\Vert _{2}^{2} causing ww\mathbf{w} to stay reasonably close to wkwk\mathbf{w}^{k}. Kudos to you if you've stuck around to this point! Your reward is that you now know how mirror descent works: just replace 12α∥∥w−wk∥∥2212α‖w−wk‖22\frac{1}{2\alpha}\left\Vert \mathbf{w}-\mathbf{w}^{k}\right\Vert _{2}^{2} with a different, more general kind of distance (referred to as the Bregman distance). After all, there are infinitely many ways to define similarity/distance between two points and the Euclidean distance (which we used with gradient descent) is just one of them. Therefore, mirror descent is achieved by solving minimizeg(wk)+[∇g(wk)]T(w−wk)+12αψ(w,wk)minimizeg(wk)+[∇g(wk)]T(w−wk)+12αψ(w,wk)\text{minimize}\qquad g\left(\mathbf{w}^{k}\right)+\left[\nabla g\left(\mathbf{w}^{k}\right)\right]^{T}\left(\mathbf{w}-\mathbf{w}^{k}\right)+\frac{1}{2\alpha}\psi\left(\mathbf{w},\mathbf{\,w}^{k}\right) where ψ(w,wk)ψ(w,wk)\psi\left(\mathbf{w},\mathbf{\,w}^{k}\right) is the Bregman distance between ww\mathbf{w} and wkwk\mathbf{w}^{k}. Mirror descent was first introduced by Nemirovski and Yudin. If you can stomach more math, read their original work below. *Arkadi Nemirovsky and David Yudin. Problem Complexity and Method Efficiency in Optimization. John Wiley & Sons, 1983.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/t7vqizsm1ebn9cog', 'title': 'What is mirror descent and how is it different from gradient descent?', 'score': {'original': 0.83875, 'ai': 0.16125}, 'blocks': [{'text': ""To understand mirror descent it's helpful to first look at gradient descent from a proximal perspective. What's that you ask? I'll explain. In machine learning contexts gradient descent is often introduced like this: given a cost function ggg, a steplength αα\\alpha, and the current input point wkwk\\mathbf{w}^{k} (this is the vector of all model parameters at the kkkth step of learning), we update the parameter vector and move to a new point wk+1wk+1\\mathbf{w}^{k+1}, computed as wk+1=wk−α∇g(wk)wk+1=wk−α∇g(wk)\\mathbf{w}^{k+1}=\\mathbf{w}^{k}-\\alpha\\nabla g\\left(\\mathbf{w}^{k}\\right) Simple enough: at each point you descend in the direction of (negative) gradient. However there's a different perspective on gradient decent - which I personally like a lot - which goes as follows: Forget for a moment you know about gradient descent, and imagine you want to minimize ggg starting at point wkwk\\mathbf{w}^{k}.Now, the function ggg itself can be super-complicated (think of a multi-layer neural network). So let's be lazy and replace ggg with its first order Taylor series approximation (the blue tangent line) and minimize it instead: after all, it is just a line (or hyperplane in higher dimensions). How hard can it be to minimize a simple function like that? Not difficult at all! There's one problem though: hyperplanes are (generally) unbounded meaning that minimizeg(wk)+[∇g(wk)]T(w−wk)minimizeg(wk)+[∇g(wk)]T(w−wk)\\text{minimize}\\qquad g\\left(\\mathbf{w}^{k}\\right)+\\left[\\nabla g\\left(\\mathbf{w}^{k}\\right)\\right]^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right) will return −∞−∞-\\infty and that's not useful! So let's make a compromise and adjust the objective function above by adding the term 12α∥∥w−wk∥∥22=12α(w−wk)T(w−wk)12α‖w−wk‖22=12α(w−wk)T(w−wk)\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2}=\\frac{1}{2\\alpha}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right)^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right) It's a quadratic centered at wkwk\\mathbf{w}^{k}, guaranteeing that the new objective function is now bounded from below (i.e., it now has a minimum). But why does it make sense to add this particular term? Notice, the addition of 12α∥∥w−wk∥∥2212α‖w−wk‖22\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} essentially penalizes the new objective harshly (quadratically) if you step too far away from wkwk\\mathbf{w}^{k}, or in other words, encourages you to stay in a proximity of wkwk\\mathbf{w}^{k}, which makes sense for two reasons: First, you'd hope that your algorithm eventually converges, so as it runs you expect the updates to become closer and closer to one another, and eventually converge. Secondly, the validity of our Taylor series approximation deteriorates as we move away from the center point wkwk\\mathbf{w}^{k}. In other words, the closer you are to wkwk\\mathbf{w}^{k}, the better the approximation provided by the Taylor series. Okay, I said all of this to bring us to this point to minimizeg(wk)+[∇g(wk)]T(w−wk)+12α∥∥w−wk∥∥22minimizeg(wk)+[∇g(wk)]T(w−wk)+12α‖w−wk‖22\\text{minimize}\\qquad g\\left(\\mathbf{w}^{k}\\right)+\\left[\\nabla g\\left(\\mathbf{w}^{k}\\right)\\right]^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right)+\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} If you're excited to see a new algorithm emerge, I'm afraid it won't happen. Turns out what minimizes the objective above is, our old buddy, gradient descent. That is, w=wk−α∇g(wk)w=wk−α∇g(wk)\\mathbf{w}=\\mathbf{w}^{k}-\\alpha\\nabla g\\left(\\mathbf{w}^{k}\\right) is the solution to the last minimization problem. You can verify it yourself by taking the gradient and set it to zero, or by completing the square - whichever you're more comfortable with. So, lets wrap this up. We just saw that gradient descent can be derived using a first order Taylor series approximation (the blue line) augmented by addition of the proximity term 12α∥∥w−wk∥∥2212α‖w−wk‖22\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} causing ww\\mathbf{w} to stay reasonably close to wkwk\\mathbf{w}^{k}. Kudos to you if you've stuck around to this point! Your reward is that you now know how mirror"", 'result': {'fake': 0.0131, 'real': 0.9869}, 'status': 'success'}, {'text': 'descent works: just replace 12α∥∥w−wk∥∥2212α‖w−wk‖22\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} with a different, more general kind of distance (referred to as the Bregman distance). After all, there are infinitely many ways to define similarity/distance between two points and the Euclidean distance (which we used with gradient descent) is just one of them. Therefore, mirror descent is achieved by solving minimizeg(wk)+[∇g(wk)]T(w−wk)+12αψ(w,wk)minimizeg(wk)+[∇g(wk)]T(w−wk)+12αψ(w,wk)\\text{minimize}\\qquad g\\left(\\mathbf{w}^{k}\\right)+\\left[\\nabla g\\left(\\mathbf{w}^{k}\\right)\\right]^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right)+\\frac{1}{2\\alpha}\\psi\\left(\\mathbf{w},\\mathbf{\\,w}^{k}\\right) where ψ(w,wk)ψ(w,wk)\\psi\\left(\\mathbf{w},\\mathbf{\\,w}^{k}\\right) is the Bregman distance between ww\\mathbf{w} and wkwk\\mathbf{w}^{k}. Mirror descent was first introduced by Nemirovski and Yudin. If you can stomach more math, read their original work below. *Arkadi Nemirovsky and David Yudin. Problem Complexity and Method Efficiency in Optimization. John Wiley & Sons, 1983.', 'result': {'fake': 0.5163, 'real': 0.4837}, 'status': 'success'}], 'credits_used': 10, 'credits': 1982947, 'subscription': 0, 'content': ""To understand mirror descent it's helpful to first look at gradient descent from a proximal perspective. What's that you ask? I'll explain. In machine learning contexts gradient descent is often introduced like this: given a cost function ggg, a steplength αα\\alpha, and the current input point wkwk\\mathbf{w}^{k} (this is the vector of all model parameters at the kkkth step of learning), we update the parameter vector and move to a new point wk+1wk+1\\mathbf{w}^{k+1}, computed as wk+1=wk−α∇g(wk)wk+1=wk−α∇g(wk)\\mathbf{w}^{k+1}=\\mathbf{w}^{k}-\\alpha\\nabla g\\left(\\mathbf{w}^{k}\\right) Simple enough: at each point you descend in the direction of (negative) gradient. However there's a different perspective on gradient decent - which I personally like a lot - which goes as follows: Forget for a moment you know about gradient descent, and imagine you want to minimize ggg starting at point wkwk\\mathbf{w}^{k}.Now, the function ggg itself can be super-complicated (think of a multi-layer neural network). So let's be lazy and replace ggg with its first order Taylor series approximation (the blue tangent line) and minimize it instead: after all, it is just a line (or hyperplane in higher dimensions). How hard can it be to minimize a simple function like that? Not difficult at all! There's one problem though: hyperplanes are (generally) unbounded meaning that minimizeg(wk)+[∇g(wk)]T(w−wk)minimizeg(wk)+[∇g(wk)]T(w−wk)\\text{minimize}\\qquad g\\left(\\mathbf{w}^{k}\\right)+\\left[\\nabla g\\left(\\mathbf{w}^{k}\\right)\\right]^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right) will return −∞−∞-\\infty and that's not useful! So let's make a compromise and adjust the objective function above by adding the term 12α∥∥w−wk∥∥22=12α(w−wk)T(w−wk)12α‖w−wk‖22=12α(w−wk)T(w−wk)\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2}=\\frac{1}{2\\alpha}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right)^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right) It's a quadratic centered at wkwk\\mathbf{w}^{k}, guaranteeing that the new objective function is now bounded from below (i.e., it now has a minimum). But why does it make sense to add this particular term? Notice, the addition of 12α∥∥w−wk∥∥2212α‖w−wk‖22\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} essentially penalizes the new objective harshly (quadratically) if you step too far away from wkwk\\mathbf{w}^{k}, or in other words, encourages you to stay in a proximity of wkwk\\mathbf{w}^{k}, which makes sense for two reasons: First, you'd hope that your algorithm eventually converges, so as it runs you expect the updates to become closer and closer to one another, and eventually converge. Secondly, the validity of our Taylor series approximation deteriorates as we move away from the center point wkwk\\mathbf{w}^{k}. In other words, the closer you are to wkwk\\mathbf{w}^{k}, the better the approximation provided by the Taylor series. Okay, I said all of this to bring us to this point to minimizeg(wk)+[∇g(wk)]T(w−wk)+12α∥∥w−wk∥∥22minimizeg(wk)+[∇g(wk)]T(w−wk)+12α‖w−wk‖22\\text{minimize}\\qquad g\\left(\\mathbf{w}^{k}\\right)+\\left[\\nabla g\\left(\\mathbf{w}^{k}\\right)\\right]^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right)+\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} If you're excited to see a new algorithm emerge, I'm afraid it won't happen. Turns out what minimizes the objective above is, our old buddy, gradient descent. That is, w=wk−α∇g(wk)w=wk−α∇g(wk)\\mathbf{w}=\\mathbf{w}^{k}-\\alpha\\nabla g\\left(\\mathbf{w}^{k}\\right) is the solution to the last minimization problem. You can verify it yourself by taking the gradient and set it to zero, or by completing the square - whichever you're more comfortable with. So, lets wrap this up. We just saw that gradient descent can be derived using a first order Taylor series approximation (the blue line) augmented by addition of the proximity term 12α∥∥w−wk∥∥2212α‖w−wk‖22\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} causing ww\\mathbf{w} to stay reasonably close to wkwk\\mathbf{w}^{k}. Kudos to you if you've stuck around to this point! Your reward is that you now know how mirror descent works: just replace 12α∥∥w−wk∥∥2212α‖w−wk‖22\\frac{1}{2\\alpha}\\left\\Vert \\mathbf{w}-\\mathbf{w}^{k}\\right\\Vert _{2}^{2} with a different, more general kind of distance (referred to as the Bregman distance). After all, there are infinitely many ways to define similarity/distance between two points and the Euclidean distance (which we used with gradient descent) is just one of them. Therefore, mirror descent is achieved by solving minimizeg(wk)+[∇g(wk)]T(w−wk)+12αψ(w,wk)minimizeg(wk)+[∇g(wk)]T(w−wk)+12αψ(w,wk)\\text{minimize}\\qquad g\\left(\\mathbf{w}^{k}\\right)+\\left[\\nabla g\\left(\\mathbf{w}^{k}\\right)\\right]^{T}\\left(\\mathbf{w}-\\mathbf{w}^{k}\\right)+\\frac{1}{2\\alpha}\\psi\\left(\\mathbf{w},\\mathbf{\\,w}^{k}\\right) where ψ(w,wk)ψ(w,wk)\\psi\\left(\\mathbf{w},\\mathbf{\\,w}^{k}\\right) is the Bregman distance between ww\\mathbf{w} and wkwk\\mathbf{w}^{k}. Mirror descent was first introduced by Nemirovski and Yudin. If you can stomach more math, read their original work below. *Arkadi Nemirovsky and David Yudin. Problem Complexity and Method Efficiency in Optimization. John Wiley & Sons, 1983."", 'aiModelVersion': '1'}",0.83875
Franklin Veaux,2y,Why can't Quora spot harassing posts before they're published? How hard would it be to train an AI to recognise the infantile sentiments and reject them?,"To answer the second part: very hard. I wouldn’t be surprised to learn that’s an AI-complete problem
, meaning it would require a true general AI, not a simple pattern recognition or Bayesian analysis algorithm.

Parsing text is hard. Doable, but hard. It requires a lot of grunt, though.

Understanding context? That’s…a whole different kettle of fish. Understanding context is a whole new level of hard. If Siri and Alexa are trash mobs in an MMO, understand context is a raid boss.

Put it this way: if Quora could solve that problem with machine intelligence, Google would buy them in a heartbeat, and someone might be up for a Nobel Prize in mathematics (if there were such a thing).

Harassment often relies on subtext, innuendo (no, in YOUR end-o!), metaphor, implication, sarcasm…these aren’t things machine learning algos are good at.

I can pretty much guarantee this problem would be far cheaper and easier to handle with a Mechanical Turk solution than with machine learning.

The answer to your question is: really, really, really hard.

Obligatory XKCD
 directly on point about this:

My stalker is once again creating fake profiles that look just like mine to send rape threats to other people. If you receive an abusive PM or comment, please check the profile carefully. It isn’t me.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/a1u85lyectvgk24m', 'title': ""Why can't Quora spot harassing posts before they're published? How hard would it be to train an AI to recognise the infantile sentiments and reject them?"", 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'To answer the second part: very hard. I wouldn’t be surprised to learn that’s an AI-complete problem\n, meaning it would require a true general AI, not a simple pattern recognition or Bayesian analysis algorithm.\n\nParsing text is hard. Doable, but hard. It requires a lot of grunt, though.\n\nUnderstanding context? That’s…a whole different kettle of fish. Understanding context is a whole new level of hard. If Siri and Alexa are trash mobs in an MMO, understand context is a raid boss.\n\nPut it this way: if Quora could solve that problem with machine intelligence, Google would buy them in a heartbeat, and someone might be up for a Nobel Prize in mathematics (if there were such a thing).\n\nHarassment often relies on subtext, innuendo (no, in YOUR end-o!), metaphor, implication, sarcasm…these aren’t things machine learning algos are good at.\n\nI can pretty much guarantee this problem would be far cheaper and easier to handle with a Mechanical Turk solution than with machine learning.\n\nThe answer to your question is: really, really, really hard.\n\nObligatory XKCD\n directly on point about this:\n\nMy stalker is once again creating fake profiles that look just like mine to send rape threats to other people. If you receive an abusive PM or comment, please check the profile carefully. It isn’t me.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982944, 'subscription': 0, 'content': 'To answer the second part: very hard. I wouldn’t be surprised to learn that’s an AI-complete problem\n, meaning it would require a true general AI, not a simple pattern recognition or Bayesian analysis algorithm.\n\nParsing text is hard. Doable, but hard. It requires a lot of grunt, though.\n\nUnderstanding context? That’s…a whole different kettle of fish. Understanding context is a whole new level of hard. If Siri and Alexa are trash mobs in an MMO, understand context is a raid boss.\n\nPut it this way: if Quora could solve that problem with machine intelligence, Google would buy them in a heartbeat, and someone might be up for a Nobel Prize in mathematics (if there were such a thing).\n\nHarassment often relies on subtext, innuendo (no, in YOUR end-o!), metaphor, implication, sarcasm…these aren’t things machine learning algos are good at.\n\nI can pretty much guarantee this problem would be far cheaper and easier to handle with a Mechanical Turk solution than with machine learning.\n\nThe answer to your question is: really, really, really hard.\n\nObligatory XKCD\n directly on point about this:\n\nMy stalker is once again creating fake profiles that look just like mine to send rape threats to other people. If you receive an abusive PM or comment, please check the profile carefully. It isn’t me.', 'aiModelVersion': '1'}",0.9996
Håkon Hapnes Strand,4y,How secure will available jobs as a machine learning engineer be in 2026?,"Six years ago, no one knew what a machine learning engineer was. Six years before that, no one knew what a data scientist was. The technology trends are changing fast, and the buzzwords and job titles are changing even faster. Who knows what these jobs will be called six years into the future.

What I do know is this: If you're a skilled machine learning engineer today, if you stay on the same trajectory and keep up with current trends, you will definitely be in high demand in 2026.

What do machine learning engineers do anyway? We set up efficient data solutions, leverage the data for AI and advanced analytics and make sure that all of this works in production.

Those skills will not get outdated any time soon. They were in demand in 1996, they were in demand in 2016 and they will be in demand in 2026.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/lsrxjmct7oni1eya', 'title': 'How secure will available jobs as a machine learning engineer be in 2026?', 'score': {'original': 0.996, 'ai': 0.004}, 'blocks': [{'text': ""Six years ago, no one knew what a machine learning engineer was. Six years before that, no one knew what a data scientist was. The technology trends are changing fast, and the buzzwords and job titles are changing even faster. Who knows what these jobs will be called six years into the future.\n\nWhat I do know is this: If you're a skilled machine learning engineer today, if you stay on the same trajectory and keep up with current trends, you will definitely be in high demand in 2026.\n\nWhat do machine learning engineers do anyway? We set up efficient data solutions, leverage the data for AI and advanced analytics and make sure that all of this works in production.\n\nThose skills will not get outdated any time soon. They were in demand in 1996, they were in demand in 2016 and they will be in demand in 2026."", 'result': {'fake': 0.004, 'real': 0.996}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982942, 'subscription': 0, 'content': ""Six years ago, no one knew what a machine learning engineer was. Six years before that, no one knew what a data scientist was. The technology trends are changing fast, and the buzzwords and job titles are changing even faster. Who knows what these jobs will be called six years into the future.\n\nWhat I do know is this: If you're a skilled machine learning engineer today, if you stay on the same trajectory and keep up with current trends, you will definitely be in high demand in 2026.\n\nWhat do machine learning engineers do anyway? We set up efficient data solutions, leverage the data for AI and advanced analytics and make sure that all of this works in production.\n\nThose skills will not get outdated any time soon. They were in demand in 1996, they were in demand in 2016 and they will be in demand in 2026."", 'aiModelVersion': '1'}",0.996
Nipun Jain,6mo,What are prerequisites to start learning machine learning?,"Learning machine learning can seem like a daunting task, but it doesn't have to be. With the right preparation, anyone can start on the path to mastering machine learning.

What are the prerequisites to start learning machine learning?

The first prerequisite for learning machine learning is basic programming knowledge. Knowing how to code in a language such as Python is essential for understanding machine learning concepts. Additionally, having a basic understanding of linear algebra and calculus is also beneficial.
The second prerequisite is an understanding of data science. Knowing how to work with data and analyze it is an important part of understanding machine learning. Data visualization and data mining are important skills to have when it comes to working with data.
Third, having a basic understanding of statistics is important. Statistics are used to measure and analyze data, which is a key part of machine learning. Being able to interpret the results of data analyses is essential to understanding and using machine learning effectively.
Finally, having a good understanding of machine learning algorithms is important. Having a basic understanding of how different algorithms work, such as linear regression, support vector machines, and artificial neural networks, is essential for understanding how machine learning works. These are the prerequisites to start learning machine learning.

However, the best and most affordable way to learn machine learning is through online courses as they offer flexible schedules and a better environment for working professionals.

Some of the best online platforms that offer machine learning courses are:

Course name:- Artificial Intelligence and Machine Learning Program

Features:

Get a chance to work on 2 Capstone and 10 Advanced projects as these projects will increase your technical knowledge and ability to resolve complex problems. Some of the projects are as follows:
Recommendation system with customer lifetime value analysis(CLV)
Condition-based preventative maintenance and fault prediction in depth
Can choose one domain of your choice and master in that field. As domain specialization helps the recruiter to help you in your job quest and land you in your dream role. Some of the domains are Manufacturing, Retail, Healthcare, and BFSI.
Post-course completion you will receive certificates from IBM and Microsoft and along with that will receive 1 Capstone certificate from IBM.
Students will get a flexible mode of learning where they can learn in both online and offline modes. In their offline mode of learning, students get the chance to visit their Experience Project Centers which are located in cities like Bangalore, Pune, Hyderabad, and Delhi.
Post-course completion students will be assisted by the institute in various aspects so that they get the desired job. Some of the features include resume-building sessions, a chance to attend seminars and webinars with experts, mock tests and interviews, and much more. This feature is also available in offline centers in cities like Bangalore, Pune, Hyderabad, and Delhi.

Course: Introduction to Machine Learning for Data Science

Features:

When it comes to short machine-learning courses, this institute is a wonderful place to start. This program is designed in a well-defined manner from where beginners can start their learning process.

However, this course offers pre-recorded videos and lectures, there is no live interaction with mentors and also it lacks domain-specific training.

Conclusion

If you're looking for machine learning courses, then there are plenty of institutes available in the market, but before making any final decision measure all the parameters.

Thanks for reading!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0v9ays4zcgtilufk', 'title': 'What are prerequisites to start learning machine learning?', 'score': {'original': 0.00015, 'ai': 0.99985}, 'blocks': [{'text': ""Learning machine learning can seem like a daunting task, but it doesn't have to be. With the right preparation, anyone can start on the path to mastering machine learning.\n\nWhat are the prerequisites to start learning machine learning?\n\nThe first prerequisite for learning machine learning is basic programming knowledge. Knowing how to code in a language such as Python is essential for understanding machine learning concepts. Additionally, having a basic understanding of linear algebra and calculus is also beneficial.\nThe second prerequisite is an understanding of data science. Knowing how to work with data and analyze it is an important part of understanding machine learning. Data visualization and data mining are important skills to have when it comes to working with data.\nThird, having a basic understanding of statistics is important. Statistics are used to measure and analyze data, which is a key part of machine learning. Being able to interpret the results of data analyses is essential to understanding and using machine learning effectively.\nFinally, having a good understanding of machine learning algorithms is important. Having a basic understanding of how different algorithms work, such as linear regression, support vector machines, and artificial neural networks, is essential for understanding how machine learning works. These are the prerequisites to start learning machine learning.\n\nHowever, the best and most affordable way to learn machine learning is through online courses as they offer flexible schedules and a better environment for working professionals.\n\nSome of the best online platforms that offer machine learning courses are:\n\nCourse name:- Artificial Intelligence and Machine Learning Program\n\nFeatures:\n\nGet a chance to work on 2 Capstone and 10 Advanced projects as these projects will increase your technical knowledge and ability to resolve complex problems. Some of the projects are as follows:\nRecommendation system with customer lifetime value analysis(CLV)\nCondition-based preventative maintenance and fault prediction in depth\nCan choose one domain of your choice and master in that field. As domain specialization helps the recruiter to help you in your job quest and land you in your dream role. Some of the domains are Manufacturing, Retail, Healthcare, and BFSI.\nPost-course completion you will receive certificates from IBM and Microsoft and along with that will receive 1 Capstone certificate from IBM.\nStudents will get a flexible mode of learning where they can learn in both online and offline modes. In their offline mode of learning, students get the chance to visit their Experience Project Centers which are located in cities like Bangalore, Pune, Hyderabad, and Delhi.\nPost-course completion students will be assisted by the institute in various aspects so that they get the desired job. Some of the features include resume-building sessions, a chance to attend seminars and webinars with experts, mock tests and interviews, and much more. This feature is also available in offline centers in cities like Bangalore, Pune, Hyderabad, and Delhi.\n\nCourse: Introduction to Machine Learning for Data Science\n\nFeatures:\n\nWhen it comes to short machine-learning courses, this institute is a wonderful place to start. This program is designed in a well-defined manner from where beginners can start their learning process.\n\nHowever, this course offers pre-recorded videos and lectures, there is no live interaction with mentors and also"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""it lacks domain-specific training.\n\nConclusion\n\nIf you're looking for machine learning courses, then there are plenty of institutes available in the market, but before making any final decision measure all the parameters.\n\nThanks for reading!"", 'result': {'fake': 0.9998, 'real': 0.0002}, 'status': 'success'}], 'credits_used': 6, 'credits': 1982936, 'subscription': 0, 'content': ""Learning machine learning can seem like a daunting task, but it doesn't have to be. With the right preparation, anyone can start on the path to mastering machine learning.\n\nWhat are the prerequisites to start learning machine learning?\n\nThe first prerequisite for learning machine learning is basic programming knowledge. Knowing how to code in a language such as Python is essential for understanding machine learning concepts. Additionally, having a basic understanding of linear algebra and calculus is also beneficial.\nThe second prerequisite is an understanding of data science. Knowing how to work with data and analyze it is an important part of understanding machine learning. Data visualization and data mining are important skills to have when it comes to working with data.\nThird, having a basic understanding of statistics is important. Statistics are used to measure and analyze data, which is a key part of machine learning. Being able to interpret the results of data analyses is essential to understanding and using machine learning effectively.\nFinally, having a good understanding of machine learning algorithms is important. Having a basic understanding of how different algorithms work, such as linear regression, support vector machines, and artificial neural networks, is essential for understanding how machine learning works. These are the prerequisites to start learning machine learning.\n\nHowever, the best and most affordable way to learn machine learning is through online courses as they offer flexible schedules and a better environment for working professionals.\n\nSome of the best online platforms that offer machine learning courses are:\n\nCourse name:- Artificial Intelligence and Machine Learning Program\n\nFeatures:\n\nGet a chance to work on 2 Capstone and 10 Advanced projects as these projects will increase your technical knowledge and ability to resolve complex problems. Some of the projects are as follows:\nRecommendation system with customer lifetime value analysis(CLV)\nCondition-based preventative maintenance and fault prediction in depth\nCan choose one domain of your choice and master in that field. As domain specialization helps the recruiter to help you in your job quest and land you in your dream role. Some of the domains are Manufacturing, Retail, Healthcare, and BFSI.\nPost-course completion you will receive certificates from IBM and Microsoft and along with that will receive 1 Capstone certificate from IBM.\nStudents will get a flexible mode of learning where they can learn in both online and offline modes. In their offline mode of learning, students get the chance to visit their Experience Project Centers which are located in cities like Bangalore, Pune, Hyderabad, and Delhi.\nPost-course completion students will be assisted by the institute in various aspects so that they get the desired job. Some of the features include resume-building sessions, a chance to attend seminars and webinars with experts, mock tests and interviews, and much more. This feature is also available in offline centers in cities like Bangalore, Pune, Hyderabad, and Delhi.\n\nCourse: Introduction to Machine Learning for Data Science\n\nFeatures:\n\nWhen it comes to short machine-learning courses, this institute is a wonderful place to start. This program is designed in a well-defined manner from where beginners can start their learning process.\n\nHowever, this course offers pre-recorded videos and lectures, there is no live interaction with mentors and also it lacks domain-specific training.\n\nConclusion\n\nIf you're looking for machine learning courses, then there are plenty of institutes available in the market, but before making any final decision measure all the parameters.\n\nThanks for reading!"", 'aiModelVersion': '1'}",0.00015
Mark Phaedrus,Updated 4y,How can an AI train itself if no one is telling it if its answer is correct or wrong?,"“How can an AI train itself if no one is telling it if its answer is correct or wrong?”

This is a great question.

In order to learn how to solve a problem correctly, the AI has to be told how to recognize a “correct” solution to the problem. In order to learn how to better solve a problem that has no one correct answer, the AI has to be told how to recognize when it’s doing “better”.

So at first it seems like the answer to your question is “It can’t.”

But the nuance is in what we mean by “has to be told”. There doesn’t have to be a human trainer there saying “this is right” or “this is better”. The thing giving the AI its feedback on “right” or “better” can be just another computer program.

The most basic example of this is a “maximizer AI”. A human writes a computer program that can look at the current state of the problem and assign it a point score. The higher the score, the better the current state of the problem is. Now the AI can rapidly try different approaches to the problem without needing a human trainer to tell it the results of each attempt. The feedback is immediate and automatic: if approach A produces a higher score than approach B, then the AI knows that approach A is better.

The most classic example of this approach is the typical computer chess program. But that’s been discussed to death. So let me give you a less common and perhaps more fun example: video games.

Specifically, old Atari 2600 video games back from the dawn of game consoles.

There are two useful things about those games from a modern perspective:

1. Simple controls.

The controller used for most Atari games had a very simple eight-direction joystick and one button. That’s it. No analog controls, no complicated six-button control schemes. That means that there’s a very small number of possible actions that a player can take at any given moment.

2. Immediate scoring feedback.

The games typically displayed a score, and the object of the game was to get the highest score possible. What’s more, points were typically scored for very simple actions (like “eat that dot - 1 point”), rather than complex multi-step actions (like “Make your way across this level, avoid getting shot, find the flag at the center of the level, pick it up, and bring it back to your base — 1 point”). That means that the training data the AI needs is quite literally there on the screen. If you do something that makes the score go up, that’s good. If you do something that makes the score go up faster, that’s better.

Starting from Scratch

An AI researcher decided to take advantage of these facts by running an experiment in which an AI learned how to play the games. Note that I didn’t say “in which the researcher taught the AI how to play the games.” The AI started with absolutely no knowledge of the rules of the games. Its baseline knowledge was essentially “Here is a simplified view of what’s on the screen at this moment; here are the possible control signals you can give; and here’s the score. High scores are better. Go.”

This led to a wonderful paper, “Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari
”. It’s a fun read if you can get past the terminology. But let me pick out my favorite example, Q*bert, which showed that this approach can lead to strategies that a human trainer would never have thought of.

Q*bert is a game that originally came out in arcades in 1982. The player controls a little aardvark-looking character that must hop around a pyramid of cubes to change their colors. A few enemies pursue the character and sometimes mess up the colors, requiring the player to fix them. The arcade version looks like this:

The Atari 2600 version… leaves a lot more to the imagination:

But the experiment was about AI gameplay, not art appreciation. So they hooked the AI up to the controls, let it see the screen, told it that high scores are good, and let it loose.

The researchers’ expectation was that the AI would learn to play Q*bert the way a human would — try to find some reasonably-efficient way of covering the board while dodging the bad guys. But two of the test runs went in particularly interesting directions.

Free-fallin’

In the first interesting run, the AI finds a weakness in the way the 2600 version scores the game and awards extra lives (which is different from the arcade version). The AI positions Q*bert on a cube near the edge of the board, waits for the smartest enemy to approach, and then deliberately sends Q*bert to his death by leaping off the board. But the enemy leaps off the board as well in pursuit, and the player gets 500 points for killing that enemy — enough for an extra life. So the AI can simply repeat this indefinitely, methodically and endlessly scoring points.

Breaking the Game

In the second interesting run, the AI finds an even more bizarre bug to exploit. As can be seen in the video, this AI’s Q*bert play skill is suspect at best. But all it has to do is to reach the end of level 1. The game then displays what’s normally a brief animation where the cubes flash for a bit and the player gets some bonus points before moving on to the next level. But the AI has stumbled onto the fact that if it makes a particular series of controller moves during this bonus sequence, the sequence never ends — it just keeps on giving the player points forever and ever and ever. All the AI has to do at that point is avoid committing suicide, and presto, infinite score.

What have we learned?

We’ve seen some of the power and some of the hazards of AI. With very simple automated training data, the AI can learn how to solve surprisingly complex problems completely on its own. But this is also problematic.

The AI’s only concern is on getting that score higher — not on performing the underlying task in a way that a human would consider “normal” or “proper”.
The AI will ignore all the other assumptions we would make about the problem, like “Deliberately killing your own character is bad”.
The AI may even find a solution that seems completely unrelated to the problem, like “Break the bonus screen and score infinite points.”

All of this is fun to watch, as long as the AI is playing video games. If the AI is, say, controlling a patient’s medication or monitoring a nuclear plant, we really don’t want this sort of “creativity”.

So when we use this sort of AI to solve a real-world problem, we need to be extremely careful. We need to make sure that the AI is constrained in what it can do — if we never want it to perform some action, then we need to make sure it can’t. And most importantly, we need to make sure that the scoring system truly reflects the outcome we want.

And this is just a problem with AIs, right?

Absolutely not! We humans exploit faulty scoring systems all the time. We’re so prone to it — and good at it — that English has a wonderfully evocative phrase for it: “gaming the system
”. We perform our tasks in all sorts of crazy ways designed to maximize the things we’ll be rewarded for, at the expense of the actual usefulness of the things we’re doing. So it’s not enough to be careful about designing a scoring system when we’re training our AIs. We need to be just as careful, if not more so, when we’re designing scoring systems for ourselves.

Many thanks for all the upvotes and encouraging feedback!

Other interesting takes on this topic:

Universal Paperclips
, a web game based on the Paperclip Maximizer thought experiment — what happens when you give an AI the goal of “maximize paperclip production” and no other constraints (thanks to Azfar Hisham for mentioning this experiment in the comments).
“AI masters 49 2600 games without instructions”
, an Ars Technica article about a similar AI experiment.
“Human-level control through deep reinforcement learning
”, the paper published in Nature about the experiment.
Minimax Algorithm
 — a Wikipedia article describing a typical approach to turn-based game AI. In this approach, using the same sort of “high scores are better for me” scoring algorithm, the AI picks the best moves for itself by also considering what the best moves are for its opponent. Simplifying greatly, the AI picks some moves that seem promising, because they yield high scores. But instead of simply making the move with the highest score, the AI then considers what the opponent will do in response — it finds the responsive move that yields the lowest score (the one that’s best for the opponent). The move the AI ultimately picks is the one that provokes the least-damaging response from the opponent.
My Little Pony: Friendship is Optimal
 — an MLP fanfic demonstrating quite convincingly why designing an intelligent game AI to maximize the goal of “satisfy human values through friendship and ponies” is probably a very bad idea.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/o7hjcxtwr2bpq849', 'title': 'How can an AI train itself if no one is telling it if its answer is correct or wrong?', 'score': {'original': 0.56185, 'ai': 0.43815}, 'blocks': [{'text': '“How can an AI train itself if no one is telling it if its answer is correct or wrong?”\n\nThis is a great question.\n\nIn order to learn how to solve a problem correctly, the AI has to be told how to recognize a “correct” solution to the problem. In order to learn how to better solve a problem that has no one correct answer, the AI has to be told how to recognize when it’s doing “better”.\n\nSo at first it seems like the answer to your question is “It can’t.”\n\nBut the nuance is in what we mean by “has to be told”. There doesn’t have to be a human trainer there saying “this is right” or “this is better”. The thing giving the AI its feedback on “right” or “better” can be just another computer program.\n\nThe most basic example of this is a “maximizer AI”. A human writes a computer program that can look at the current state of the problem and assign it a point score. The higher the score, the better the current state of the problem is. Now the AI can rapidly try different approaches to the problem without needing a human trainer to tell it the results of each attempt. The feedback is immediate and automatic: if approach A produces a higher score than approach B, then the AI knows that approach A is better.\n\nThe most classic example of this approach is the typical computer chess program. But that’s been discussed to death. So let me give you a less common and perhaps more fun example: video games.\n\nSpecifically, old Atari 2600 video games back from the dawn of game consoles.\n\nThere are two useful things about those games from a modern perspective:\n\n1. Simple controls.\n\nThe controller used for most Atari games had a very simple eight-direction joystick and one button. That’s it. No analog controls, no complicated six-button control schemes. That means that there’s a very small number of possible actions that a player can take at any given moment.\n\n2. Immediate scoring feedback.\n\nThe games typically displayed a score, and the object of the game was to get the highest score possible. What’s more, points were typically scored for very simple actions (like “eat that dot - 1 point”), rather than complex multi-step actions (like “Make your way across this level, avoid getting shot, find the flag at the center of the level, pick it up, and bring it back to your base — 1 point”). That means that the training data the AI needs is quite literally there on the screen. If you do something that makes the score go up, that’s good. If you do something that makes the score go up faster, that’s better.\n\nStarting from Scratch\n\nAn AI researcher decided to take advantage of these facts by running an experiment in which an AI learned how to play the games. Note that I didn’t say “in which the researcher taught the AI how to play the games.” The AI started with absolutely no knowledge of the rules of the games. Its baseline knowledge was essentially “Here is a simplified view', 'result': {'fake': 0.0159, 'real': 0.9841}, 'status': 'success'}, {'text': 'of what’s on the screen at this moment; here are the possible control signals you can give; and here’s the score. High scores are better. Go.”\n\nThis led to a wonderful paper, “Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari\n”. It’s a fun read if you can get past the terminology. But let me pick out my favorite example, Q*bert, which showed that this approach can lead to strategies that a human trainer would never have thought of.\n\nQ*bert is a game that originally came out in arcades in 1982. The player controls a little aardvark-looking character that must hop around a pyramid of cubes to change their colors. A few enemies pursue the character and sometimes mess up the colors, requiring the player to fix them. The arcade version looks like this:\n\nThe Atari 2600 version… leaves a lot more to the imagination:\n\nBut the experiment was about AI gameplay, not art appreciation. So they hooked the AI up to the controls, let it see the screen, told it that high scores are good, and let it loose.\n\nThe researchers’ expectation was that the AI would learn to play Q*bert the way a human would — try to find some reasonably-efficient way of covering the board while dodging the bad guys. But two of the test runs went in particularly interesting directions.\n\nFree-fallin’\n\nIn the first interesting run, the AI finds a weakness in the way the 2600 version scores the game and awards extra lives (which is different from the arcade version). The AI positions Q*bert on a cube near the edge of the board, waits for the smartest enemy to approach, and then deliberately sends Q*bert to his death by leaping off the board. But the enemy leaps off the board as well in pursuit, and the player gets 500 points for killing that enemy — enough for an extra life. So the AI can simply repeat this indefinitely, methodically and endlessly scoring points.\n\nBreaking the Game\n\nIn the second interesting run, the AI finds an even more bizarre bug to exploit. As can be seen in the video, this AI’s Q*bert play skill is suspect at best. But all it has to do is to reach the end of level 1. The game then displays what’s normally a brief animation where the cubes flash for a bit and the player gets some bonus points before moving on to the next level. But the AI has stumbled onto the fact that if it makes a particular series of controller moves during this bonus sequence, the sequence never ends — it just keeps on giving the player points forever and ever and ever. All the AI has to do at that point is avoid committing suicide, and presto, infinite score.\n\nWhat have we learned?\n\nWe’ve seen some of the power and some of the hazards of AI. With very simple automated training data, the AI can learn how to solve surprisingly complex problems completely on its own. But this is also problematic.\n\nThe AI’s only concern is on getting that score higher — not on performing the underlying task in', 'result': {'fake': 0.8932, 'real': 0.1068}, 'status': 'success'}, {'text': 'a way that a human would consider “normal” or “proper”.\nThe AI will ignore all the other assumptions we would make about the problem, like “Deliberately killing your own character is bad”.\nThe AI may even find a solution that seems completely unrelated to the problem, like “Break the bonus screen and score infinite points.”\n\nAll of this is fun to watch, as long as the AI is playing video games. If the AI is, say, controlling a patient’s medication or monitoring a nuclear plant, we really don’t want this sort of “creativity”.\n\nSo when we use this sort of AI to solve a real-world problem, we need to be extremely careful. We need to make sure that the AI is constrained in what it can do — if we never want it to perform some action, then we need to make sure it can’t. And most importantly, we need to make sure that the scoring system truly reflects the outcome we want.\n\nAnd this is just a problem with AIs, right?\n\nAbsolutely not! We humans exploit faulty scoring systems all the time. We’re so prone to it — and good at it — that English has a wonderfully evocative phrase for it: “gaming the system\n”. We perform our tasks in all sorts of crazy ways designed to maximize the things we’ll be rewarded for, at the expense of the actual usefulness of the things we’re doing. So it’s not enough to be careful about designing a scoring system when we’re training our AIs. We need to be just as careful, if not more so, when we’re designing scoring systems for ourselves.\n\nMany thanks for all the upvotes and encouraging feedback!\n\nOther interesting takes on this topic:\n\nUniversal Paperclips\n, a web game based on the Paperclip Maximizer thought experiment — what happens when you give an AI the goal of “maximize paperclip production” and no other constraints (thanks to Azfar Hisham for mentioning this experiment in the comments).\n“AI masters 49 2600 games without instructions”\n, an Ars Technica article about a similar AI experiment.\n“Human-level control through deep reinforcement learning\n”, the paper published in Nature about the experiment.\nMinimax Algorithm\n — a Wikipedia article describing a typical approach to turn-based game AI. In this approach, using the same sort of “high scores are better for me” scoring algorithm, the AI picks the best moves for itself by also considering what the best moves are for its opponent. Simplifying greatly, the AI picks some moves that seem promising, because they yield high scores. But instead of simply making the move with the highest score, the AI then considers what the opponent will do in response — it finds the responsive move that yields the lowest score (the one that’s best for the opponent). The move the AI ultimately picks is the one that provokes the least-damaging response from the opponent.\nMy Little Pony: Friendship is Optimal\n — an MLP fanfic demonstrating quite convincingly why designing an intelligent game AI to maximize the goal of “satisfy human values through friendship and ponies” is probably a very bad idea.', 'result': {'fake': 0.921, 'real': 0.079}, 'status': 'success'}], 'credits_used': 16, 'credits': 1982920, 'subscription': 0, 'content': '“How can an AI train itself if no one is telling it if its answer is correct or wrong?”\n\nThis is a great question.\n\nIn order to learn how to solve a problem correctly, the AI has to be told how to recognize a “correct” solution to the problem. In order to learn how to better solve a problem that has no one correct answer, the AI has to be told how to recognize when it’s doing “better”.\n\nSo at first it seems like the answer to your question is “It can’t.”\n\nBut the nuance is in what we mean by “has to be told”. There doesn’t have to be a human trainer there saying “this is right” or “this is better”. The thing giving the AI its feedback on “right” or “better” can be just another computer program.\n\nThe most basic example of this is a “maximizer AI”. A human writes a computer program that can look at the current state of the problem and assign it a point score. The higher the score, the better the current state of the problem is. Now the AI can rapidly try different approaches to the problem without needing a human trainer to tell it the results of each attempt. The feedback is immediate and automatic: if approach A produces a higher score than approach B, then the AI knows that approach A is better.\n\nThe most classic example of this approach is the typical computer chess program. But that’s been discussed to death. So let me give you a less common and perhaps more fun example: video games.\n\nSpecifically, old Atari 2600 video games back from the dawn of game consoles.\n\nThere are two useful things about those games from a modern perspective:\n\n1. Simple controls.\n\nThe controller used for most Atari games had a very simple eight-direction joystick and one button. That’s it. No analog controls, no complicated six-button control schemes. That means that there’s a very small number of possible actions that a player can take at any given moment.\n\n2. Immediate scoring feedback.\n\nThe games typically displayed a score, and the object of the game was to get the highest score possible. What’s more, points were typically scored for very simple actions (like “eat that dot - 1 point”), rather than complex multi-step actions (like “Make your way across this level, avoid getting shot, find the flag at the center of the level, pick it up, and bring it back to your base — 1 point”). That means that the training data the AI needs is quite literally there on the screen. If you do something that makes the score go up, that’s good. If you do something that makes the score go up faster, that’s better.\n\nStarting from Scratch\n\nAn AI researcher decided to take advantage of these facts by running an experiment in which an AI learned how to play the games. Note that I didn’t say “in which the researcher taught the AI how to play the games.” The AI started with absolutely no knowledge of the rules of the games. Its baseline knowledge was essentially “Here is a simplified view of what’s on the screen at this moment; here are the possible control signals you can give; and here’s the score. High scores are better. Go.”\n\nThis led to a wonderful paper, “Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari\n”. It’s a fun read if you can get past the terminology. But let me pick out my favorite example, Q*bert, which showed that this approach can lead to strategies that a human trainer would never have thought of.\n\nQ*bert is a game that originally came out in arcades in 1982. The player controls a little aardvark-looking character that must hop around a pyramid of cubes to change their colors. A few enemies pursue the character and sometimes mess up the colors, requiring the player to fix them. The arcade version looks like this:\n\nThe Atari 2600 version… leaves a lot more to the imagination:\n\nBut the experiment was about AI gameplay, not art appreciation. So they hooked the AI up to the controls, let it see the screen, told it that high scores are good, and let it loose.\n\nThe researchers’ expectation was that the AI would learn to play Q*bert the way a human would — try to find some reasonably-efficient way of covering the board while dodging the bad guys. But two of the test runs went in particularly interesting directions.\n\nFree-fallin’\n\nIn the first interesting run, the AI finds a weakness in the way the 2600 version scores the game and awards extra lives (which is different from the arcade version). The AI positions Q*bert on a cube near the edge of the board, waits for the smartest enemy to approach, and then deliberately sends Q*bert to his death by leaping off the board. But the enemy leaps off the board as well in pursuit, and the player gets 500 points for killing that enemy — enough for an extra life. So the AI can simply repeat this indefinitely, methodically and endlessly scoring points.\n\nBreaking the Game\n\nIn the second interesting run, the AI finds an even more bizarre bug to exploit. As can be seen in the video, this AI’s Q*bert play skill is suspect at best. But all it has to do is to reach the end of level 1. The game then displays what’s normally a brief animation where the cubes flash for a bit and the player gets some bonus points before moving on to the next level. But the AI has stumbled onto the fact that if it makes a particular series of controller moves during this bonus sequence, the sequence never ends — it just keeps on giving the player points forever and ever and ever. All the AI has to do at that point is avoid committing suicide, and presto, infinite score.\n\nWhat have we learned?\n\nWe’ve seen some of the power and some of the hazards of AI. With very simple automated training data, the AI can learn how to solve surprisingly complex problems completely on its own. But this is also problematic.\n\nThe AI’s only concern is on getting that score higher — not on performing the underlying task in a way that a human would consider “normal” or “proper”.\nThe AI will ignore all the other assumptions we would make about the problem, like “Deliberately killing your own character is bad”.\nThe AI may even find a solution that seems completely unrelated to the problem, like “Break the bonus screen and score infinite points.”\n\nAll of this is fun to watch, as long as the AI is playing video games. If the AI is, say, controlling a patient’s medication or monitoring a nuclear plant, we really don’t want this sort of “creativity”.\n\nSo when we use this sort of AI to solve a real-world problem, we need to be extremely careful. We need to make sure that the AI is constrained in what it can do — if we never want it to perform some action, then we need to make sure it can’t. And most importantly, we need to make sure that the scoring system truly reflects the outcome we want.\n\nAnd this is just a problem with AIs, right?\n\nAbsolutely not! We humans exploit faulty scoring systems all the time. We’re so prone to it — and good at it — that English has a wonderfully evocative phrase for it: “gaming the system\n”. We perform our tasks in all sorts of crazy ways designed to maximize the things we’ll be rewarded for, at the expense of the actual usefulness of the things we’re doing. So it’s not enough to be careful about designing a scoring system when we’re training our AIs. We need to be just as careful, if not more so, when we’re designing scoring systems for ourselves.\n\nMany thanks for all the upvotes and encouraging feedback!\n\nOther interesting takes on this topic:\n\nUniversal Paperclips\n, a web game based on the Paperclip Maximizer thought experiment — what happens when you give an AI the goal of “maximize paperclip production” and no other constraints (thanks to Azfar Hisham for mentioning this experiment in the comments).\n“AI masters 49 2600 games without instructions”\n, an Ars Technica article about a similar AI experiment.\n“Human-level control through deep reinforcement learning\n”, the paper published in Nature about the experiment.\nMinimax Algorithm\n — a Wikipedia article describing a typical approach to turn-based game AI. In this approach, using the same sort of “high scores are better for me” scoring algorithm, the AI picks the best moves for itself by also considering what the best moves are for its opponent. Simplifying greatly, the AI picks some moves that seem promising, because they yield high scores. But instead of simply making the move with the highest score, the AI then considers what the opponent will do in response — it finds the responsive move that yields the lowest score (the one that’s best for the opponent). The move the AI ultimately picks is the one that provokes the least-damaging response from the opponent.\nMy Little Pony: Friendship is Optimal\n — an MLP fanfic demonstrating quite convincingly why designing an intelligent game AI to maximize the goal of “satisfy human values through friendship and ponies” is probably a very bad idea.', 'aiModelVersion': '1'}",0.56185
Kiran Kannar,Updated 5y,What are the dangers of using machine learning libraries without any understanding?,"Here’s a picture of a cat right? Google’s Inception model thinks it’s a guacamole. As much as the image looks like a cat, the image is digitally altered which confused the model.

Slightly rotating the image led the model to correctly classify the image as a cat (and as an animal).

The above image is what’s called as an adversarial image, trying to fool your model into thinking the image is something you want it to be instead of what the image actually is.

I bring this example to illustrate the problem with the current AI trend. We have many technologies that allow us to use AI/ML (and not just deep learning) as a blackbox. The real danger is in the application, especially in healthcare and defense. For example, how would you convince that your model for predicting cancer actually works? How do you know your model is not susceptible to noise? How do you know that your model has actually learnt what it is supposed to be learning? How do you actually read interpretability here? If you can’t interpret what the model has learnt, then you can’t sell it.

Images source: Google’s AI thinks this turtle looks like a gun, which is a problem

A lot of the discussion in the comments is primarily on the adversarial example and not exactly what I intended to point on— interpretability. You could take a simple problem and use a complex solution involving weeks of training. It may give you fairly good results on training data within the prototyping phase. But the questions to be asked are:

How would you now ensure that the results be consistent in application, without posing any vulnerabilities or risk?
Did you really need a AI/ML solution?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/gqkw1cs2ir9xjoml', 'title': 'What are the dangers of using machine learning libraries without any understanding?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Here’s a picture of a cat right? Google’s Inception model thinks it’s a guacamole. As much as the image looks like a cat, the image is digitally altered which confused the model.\n\nSlightly rotating the image led the model to correctly classify the image as a cat (and as an animal).\n\nThe above image is what’s called as an adversarial image, trying to fool your model into thinking the image is something you want it to be instead of what the image actually is.\n\nI bring this example to illustrate the problem with the current AI trend. We have many technologies that allow us to use AI/ML (and not just deep learning) as a blackbox. The real danger is in the application, especially in healthcare and defense. For example, how would you convince that your model for predicting cancer actually works? How do you know your model is not susceptible to noise? How do you know that your model has actually learnt what it is supposed to be learning? How do you actually read interpretability here? If you can’t interpret what the model has learnt, then you can’t sell it.\n\nImages source: Google’s AI thinks this turtle looks like a gun, which is a problem\n\nA lot of the discussion in the comments is primarily on the adversarial example and not exactly what I intended to point on— interpretability. You could take a simple problem and use a complex solution involving weeks of training. It may give you fairly good results on training data within the prototyping phase. But the questions to be asked are:\n\nHow would you now ensure that the results be consistent in application, without posing any vulnerabilities or risk?\nDid you really need a AI/ML solution?', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982917, 'subscription': 0, 'content': 'Here’s a picture of a cat right? Google’s Inception model thinks it’s a guacamole. As much as the image looks like a cat, the image is digitally altered which confused the model.\n\nSlightly rotating the image led the model to correctly classify the image as a cat (and as an animal).\n\nThe above image is what’s called as an adversarial image, trying to fool your model into thinking the image is something you want it to be instead of what the image actually is.\n\nI bring this example to illustrate the problem with the current AI trend. We have many technologies that allow us to use AI/ML (and not just deep learning) as a blackbox. The real danger is in the application, especially in healthcare and defense. For example, how would you convince that your model for predicting cancer actually works? How do you know your model is not susceptible to noise? How do you know that your model has actually learnt what it is supposed to be learning? How do you actually read interpretability here? If you can’t interpret what the model has learnt, then you can’t sell it.\n\nImages source: Google’s AI thinks this turtle looks like a gun, which is a problem\n\nA lot of the discussion in the comments is primarily on the adversarial example and not exactly what I intended to point on— interpretability. You could take a simple problem and use a complex solution involving weeks of training. It may give you fairly good results on training data within the prototyping phase. But the questions to be asked are:\n\nHow would you now ensure that the results be consistent in application, without posing any vulnerabilities or risk?\nDid you really need a AI/ML solution?', 'aiModelVersion': '1'}",0.9998
Pararth Shah,11y,How do you explain Machine Learning and Data Mining to non Computer Science people?,"Mango Shopping

Suppose you go shopping for mangoes one day. The vendor has laid out a cart full of mangoes. You can handpick the mangoes, the vendor will weigh them, and you pay according to a fixed Rs per Kg rate (typical story in India).


Obviously, you want to pick the sweetest, most ripe mangoes for yourself (since you are paying by weight and not by quality). How do you choose the mangoes?

You remember your grandmother saying that bright yellow mangoes are sweeter than pale yellow ones. So you make a simple rule: pick only from the bright yellow mangoes. You check the color of the mangoes, pick the bright yellow ones, pay up, and return home. Happy ending?

Not quite.

Life is complicated

Suppose you go home and taste the mangoes. Some of them are not sweet as you'd like. You are worried. Apparently, your grandmother's wisdom is insufficient. There is more to mangoes than just color.

After a lot of pondering (and tasting different types of mangoes), you conclude that the bigger, bright yellow mangoes are guaranteed to be sweet, while the smaller, bright yellow mangoes are sweet only half the time (i.e. if you buy 100 bright yellow mangoes, out of which 50 are big in size and 50 are small, then the 50 big mangoes will all be sweet, while out of the 50 small ones, on average only 25 mangoes will turn out to be sweet).

You are happy with your findings, and you keep them in mind the next time you go mango shopping. But next time at the market, you see that your favorite vendor has gone out of town. You decide to buy from a different vendor, who supplies mangoes grown from a different part of the country. Now, you realize that the rule which you had learnt (that big, bright yellow mangoes are the sweetest) is no longer applicable. You have to learn from scratch. You taste a mango of each kind from this vendor, and realize that the small, pale yellow ones are in fact the sweetest of all.

Now, a distant cousin visits you from another city. You decide to treat her with mangoes. But she mentions that she doesn't care about the sweetness of a mango, she only wants the most juicy ones. Once again, you run your experiments, tasting all kinds of mangoes, and realizing that the softer ones are more juicy.

Now, you move to a different part of the world. Here, mangoes taste surprisingly different from your home country. You realize that the green mangoes are in fact tastier than the yellow ones.

You marry someone who hates mangoes. She loves apples instead. You go apple shopping. Now, all your accumulated knowledge about mangoes is worthless. You have to learn everything about the correlation between the physical characteristics and the taste of apples, by the same method of experimentation. You do it, because you love her.

Enter computer programs

Now, imagine that all this while, you were writing a computer program to help you choose your mangoes (or apples). You would write rules of the following kind:

if (color is bright yellow and size is big and sold by favorite vendor): mango is sweet.
if (soft): mango is juicy.
etc.

You would use these rules to choose the mangoes. You could even send your younger brother with this list of rules to buy the mangoes, and you would be assured that he will pick only the mangoes of your choice.

But every time you make a new observation from your experiments, you have to manually modify the list of rules. You have to understand the intricate details of all the factors affecting the quality of mangoes. If the problem gets complicated enough, it can get really difficult to make accurate rules by hand that cover all possible types of mangoes. Your research could earn you a PhD in Mango Science (if there is one).

But not everyone has that kind of time.

Enter Machine Learning algorithms

ML algorithms are an evolution over normal algorithms. They make your programs ""smarter"", by allowing them to automatically learn from the data you provide.

You take a randomly selected specimen of mangoes from the market (training data), make a table of all the physical characteristics of each mango, like color, size, shape, grown in which part of the country, sold by which vendor, etc (features), along with the sweetness, juicyness, ripeness of that mango (output variables). You feed this data to the machine learning algorithm (classification/regression), and it learns a model of the correlation between an average mango's physical characteristics, and its quality.

Next time you go to the market, you measure the characteristics of the mangoes on sale (test data), and feed it to the ML algorithm. It will use the model computed earlier to predict which mangoes are sweet, ripe and/or juicy. The algorithm may internally use rules similar to the rules you manually wrote earlier (for eg, a decision tree), or it may use something more involved, but you don't need to worry about that, to a large extent.

Voila, you can now shop for mangoes with great confidence, without worrying about the details of how to choose the best mangoes. And what's more, you can make your algorithm improve over time (reinforcement learning), so that it will improve its accuracy as it reads more training data, and modifies itself when it makes a wrong prediction. But the best part is, you can use the same algorithm to train different models, one each for predicting the quality of apples, oranges, bananas, grapes, cherries and watermelons, and keep all your loved ones happy :)

And that, is Machine Learning for you. Tell me if it isn't cool.

Machine Learning: Making your algorithms smart, so that you don't need to be. ;)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/v0koz93tgxq14b7e', 'title': 'How do you explain Machine Learning and Data Mining to non Computer Science people?', 'score': {'original': 0.39875, 'ai': 0.60125}, 'blocks': [{'text': ""Mango Shopping\n\nSuppose you go shopping for mangoes one day. The vendor has laid out a cart full of mangoes. You can handpick the mangoes, the vendor will weigh them, and you pay according to a fixed Rs per Kg rate (typical story in India).\n\n\nObviously, you want to pick the sweetest, most ripe mangoes for yourself (since you are paying by weight and not by quality). How do you choose the mangoes?\n\nYou remember your grandmother saying that bright yellow mangoes are sweeter than pale yellow ones. So you make a simple rule: pick only from the bright yellow mangoes. You check the color of the mangoes, pick the bright yellow ones, pay up, and return home. Happy ending?\n\nNot quite.\n\nLife is complicated\n\nSuppose you go home and taste the mangoes. Some of them are not sweet as you'd like. You are worried. Apparently, your grandmother's wisdom is insufficient. There is more to mangoes than just color.\n\nAfter a lot of pondering (and tasting different types of mangoes), you conclude that the bigger, bright yellow mangoes are guaranteed to be sweet, while the smaller, bright yellow mangoes are sweet only half the time (i.e. if you buy 100 bright yellow mangoes, out of which 50 are big in size and 50 are small, then the 50 big mangoes will all be sweet, while out of the 50 small ones, on average only 25 mangoes will turn out to be sweet).\n\nYou are happy with your findings, and you keep them in mind the next time you go mango shopping. But next time at the market, you see that your favorite vendor has gone out of town. You decide to buy from a different vendor, who supplies mangoes grown from a different part of the country. Now, you realize that the rule which you had learnt (that big, bright yellow mangoes are the sweetest) is no longer applicable. You have to learn from scratch. You taste a mango of each kind from this vendor, and realize that the small, pale yellow ones are in fact the sweetest of all.\n\nNow, a distant cousin visits you from another city. You decide to treat her with mangoes. But she mentions that she doesn't care about the sweetness of a mango, she only wants the most juicy ones. Once again, you run your experiments, tasting all kinds of mangoes, and realizing that the softer ones are more juicy.\n\nNow, you move to a different part of the world. Here, mangoes taste surprisingly different from your home country. You realize that the green mangoes are in fact tastier than the yellow ones.\n\nYou marry someone who hates mangoes. She loves apples instead. You go apple shopping. Now, all your accumulated knowledge about mangoes is worthless. You have to learn everything about the correlation between the physical characteristics and the taste of apples, by the same method of experimentation. You do it, because you love her.\n\nEnter computer programs\n\nNow, imagine that all this while, you were writing a computer program to help you choose your mangoes (or apples). You would write rules of the following kind:\n\nif (color is"", 'result': {'fake': 0.1193, 'real': 0.8807}, 'status': 'success'}, {'text': 'bright yellow and size is big and sold by favorite vendor): mango is sweet.\nif (soft): mango is juicy.\netc.\n\nYou would use these rules to choose the mangoes. You could even send your younger brother with this list of rules to buy the mangoes, and you would be assured that he will pick only the mangoes of your choice.\n\nBut every time you make a new observation from your experiments, you have to manually modify the list of rules. You have to understand the intricate details of all the factors affecting the quality of mangoes. If the problem gets complicated enough, it can get really difficult to make accurate rules by hand that cover all possible types of mangoes. Your research could earn you a PhD in Mango Science (if there is one).\n\nBut not everyone has that kind of time.\n\nEnter Machine Learning algorithms\n\nML algorithms are an evolution over normal algorithms. They make your programs ""smarter"", by allowing them to automatically learn from the data you provide.\n\nYou take a randomly selected specimen of mangoes from the market (training data), make a table of all the physical characteristics of each mango, like color, size, shape, grown in which part of the country, sold by which vendor, etc (features), along with the sweetness, juicyness, ripeness of that mango (output variables). You feed this data to the machine learning algorithm (classification/regression), and it learns a model of the correlation between an average mango\'s physical characteristics, and its quality.\n\nNext time you go to the market, you measure the characteristics of the mangoes on sale (test data), and feed it to the ML algorithm. It will use the model computed earlier to predict which mangoes are sweet, ripe and/or juicy. The algorithm may internally use rules similar to the rules you manually wrote earlier (for eg, a decision tree), or it may use something more involved, but you don\'t need to worry about that, to a large extent.\n\nVoila, you can now shop for mangoes with great confidence, without worrying about the details of how to choose the best mangoes. And what\'s more, you can make your algorithm improve over time (reinforcement learning), so that it will improve its accuracy as it reads more training data, and modifies itself when it makes a wrong prediction. But the best part is, you can use the same algorithm to train different models, one each for predicting the quality of apples, oranges, bananas, grapes, cherries and watermelons, and keep all your loved ones happy :)\n\nAnd that, is Machine Learning for you. Tell me if it isn\'t cool.\n\nMachine Learning: Making your algorithms smart, so that you don\'t need to be. ;)', 'result': {'fake': 0.69, 'real': 0.31}, 'status': 'success'}], 'credits_used': 10, 'credits': 1982907, 'subscription': 0, 'content': 'Mango Shopping\n\nSuppose you go shopping for mangoes one day. The vendor has laid out a cart full of mangoes. You can handpick the mangoes, the vendor will weigh them, and you pay according to a fixed Rs per Kg rate (typical story in India).\n\n\nObviously, you want to pick the sweetest, most ripe mangoes for yourself (since you are paying by weight and not by quality). How do you choose the mangoes?\n\nYou remember your grandmother saying that bright yellow mangoes are sweeter than pale yellow ones. So you make a simple rule: pick only from the bright yellow mangoes. You check the color of the mangoes, pick the bright yellow ones, pay up, and return home. Happy ending?\n\nNot quite.\n\nLife is complicated\n\nSuppose you go home and taste the mangoes. Some of them are not sweet as you\'d like. You are worried. Apparently, your grandmother\'s wisdom is insufficient. There is more to mangoes than just color.\n\nAfter a lot of pondering (and tasting different types of mangoes), you conclude that the bigger, bright yellow mangoes are guaranteed to be sweet, while the smaller, bright yellow mangoes are sweet only half the time (i.e. if you buy 100 bright yellow mangoes, out of which 50 are big in size and 50 are small, then the 50 big mangoes will all be sweet, while out of the 50 small ones, on average only 25 mangoes will turn out to be sweet).\n\nYou are happy with your findings, and you keep them in mind the next time you go mango shopping. But next time at the market, you see that your favorite vendor has gone out of town. You decide to buy from a different vendor, who supplies mangoes grown from a different part of the country. Now, you realize that the rule which you had learnt (that big, bright yellow mangoes are the sweetest) is no longer applicable. You have to learn from scratch. You taste a mango of each kind from this vendor, and realize that the small, pale yellow ones are in fact the sweetest of all.\n\nNow, a distant cousin visits you from another city. You decide to treat her with mangoes. But she mentions that she doesn\'t care about the sweetness of a mango, she only wants the most juicy ones. Once again, you run your experiments, tasting all kinds of mangoes, and realizing that the softer ones are more juicy.\n\nNow, you move to a different part of the world. Here, mangoes taste surprisingly different from your home country. You realize that the green mangoes are in fact tastier than the yellow ones.\n\nYou marry someone who hates mangoes. She loves apples instead. You go apple shopping. Now, all your accumulated knowledge about mangoes is worthless. You have to learn everything about the correlation between the physical characteristics and the taste of apples, by the same method of experimentation. You do it, because you love her.\n\nEnter computer programs\n\nNow, imagine that all this while, you were writing a computer program to help you choose your mangoes (or apples). You would write rules of the following kind:\n\nif (color is bright yellow and size is big and sold by favorite vendor): mango is sweet.\nif (soft): mango is juicy.\netc.\n\nYou would use these rules to choose the mangoes. You could even send your younger brother with this list of rules to buy the mangoes, and you would be assured that he will pick only the mangoes of your choice.\n\nBut every time you make a new observation from your experiments, you have to manually modify the list of rules. You have to understand the intricate details of all the factors affecting the quality of mangoes. If the problem gets complicated enough, it can get really difficult to make accurate rules by hand that cover all possible types of mangoes. Your research could earn you a PhD in Mango Science (if there is one).\n\nBut not everyone has that kind of time.\n\nEnter Machine Learning algorithms\n\nML algorithms are an evolution over normal algorithms. They make your programs ""smarter"", by allowing them to automatically learn from the data you provide.\n\nYou take a randomly selected specimen of mangoes from the market (training data), make a table of all the physical characteristics of each mango, like color, size, shape, grown in which part of the country, sold by which vendor, etc (features), along with the sweetness, juicyness, ripeness of that mango (output variables). You feed this data to the machine learning algorithm (classification/regression), and it learns a model of the correlation between an average mango\'s physical characteristics, and its quality.\n\nNext time you go to the market, you measure the characteristics of the mangoes on sale (test data), and feed it to the ML algorithm. It will use the model computed earlier to predict which mangoes are sweet, ripe and/or juicy. The algorithm may internally use rules similar to the rules you manually wrote earlier (for eg, a decision tree), or it may use something more involved, but you don\'t need to worry about that, to a large extent.\n\nVoila, you can now shop for mangoes with great confidence, without worrying about the details of how to choose the best mangoes. And what\'s more, you can make your algorithm improve over time (reinforcement learning), so that it will improve its accuracy as it reads more training data, and modifies itself when it makes a wrong prediction. But the best part is, you can use the same algorithm to train different models, one each for predicting the quality of apples, oranges, bananas, grapes, cherries and watermelons, and keep all your loved ones happy :)\n\nAnd that, is Machine Learning for you. Tell me if it isn\'t cool.\n\nMachine Learning: Making your algorithms smart, so that you don\'t need to be. ;)', 'aiModelVersion': '1'}",0.39875
Yariv Adan,5y,Is it possible for a neural network to be too deep?,"Yes, historically there have been multiple issues with networks being too deep. Some of these issues have been addressed with solutions, while others are still important to take into consideration:

1. Deeper is not necessarily better - data and model power go hand in hand - very deep models with many parameters require a lot of data to be well tuned. If you don't have enough training data, your powerful model will easily overfit your training data - resulting in very low training loss, but will most probably perform poorly on the test / production data. A classic and simple example that illustrates this is this one:

Imagine you have only 6 training samples, yet a powerful model that can represent a 6th degree polynomial. The model will easily fit all your training data with this complicated curve. However, when you get a new unseen test sample, the model will overfit, and predict a value somewhere on the curve (the blue point in the image above) - resulting in a high error. Contrast that with the much simpler linear model in the image. All sorts of regularization and other methods have been constructed to deal with this issue. However, these won't work if there is an inherent dissonance between your model and your data, and you will definitely won't be able to enjoy the depth of your network.

2. Vanishing gradients - when the network becomes too deep, you hit the issue of vanishing gradients. In fact, this was one of the key issues that prevented deep CNNs from going beyond a couple dozen dozen layers for quite some time (CNNs are used mostly for visual recognition - which are data rich tasks, blessed with billions of dense and complex training samples. So at least in theory, they would benefit from complex deep networks that can represent their complex structures).

The vanishing gradient problem is a difficulty found in training artificial neural networks
 with gradient-based learning methods
 and backpropagation
. In such methods, each of the neural network's weights receives an update proportional to the partial derivative
 of the error function
 with respect to the current weight in each iteration of training. The problem is that in some cases - especially in very deep networks, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.

So the depth of a network becomes its Achilles heel, and it can't be trained. In 2015, Microsoft introduced an exceptionally deep ""Residual Network""
, and won the 1st place in the ILSRVC of that year, with an astonishing 3.7% error rate (beating human error rate for the first time). The residual network addresses the problem of vanishing gradients by splitting the network into building blocks, with ""highway paths"" between the blocks that allow gradients to flow freely along the lawyers in parallel to the ""regular"" paths. Since then, many other flavors of residual networks have been developed.

3. Compute and storage - as a model becomes exceptionally big, and as a result also its data set, training can become very challenging and costly in terms of compute and storage. You just can't train the model on a single machine - it's too big. There are various solutions out there for distributed parallel training - capable of splitting both the date and model across training agents. However, this introduces significant complexity, and it's not always clear that the lemonade is worth the squeeze.

So to sum it up: as with everything in life, it's always a trade-off, and bigger, stronger, and deeper is not necessarily also better. Often, well informed creativity and flexibility can lead to more effective and efficient solutions.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/7gxeyokanltqf4p6', 'title': 'Is it possible for a neural network to be too deep?', 'score': {'original': 0.29635, 'ai': 0.70365}, 'blocks': [{'text': 'Yes, historically there have been multiple issues with networks being too deep. Some of these issues have been addressed with solutions, while others are still important to take into consideration:\n\n1. Deeper is not necessarily better - data and model power go hand in hand - very deep models with many parameters require a lot of data to be well tuned. If you don\'t have enough training data, your powerful model will easily overfit your training data - resulting in very low training loss, but will most probably perform poorly on the test / production data. A classic and simple example that illustrates this is this one:\n\nImagine you have only 6 training samples, yet a powerful model that can represent a 6th degree polynomial. The model will easily fit all your training data with this complicated curve. However, when you get a new unseen test sample, the model will overfit, and predict a value somewhere on the curve (the blue point in the image above) - resulting in a high error. Contrast that with the much simpler linear model in the image. All sorts of regularization and other methods have been constructed to deal with this issue. However, these won\'t work if there is an inherent dissonance between your model and your data, and you will definitely won\'t be able to enjoy the depth of your network.\n\n2. Vanishing gradients - when the network becomes too deep, you hit the issue of vanishing gradients. In fact, this was one of the key issues that prevented deep CNNs from going beyond a couple dozen dozen layers for quite some time (CNNs are used mostly for visual recognition - which are data rich tasks, blessed with billions of dense and complex training samples. So at least in theory, they would benefit from complex deep networks that can represent their complex structures).\n\nThe vanishing gradient problem is a difficulty found in training artificial neural networks\n with gradient-based learning methods\n and backpropagation\n. In such methods, each of the neural network\'s weights receives an update proportional to the partial derivative\n of the error function\n with respect to the current weight in each iteration of training. The problem is that in some cases - especially in very deep networks, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.\n\nSo the depth of a network becomes its Achilles heel, and it can\'t be trained. In 2015, Microsoft introduced an exceptionally deep ""Residual Network""\n, and won the 1st place in the ILSRVC of that year, with an astonishing 3.7% error rate (beating human error rate for the first time). The residual network addresses the problem of vanishing gradients by splitting the network into building blocks, with ""highway paths"" between the blocks that allow gradients to flow freely along the lawyers in parallel to the ""regular"" paths. Since then, many other flavors of residual networks have been developed.\n\n3. Compute and storage - as a model becomes exceptionally big, and as a result also its data set,', 'result': {'fake': 0.5086, 'real': 0.4914}, 'status': 'success'}, {'text': ""training can become very challenging and costly in terms of compute and storage. You just can't train the model on a single machine - it's too big. There are various solutions out there for distributed parallel training - capable of splitting both the date and model across training agents. However, this introduces significant complexity, and it's not always clear that the lemonade is worth the squeeze.\n\nSo to sum it up: as with everything in life, it's always a trade-off, and bigger, stronger, and deeper is not necessarily also better. Often, well informed creativity and flexibility can lead to more effective and efficient solutions."", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982900, 'subscription': 0, 'content': 'Yes, historically there have been multiple issues with networks being too deep. Some of these issues have been addressed with solutions, while others are still important to take into consideration:\n\n1. Deeper is not necessarily better - data and model power go hand in hand - very deep models with many parameters require a lot of data to be well tuned. If you don\'t have enough training data, your powerful model will easily overfit your training data - resulting in very low training loss, but will most probably perform poorly on the test / production data. A classic and simple example that illustrates this is this one:\n\nImagine you have only 6 training samples, yet a powerful model that can represent a 6th degree polynomial. The model will easily fit all your training data with this complicated curve. However, when you get a new unseen test sample, the model will overfit, and predict a value somewhere on the curve (the blue point in the image above) - resulting in a high error. Contrast that with the much simpler linear model in the image. All sorts of regularization and other methods have been constructed to deal with this issue. However, these won\'t work if there is an inherent dissonance between your model and your data, and you will definitely won\'t be able to enjoy the depth of your network.\n\n2. Vanishing gradients - when the network becomes too deep, you hit the issue of vanishing gradients. In fact, this was one of the key issues that prevented deep CNNs from going beyond a couple dozen dozen layers for quite some time (CNNs are used mostly for visual recognition - which are data rich tasks, blessed with billions of dense and complex training samples. So at least in theory, they would benefit from complex deep networks that can represent their complex structures).\n\nThe vanishing gradient problem is a difficulty found in training artificial neural networks\n with gradient-based learning methods\n and backpropagation\n. In such methods, each of the neural network\'s weights receives an update proportional to the partial derivative\n of the error function\n with respect to the current weight in each iteration of training. The problem is that in some cases - especially in very deep networks, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.\n\nSo the depth of a network becomes its Achilles heel, and it can\'t be trained. In 2015, Microsoft introduced an exceptionally deep ""Residual Network""\n, and won the 1st place in the ILSRVC of that year, with an astonishing 3.7% error rate (beating human error rate for the first time). The residual network addresses the problem of vanishing gradients by splitting the network into building blocks, with ""highway paths"" between the blocks that allow gradients to flow freely along the lawyers in parallel to the ""regular"" paths. Since then, many other flavors of residual networks have been developed.\n\n3. Compute and storage - as a model becomes exceptionally big, and as a result also its data set, training can become very challenging and costly in terms of compute and storage. You just can\'t train the model on a single machine - it\'s too big. There are various solutions out there for distributed parallel training - capable of splitting both the date and model across training agents. However, this introduces significant complexity, and it\'s not always clear that the lemonade is worth the squeeze.\n\nSo to sum it up: as with everything in life, it\'s always a trade-off, and bigger, stronger, and deeper is not necessarily also better. Often, well informed creativity and flexibility can lead to more effective and efficient solutions.', 'aiModelVersion': '1'}",0.29635
Rashmi Khurana,Aug 27,What are prerequisites to start learning machine learning?,"Machine learning is an extremely complex field - essentially you're trying to teach a computer how to think for itself. There aren't any primers or short courses that will help you get started, so it's best if you've got some background knowledge in math, programming, and stats before diving in headfirst.

Prerequisites to start learning machine learning:

Get a copy of the book 'Machine Learning to learn general machine learning theory
Learn how to use MATLAB or R to do basic linear algebra and calculus
Try some statistics, preferably in R or MATLAB before your machine-learning journey begins
Learn basic probability, e.g., using the binomial distribution in R or MATLAB.

How to go about learning the abilities needed to become a machine learning analyst:

For those who possess a thorough understanding of data, statistics, and computer science, there are numerous certificate programs available.

It introduces users to machine learning principles and covers its fundamental topics.
The classes also teach you how to expand your knowledge of machine learning and how to analyze data.
You will learn how to understand data using statistical analysis from these programs.
Additionally, it covers the skills necessary to excel as a data mining and machine learning analyst.

Which platform provides the best ML certification?

This institute offers the best machine learning courses in Bangalore, which can be accessed from anywhere in the world. It is a great place to learn about machine learning. The best aspect of this institute is the availability of courses with a particular industrial specialization.

Artificial Intelligence and Machine Learning Program

Here are a few of its features to look at:

Real-time initiatives: There are too many ML principles to fully comprehend without useful applications. Because of the practical instruction, every student will be able to use the subject in business. You will have the opportunity to work on practical assignments that require you to use the theoretical knowledge you have gained in class. Actual commercial endeavors from 12+ firms, including Samsung, Netflix, Amazon, Uber, and others. Programs that use machine learning can be useful for both novices and specialists. AI initiatives include tasks like Fraud Deduction, Portfolio Management, etc.
Domain specialization is valuable since many occupations today require a certain set of subject-matter expertise before you can grow in your career. However, I believe focusing on a certain field is the best way to advance your career. You can develop your career while learning more about a particular subject with this type of training. High-profile companies seek professionals who have a thorough understanding of their industry. Working professionals should think carefully before enrolling in domain-specific training courses to advance their abilities. Listed below are a few of these Oil and Gas, Healthcare, Energy, Manufacturing, BFSI, etc.
An IBM and Microsoft certification is the result of the Artificial Intelligence and Machine Learning Program. Such a qualification will strengthen the reliability and competence of your resume. After finishing the course, you will be awarded an IBM and Microsoft and along with that will receive 1 Capstone certificate from IBM.
When hybrid learning formats are used, students in this institute can work on a variety of academic tasks at once. Through hybrid training, for instance, employees can continue to fulfill their current duties while picking up new abilities.
Professionals from MNC and MAANG participate in both life and recorded experience centers or sessions. At least seven cities, including Bangalore, Hyderabad, Delhi, and Pune, have experience centers.
The institute provides a range of job support services, including career counseling, help with job searches, and workshops for career development. Through the offline centers located in places like Bangalore, Hyderabad, Delhi, and Pune, you can also get career services.

If not the above institute, you can also look for other options like:

The greatest instructors from Stanford University and other universities are available to teach a variety of courses on Udacity, a great resource for learning about machine learning. The website also provides a few certification programs that help determine rankings. However, this platform doesn't have a thorough curriculum for advanced ML programs.

Bottom Line:

On a final note, machine learning is a top trend that can drive change in the digital world. It is the technology that will enable ML to realize its full potential.

Make the right choice

Thank you","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/xh0f2uiqmkzcnev5', 'title': 'What are prerequisites to start learning machine learning?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""Machine learning is an extremely complex field - essentially you're trying to teach a computer how to think for itself. There aren't any primers or short courses that will help you get started, so it's best if you've got some background knowledge in math, programming, and stats before diving in headfirst.\n\nPrerequisites to start learning machine learning:\n\nGet a copy of the book 'Machine Learning to learn general machine learning theory\nLearn how to use MATLAB or R to do basic linear algebra and calculus\nTry some statistics, preferably in R or MATLAB before your machine-learning journey begins\nLearn basic probability, e.g., using the binomial distribution in R or MATLAB.\n\nHow to go about learning the abilities needed to become a machine learning analyst:\n\nFor those who possess a thorough understanding of data, statistics, and computer science, there are numerous certificate programs available.\n\nIt introduces users to machine learning principles and covers its fundamental topics.\nThe classes also teach you how to expand your knowledge of machine learning and how to analyze data.\nYou will learn how to understand data using statistical analysis from these programs.\nAdditionally, it covers the skills necessary to excel as a data mining and machine learning analyst.\n\nWhich platform provides the best ML certification?\n\nThis institute offers the best machine learning courses in Bangalore, which can be accessed from anywhere in the world. It is a great place to learn about machine learning. The best aspect of this institute is the availability of courses with a particular industrial specialization.\n\nArtificial Intelligence and Machine Learning Program\n\nHere are a few of its features to look at:\n\nReal-time initiatives: There are too many ML principles to fully comprehend without useful applications. Because of the practical instruction, every student will be able to use the subject in business. You will have the opportunity to work on practical assignments that require you to use the theoretical knowledge you have gained in class. Actual commercial endeavors from 12+ firms, including Samsung, Netflix, Amazon, Uber, and others. Programs that use machine learning can be useful for both novices and specialists. AI initiatives include tasks like Fraud Deduction, Portfolio Management, etc.\nDomain specialization is valuable since many occupations today require a certain set of subject-matter expertise before you can grow in your career. However, I believe focusing on a certain field is the best way to advance your career. You can develop your career while learning more about a particular subject with this type of training. High-profile companies seek professionals who have a thorough understanding of their industry. Working professionals should think carefully before enrolling in domain-specific training courses to advance their abilities. Listed below are a few of these Oil and Gas, Healthcare, Energy, Manufacturing, BFSI, etc.\nAn IBM and Microsoft certification is the result of the Artificial Intelligence and Machine Learning Program. Such a qualification will strengthen the reliability and competence of your resume. After finishing the course, you will be awarded an IBM and Microsoft and along with that will receive 1 Capstone certificate from IBM.\nWhen hybrid learning formats are used, students in this institute can work on a variety of academic tasks at once. Through hybrid training, for"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""instance, employees can continue to fulfill their current duties while picking up new abilities.\nProfessionals from MNC and MAANG participate in both life and recorded experience centers or sessions. At least seven cities, including Bangalore, Hyderabad, Delhi, and Pune, have experience centers.\nThe institute provides a range of job support services, including career counseling, help with job searches, and workshops for career development. Through the offline centers located in places like Bangalore, Hyderabad, Delhi, and Pune, you can also get career services.\n\nIf not the above institute, you can also look for other options like:\n\nThe greatest instructors from Stanford University and other universities are available to teach a variety of courses on Udacity, a great resource for learning about machine learning. The website also provides a few certification programs that help determine rankings. However, this platform doesn't have a thorough curriculum for advanced ML programs.\n\nBottom Line:\n\nOn a final note, machine learning is a top trend that can drive change in the digital world. It is the technology that will enable ML to realize its full potential.\n\nMake the right choice\n\nThank you"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 8, 'credits': 1982892, 'subscription': 0, 'content': ""Machine learning is an extremely complex field - essentially you're trying to teach a computer how to think for itself. There aren't any primers or short courses that will help you get started, so it's best if you've got some background knowledge in math, programming, and stats before diving in headfirst.\n\nPrerequisites to start learning machine learning:\n\nGet a copy of the book 'Machine Learning to learn general machine learning theory\nLearn how to use MATLAB or R to do basic linear algebra and calculus\nTry some statistics, preferably in R or MATLAB before your machine-learning journey begins\nLearn basic probability, e.g., using the binomial distribution in R or MATLAB.\n\nHow to go about learning the abilities needed to become a machine learning analyst:\n\nFor those who possess a thorough understanding of data, statistics, and computer science, there are numerous certificate programs available.\n\nIt introduces users to machine learning principles and covers its fundamental topics.\nThe classes also teach you how to expand your knowledge of machine learning and how to analyze data.\nYou will learn how to understand data using statistical analysis from these programs.\nAdditionally, it covers the skills necessary to excel as a data mining and machine learning analyst.\n\nWhich platform provides the best ML certification?\n\nThis institute offers the best machine learning courses in Bangalore, which can be accessed from anywhere in the world. It is a great place to learn about machine learning. The best aspect of this institute is the availability of courses with a particular industrial specialization.\n\nArtificial Intelligence and Machine Learning Program\n\nHere are a few of its features to look at:\n\nReal-time initiatives: There are too many ML principles to fully comprehend without useful applications. Because of the practical instruction, every student will be able to use the subject in business. You will have the opportunity to work on practical assignments that require you to use the theoretical knowledge you have gained in class. Actual commercial endeavors from 12+ firms, including Samsung, Netflix, Amazon, Uber, and others. Programs that use machine learning can be useful for both novices and specialists. AI initiatives include tasks like Fraud Deduction, Portfolio Management, etc.\nDomain specialization is valuable since many occupations today require a certain set of subject-matter expertise before you can grow in your career. However, I believe focusing on a certain field is the best way to advance your career. You can develop your career while learning more about a particular subject with this type of training. High-profile companies seek professionals who have a thorough understanding of their industry. Working professionals should think carefully before enrolling in domain-specific training courses to advance their abilities. Listed below are a few of these Oil and Gas, Healthcare, Energy, Manufacturing, BFSI, etc.\nAn IBM and Microsoft certification is the result of the Artificial Intelligence and Machine Learning Program. Such a qualification will strengthen the reliability and competence of your resume. After finishing the course, you will be awarded an IBM and Microsoft and along with that will receive 1 Capstone certificate from IBM.\nWhen hybrid learning formats are used, students in this institute can work on a variety of academic tasks at once. Through hybrid training, for instance, employees can continue to fulfill their current duties while picking up new abilities.\nProfessionals from MNC and MAANG participate in both life and recorded experience centers or sessions. At least seven cities, including Bangalore, Hyderabad, Delhi, and Pune, have experience centers.\nThe institute provides a range of job support services, including career counseling, help with job searches, and workshops for career development. Through the offline centers located in places like Bangalore, Hyderabad, Delhi, and Pune, you can also get career services.\n\nIf not the above institute, you can also look for other options like:\n\nThe greatest instructors from Stanford University and other universities are available to teach a variety of courses on Udacity, a great resource for learning about machine learning. The website also provides a few certification programs that help determine rankings. However, this platform doesn't have a thorough curriculum for advanced ML programs.\n\nBottom Line:\n\nOn a final note, machine learning is a top trend that can drive change in the digital world. It is the technology that will enable ML to realize its full potential.\n\nMake the right choice\n\nThank you"", 'aiModelVersion': '1'}",0.0
Markus Schmaus,5y,What does it mean that we don't really understand what happens in neural networks?,"[1412.6572] Explaining and Harnessing Adversarial Examples

To you and me these are two pictures of a panda. While a neural network would agree with us on the left hand picture, it is absolutely certain that the right hand picture shows a gibbon. And we cannot explain why.

Since we don’t understand why it insists that the right picture shows a gibbon, we also don’t really understand why the neural network thinks the left picture shows a panda.

When we look at the neural network we can identify individual neurons which tend to activate when certain features are present, like two tilted black ovals in the middle of a big white circle. So it’s tempting to claim that it thinks the picture shows a panda because these features are present, but the same features are also present in the other picture, so this isn’t a correct description of how the neural network works, and we don’t really understand how it really works.

There are some things we do understand. The picture on the right is called an adversarial example and it was creating by perturbing the picture on the left by the noise term shown in the middle. And we understand how we can find such noise terms. We have also made progress in understanding why neural networks are susceptible to such images, which has a lot to do with the curse of dimensionality and the decision boundary between pictures of different objects. But unfortunately we don’t understand this well enough to actually fix it.

You might think, we could simply fix this by providing additional learning data, including the picture on the right. Unfortunately this doesn’t work. While this allows us to defend against specific examples, it’s always possible to create new examples. And all network architectures we have explored so far are susceptible to adversarial examples.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/bug4o7npmrwx1yi0', 'title': ""What does it mean that we don't really understand what happens in neural networks?"", 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': '[1412.6572] Explaining and Harnessing Adversarial Examples\n\nTo you and me these are two pictures of a panda. While a neural network would agree with us on the left hand picture, it is absolutely certain that the right hand picture shows a gibbon. And we cannot explain why.\n\nSince we don’t understand why it insists that the right picture shows a gibbon, we also don’t really understand why the neural network thinks the left picture shows a panda.\n\nWhen we look at the neural network we can identify individual neurons which tend to activate when certain features are present, like two tilted black ovals in the middle of a big white circle. So it’s tempting to claim that it thinks the picture shows a panda because these features are present, but the same features are also present in the other picture, so this isn’t a correct description of how the neural network works, and we don’t really understand how it really works.\n\nThere are some things we do understand. The picture on the right is called an adversarial example and it was creating by perturbing the picture on the left by the noise term shown in the middle. And we understand how we can find such noise terms. We have also made progress in understanding why neural networks are susceptible to such images, which has a lot to do with the curse of dimensionality and the decision boundary between pictures of different objects. But unfortunately we don’t understand this well enough to actually fix it.\n\nYou might think, we could simply fix this by providing additional learning data, including the picture on the right. Unfortunately this doesn’t work. While this allows us to defend against specific examples, it’s always possible to create new examples. And all network architectures we have explored so far are susceptible to adversarial examples.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 4, 'credits': 1982888, 'subscription': 0, 'content': '[1412.6572] Explaining and Harnessing Adversarial Examples\n\nTo you and me these are two pictures of a panda. While a neural network would agree with us on the left hand picture, it is absolutely certain that the right hand picture shows a gibbon. And we cannot explain why.\n\nSince we don’t understand why it insists that the right picture shows a gibbon, we also don’t really understand why the neural network thinks the left picture shows a panda.\n\nWhen we look at the neural network we can identify individual neurons which tend to activate when certain features are present, like two tilted black ovals in the middle of a big white circle. So it’s tempting to claim that it thinks the picture shows a panda because these features are present, but the same features are also present in the other picture, so this isn’t a correct description of how the neural network works, and we don’t really understand how it really works.\n\nThere are some things we do understand. The picture on the right is called an adversarial example and it was creating by perturbing the picture on the left by the noise term shown in the middle. And we understand how we can find such noise terms. We have also made progress in understanding why neural networks are susceptible to such images, which has a lot to do with the curse of dimensionality and the decision boundary between pictures of different objects. But unfortunately we don’t understand this well enough to actually fix it.\n\nYou might think, we could simply fix this by providing additional learning data, including the picture on the right. Unfortunately this doesn’t work. While this allows us to defend against specific examples, it’s always possible to create new examples. And all network architectures we have explored so far are susceptible to adversarial examples.', 'aiModelVersion': '1'}",0.9997
Gourav Bansal,Aug 27,What are prerequisites to start learning machine learning?,"Machine learning (ML) is an emerging field in data science and engineering that enables several AI systems to make accurate decisions on their own. This is because machine learning is based on the ability of statistical models to provide results on unknown or new sets of data based on the past data that it is trained on. And to get the data trained, you need programming skills.

Note: So the first point in the answer to your question is that statistics and programming are fundamental prerequisites for ML. The next step would be to identify an institute where you can learn these things.

If you take up a machine learning programming course online, you will be taught all the underlying concepts and algorithms that make an ML model. There are many institutes that you can consider for an online ML course, like Coursera, Udemy, upGrad, Skillshare, Learnbay, etc.

The prerequisites for ML can be easily built in the following steps.

Step 1: Build a strong base in statistics and mathematics

Theoretically, ML has many overlaps with predictive statistics. In fact, a distinct feature of ML is that it typically deals with big data, but the techniques applied to the data all have their roots in predictive statistics.

However, you cannot directly start learning these models without first understanding descriptive and inferential statistics.
So you should begin by brushing up on your basic statistical concepts like mean, median, mode, etc., and the different types of graphs and charts.
The next thing would be to move on to concepts of inferential statistics.
Some of the most important things to learn here for ML would be the different probability distributions, hypothesis testing, chi-square tests, ANOVA, etc.

In this way, you can gradually enter into predictive statistics, like regression analysis, time series forecasting, etc. These are the methods that will form the foundation for elementary ML concepts like CART (classification and regression trees), validation techniques, clustering, discriminant analysis, artificial neural networks, etc.

This covers the prerequisites for learning ML from a conceptual point of view.

The next step is about where you can get the proper guidance.

Step 2: Pick the right institute to help you in learning these subjects

You will find many online resources like books and videos to learn everything mentioned above. However, these are difficult to follow for those who are not already familiar with statistical analyses to some extent. This is why I strongly suggest you enrol in an online certification program for ML. Many platforms like Udemy, Skillshare, upGrad, Simplilearn, Great Learning, etc. offer such programs and each has its own distinct advantages.

Let’s look at some:

In my view, the best program for ML training for working professionals is offered by this institute. The Advanced AI & ML Certification Program offered by this institute is perfect for you to start a career as an ML expert in product-based MNCs and startups.

Some of the most exciting features of this course are -

In addition to it being a full-stack program in ML, you have the opportunity to choose a domain specialization along with the course. This means that you get to work on a hands-on industry project offered in real-time by leading companies like Bosch, Samsung, etc. You can choose a project from any industry of your liking, such as Oil and Gas, Healthcare, Energy, Manufacturing, BFSI, etc.
The classes are conducted live, and you have a lot of scope for faculty interaction during sessions. You can also approach them after class for 1-1 sessions to get your doubts cleared. It also has a hybrid way of learning.
Students who successfully complete the Artificial Intelligence and Machine Learning Programme receive an IBM course completion certificate. You will earn an IBM and Microsoft certificate after completing the course, along with 1 IBM Capstone certificate.
The institute provides a range of employment aid services, including career counseling, help with job searches, and training for career development. The offline centers located in places like Bangalore, Hyderabad, Delhi, and Pune allow you to access career services offline as well.
The main advantage of the Artificial Intelligence and Machine Learning Programme is the chance to work on real machine learning projects. Future sales forecasting is one of the many subjects covered, in addition to examining fraud deduction and portfolio management. These experience centers may be located in Delhi, Pune, Hyderabad, and Bangalore in addition to other cities across the nation.

Course: Introduction to Machine Learning for Data Science

Features:

This institute is a great place to start if you're interested in taking a few quick machine-learning courses. This program is made in such a way that beginners can begin their learning journey from the beginning.

However, this course just provides pre-recorded lectures and videos; there is no real mentoring interaction; and there is no instruction in particular domains.

But in the end, you have to ensure that you have an adequate passion to earn the prerequisites, otherwise, no one can help you.

I hope that my answer helps you kickstart your ML journey in the right direction.

All the best.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/4cx6eqrj8wuz1isn', 'title': 'What are prerequisites to start learning machine learning?', 'score': {'original': 0.01575, 'ai': 0.98425}, 'blocks': [{'text': 'Machine learning (ML) is an emerging field in data science and engineering that enables several AI systems to make accurate decisions on their own. This is because machine learning is based on the ability of statistical models to provide results on unknown or new sets of data based on the past data that it is trained on. And to get the data trained, you need programming skills.\n\nNote: So the first point in the answer to your question is that statistics and programming are fundamental prerequisites for ML. The next step would be to identify an institute where you can learn these things.\n\nIf you take up a machine learning programming course online, you will be taught all the underlying concepts and algorithms that make an ML model. There are many institutes that you can consider for an online ML course, like Coursera, Udemy, upGrad, Skillshare, Learnbay, etc.\n\nThe prerequisites for ML can be easily built in the following steps.\n\nStep 1: Build a strong base in statistics and mathematics\n\nTheoretically, ML has many overlaps with predictive statistics. In fact, a distinct feature of ML is that it typically deals with big data, but the techniques applied to the data all have their roots in predictive statistics.\n\nHowever, you cannot directly start learning these models without first understanding descriptive and inferential statistics.\nSo you should begin by brushing up on your basic statistical concepts like mean, median, mode, etc., and the different types of graphs and charts.\nThe next thing would be to move on to concepts of inferential statistics.\nSome of the most important things to learn here for ML would be the different probability distributions, hypothesis testing, chi-square tests, ANOVA, etc.\n\nIn this way, you can gradually enter into predictive statistics, like regression analysis, time series forecasting, etc. These are the methods that will form the foundation for elementary ML concepts like CART (classification and regression trees), validation techniques, clustering, discriminant analysis, artificial neural networks, etc.\n\nThis covers the prerequisites for learning ML from a conceptual point of view.\n\nThe next step is about where you can get the proper guidance.\n\nStep 2: Pick the right institute to help you in learning these subjects\n\nYou will find many online resources like books and videos to learn everything mentioned above. However, these are difficult to follow for those who are not already familiar with statistical analyses to some extent. This is why I strongly suggest you enrol in an online certification program for ML. Many platforms like Udemy, Skillshare, upGrad, Simplilearn, Great Learning, etc. offer such programs and each has its own distinct advantages.\n\nLet’s look at some:\n\nIn my view, the best program for ML training for working professionals is offered by this institute. The Advanced AI & ML Certification Program offered by this institute is perfect for you to start a career as an ML expert in product-based MNCs and startups.\n\nSome of the most exciting features of this course are -\n\nIn addition to it being a full-stack program in ML, you have the opportunity to choose a domain specialization along with the course. This means that you get to work on a hands-on industry project', 'result': {'fake': 0.9297, 'real': 0.0703}, 'status': 'success'}, {'text': ""offered in real-time by leading companies like Bosch, Samsung, etc. You can choose a project from any industry of your liking, such as Oil and Gas, Healthcare, Energy, Manufacturing, BFSI, etc.\nThe classes are conducted live, and you have a lot of scope for faculty interaction during sessions. You can also approach them after class for 1-1 sessions to get your doubts cleared. It also has a hybrid way of learning.\nStudents who successfully complete the Artificial Intelligence and Machine Learning Programme receive an IBM course completion certificate. You will earn an IBM and Microsoft certificate after completing the course, along with 1 IBM Capstone certificate.\nThe institute provides a range of employment aid services, including career counseling, help with job searches, and training for career development. The offline centers located in places like Bangalore, Hyderabad, Delhi, and Pune allow you to access career services offline as well.\nThe main advantage of the Artificial Intelligence and Machine Learning Programme is the chance to work on real machine learning projects. Future sales forecasting is one of the many subjects covered, in addition to examining fraud deduction and portfolio management. These experience centers may be located in Delhi, Pune, Hyderabad, and Bangalore in addition to other cities across the nation.\n\nCourse: Introduction to Machine Learning for Data Science\n\nFeatures:\n\nThis institute is a great place to start if you're interested in taking a few quick machine-learning courses. This program is made in such a way that beginners can begin their learning journey from the beginning.\n\nHowever, this course just provides pre-recorded lectures and videos; there is no real mentoring interaction; and there is no instruction in particular domains.\n\nBut in the end, you have to ensure that you have an adequate passion to earn the prerequisites, otherwise, no one can help you.\n\nI hope that my answer helps you kickstart your ML journey in the right direction.\n\nAll the best."", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 9, 'credits': 1982879, 'subscription': 0, 'content': ""Machine learning (ML) is an emerging field in data science and engineering that enables several AI systems to make accurate decisions on their own. This is because machine learning is based on the ability of statistical models to provide results on unknown or new sets of data based on the past data that it is trained on. And to get the data trained, you need programming skills.\n\nNote: So the first point in the answer to your question is that statistics and programming are fundamental prerequisites for ML. The next step would be to identify an institute where you can learn these things.\n\nIf you take up a machine learning programming course online, you will be taught all the underlying concepts and algorithms that make an ML model. There are many institutes that you can consider for an online ML course, like Coursera, Udemy, upGrad, Skillshare, Learnbay, etc.\n\nThe prerequisites for ML can be easily built in the following steps.\n\nStep 1: Build a strong base in statistics and mathematics\n\nTheoretically, ML has many overlaps with predictive statistics. In fact, a distinct feature of ML is that it typically deals with big data, but the techniques applied to the data all have their roots in predictive statistics.\n\nHowever, you cannot directly start learning these models without first understanding descriptive and inferential statistics.\nSo you should begin by brushing up on your basic statistical concepts like mean, median, mode, etc., and the different types of graphs and charts.\nThe next thing would be to move on to concepts of inferential statistics.\nSome of the most important things to learn here for ML would be the different probability distributions, hypothesis testing, chi-square tests, ANOVA, etc.\n\nIn this way, you can gradually enter into predictive statistics, like regression analysis, time series forecasting, etc. These are the methods that will form the foundation for elementary ML concepts like CART (classification and regression trees), validation techniques, clustering, discriminant analysis, artificial neural networks, etc.\n\nThis covers the prerequisites for learning ML from a conceptual point of view.\n\nThe next step is about where you can get the proper guidance.\n\nStep 2: Pick the right institute to help you in learning these subjects\n\nYou will find many online resources like books and videos to learn everything mentioned above. However, these are difficult to follow for those who are not already familiar with statistical analyses to some extent. This is why I strongly suggest you enrol in an online certification program for ML. Many platforms like Udemy, Skillshare, upGrad, Simplilearn, Great Learning, etc. offer such programs and each has its own distinct advantages.\n\nLet’s look at some:\n\nIn my view, the best program for ML training for working professionals is offered by this institute. The Advanced AI & ML Certification Program offered by this institute is perfect for you to start a career as an ML expert in product-based MNCs and startups.\n\nSome of the most exciting features of this course are -\n\nIn addition to it being a full-stack program in ML, you have the opportunity to choose a domain specialization along with the course. This means that you get to work on a hands-on industry project offered in real-time by leading companies like Bosch, Samsung, etc. You can choose a project from any industry of your liking, such as Oil and Gas, Healthcare, Energy, Manufacturing, BFSI, etc.\nThe classes are conducted live, and you have a lot of scope for faculty interaction during sessions. You can also approach them after class for 1-1 sessions to get your doubts cleared. It also has a hybrid way of learning.\nStudents who successfully complete the Artificial Intelligence and Machine Learning Programme receive an IBM course completion certificate. You will earn an IBM and Microsoft certificate after completing the course, along with 1 IBM Capstone certificate.\nThe institute provides a range of employment aid services, including career counseling, help with job searches, and training for career development. The offline centers located in places like Bangalore, Hyderabad, Delhi, and Pune allow you to access career services offline as well.\nThe main advantage of the Artificial Intelligence and Machine Learning Programme is the chance to work on real machine learning projects. Future sales forecasting is one of the many subjects covered, in addition to examining fraud deduction and portfolio management. These experience centers may be located in Delhi, Pune, Hyderabad, and Bangalore in addition to other cities across the nation.\n\nCourse: Introduction to Machine Learning for Data Science\n\nFeatures:\n\nThis institute is a great place to start if you're interested in taking a few quick machine-learning courses. This program is made in such a way that beginners can begin their learning journey from the beginning.\n\nHowever, this course just provides pre-recorded lectures and videos; there is no real mentoring interaction; and there is no instruction in particular domains.\n\nBut in the end, you have to ensure that you have an adequate passion to earn the prerequisites, otherwise, no one can help you.\n\nI hope that my answer helps you kickstart your ML journey in the right direction.\n\nAll the best."", 'aiModelVersion': '1'}",0.01575
Håkon Hapnes Strand,5y,What fallacies do amateur machine learning engineers make that experienced ones don't?,"There are some fallacies I’ve seen inexperienced machine learning engineers make frequently that seniors tend to know about and avoid. I must stress that these are just propensities. Some juniors, especially those with a solid background in statistics, are very careful about the assumptions they make. And some seniors have a cowboy attitude to data and hack away without much thought. This list is based on my own anecdotal experience.

Assuming correlation implies causation. This is a big one, and the most well-known. Just because there is correlation between two variables, that doesn’t necessarily mean that one is causing the other, and it doesn’t mean that the relationship is any useful in a machine learning model. A common one I often hear is “we’ve identified that the target variable is correlated with weather”. Then the company spends weeks setting up a weather data collector and integrates it into a model, and it turns out the weather data didn’t help at all. Even if there was an indirect relationship, it was probably captured in another feature that was readily available in the data.
Historian’s fallacy. This is pretty basic, but a model should always be evaluated with the data that would have been available at the time of prediction. If not, you get data leakage. Although most machine learning engineers intuititively understand this, it’s harder to avoid in practice than one would think. Setting up proper cross-validation routines requires some practice.
Selection bias. The data is collected and selected in a way so that the population is affected by the sampling approach. Sometimes this is unavoidable, but the ability to sniff that out is the mark of an experienced machine learning engineer.
Handling missing values dogmatically. There is no silver bullet for treating missing values. Inexperienced machine learning engineers will do things like replace missing values with zero, or remove them completely. When they figure out that the missing values are a problem, they will ask for the canonical way to handle them, but the truth is it doesn’t exist. It’s a complicated, situational problem. Solving it correctly requires experience.
Using accuracy as the only metric to evaluate classifiers. It’s rare that the performance of a classification model can be described precisely using only a single metric. It’s even worse when that metric is plain accuracy. That would assume that false positives and false negatives are equally bad, which is almost never the case. Even ROC AUC falls short in most cases. The necessity of assessing models holistically is often underestimated by inexperienced machine learning engineers.
Motivated reasoning. Because they want the model to work, they find ways to make it look better than it really is, losing objectivity in the process. A common attitude among juniors is that failure is not an option. Seniors will understand that failing is a natural part of a scientific process. The job of a data scientist is to test hypotheses. Sometimes, the hypothesis must be rejected, meaning there is insufficient data to build a predictive model. It doesn’t help when machine learning engineers try to fit squares into circles.
Cherry picking. Demonstrating the effectiveness of a model by pointing at examples where it made good predictions instead of providing the full picture. This goes hand in hand with motivated reasoning.
The quantitative fallacy. This is very common even among more experienced machine learning engineers and data scientists. It’s when you make decisions based solely on numbers, disregarding any qualitative considerations. The result is often a model that is technically sound but useless, because it ignores fundamental questions like “how will we use this model” and “how will it help our business”. Ignoring qualititative aspects also leads to a poor understanding of the domain which again hampers modelling.
Preferring complexity over simplicity. Any experienced machine learning engineer will tell you that a simpler model is better than a complex one, all else being equal. Inexperienced machine learning engineers will often dive right into complex neural networks before testing simpler models, merely because they want to get their feet wet.
Overestimating the importance of hyperparameter tuning. Junior practitioners will often spend too much time tweaking hyperparameters for small performance gains when much greater gains could have been made by working with the underlying data and features.
Underestimating the importance of feature engineering. See above.
Regarding domain knowledge as irrelevant. Domain knowledge is always relevant, even if you’re just implementing an already defined machine learning model. That’s why every machine learning engineer needs to be a data scientist too. It takes a bit of trial and error until you fully appreciate just how important domain knowledge is.
Treating the modelling process linearly. Iteration is key, not just in the training algorithm, but in the modelling and experimentation process as well. Because of the nondeterministic nature of machine learning, you can’t specify how to solve a problem upfront, because there is no way of knowing what the best solution is before actually trying it out. Many beginners (and managers) make the mistake of planning a machine learning project like a software development project.
Ignoring model decay. No, I’m not talking about learning rate decay, which is something completely different. I’m talking about the fact that models will degrade over time after being put into production. Sometimes they even affect the very phenomenon they’re trying to model, leading to self-reinforcing feedback. This is also known as the Hawthorne effect. Even though you’ve trained, validated and tested a model on three different datasets, it’s not necessarily the model that performs best in that test that will prove to be the best over time. This touches on preferring complexity over simplicity, since simpler models are less prone to degrade. It’s hard to grasp this concept intuitively without practical experience.
Ignoring the cobra effect. Sometimes the solution to a problem can actually make the problem worse. Just because you have identified that some customers are more likely to buy a product, that doesn’t mean that a targeted marketing ad will increase sales. In some occasions, marketing actually produces the opposite outcome. This is unintuitive, but happens in practice.

I’m sure there are many more. If you know about some common fallacies I’ve forgotten, feel free to leave a comment.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/honc3rkbjqst2dmg', 'title': ""What fallacies do amateur machine learning engineers make that experienced ones don't?"", 'score': {'original': 0.35466666666667, 'ai': 0.64533333333333}, 'blocks': [{'text': 'There are some fallacies I’ve seen inexperienced machine learning engineers make frequently that seniors tend to know about and avoid. I must stress that these are just propensities. Some juniors, especially those with a solid background in statistics, are very careful about the assumptions they make. And some seniors have a cowboy attitude to data and hack away without much thought. This list is based on my own anecdotal experience.\n\nAssuming correlation implies causation. This is a big one, and the most well-known. Just because there is correlation between two variables, that doesn’t necessarily mean that one is causing the other, and it doesn’t mean that the relationship is any useful in a machine learning model. A common one I often hear is “we’ve identified that the target variable is correlated with weather”. Then the company spends weeks setting up a weather data collector and integrates it into a model, and it turns out the weather data didn’t help at all. Even if there was an indirect relationship, it was probably captured in another feature that was readily available in the data.\nHistorian’s fallacy. This is pretty basic, but a model should always be evaluated with the data that would have been available at the time of prediction. If not, you get data leakage. Although most machine learning engineers intuititively understand this, it’s harder to avoid in practice than one would think. Setting up proper cross-validation routines requires some practice.\nSelection bias. The data is collected and selected in a way so that the population is affected by the sampling approach. Sometimes this is unavoidable, but the ability to sniff that out is the mark of an experienced machine learning engineer.\nHandling missing values dogmatically. There is no silver bullet for treating missing values. Inexperienced machine learning engineers will do things like replace missing values with zero, or remove them completely. When they figure out that the missing values are a problem, they will ask for the canonical way to handle them, but the truth is it doesn’t exist. It’s a complicated, situational problem. Solving it correctly requires experience.\nUsing accuracy as the only metric to evaluate classifiers. It’s rare that the performance of a classification model can be described precisely using only a single metric. It’s even worse when that metric is plain accuracy. That would assume that false positives and false negatives are equally bad, which is almost never the case. Even ROC AUC falls short in most cases. The necessity of assessing models holistically is often underestimated by inexperienced machine learning engineers.\nMotivated reasoning. Because they want the model to work, they find ways to make it look better than it really is, losing objectivity in the process. A common attitude among juniors is that failure is not an option. Seniors will understand that failing is a natural part of a scientific process. The job of a data scientist is to test hypotheses. Sometimes, the hypothesis must be rejected, meaning there is insufficient data to build a predictive model. It doesn’t help when machine learning engineers try to fit squares into circles.\nCherry picking. Demonstrating the', 'result': {'fake': 0.0051, 'real': 0.9949}, 'status': 'success'}, {'text': 'effectiveness of a model by pointing at examples where it made good predictions instead of providing the full picture. This goes hand in hand with motivated reasoning.\nThe quantitative fallacy. This is very common even among more experienced machine learning engineers and data scientists. It’s when you make decisions based solely on numbers, disregarding any qualitative considerations. The result is often a model that is technically sound but useless, because it ignores fundamental questions like “how will we use this model” and “how will it help our business”. Ignoring qualititative aspects also leads to a poor understanding of the domain which again hampers modelling.\nPreferring complexity over simplicity. Any experienced machine learning engineer will tell you that a simpler model is better than a complex one, all else being equal. Inexperienced machine learning engineers will often dive right into complex neural networks before testing simpler models, merely because they want to get their feet wet.\nOverestimating the importance of hyperparameter tuning. Junior practitioners will often spend too much time tweaking hyperparameters for small performance gains when much greater gains could have been made by working with the underlying data and features.\nUnderestimating the importance of feature engineering. See above.\nRegarding domain knowledge as irrelevant. Domain knowledge is always relevant, even if you’re just implementing an already defined machine learning model. That’s why every machine learning engineer needs to be a data scientist too. It takes a bit of trial and error until you fully appreciate just how important domain knowledge is.\nTreating the modelling process linearly. Iteration is key, not just in the training algorithm, but in the modelling and experimentation process as well. Because of the nondeterministic nature of machine learning, you can’t specify how to solve a problem upfront, because there is no way of knowing what the best solution is before actually trying it out. Many beginners (and managers) make the mistake of planning a machine learning project like a software development project.\nIgnoring model decay. No, I’m not talking about learning rate decay, which is something completely different. I’m talking about the fact that models will degrade over time after being put into production. Sometimes they even affect the very phenomenon they’re trying to model, leading to self-reinforcing feedback. This is also known as the Hawthorne effect. Even though you’ve trained, validated and tested a model on three different datasets, it’s not necessarily the model that performs best in that test that will prove to be the best over time. This touches on preferring complexity over simplicity, since simpler models are less prone to degrade. It’s hard to grasp this concept intuitively without practical experience.\nIgnoring the cobra effect. Sometimes the solution to a problem can actually make the problem worse. Just because you have identified that some customers are more likely to buy a product, that doesn’t mean that a targeted marketing ad will increase sales. In some occasions, marketing actually produces the opposite outcome. This is unintuitive, but happens in practice.\n\nI’m sure there are many more. If you know about some common fallacies I’ve forgotten, feel free to leave a comment.', 'result': {'fake': 0.728, 'real': 0.272}, 'status': 'success'}], 'credits_used': 11, 'credits': 1982868, 'subscription': 0, 'content': 'There are some fallacies I’ve seen inexperienced machine learning engineers make frequently that seniors tend to know about and avoid. I must stress that these are just propensities. Some juniors, especially those with a solid background in statistics, are very careful about the assumptions they make. And some seniors have a cowboy attitude to data and hack away without much thought. This list is based on my own anecdotal experience.\n\nAssuming correlation implies causation. This is a big one, and the most well-known. Just because there is correlation between two variables, that doesn’t necessarily mean that one is causing the other, and it doesn’t mean that the relationship is any useful in a machine learning model. A common one I often hear is “we’ve identified that the target variable is correlated with weather”. Then the company spends weeks setting up a weather data collector and integrates it into a model, and it turns out the weather data didn’t help at all. Even if there was an indirect relationship, it was probably captured in another feature that was readily available in the data.\nHistorian’s fallacy. This is pretty basic, but a model should always be evaluated with the data that would have been available at the time of prediction. If not, you get data leakage. Although most machine learning engineers intuititively understand this, it’s harder to avoid in practice than one would think. Setting up proper cross-validation routines requires some practice.\nSelection bias. The data is collected and selected in a way so that the population is affected by the sampling approach. Sometimes this is unavoidable, but the ability to sniff that out is the mark of an experienced machine learning engineer.\nHandling missing values dogmatically. There is no silver bullet for treating missing values. Inexperienced machine learning engineers will do things like replace missing values with zero, or remove them completely. When they figure out that the missing values are a problem, they will ask for the canonical way to handle them, but the truth is it doesn’t exist. It’s a complicated, situational problem. Solving it correctly requires experience.\nUsing accuracy as the only metric to evaluate classifiers. It’s rare that the performance of a classification model can be described precisely using only a single metric. It’s even worse when that metric is plain accuracy. That would assume that false positives and false negatives are equally bad, which is almost never the case. Even ROC AUC falls short in most cases. The necessity of assessing models holistically is often underestimated by inexperienced machine learning engineers.\nMotivated reasoning. Because they want the model to work, they find ways to make it look better than it really is, losing objectivity in the process. A common attitude among juniors is that failure is not an option. Seniors will understand that failing is a natural part of a scientific process. The job of a data scientist is to test hypotheses. Sometimes, the hypothesis must be rejected, meaning there is insufficient data to build a predictive model. It doesn’t help when machine learning engineers try to fit squares into circles.\nCherry picking. Demonstrating the effectiveness of a model by pointing at examples where it made good predictions instead of providing the full picture. This goes hand in hand with motivated reasoning.\nThe quantitative fallacy. This is very common even among more experienced machine learning engineers and data scientists. It’s when you make decisions based solely on numbers, disregarding any qualitative considerations. The result is often a model that is technically sound but useless, because it ignores fundamental questions like “how will we use this model” and “how will it help our business”. Ignoring qualititative aspects also leads to a poor understanding of the domain which again hampers modelling.\nPreferring complexity over simplicity. Any experienced machine learning engineer will tell you that a simpler model is better than a complex one, all else being equal. Inexperienced machine learning engineers will often dive right into complex neural networks before testing simpler models, merely because they want to get their feet wet.\nOverestimating the importance of hyperparameter tuning. Junior practitioners will often spend too much time tweaking hyperparameters for small performance gains when much greater gains could have been made by working with the underlying data and features.\nUnderestimating the importance of feature engineering. See above.\nRegarding domain knowledge as irrelevant. Domain knowledge is always relevant, even if you’re just implementing an already defined machine learning model. That’s why every machine learning engineer needs to be a data scientist too. It takes a bit of trial and error until you fully appreciate just how important domain knowledge is.\nTreating the modelling process linearly. Iteration is key, not just in the training algorithm, but in the modelling and experimentation process as well. Because of the nondeterministic nature of machine learning, you can’t specify how to solve a problem upfront, because there is no way of knowing what the best solution is before actually trying it out. Many beginners (and managers) make the mistake of planning a machine learning project like a software development project.\nIgnoring model decay. No, I’m not talking about learning rate decay, which is something completely different. I’m talking about the fact that models will degrade over time after being put into production. Sometimes they even affect the very phenomenon they’re trying to model, leading to self-reinforcing feedback. This is also known as the Hawthorne effect. Even though you’ve trained, validated and tested a model on three different datasets, it’s not necessarily the model that performs best in that test that will prove to be the best over time. This touches on preferring complexity over simplicity, since simpler models are less prone to degrade. It’s hard to grasp this concept intuitively without practical experience.\nIgnoring the cobra effect. Sometimes the solution to a problem can actually make the problem worse. Just because you have identified that some customers are more likely to buy a product, that doesn’t mean that a targeted marketing ad will increase sales. In some occasions, marketing actually produces the opposite outcome. This is unintuitive, but happens in practice.\n\nI’m sure there are many more. If you know about some common fallacies I’ve forgotten, feel free to leave a comment.', 'aiModelVersion': '1'}",0.35466666666667
Bryan Walsh,Updated 4y,What's the coolest thing that AI has achieved so far?,"This is Speech2Face[1], a neural network model that can re-create someone’s face just from their voice.

The results aren’t perfect, but the resemblance is apparent.

It was developed at MIT and the relevant research paper[2] was published last week (Late May 2019). Millions of video segments from YouTube and other sources were used to train it.

Given enough data, computing power, and time, a neural network can 'learn' how to recreate human faces by analyzing the video segments, pairing the voices to faces, and finding the patterns and trends between the two.

Notice that the generated faces accurately recreate the nose, lips, cheeks, and bone structure - all except for their eyes. This provides insight on how this ML model works. The accurately represented features have direct correlations between appearance and speech. For example, people with deeper voices might have wider noses, or wider jaws than people with higher pitched voices, whereas eye shape and size generally don’t have much impact on how someone sounds.

Here are all of the correlating features that the authors listed. Correlation ranges from -1 to 1, with 0 having no correlation and 
±
±
 1 being absolute correlation:

(Source: Referenced paper)

But how is this possible?

Think of a time that you spoke on the phone with someone that you’ve never seen before, like customer support or a doctor’s office. You might not realize that during the phone call, your brain is trying to visualize what they might look like based on how they sound. If you’ve ever listened to a podcast or been in conference call, you’re able to keep track of who’s speaking at any moment without even trying.

Try this:

Visualize two guys having a conversation at a bar
Imagine their dialogue and tone
Now give both of them an Australian accent

What do they look like? What are their hair colors? The image in your head might be different from mine, but what matters is that we both came up with some generic, “average” image for what a male with an Australian accent looks like. The two guys you pictured might look like Australians you know in real life, or a combination of all of the Australians you’ve ever seen.

This is what Speech2Face is doing, but with hundreds of times more patterns to factor in. Interestingly enough, our brains can already partially accomplish what Speech2Face does, such as being able to identify our friends just by the sound of their voice.

Here are some more results from Speech2Face:

Additional Remarks & Info:

Now combine Speech2Face with Nvidia’s GAN[3] (Generative Adversarial Network):

All of these faces were generated by GAN, none of them exist in real-life. They’re the results of a neural network that learned the common features and patterns of actual people well enough to create them on its own. Even more, the output of the neural network can be sent back to the input to further train itself, resulting in near-infinite data for it to train on (Given that the output is already quite lifelike).

GAN can create non-existent people from nothing, and Speech2Face is currently working from the other end. WaveNet[4] is already scarily close to sounding like a real human voice, and it’s only a matter of time before they meet in the middle.

We’re probably only < 10 years away from an artificial life-like human that sounds and looks like anyone else, and it will be difficult to tell the difference.

I discussed the ethical concerns with this technology on another question recently.

Footnotes

[1] Speech2Face: Learning the Face Behind a Voice
[2] Speech2Face: Learning the Face Behind a Voice
[3] Progressive Growing of GANs for Improved Quality, Stability, and Variation
[4] WaveNet: A Generative Model for Raw Audio | DeepMind","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/h8mwv34c5i2krtja', 'title': ""What's the coolest thing that AI has achieved so far?"", 'score': {'original': 0.496, 'ai': 0.504}, 'blocks': [{'text': ""This is Speech2Face[1], a neural network model that can re-create someone’s face just from their voice.\n\nThe results aren’t perfect, but the resemblance is apparent.\n\nIt was developed at MIT and the relevant research paper[2] was published last week (Late May 2019). Millions of video segments from YouTube and other sources were used to train it.\n\nGiven enough data, computing power, and time, a neural network can 'learn' how to recreate human faces by analyzing the video segments, pairing the voices to faces, and finding the patterns and trends between the two.\n\nNotice that the generated faces accurately recreate the nose, lips, cheeks, and bone structure - all except for their eyes. This provides insight on how this ML model works. The accurately represented features have direct correlations between appearance and speech. For example, people with deeper voices might have wider noses, or wider jaws than people with higher pitched voices, whereas eye shape and size generally don’t have much impact on how someone sounds.\n\nHere are all of the correlating features that the authors listed. Correlation ranges from -1 to 1, with 0 having no correlation and \n±\n±\n 1 being absolute correlation:\n\n(Source: Referenced paper)\n\nBut how is this possible?\n\nThink of a time that you spoke on the phone with someone that you’ve never seen before, like customer support or a doctor’s office. You might not realize that during the phone call, your brain is trying to visualize what they might look like based on how they sound. If you’ve ever listened to a podcast or been in conference call, you’re able to keep track of who’s speaking at any moment without even trying.\n\nTry this:\n\nVisualize two guys having a conversation at a bar\nImagine their dialogue and tone\nNow give both of them an Australian accent\n\nWhat do they look like? What are their hair colors? The image in your head might be different from mine, but what matters is that we both came up with some generic, “average” image for what a male with an Australian accent looks like. The two guys you pictured might look like Australians you know in real life, or a combination of all of the Australians you’ve ever seen.\n\nThis is what Speech2Face is doing, but with hundreds of times more patterns to factor in. Interestingly enough, our brains can already partially accomplish what Speech2Face does, such as being able to identify our friends just by the sound of their voice.\n\nHere are some more results from Speech2Face:\n\nAdditional Remarks & Info:\n\nNow combine Speech2Face with Nvidia’s GAN[3] (Generative Adversarial Network):\n\nAll of these faces were generated by GAN, none of them exist in real-life. They’re the results of a neural network that learned the common features and patterns of actual people well enough to create them on its own. Even more, the output of the neural network can be sent back to the input to further train itself, resulting in near-infinite data for it to train on (Given that the output is already quite lifelike).\n\nGAN can create non-existent people from nothing, and Speech2Face is currently working from the other end. WaveNet[4] is already scarily close to sounding like a"", 'result': {'fake': 0.0055, 'real': 0.9945}, 'status': 'success'}, {'text': 'real human voice, and it’s only a matter of time before they meet in the middle.\n\nWe’re probably only < 10 years away from an artificial life-like human that sounds and looks like anyone else, and it will be difficult to tell the difference.\n\nI discussed the ethical concerns with this technology on another question recently.\n\nFootnotes\n\n[1] Speech2Face: Learning the Face Behind a Voice\n[2] Speech2Face: Learning the Face Behind a Voice\n[3] Progressive Growing of GANs for Improved Quality, Stability, and Variation\n[4] WaveNet: A Generative Model for Raw Audio | DeepMind', 'result': {'fake': 0.9999, 'real': 0.0001}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982861, 'subscription': 0, 'content': ""This is Speech2Face[1], a neural network model that can re-create someone’s face just from their voice.\n\nThe results aren’t perfect, but the resemblance is apparent.\n\nIt was developed at MIT and the relevant research paper[2] was published last week (Late May 2019). Millions of video segments from YouTube and other sources were used to train it.\n\nGiven enough data, computing power, and time, a neural network can 'learn' how to recreate human faces by analyzing the video segments, pairing the voices to faces, and finding the patterns and trends between the two.\n\nNotice that the generated faces accurately recreate the nose, lips, cheeks, and bone structure - all except for their eyes. This provides insight on how this ML model works. The accurately represented features have direct correlations between appearance and speech. For example, people with deeper voices might have wider noses, or wider jaws than people with higher pitched voices, whereas eye shape and size generally don’t have much impact on how someone sounds.\n\nHere are all of the correlating features that the authors listed. Correlation ranges from -1 to 1, with 0 having no correlation and \n±\n±\n 1 being absolute correlation:\n\n(Source: Referenced paper)\n\nBut how is this possible?\n\nThink of a time that you spoke on the phone with someone that you’ve never seen before, like customer support or a doctor’s office. You might not realize that during the phone call, your brain is trying to visualize what they might look like based on how they sound. If you’ve ever listened to a podcast or been in conference call, you’re able to keep track of who’s speaking at any moment without even trying.\n\nTry this:\n\nVisualize two guys having a conversation at a bar\nImagine their dialogue and tone\nNow give both of them an Australian accent\n\nWhat do they look like? What are their hair colors? The image in your head might be different from mine, but what matters is that we both came up with some generic, “average” image for what a male with an Australian accent looks like. The two guys you pictured might look like Australians you know in real life, or a combination of all of the Australians you’ve ever seen.\n\nThis is what Speech2Face is doing, but with hundreds of times more patterns to factor in. Interestingly enough, our brains can already partially accomplish what Speech2Face does, such as being able to identify our friends just by the sound of their voice.\n\nHere are some more results from Speech2Face:\n\nAdditional Remarks & Info:\n\nNow combine Speech2Face with Nvidia’s GAN[3] (Generative Adversarial Network):\n\nAll of these faces were generated by GAN, none of them exist in real-life. They’re the results of a neural network that learned the common features and patterns of actual people well enough to create them on its own. Even more, the output of the neural network can be sent back to the input to further train itself, resulting in near-infinite data for it to train on (Given that the output is already quite lifelike).\n\nGAN can create non-existent people from nothing, and Speech2Face is currently working from the other end. WaveNet[4] is already scarily close to sounding like a real human voice, and it’s only a matter of time before they meet in the middle.\n\nWe’re probably only < 10 years away from an artificial life-like human that sounds and looks like anyone else, and it will be difficult to tell the difference.\n\nI discussed the ethical concerns with this technology on another question recently.\n\nFootnotes\n\n[1] Speech2Face: Learning the Face Behind a Voice\n[2] Speech2Face: Learning the Face Behind a Voice\n[3] Progressive Growing of GANs for Improved Quality, Stability, and Variation\n[4] WaveNet: A Generative Model for Raw Audio | DeepMind"", 'aiModelVersion': '1'}",0.496
Roman Trusov,6y,Should you spend money to buy a good GPU for studying deep learning (I’m not referring to production runs)?,"Here’s my story and cost analysis that went into it.

When I was starting my Master’s I was certain that no matter what kind of architectures I am going to work on, they will be heavy and memory-consuming, certainly not the job for a mobile card. GPUs from the university cluster were not the best option, because the machine time was shared and I couldn’t have as much as I wanted.

So, that left me with two options:

Buy my own dev server.
Rent an equivalent AWS.

So, to make the decision, I needed to know:

Total cost of the server.
Projected utilization of the server/AWS.
Total cost of an equivalent AWS unit calculated for, let’s say, a year.

The first part was simple, I chose GTX 1070 as a GPU (at the time 1080 and Titans were twice as expensive and not worth it), used 28-core Xeon for parallel CPU stuff, 64 GB RAM with an option to buy 64 more, an SSD drive for change. GPU was the most expensive part, something around $440.

Total cost: ~$2000.

What would that money get me?

I figured that if I train something at least 8 hours every day on average, which turned out to be true, I am looking at 56 hours per week or 2920 hours per year. So, if I get myself pay-per-hour server on AWS and run it for 2920 hours, anything that costs more than $0.685 will be more expensive than my own PC. I don’t include an electricity bill in the analysis, it’s a negligible fraction of that price (depending on where you live, your experience may vary, and also beware of power outages, they almost never happen where I live).

AWS doesn’t offer any reasonably good GPU solution for $0.685/hr, even as a spot instance. So, that was a no-brainer.

Was it worth it? Absolutely. Two grand for the vital piece of my education is nothing.

Do you need it? If you are serious about studying DL, yes. Understanding an architecture or an algorithm and getting it to work are two very different stories, the only real way to acquire knowledge is to try things for yourself and analyze the results.

If you consider buying multiple cheap GPUs to learn how to work with them - don’t. If your framework supports distributed computation, it does everything in a painless way. If it doesn’t, this is not a task for a beginner, and generally a pain in the rearward.

For training modern architectures CPUs are not a substitute for a GPU in any way. I have a really damn good CPU and it would take weeks to train a network that I usually train overnight. A consumer-grade i5 (I don’t think that overpaying for i7 is a great idea) is even slower.

Finally, some tips and lessons learned along the way:

Don’t ever buy a gaming laptop, the very idea is ridiculous, both for gaming and for deep learning.
You can get a used Xeon CPU very cheap. Even better, you can get a decommissioned server, but this kind of shopping is for hardware-savvy.
ECC RAM is an overkill for this kind of workstation and you can save some money on it.
Small laptop like Chromebook or Thinkpad connected via mosh to your home server is the best configuration.

P.S.: Aaron Yip added some excellent links, check it out:

The $1700 great Deep Learning box: Assembly, setup and benchmarks

System Build - Core i5-7600K 3.8GHz Quad-Core, GeForce GTX 1070 8GB SC... - PCPartPicker

System Build - Core i5-7600K 3.8GHz Quad-Core, GeForce GTX 1070 8GB Mi... - PCPartPicker","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ad612i0jmqhpnekv', 'title': 'Should you spend money to buy a good GPU for studying deep learning (I’m not referring to production runs)?', 'score': {'original': 0.97025, 'ai': 0.02975}, 'blocks': [{'text': 'Here’s my story and cost analysis that went into it.\n\nWhen I was starting my Master’s I was certain that no matter what kind of architectures I am going to work on, they will be heavy and memory-consuming, certainly not the job for a mobile card. GPUs from the university cluster were not the best option, because the machine time was shared and I couldn’t have as much as I wanted.\n\nSo, that left me with two options:\n\nBuy my own dev server.\nRent an equivalent AWS.\n\nSo, to make the decision, I needed to know:\n\nTotal cost of the server.\nProjected utilization of the server/AWS.\nTotal cost of an equivalent AWS unit calculated for, let’s say, a year.\n\nThe first part was simple, I chose GTX 1070 as a GPU (at the time 1080 and Titans were twice as expensive and not worth it), used 28-core Xeon for parallel CPU stuff, 64 GB RAM with an option to buy 64 more, an SSD drive for change. GPU was the most expensive part, something around $440.\n\nTotal cost: ~$2000.\n\nWhat would that money get me?\n\nI figured that if I train something at least 8 hours every day on average, which turned out to be true, I am looking at 56 hours per week or 2920 hours per year. So, if I get myself pay-per-hour server on AWS and run it for 2920 hours, anything that costs more than $0.685 will be more expensive than my own PC. I don’t include an electricity bill in the analysis, it’s a negligible fraction of that price (depending on where you live, your experience may vary, and also beware of power outages, they almost never happen where I live).\n\nAWS doesn’t offer any reasonably good GPU solution for $0.685/hr, even as a spot instance. So, that was a no-brainer.\n\nWas it worth it? Absolutely. Two grand for the vital piece of my education is nothing.\n\nDo you need it? If you are serious about studying DL, yes. Understanding an architecture or an algorithm and getting it to work are two very different stories, the only real way to acquire knowledge is to try things for yourself and analyze the results.\n\nIf you consider buying multiple cheap GPUs to learn how to work with them - don’t. If your framework supports distributed computation, it does everything in a painless way. If it doesn’t, this is not a task for a beginner, and generally a pain in the rearward.\n\nFor training modern architectures CPUs are not a substitute for a GPU in any way. I have a really damn good CPU and it would take weeks to train a network that I usually train overnight. A consumer-grade i5 (I don’t think that overpaying for i7 is a great idea) is even slower.\n\nFinally, some tips and lessons learned along the way:\n\nDon’t ever buy a gaming laptop, the very idea is ridiculous, both for gaming and for deep learning.\nYou can get a used Xeon CPU very cheap. Even better, you can get a decommissioned server, but this kind of shopping is for hardware-savvy.\nECC RAM is an overkill for this kind of workstation and you can save', 'result': {'fake': 0.0518, 'real': 0.9482}, 'status': 'success'}, {'text': 'some money on it.\nSmall laptop like Chromebook or Thinkpad connected via mosh to your home server is the best configuration.\n\nP.S.: Aaron Yip added some excellent links, check it out:\n\nThe $1700 great Deep Learning box: Assembly, setup and benchmarks\n\nSystem Build - Core i5-7600K 3.8GHz Quad-Core, GeForce GTX 1070 8GB SC... - PCPartPicker\n\nSystem Build - Core i5-7600K 3.8GHz Quad-Core, GeForce GTX 1070 8GB Mi... - PCPartPicker', 'result': {'fake': 0.7856, 'real': 0.2144}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982854, 'subscription': 0, 'content': 'Here’s my story and cost analysis that went into it.\n\nWhen I was starting my Master’s I was certain that no matter what kind of architectures I am going to work on, they will be heavy and memory-consuming, certainly not the job for a mobile card. GPUs from the university cluster were not the best option, because the machine time was shared and I couldn’t have as much as I wanted.\n\nSo, that left me with two options:\n\nBuy my own dev server.\nRent an equivalent AWS.\n\nSo, to make the decision, I needed to know:\n\nTotal cost of the server.\nProjected utilization of the server/AWS.\nTotal cost of an equivalent AWS unit calculated for, let’s say, a year.\n\nThe first part was simple, I chose GTX 1070 as a GPU (at the time 1080 and Titans were twice as expensive and not worth it), used 28-core Xeon for parallel CPU stuff, 64 GB RAM with an option to buy 64 more, an SSD drive for change. GPU was the most expensive part, something around $440.\n\nTotal cost: ~$2000.\n\nWhat would that money get me?\n\nI figured that if I train something at least 8 hours every day on average, which turned out to be true, I am looking at 56 hours per week or 2920 hours per year. So, if I get myself pay-per-hour server on AWS and run it for 2920 hours, anything that costs more than $0.685 will be more expensive than my own PC. I don’t include an electricity bill in the analysis, it’s a negligible fraction of that price (depending on where you live, your experience may vary, and also beware of power outages, they almost never happen where I live).\n\nAWS doesn’t offer any reasonably good GPU solution for $0.685/hr, even as a spot instance. So, that was a no-brainer.\n\nWas it worth it? Absolutely. Two grand for the vital piece of my education is nothing.\n\nDo you need it? If you are serious about studying DL, yes. Understanding an architecture or an algorithm and getting it to work are two very different stories, the only real way to acquire knowledge is to try things for yourself and analyze the results.\n\nIf you consider buying multiple cheap GPUs to learn how to work with them - don’t. If your framework supports distributed computation, it does everything in a painless way. If it doesn’t, this is not a task for a beginner, and generally a pain in the rearward.\n\nFor training modern architectures CPUs are not a substitute for a GPU in any way. I have a really damn good CPU and it would take weeks to train a network that I usually train overnight. A consumer-grade i5 (I don’t think that overpaying for i7 is a great idea) is even slower.\n\nFinally, some tips and lessons learned along the way:\n\nDon’t ever buy a gaming laptop, the very idea is ridiculous, both for gaming and for deep learning.\nYou can get a used Xeon CPU very cheap. Even better, you can get a decommissioned server, but this kind of shopping is for hardware-savvy.\nECC RAM is an overkill for this kind of workstation and you can save some money on it.\nSmall laptop like Chromebook or Thinkpad connected via mosh to your home server is the best configuration.\n\nP.S.: Aaron Yip added some excellent links, check it out:\n\nThe $1700 great Deep Learning box: Assembly, setup and benchmarks\n\nSystem Build - Core i5-7600K 3.8GHz Quad-Core, GeForce GTX 1070 8GB SC... - PCPartPicker\n\nSystem Build - Core i5-7600K 3.8GHz Quad-Core, GeForce GTX 1070 8GB Mi... - PCPartPicker', 'aiModelVersion': '1'}",0.97025
Huyen Nguyen,4y,What are some embarrassing statistics mistakes that people with fancy credentials often make?,"This has to be my favorite common mistake ever. Can you explain this “paradox”?

“The median weekly wage, adjusted for inflation, in the US has grown by just 0.9 per cent since 2000. But the median wage has actually fallen for high-school dropouts (by 7.9 per cent), high-school graduates (by 4.7 per cent), people with some college education (by 7.6 per cent) and people with at least one degree (by 1.2 per cent). In other words, every sub-group’s wage fell, even as the overall wage rose.”[1]

How is that possible? You would think if every sub-group fell, then shouldn’t the average (or median) be lower as well?

The answer is that the relative size of the groups changed greatly over those 13 years. The number of college graduates increased while the number of workers with a high school education or less dropped[2] (first plot below). People with a college degree earn significantly more than those without[3] (second plot below), thus pulling up the median wage of the total population.

Similarly, when you look at wages broken down by race and gender: Between 1980 and 2005, wages for all US workers increased by 3 per cent, but wages for every demographic subgroup increased by far more
. That result was because of the steady increase in participation in the labour force by women and people of colour, people who tend to earn less on average.

This is called Simpson’s Paradox, when the direction of a correlation disappears or reverses when the data is disaggregated into different subgroups.[4] Habib Fanny did a very similar detailed analysis on the gender wage gap.[5] It’s too complicated to explain here but his conclusion is when you break the data into smaller groups and account for all the different factors, the wage gap still exists but is much smaller than the 21% often reported in the media.

This is a very subtle problem that can lead to misguided policy when we are not careful. You might think great, we have more educated, better-paid people so it’s a good thing. Or you might think wages have declined for every educational level, it’s a bad thing. Both of these conclusions are correct but they will lead to very different policy priorities. Always be extra careful when you see aggregate trends because they might mask the full picture.

Footnotes

[1] The most misleading statistics of all, thanks to Simpson's Paradox
[2] Profile of the labor force by educational attainment
[3] https://fas.org/sgp/crs/misc/R45090.pdf
[4] Simpson's paradox - Wikipedia
[5] Habib Fanny's answer to Is there a gender gap?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/7fqdp564e09trwv3', 'title': 'What are some embarrassing statistics mistakes that people with fancy credentials often make?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': ""This has to be my favorite common mistake ever. Can you explain this “paradox”?\n\n“The median weekly wage, adjusted for inflation, in the US has grown by just 0.9 per cent since 2000. But the median wage has actually fallen for high-school dropouts (by 7.9 per cent), high-school graduates (by 4.7 per cent), people with some college education (by 7.6 per cent) and people with at least one degree (by 1.2 per cent). In other words, every sub-group’s wage fell, even as the overall wage rose.”[1]\n\nHow is that possible? You would think if every sub-group fell, then shouldn’t the average (or median) be lower as well?\n\nThe answer is that the relative size of the groups changed greatly over those 13 years. The number of college graduates increased while the number of workers with a high school education or less dropped[2] (first plot below). People with a college degree earn significantly more than those without[3] (second plot below), thus pulling up the median wage of the total population.\n\nSimilarly, when you look at wages broken down by race and gender: Between 1980 and 2005, wages for all US workers increased by 3 per cent, but wages for every demographic subgroup increased by far more\n. That result was because of the steady increase in participation in the labour force by women and people of colour, people who tend to earn less on average.\n\nThis is called Simpson’s Paradox, when the direction of a correlation disappears or reverses when the data is disaggregated into different subgroups.[4] Habib Fanny did a very similar detailed analysis on the gender wage gap.[5] It’s too complicated to explain here but his conclusion is when you break the data into smaller groups and account for all the different factors, the wage gap still exists but is much smaller than the 21% often reported in the media.\n\nThis is a very subtle problem that can lead to misguided policy when we are not careful. You might think great, we have more educated, better-paid people so it’s a good thing. Or you might think wages have declined for every educational level, it’s a bad thing. Both of these conclusions are correct but they will lead to very different policy priorities. Always be extra careful when you see aggregate trends because they might mask the full picture.\n\nFootnotes\n\n[1] The most misleading statistics of all, thanks to Simpson's Paradox\n[2] Profile of the labor force by educational attainment\n[3] https://fas.org/sgp/crs/misc/R45090.pdf\n[4] Simpson's paradox - Wikipedia\n[5] Habib Fanny's answer to Is there a gender gap?"", 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 5, 'credits': 1982849, 'subscription': 0, 'content': ""This has to be my favorite common mistake ever. Can you explain this “paradox”?\n\n“The median weekly wage, adjusted for inflation, in the US has grown by just 0.9 per cent since 2000. But the median wage has actually fallen for high-school dropouts (by 7.9 per cent), high-school graduates (by 4.7 per cent), people with some college education (by 7.6 per cent) and people with at least one degree (by 1.2 per cent). In other words, every sub-group’s wage fell, even as the overall wage rose.”[1]\n\nHow is that possible? You would think if every sub-group fell, then shouldn’t the average (or median) be lower as well?\n\nThe answer is that the relative size of the groups changed greatly over those 13 years. The number of college graduates increased while the number of workers with a high school education or less dropped[2] (first plot below). People with a college degree earn significantly more than those without[3] (second plot below), thus pulling up the median wage of the total population.\n\nSimilarly, when you look at wages broken down by race and gender: Between 1980 and 2005, wages for all US workers increased by 3 per cent, but wages for every demographic subgroup increased by far more\n. That result was because of the steady increase in participation in the labour force by women and people of colour, people who tend to earn less on average.\n\nThis is called Simpson’s Paradox, when the direction of a correlation disappears or reverses when the data is disaggregated into different subgroups.[4] Habib Fanny did a very similar detailed analysis on the gender wage gap.[5] It’s too complicated to explain here but his conclusion is when you break the data into smaller groups and account for all the different factors, the wage gap still exists but is much smaller than the 21% often reported in the media.\n\nThis is a very subtle problem that can lead to misguided policy when we are not careful. You might think great, we have more educated, better-paid people so it’s a good thing. Or you might think wages have declined for every educational level, it’s a bad thing. Both of these conclusions are correct but they will lead to very different policy priorities. Always be extra careful when you see aggregate trends because they might mask the full picture.\n\nFootnotes\n\n[1] The most misleading statistics of all, thanks to Simpson's Paradox\n[2] Profile of the labor force by educational attainment\n[3] https://fas.org/sgp/crs/misc/R45090.pdf\n[4] Simpson's paradox - Wikipedia\n[5] Habib Fanny's answer to Is there a gender gap?"", 'aiModelVersion': '1'}",0.9998
Aj Santosh,1y,Why is data science important?,"Data science aims to make experiences and figure out data by uniting domain ability from programming, arithmetic, and insights. When we consider why data science is turning out to be progressively significant, the appropriate response lies in how the worth of information is rising. Did you realize that Southwest Airlines had the option to save $100 million because of utilizing data? By lessening planes' landing area holding up time, they could drive an adjustment of how assets are used.

Why data science?

It is straightforward [if you are intrigued enough]. Sorting out information will diminish the detestations of vulnerability for associations. Data science is a quickly developing capacity, yet industry specialists say it is in its earliest stages. The objective of data science is to integrate subject knowledge from programming, mathematics, and statistics to produce insight for extracting meaningful information.

Significance of Data Science:

Data science assists organizations with diminishing danger. Beginning a computerized firm, conveying an incredible item, and getting ubiquity is becoming more straightforward due to further developed association, diminished handling costs, distributed storage, and simple admittance to appropriation channels. With the rising buzz around AI and machine insight, the business is moving in unique ways. Simply put, data science aids companies in reducing risk.

The power of data can be unleashed. Industries need data to make well-informed decisions. Data science transforms raw data into actionable information. As a result, many companies are turning to data science for assistance. When it comes to magic, a Data Scientist is a magician who knows how to work with data. Any data that a skilled Data Scientist comes across will be able to be mined for useful information.

The Data Science field has a lot of advantages:

It will advance your vocation with adaptability, opportunity, and choices which are three words that ring a bell while contemplating adaptability.
You can learn about the most broadly utilized data science instruments and how to apply hypothetical ideas to certifiable issues.
Keeps you informed with regards to the latest business patterns.
You Can Easily Demonstrate Your Expertise.

Now that you know what Data Science is and why Data Science is important, I will suggest you look for a reputable institute/course that will ensure that your data science path is smooth and lots of wonderful projects which provide you with the best and simplest experience and exposure. There are several data science training institutes that also provide domain specialization, which helps students master this field and get a lot of expertise.

How Domain Specialization can be helpful?

I strongly recommend that you enroll in an institute that provides specialized training and employment opportunities. You'd learn the skills you need for your chosen field of study at these institutions. With such a specialization, you will not only gain valuable experience but you may also be presented with numerous opportunities that will greatly enhance your abilities. Anyone who wants to improve their future or career, especially in data science, understands that there is a lot of competition out there today. It's possible to gain extensive knowledge with the help of domain specialization. Domain specialization and its significance are two things you should be familiar with.

So, which institution provides courses that focus on a certain domain?

[Note: After the most extreme perception in the field of DS, I've closed with the underneath establishments.]

Learnbay [specially intended for working professionals]

Learnbay has been the most well-known organization for offering data science courses in Bangalore. It is exceptionally created for working professionals. This establishment has dispatched an explicit Data Science space course, which procured huge advantages for working professionals.

Learnbay essentially centers around Domain Elective courses which is a major plus point:

It's one of the most popular and well-received features on the list. ' A wide range of industries rely on Learnbay's expertise in data science. Your field knowledge and data science skills can then be applied more effectively if you have a solid foundation of critical thinking skills. One of the main advantages of this Learnbay’s Advanced Data Science and AI Program is that it allows you to select a specific domain of study.

Disciplines in Domain Electives are as per the following:

Oil, Gas & Energy
Manufacturing
E-commerce and supply chain
Insurance and finance
Hospitality and haulage

For competitors holding over an extended period of involvement, I would strongly suggest the referenced course by Learnbay.

Advanced Data Science & AI Program

Duration for Weekday Batch: 7 months
Duration for Weekend Batches: 9 months

Some of the other features of Learnbay include:

Live Interactive sessions: To avoid losing interest in the training, Learnbay gives live sessions instead of pre-recorded sessions. As a result, it helps you stay focused while also boosting your interest in the topic. Students have the opportunity to learn in real-time in virtual classrooms and practice implementation in person at the center thanks to this.
Real-time and Capstone Projects: With the Advanced Data Science and AI Program, real-time and capstone-based learning experiences are also available. They develop Real-Time and Capstone Projects that are accurate, dependable, and serve as stepping stones under the leadership of a knowledgeable instructor. In addition, there are a number of student-led initiatives, including credit card fraud detection. At many project innovation centers in seven or more Indian cities, including Delhi, Pune, etc., experts from MNCs and MAANG assist with both online and offline project sessions.
IBM Certificates: It's not only the course completion certificate, Learnbay also offers the project certificate and the multiple skills certificate all accredited by IBM which is offered with the Advance Data Science and AI Program of Learnbay.

The above highlights appear to streak for me as for the cutthroat courses. Aside from this, the courses offer plentiful extents of learning with life expectancy admittance to the exceptional learning material and/or go for live classes.

Data Science Beginners: 2021 [Edureka]

In addition to the data science course, Edureka can assist students to learn python. In the event that you are a complete newcomer to this field, this institute contains a particular section just for you. These courses are taught by top-notch professors who can guide you through the data scientist's journey. Despite its numerous advantages, this institution does not have any domain specialty.

Final word:

Moreover, before going on a vacation in any subject, you should understand and have prior knowledge of that field. The equivalent applies if you are going for a vocation in Data Science. So assuming that you don't have the fundamental information, I recommend that you take a gander at the Data Science course at Learnbay. I've always found Learnbay to be an excellent resource for learning about data science. For many years, Learnbay has been regarded as one of the best educational institutions in the world.

I hope this information will be helpful to you.

Best of Luck","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/sdrbifc8unxwkm73', 'title': 'Why is data science important?', 'score': {'original': 3.3333333333333e-05, 'ai': 0.99996666666667}, 'blocks': [{'text': ""Data science aims to make experiences and figure out data by uniting domain ability from programming, arithmetic, and insights. When we consider why data science is turning out to be progressively significant, the appropriate response lies in how the worth of information is rising. Did you realize that Southwest Airlines had the option to save $100 million because of utilizing data? By lessening planes' landing area holding up time, they could drive an adjustment of how assets are used.\n\nWhy data science?\n\nIt is straightforward [if you are intrigued enough]. Sorting out information will diminish the detestations of vulnerability for associations. Data science is a quickly developing capacity, yet industry specialists say it is in its earliest stages. The objective of data science is to integrate subject knowledge from programming, mathematics, and statistics to produce insight for extracting meaningful information.\n\nSignificance of Data Science:\n\nData science assists organizations with diminishing danger. Beginning a computerized firm, conveying an incredible item, and getting ubiquity is becoming more straightforward due to further developed association, diminished handling costs, distributed storage, and simple admittance to appropriation channels. With the rising buzz around AI and machine insight, the business is moving in unique ways. Simply put, data science aids companies in reducing risk.\n\nThe power of data can be unleashed. Industries need data to make well-informed decisions. Data science transforms raw data into actionable information. As a result, many companies are turning to data science for assistance. When it comes to magic, a Data Scientist is a magician who knows how to work with data. Any data that a skilled Data Scientist comes across will be able to be mined for useful information.\n\nThe Data Science field has a lot of advantages:\n\nIt will advance your vocation with adaptability, opportunity, and choices which are three words that ring a bell while contemplating adaptability.\nYou can learn about the most broadly utilized data science instruments and how to apply hypothetical ideas to certifiable issues.\nKeeps you informed with regards to the latest business patterns.\nYou Can Easily Demonstrate Your Expertise.\n\nNow that you know what Data Science is and why Data Science is important, I will suggest you look for a reputable institute/course that will ensure that your data science path is smooth and lots of wonderful projects which provide you with the best and simplest experience and exposure. There are several data science training institutes that also provide domain specialization, which helps students master this field and get a lot of expertise.\n\nHow Domain Specialization can be helpful?\n\nI strongly recommend that you enroll in an institute that provides specialized training and employment opportunities. You'd learn the skills you need for your chosen field of study at these institutions. With such a specialization, you will not only gain valuable experience but you may also be presented with numerous opportunities that will greatly enhance your abilities. Anyone who wants to improve their future or career, especially in data science, understands that there is a lot of competition out there today. It's possible to gain extensive knowledge with the help of domain specialization. Domain specialization and its significance are two things you should"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""be familiar with.\n\nSo, which institution provides courses that focus on a certain domain?\n\n[Note: After the most extreme perception in the field of DS, I've closed with the underneath establishments.]\n\nLearnbay [specially intended for working professionals]\n\nLearnbay has been the most well-known organization for offering data science courses in Bangalore. It is exceptionally created for working professionals. This establishment has dispatched an explicit Data Science space course, which procured huge advantages for working professionals.\n\nLearnbay essentially centers around Domain Elective courses which is a major plus point:\n\nIt's one of the most popular and well-received features on the list. ' A wide range of industries rely on Learnbay's expertise in data science. Your field knowledge and data science skills can then be applied more effectively if you have a solid foundation of critical thinking skills. One of the main advantages of this Learnbay’s Advanced Data Science and AI Program is that it allows you to select a specific domain of study.\n\nDisciplines in Domain Electives are as per the following:\n\nOil, Gas & Energy\nManufacturing\nE-commerce and supply chain\nInsurance and finance\nHospitality and haulage\n\nFor competitors holding over an extended period of involvement, I would strongly suggest the referenced course by Learnbay.\n\nAdvanced Data Science & AI Program\n\nDuration for Weekday Batch: 7 months\nDuration for Weekend Batches: 9 months\n\nSome of the other features of Learnbay include:\n\nLive Interactive sessions: To avoid losing interest in the training, Learnbay gives live sessions instead of pre-recorded sessions. As a result, it helps you stay focused while also boosting your interest in the topic. Students have the opportunity to learn in real-time in virtual classrooms and practice implementation in person at the center thanks to this.\nReal-time and Capstone Projects: With the Advanced Data Science and AI Program, real-time and capstone-based learning experiences are also available. They develop Real-Time and Capstone Projects that are accurate, dependable, and serve as stepping stones under the leadership of a knowledgeable instructor. In addition, there are a number of student-led initiatives, including credit card fraud detection. At many project innovation centers in seven or more Indian cities, including Delhi, Pune, etc., experts from MNCs and MAANG assist with both online and offline project sessions.\nIBM Certificates: It's not only the course completion certificate, Learnbay also offers the project certificate and the multiple skills certificate all accredited by IBM which is offered with the Advance Data Science and AI Program of Learnbay.\n\nThe above highlights appear to streak for me as for the cutthroat courses. Aside from this, the courses offer plentiful extents of learning with life expectancy admittance to the exceptional learning material and/or go for live classes.\n\nData Science Beginners: 2021 [Edureka]\n\nIn addition to the data science course, Edureka can assist students to learn python. In the event that you are a complete newcomer to this field, this institute contains a particular section just for you. These courses are taught by top-notch professors who can guide you through the data scientist's journey. Despite its numerous advantages, this institution does not have any domain specialty.\n\nFinal word:\n\nMoreover, before going on a vacation in any subject, you should understand and have prior knowledge of that field. The equivalent applies if"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""you are going for a vocation in Data Science. So assuming that you don't have the fundamental information, I recommend that you take a gander at the Data Science course at Learnbay. I've always found Learnbay to be an excellent resource for learning about data science. For many years, Learnbay has been regarded as one of the best educational institutions in the world.\n\nI hope this information will be helpful to you.\n\nBest of Luck"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 12, 'credits': 1982837, 'subscription': 0, 'content': ""Data science aims to make experiences and figure out data by uniting domain ability from programming, arithmetic, and insights. When we consider why data science is turning out to be progressively significant, the appropriate response lies in how the worth of information is rising. Did you realize that Southwest Airlines had the option to save $100 million because of utilizing data? By lessening planes' landing area holding up time, they could drive an adjustment of how assets are used.\n\nWhy data science?\n\nIt is straightforward [if you are intrigued enough]. Sorting out information will diminish the detestations of vulnerability for associations. Data science is a quickly developing capacity, yet industry specialists say it is in its earliest stages. The objective of data science is to integrate subject knowledge from programming, mathematics, and statistics to produce insight for extracting meaningful information.\n\nSignificance of Data Science:\n\nData science assists organizations with diminishing danger. Beginning a computerized firm, conveying an incredible item, and getting ubiquity is becoming more straightforward due to further developed association, diminished handling costs, distributed storage, and simple admittance to appropriation channels. With the rising buzz around AI and machine insight, the business is moving in unique ways. Simply put, data science aids companies in reducing risk.\n\nThe power of data can be unleashed. Industries need data to make well-informed decisions. Data science transforms raw data into actionable information. As a result, many companies are turning to data science for assistance. When it comes to magic, a Data Scientist is a magician who knows how to work with data. Any data that a skilled Data Scientist comes across will be able to be mined for useful information.\n\nThe Data Science field has a lot of advantages:\n\nIt will advance your vocation with adaptability, opportunity, and choices which are three words that ring a bell while contemplating adaptability.\nYou can learn about the most broadly utilized data science instruments and how to apply hypothetical ideas to certifiable issues.\nKeeps you informed with regards to the latest business patterns.\nYou Can Easily Demonstrate Your Expertise.\n\nNow that you know what Data Science is and why Data Science is important, I will suggest you look for a reputable institute/course that will ensure that your data science path is smooth and lots of wonderful projects which provide you with the best and simplest experience and exposure. There are several data science training institutes that also provide domain specialization, which helps students master this field and get a lot of expertise.\n\nHow Domain Specialization can be helpful?\n\nI strongly recommend that you enroll in an institute that provides specialized training and employment opportunities. You'd learn the skills you need for your chosen field of study at these institutions. With such a specialization, you will not only gain valuable experience but you may also be presented with numerous opportunities that will greatly enhance your abilities. Anyone who wants to improve their future or career, especially in data science, understands that there is a lot of competition out there today. It's possible to gain extensive knowledge with the help of domain specialization. Domain specialization and its significance are two things you should be familiar with.\n\nSo, which institution provides courses that focus on a certain domain?\n\n[Note: After the most extreme perception in the field of DS, I've closed with the underneath establishments.]\n\nLearnbay [specially intended for working professionals]\n\nLearnbay has been the most well-known organization for offering data science courses in Bangalore. It is exceptionally created for working professionals. This establishment has dispatched an explicit Data Science space course, which procured huge advantages for working professionals.\n\nLearnbay essentially centers around Domain Elective courses which is a major plus point:\n\nIt's one of the most popular and well-received features on the list. ' A wide range of industries rely on Learnbay's expertise in data science. Your field knowledge and data science skills can then be applied more effectively if you have a solid foundation of critical thinking skills. One of the main advantages of this Learnbay’s Advanced Data Science and AI Program is that it allows you to select a specific domain of study.\n\nDisciplines in Domain Electives are as per the following:\n\nOil, Gas & Energy\nManufacturing\nE-commerce and supply chain\nInsurance and finance\nHospitality and haulage\n\nFor competitors holding over an extended period of involvement, I would strongly suggest the referenced course by Learnbay.\n\nAdvanced Data Science & AI Program\n\nDuration for Weekday Batch: 7 months\nDuration for Weekend Batches: 9 months\n\nSome of the other features of Learnbay include:\n\nLive Interactive sessions: To avoid losing interest in the training, Learnbay gives live sessions instead of pre-recorded sessions. As a result, it helps you stay focused while also boosting your interest in the topic. Students have the opportunity to learn in real-time in virtual classrooms and practice implementation in person at the center thanks to this.\nReal-time and Capstone Projects: With the Advanced Data Science and AI Program, real-time and capstone-based learning experiences are also available. They develop Real-Time and Capstone Projects that are accurate, dependable, and serve as stepping stones under the leadership of a knowledgeable instructor. In addition, there are a number of student-led initiatives, including credit card fraud detection. At many project innovation centers in seven or more Indian cities, including Delhi, Pune, etc., experts from MNCs and MAANG assist with both online and offline project sessions.\nIBM Certificates: It's not only the course completion certificate, Learnbay also offers the project certificate and the multiple skills certificate all accredited by IBM which is offered with the Advance Data Science and AI Program of Learnbay.\n\nThe above highlights appear to streak for me as for the cutthroat courses. Aside from this, the courses offer plentiful extents of learning with life expectancy admittance to the exceptional learning material and/or go for live classes.\n\nData Science Beginners: 2021 [Edureka]\n\nIn addition to the data science course, Edureka can assist students to learn python. In the event that you are a complete newcomer to this field, this institute contains a particular section just for you. These courses are taught by top-notch professors who can guide you through the data scientist's journey. Despite its numerous advantages, this institution does not have any domain specialty.\n\nFinal word:\n\nMoreover, before going on a vacation in any subject, you should understand and have prior knowledge of that field. The equivalent applies if you are going for a vocation in Data Science. So assuming that you don't have the fundamental information, I recommend that you take a gander at the Data Science course at Learnbay. I've always found Learnbay to be an excellent resource for learning about data science. For many years, Learnbay has been regarded as one of the best educational institutions in the world.\n\nI hope this information will be helpful to you.\n\nBest of Luck"", 'aiModelVersion': '1'}",3.3333333333333e-05
Dr Jo,Updated 9mo,Is there an AI system that translates natural language to programming code?,"Let’s see how Chat GPT performs (as of May 2023). This is a fairly long post. If you don’t like code, you can skip to the punchline at the end. Otherwise …

I’ll take a not-entirely-trivial example. The challenge here is to see how you interpret the “natural language” and then anticipate Chat GPT’s response. Are you ready?

The National Health Index (NHI)

Me: Please write me a JavaScript routine that validates the old-style New Zealand National Health Index Number (NHI). The old NHI is a seven-character text string. It comprises three alphabetic characters followed by three single digit numerics, and then a final checksum. The alphabetic characters are uppercase A to Z, excluding I and O. If the NHI contains I or O, then it is invalid.

The checksum is calculated as follows.

Calculate a check value for each of the first six characters. If the character is numeric, then the value is simply that number from zero to nine. If the character is alphabetic, then the numeric value ranges from 1 to 24, depending on the position of the character in the sequence A to Z with I and O excluded. For example A=1, B=2, ... J=9, ... P=14, ... up to Z=24.
Multiply the first (leftmost) check value by seven, the next by six and so on until the sixth, which you can work out is multiplied by two. You now have six products.
Add up all of the values from the preceding step.
Take the above sum and modulo 11. Call this the precheck.
If the precheck is 11, the NHI is invalid.
∗
∗
Otherwise, subtract the precheck from 11.
If the result is 10, subtract ten.

Finally, we have our checksum. If this checksum is equal to the numeric value of the final character digit of the NHI, the NHI is valid.

Return 1 if the NHI is valid, otherwise 0.

Chat GPT’s response…

function validateNHI(nhi) { 
  // Remove any whitespace and convert to uppercase 
  nhi = nhi.replace(/\s/g, '').toUpperCase(); 
 
  // Check if NHI length is valid 
  if (nhi.length !== 7) { 
    return 0; 
  } 
 
  // Check for invalid characters I or O 
  if (nhi.includes('I') || nhi.includes('O')) { 
    return 0; 
  } 
 
  // Calculate check values 
  const checkValues = { 
    A: 1, B: 2, C: 3, D: 4, E: 5, F: 6, G: 7, H: 8, 
    J: 9, K: 10, L: 11, M: 12, N: 13, P: 14, Q: 15, 
    R: 16, S: 17, T: 18, U: 19, V: 20, W: 21, X: 22, 
    Y: 23, Z: 24 
  }; 
 
  let precheck = 0; 
 
  for (let i = 0; i < 6; i++) { 
    const character = nhi[i]; 
    let value; 
 
    if (isNaN(parseInt(character, 10))) { 
      value = checkValues[character]; 
    } else { 
      value = parseInt(character, 10); 
    } 
 
    precheck += value * (7 - i); 
  } 
 
  precheck %= 11; 
 
  if (precheck === 11) { 
    return 0; 
  } 
 
  const checksum = precheck === 10 ? 0 : 11 - precheck; 
 
  const lastDigit = parseInt(nhi[6], 10); 
 
  // Check if the calculated checksum matches the last digit of NHI 
  return checksum === lastDigit ? 1 : 0; 
} 

Now that code could be a bit cleaner, and there are some added checks you might wish to put in, but it’s done a half-reasonable job, given my language. It works! If you’re familiar with JavaScript, you may wish to play a bit. Try e.g. validateNHI('ZZZ0016') and then vary the check digit.

With careful phrasing, I even managed to persuade Chat GPT to accommodate the new NHI format.

Here’s the thing

I was fairly precise in specifying the “natural language”. Effectively, I translated a working algorithm into careful English—and then Chat GPT translated it back again :)

It turns out that Chat GPT already knows a fair bit about the New Zealand NHI—and can still get horribly confused about the details. Trying to get the bot to work out the NHI check routine de novo, I discovered two things:

Even if asked to ignore past efforts, it incorporates them.
It does the ‘stochastic parrot’ thing[1] to code too—making up hallucinatory stuff to fill in the gaps. Even on things that used to work.

Take this fragment, which was built up from a long progression, starting with asking it to provide an example of a valid NHI (it got this wrong), extracting a correction, and then ending with a routine that purports to validate old and new NHIs…

function validateNHI(nhi) { 
  if (!nhi || typeof nhi !== 'string' || nhi.length !== 7) { 
    return 0; // Invalid format 
  } 
 
  const checkValues = {A:1,B:2,C:3,D:4,E:5,F:6,G:7,H:8,J:9,K:10,L:11,M:12,N:13,P:14,Q:15,R:16,S:17,T:18,U:19,V:20,W:21,X:22,Y:23}; 
  const firstThree = nhi.substring(0,3); 
  const lastTwo = nhi.substring(5,7); 
  const isOldFormat = !isNaN(parseInt(lastTwo)); 
 
  if (isOldFormat) { 
    const firstSix = nhi.substring(0,6); 
    const checksum = parseInt(lastTwo); 
    let sum = 0; 
    for (let i = 0; i < 6; i++) { 
      const c = firstSix.charAt(i); 
      const val = isNaN(parseInt(c)) ? checkValues[c] : parseInt(c); 
      sum += val * (i + 1); 
    } 
    const remainder = sum % 11; 
    const expected = remainder === 0 ? 0 : 11 - remainder; 
    return expected === checksum ? 1 : 0; 
  } else ... 

It comes up with a cute trick to check for the new format—but then calmly ‘forgets’ that a key part of the checksum is to multiply by (7-i). Instead, it uses (i+1). Oh Dear!

Now there’s no doubt in my mind that these abilities will progressively improve.

And there’s the problem!

In asking a computer to do something using ‘natural language’ we have to accommodate the fact that natural language can be imprecise. Currently, how a bot performs will depend on our awareness of this need. To avoid cruft and bad code, we compensate through careful use of language.

As the bot gets smarter, we will tend to get lazier and more imprecise. The bot will take on more of the work, interpreting what we “mean”. We will likely also get lazier and less competent at parsing its output.

Can you see the problem?

My 2c, Dr Jo.

Cartoon is from XKCD 1696: AI Research
.

∗
∗
 Actually, this step is incorrect. It should declare the NHI invalid if the ‘precheck’ is zero (or the value subtracted from 11 is 11). My bad. Enormous thanks to André Braga for pointing this out. (And if Chat GPT were really smart, it would have/could have/should have warned me that anything mod 11 can’t possibly be 11, so as it stands, the test is daft).

Footnotes

[1] Dr Jo's answer to What is a ""stochastic parrot""?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/89ft3vndpbqszli2', 'title': 'Is there an AI system that translates natural language to programming code?', 'score': {'original': 0.83766666666667, 'ai': 0.16233333333333}, 'blocks': [{'text': ""Let’s see how Chat GPT performs (as of May 2023). This is a fairly long post. If you don’t like code, you can skip to the punchline at the end. Otherwise …\n\nI’ll take a not-entirely-trivial example. The challenge here is to see how you interpret the “natural language” and then anticipate Chat GPT’s response. Are you ready?\n\nThe National Health Index (NHI)\n\nMe: Please write me a JavaScript routine that validates the old-style New Zealand National Health Index Number (NHI). The old NHI is a seven-character text string. It comprises three alphabetic characters followed by three single digit numerics, and then a final checksum. The alphabetic characters are uppercase A to Z, excluding I and O. If the NHI contains I or O, then it is invalid.\n\nThe checksum is calculated as follows.\n\nCalculate a check value for each of the first six characters. If the character is numeric, then the value is simply that number from zero to nine. If the character is alphabetic, then the numeric value ranges from 1 to 24, depending on the position of the character in the sequence A to Z with I and O excluded. For example A=1, B=2, ... J=9, ... P=14, ... up to Z=24.\nMultiply the first (leftmost) check value by seven, the next by six and so on until the sixth, which you can work out is multiplied by two. You now have six products.\nAdd up all of the values from the preceding step.\nTake the above sum and modulo 11. Call this the precheck.\nIf the precheck is 11, the NHI is invalid.\n∗\n∗\nOtherwise, subtract the precheck from 11.\nIf the result is 10, subtract ten.\n\nFinally, we have our checksum. If this checksum is equal to the numeric value of the final character digit of the NHI, the NHI is valid.\n\nReturn 1 if the NHI is valid, otherwise 0.\n\nChat GPT’s response…\n\nfunction validateNHI(nhi) {\xa0\n  // Remove any whitespace and convert to uppercase\xa0\n  nhi = nhi.replace(/\\s/g, '').toUpperCase();\xa0\n\xa0\n  // Check if NHI length is valid\xa0\n  if (nhi.length !== 7) {\xa0\n    return 0;\xa0\n  }\xa0\n\xa0\n  // Check for invalid characters I or O\xa0\n  if (nhi.includes('I') || nhi.includes('O')) {\xa0\n    return 0;\xa0\n  }\xa0\n\xa0\n  // Calculate check values\xa0\n  const checkValues = {\xa0\n    A: 1, B: 2, C: 3, D: 4, E: 5, F: 6, G: 7, H: 8,\xa0\n    J: 9, K: 10, L: 11, M: 12, N: 13, P: 14, Q: 15,\xa0\n    R: 16, S: 17, T: 18, U: 19, V: 20, W: 21, X: 22,\xa0\n    Y: 23, Z: 24\xa0\n  };\xa0\n\xa0\n  let precheck = 0;\xa0\n\xa0\n  for (let i = 0; i < 6; i++) {\xa0\n    const character = nhi[i];\xa0\n    let value;\xa0\n\xa0\n    if (isNaN(parseInt(character, 10))) {\xa0\n      value = checkValues[character];\xa0\n    } else {\xa0\n      value = parseInt(character, 10);\xa0\n    }\xa0\n\xa0\n    precheck += value * (7 - i);\xa0\n  }\xa0\n\xa0\n  precheck %="", 'result': {'fake': 0.1172, 'real': 0.8828}, 'status': 'success'}, {'text': ""11;\xa0\n\xa0\n  if (precheck === 11) {\xa0\n    return 0;\xa0\n  }\xa0\n\xa0\n  const checksum = precheck === 10 ? 0 : 11 - precheck;\xa0\n\xa0\n  const lastDigit = parseInt(nhi[6], 10);\xa0\n\xa0\n  // Check if the calculated checksum matches the last digit of NHI\xa0\n  return checksum === lastDigit ? 1 : 0;\xa0\n}\xa0\n\nNow that code could be a bit cleaner, and there are some added checks you might wish to put in, but it’s done a half-reasonable job, given my language. It works! If you’re familiar with JavaScript, you may wish to play a bit. Try e.g. validateNHI('ZZZ0016') and then vary the check digit.\n\nWith careful phrasing, I even managed to persuade Chat GPT to accommodate the new NHI format.\n\nHere’s the thing\n\nI was fairly precise in specifying the “natural language”. Effectively, I translated a working algorithm into careful English—and then Chat GPT translated it back again :)\n\nIt turns out that Chat GPT already knows a fair bit about the New Zealand NHI—and can still get horribly confused about the details. Trying to get the bot to work out the NHI check routine de novo, I discovered two things:\n\nEven if asked to ignore past efforts, it incorporates them.\nIt does the ‘stochastic parrot’ thing[1] to code too—making up hallucinatory stuff to fill in the gaps. Even on things that used to work.\n\nTake this fragment, which was built up from a long progression, starting with asking it to provide an example of a valid NHI (it got this wrong), extracting a correction, and then ending with a routine that purports to validate old and new NHIs…\n\nfunction validateNHI(nhi) {\xa0\n  if (!nhi || typeof nhi !== 'string' || nhi.length !== 7) {\xa0\n    return 0; // Invalid format\xa0\n  }\xa0\n\xa0\n  const checkValues = {A:1,B:2,C:3,D:4,E:5,F:6,G:7,H:8,J:9,K:10,L:11,M:12,N:13,P:14,Q:15,R:16,S:17,T:18,U:19,V:20,W:21,X:22,Y:23};\xa0\n  const firstThree = nhi.substring(0,3);\xa0\n  const lastTwo = nhi.substring(5,7);\xa0\n  const isOldFormat = !isNaN(parseInt(lastTwo));\xa0\n\xa0\n  if (isOldFormat) {\xa0\n    const firstSix = nhi.substring(0,6);\xa0\n    const checksum = parseInt(lastTwo);\xa0\n    let sum = 0;\xa0\n    for (let i = 0; i < 6; i++) {\xa0\n      const c = firstSix.charAt(i);\xa0\n      const val = isNaN(parseInt(c)) ? checkValues[c] : parseInt(c);\xa0\n      sum += val * (i + 1);\xa0\n    }\xa0\n    const remainder = sum % 11;\xa0\n    const expected = remainder === 0 ? 0 : 11 - remainder;\xa0\n    return expected === checksum ? 1 : 0;\xa0\n  } else ...\xa0\n\nIt comes up with a cute trick to check for the new format—but then calmly ‘forgets’ that a key part of the checksum is to multiply by (7-i). Instead, it uses (i+1). Oh Dear!\n\nNow there’s no doubt in my mind that these abilities will progressively improve.\n\nAnd there’s the problem!\n\nIn asking a computer to do something using ‘natural language’ we have to accommodate the fact that natural language can be imprecise. Currently, how a bot performs will depend on our awareness of this need. To avoid cruft and bad code,"", 'result': {'fake': 0.0653, 'real': 0.9347}, 'status': 'success'}, {'text': 'we compensate through careful use of language.\n\nAs the bot gets smarter, we will tend to get lazier and more imprecise. The bot will take on more of the work, interpreting what we “mean”. We will likely also get lazier and less competent at parsing its output.\n\nCan you see the problem?\n\nMy 2c, Dr Jo.\n\nCartoon is from XKCD 1696: AI Research\n.\n\n∗\n∗\n Actually, this step is incorrect. It should declare the NHI invalid if the ‘precheck’ is zero (or the value subtracted from 11 is 11). My bad. Enormous thanks to André Braga for pointing this out. (And if Chat GPT were really smart, it would have/could have/should have warned me that anything mod 11 can’t possibly be 11, so as it stands, the test is daft).\n\nFootnotes\n\n[1] Dr Jo\'s answer to What is a ""stochastic parrot""?', 'result': {'fake': 0.0924, 'real': 0.9076}, 'status': 'success'}], 'credits_used': 10, 'credits': 1982827, 'subscription': 0, 'content': 'Let’s see how Chat GPT performs (as of May 2023). This is a fairly long post. If you don’t like code, you can skip to the punchline at the end. Otherwise …\n\nI’ll take a not-entirely-trivial example. The challenge here is to see how you interpret the “natural language” and then anticipate Chat GPT’s response. Are you ready?\n\nThe National Health Index (NHI)\n\nMe: Please write me a JavaScript routine that validates the old-style New Zealand National Health Index Number (NHI). The old NHI is a seven-character text string. It comprises three alphabetic characters followed by three single digit numerics, and then a final checksum. The alphabetic characters are uppercase A to Z, excluding I and O. If the NHI contains I or O, then it is invalid.\n\nThe checksum is calculated as follows.\n\nCalculate a check value for each of the first six characters. If the character is numeric, then the value is simply that number from zero to nine. If the character is alphabetic, then the numeric value ranges from 1 to 24, depending on the position of the character in the sequence A to Z with I and O excluded. For example A=1, B=2, ... J=9, ... P=14, ... up to Z=24.\nMultiply the first (leftmost) check value by seven, the next by six and so on until the sixth, which you can work out is multiplied by two. You now have six products.\nAdd up all of the values from the preceding step.\nTake the above sum and modulo 11. Call this the precheck.\nIf the precheck is 11, the NHI is invalid.\n∗\n∗\nOtherwise, subtract the precheck from 11.\nIf the result is 10, subtract ten.\n\nFinally, we have our checksum. If this checksum is equal to the numeric value of the final character digit of the NHI, the NHI is valid.\n\nReturn 1 if the NHI is valid, otherwise 0.\n\nChat GPT’s response…\n\nfunction validateNHI(nhi) {\xa0\n  // Remove any whitespace and convert to uppercase\xa0\n  nhi = nhi.replace(/\\s/g, \'\').toUpperCase();\xa0\n\xa0\n  // Check if NHI length is valid\xa0\n  if (nhi.length !== 7) {\xa0\n    return 0;\xa0\n  }\xa0\n\xa0\n  // Check for invalid characters I or O\xa0\n  if (nhi.includes(\'I\') || nhi.includes(\'O\')) {\xa0\n    return 0;\xa0\n  }\xa0\n\xa0\n  // Calculate check values\xa0\n  const checkValues = {\xa0\n    A: 1, B: 2, C: 3, D: 4, E: 5, F: 6, G: 7, H: 8,\xa0\n    J: 9, K: 10, L: 11, M: 12, N: 13, P: 14, Q: 15,\xa0\n    R: 16, S: 17, T: 18, U: 19, V: 20, W: 21, X: 22,\xa0\n    Y: 23, Z: 24\xa0\n  };\xa0\n\xa0\n  let precheck = 0;\xa0\n\xa0\n  for (let i = 0; i < 6; i++) {\xa0\n    const character = nhi[i];\xa0\n    let value;\xa0\n\xa0\n    if (isNaN(parseInt(character, 10))) {\xa0\n      value = checkValues[character];\xa0\n    } else {\xa0\n      value = parseInt(character, 10);\xa0\n    }\xa0\n\xa0\n    precheck += value * (7 - i);\xa0\n  }\xa0\n\xa0\n  precheck %= 11;\xa0\n\xa0\n  if (precheck === 11) {\xa0\n    return 0;\xa0\n  }\xa0\n\xa0\n  const checksum = precheck === 10 ? 0 : 11 - precheck;\xa0\n\xa0\n  const lastDigit = parseInt(nhi[6], 10);\xa0\n\xa0\n  // Check if the calculated checksum matches the last digit of NHI\xa0\n  return checksum === lastDigit ? 1 : 0;\xa0\n}\xa0\n\nNow that code could be a bit cleaner, and there are some added checks you might wish to put in, but it’s done a half-reasonable job, given my language. It works! If you’re familiar with JavaScript, you may wish to play a bit. Try e.g. validateNHI(\'ZZZ0016\') and then vary the check digit.\n\nWith careful phrasing, I even managed to persuade Chat GPT to accommodate the new NHI format.\n\nHere’s the thing\n\nI was fairly precise in specifying the “natural language”. Effectively, I translated a working algorithm into careful English—and then Chat GPT translated it back again :)\n\nIt turns out that Chat GPT already knows a fair bit about the New Zealand NHI—and can still get horribly confused about the details. Trying to get the bot to work out the NHI check routine de novo, I discovered two things:\n\nEven if asked to ignore past efforts, it incorporates them.\nIt does the ‘stochastic parrot’ thing[1] to code too—making up hallucinatory stuff to fill in the gaps. Even on things that used to work.\n\nTake this fragment, which was built up from a long progression, starting with asking it to provide an example of a valid NHI (it got this wrong), extracting a correction, and then ending with a routine that purports to validate old and new NHIs…\n\nfunction validateNHI(nhi) {\xa0\n  if (!nhi || typeof nhi !== \'string\' || nhi.length !== 7) {\xa0\n    return 0; // Invalid format\xa0\n  }\xa0\n\xa0\n  const checkValues = {A:1,B:2,C:3,D:4,E:5,F:6,G:7,H:8,J:9,K:10,L:11,M:12,N:13,P:14,Q:15,R:16,S:17,T:18,U:19,V:20,W:21,X:22,Y:23};\xa0\n  const firstThree = nhi.substring(0,3);\xa0\n  const lastTwo = nhi.substring(5,7);\xa0\n  const isOldFormat = !isNaN(parseInt(lastTwo));\xa0\n\xa0\n  if (isOldFormat) {\xa0\n    const firstSix = nhi.substring(0,6);\xa0\n    const checksum = parseInt(lastTwo);\xa0\n    let sum = 0;\xa0\n    for (let i = 0; i < 6; i++) {\xa0\n      const c = firstSix.charAt(i);\xa0\n      const val = isNaN(parseInt(c)) ? checkValues[c] : parseInt(c);\xa0\n      sum += val * (i + 1);\xa0\n    }\xa0\n    const remainder = sum % 11;\xa0\n    const expected = remainder === 0 ? 0 : 11 - remainder;\xa0\n    return expected === checksum ? 1 : 0;\xa0\n  } else ...\xa0\n\nIt comes up with a cute trick to check for the new format—but then calmly ‘forgets’ that a key part of the checksum is to multiply by (7-i). Instead, it uses (i+1). Oh Dear!\n\nNow there’s no doubt in my mind that these abilities will progressively improve.\n\nAnd there’s the problem!\n\nIn asking a computer to do something using ‘natural language’ we have to accommodate the fact that natural language can be imprecise. Currently, how a bot performs will depend on our awareness of this need. To avoid cruft and bad code, we compensate through careful use of language.\n\nAs the bot gets smarter, we will tend to get lazier and more imprecise. The bot will take on more of the work, interpreting what we “mean”. We will likely also get lazier and less competent at parsing its output.\n\nCan you see the problem?\n\nMy 2c, Dr Jo.\n\nCartoon is from XKCD 1696: AI Research\n.\n\n∗\n∗\n Actually, this step is incorrect. It should declare the NHI invalid if the ‘precheck’ is zero (or the value subtracted from 11 is 11). My bad. Enormous thanks to André Braga for pointing this out. (And if Chat GPT were really smart, it would have/could have/should have warned me that anything mod 11 can’t possibly be 11, so as it stands, the test is daft).\n\nFootnotes\n\n[1] Dr Jo\'s answer to What is a ""stochastic parrot""?', 'aiModelVersion': '1'}",0.83766666666667
Jinu Abrahim,Oct 26,What is the best online course to learn AI?,"When it comes to learning AI online, there are numerous reputable courses available that can provide you with a solid foundation in this rapidly evolving field. However, defining the ""best"" course can depend on your specific needs, preferences, and prior knowledge.

Here are a few highly regarded online AI courses to consider:

""Machine Learning"" by Andrew Ng on Coursera: This course is often considered the gold standard for beginners. Andrew Ng, a renowned AI expert, covers the fundamentals of machine learning, including linear regression, neural networks, and support vector machines.
""Deep Learning Specialization"" by deep learning AI on Coursera: Created by Andrew Ng's team, this specialization is designed for individuals looking to delve deeper into the world of deep learning. The specialization consists of five courses, covering topics such as neural networks, convolutional networks, recurrent neural networks, and more.

3. ""Artificial Intelligence A-Z: Learn How to Build an AI"" on Udemy: This comprehensive course provides a hands-on approach to AI, focusing on practical applications. It covers a wide range of topics including natural language processing, image recognition, and reinforcement learning.

4. ""Intro to Artificial Intelligence"" by Sebastian Thrun and Peter Norvig on Udacity: This course offers a comprehensive introduction to AI, covering topics such as problem-solving, search algorithms, knowledge representation, and machine learning. It is suitable for beginners and provides a solid foundation in AI concepts.

5. ""CS50's Introduction to Artificial Intelligence with Python"" on edX: This course is offered by Harvard University and provides an introduction to AI using the Python programming language. It covers topics such as search algorithms, knowledge representation, and machine learning.

Note:

Professionals should consider looking up institutes that offer online master's degree courses for several reasons.

Online master's degree courses, especially in fields like data science, offer a bigger value compared to normal data science courses for a variety of reasons.
These programs offer a deep level of knowledge and expertise in their chosen field.
Online master's degree programs offered by reputable institutes are often recognized and respected within the industry.
A Master’s degree raises a professional's credibility and reputation, opening doors to better job prospects and higher salaries.

Hence, it’s important to take a look at the courses offering master’s degrees online:

But, which factors to consider when taking courses?

Look for credibility and recognition

Learnbay’s Program in Data Science and AI issues an IBM Certificate that holds significant value in various countries. This certificate is recognized and respected by educational institutions, employers, and immigration authorities in these regions.

What’s the best bet?

You receive an IBM project experience certificate.
You qualify for a work visa and permanent employment abroad.

Alternatively: The ExcelR: ""Data Science Certification Training"" course does not include any qualified certifications but basic level ones.

2. Look for specialized skills and gain expertise

The Learnbay’s Program in Data Science and AI ensures that they acquire in-depth knowledge and skills relevant to that particular industry. This specialization enables students to understand the nuances and challenges of working with data in their chosen field and to develop domain-specific expertise in various domains.

Alternatively: ExcelR does not include any domain-specific courses.

3. Look for tremendous career growth opportunities

The Learnbay’s Program in Data Science and AI guarantees interview calls for students who successfully complete the program. This assurance provides students with an added level of confidence to showcase their skills to potential employers. This guarantee helps students remain focused and determined in their job search throughout the globe, knowing that they own a Woolf certificate and ease of immigration facility.

You can reach out to centers in the cities like Bangalore, Delhi, Pune, and Hyderabad for offline assistance.

Alternatively: ExcelR provides job search and resume-building services.

4. Look for flexibility and convenience

The Learnbay’s Program in Data Science and AI offers the flexibility to access online sessions during their free time, such as in the evenings or on weekends.

Other advantages:

Additionally, they can take advantage of the option to attend in-person classes or workshop centers located in Bangalore, Delhi, Pune, and Hyderabad, allowing them to have face-to-face interactions with experts in the field.

Alternatively: ExcelR only allows students to access offline sessions-based learning.

Remember, the ""best"" course ultimately depends on your individual goals, learning style, and prior knowledge. Consider your preferences, the course syllabus, instructor expertise, and student reviews when making your decision. Additionally, exploring multiple resources and combining different courses or tutorials can provide a well-rounded understanding of AI.

All the best.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/gqbsf6yclkt091ez', 'title': 'What is the best online course to learn AI?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': 'When it comes to learning AI online, there are numerous reputable courses available that can provide you with a solid foundation in this rapidly evolving field. However, defining the ""best"" course can depend on your specific needs, preferences, and prior knowledge.\n\nHere are a few highly regarded online AI courses to consider:\n\n""Machine Learning"" by Andrew Ng on Coursera: This course is often considered the gold standard for beginners. Andrew Ng, a renowned AI expert, covers the fundamentals of machine learning, including linear regression, neural networks, and support vector machines.\n""Deep Learning Specialization"" by deep learning AI on Coursera: Created by Andrew Ng\'s team, this specialization is designed for individuals looking to delve deeper into the world of deep learning. The specialization consists of five courses, covering topics such as neural networks, convolutional networks, recurrent neural networks, and more.\n\n3. ""Artificial Intelligence A-Z: Learn How to Build an AI"" on Udemy: This comprehensive course provides a hands-on approach to AI, focusing on practical applications. It covers a wide range of topics including natural language processing, image recognition, and reinforcement learning.\n\n4. ""Intro to Artificial Intelligence"" by Sebastian Thrun and Peter Norvig on Udacity: This course offers a comprehensive introduction to AI, covering topics such as problem-solving, search algorithms, knowledge representation, and machine learning. It is suitable for beginners and provides a solid foundation in AI concepts.\n\n5. ""CS50\'s Introduction to Artificial Intelligence with Python"" on edX: This course is offered by Harvard University and provides an introduction to AI using the Python programming language. It covers topics such as search algorithms, knowledge representation, and machine learning.\n\nNote:\n\nProfessionals should consider looking up institutes that offer online master\'s degree courses for several reasons.\n\nOnline master\'s degree courses, especially in fields like data science, offer a bigger value compared to normal data science courses for a variety of reasons.\nThese programs offer a deep level of knowledge and expertise in their chosen field.\nOnline master\'s degree programs offered by reputable institutes are often recognized and respected within the industry.\nA Master’s degree raises a professional\'s credibility and reputation, opening doors to better job prospects and higher salaries.\n\nHence, it’s important to take a look at the courses offering master’s degrees online:\n\nBut, which factors to consider when taking courses?\n\nLook for credibility and recognition\n\nLearnbay’s Program in Data Science and AI issues an IBM Certificate that holds significant value in various countries. This certificate is recognized and respected by educational institutions, employers, and immigration authorities in these regions.\n\nWhat’s the best bet?\n\nYou receive an IBM project experience certificate.\nYou qualify for a work visa and permanent employment abroad.\n\nAlternatively: The ExcelR: ""Data Science Certification Training"" course does not include any qualified certifications but basic level ones.\n\n2. Look for specialized skills and gain expertise\n\nThe Learnbay’s Program in Data Science and AI ensures that they acquire in-depth knowledge and skills relevant to that particular industry. This specialization enables students to understand the nuances and challenges of working with data in their chosen field and to develop domain-specific expertise in various domains.\n\nAlternatively: ExcelR does not include any domain-specific courses.\n\n3. Look for tremendous career growth opportunities\n\nThe Learnbay’s Program in Data Science and AI guarantees interview calls for', 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': 'students who successfully complete the program. This assurance provides students with an added level of confidence to showcase their skills to potential employers. This guarantee helps students remain focused and determined in their job search throughout the globe, knowing that they own a Woolf certificate and ease of immigration facility.\n\nYou can reach out to centers in the cities like Bangalore, Delhi, Pune, and Hyderabad for offline assistance.\n\nAlternatively: ExcelR provides job search and resume-building services.\n\n4. Look for flexibility and convenience\n\nThe Learnbay’s Program in Data Science and AI offers the flexibility to access online sessions during their free time, such as in the evenings or on weekends.\n\nOther advantages:\n\nAdditionally, they can take advantage of the option to attend in-person classes or workshop centers located in Bangalore, Delhi, Pune, and Hyderabad, allowing them to have face-to-face interactions with experts in the field.\n\nAlternatively: ExcelR only allows students to access offline sessions-based learning.\n\nRemember, the ""best"" course ultimately depends on your individual goals, learning style, and prior knowledge. Consider your preferences, the course syllabus, instructor expertise, and student reviews when making your decision. Additionally, exploring multiple resources and combining different courses or tutorials can provide a well-rounded understanding of AI.\n\nAll the best.', 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 8, 'credits': 1982819, 'subscription': 0, 'content': 'When it comes to learning AI online, there are numerous reputable courses available that can provide you with a solid foundation in this rapidly evolving field. However, defining the ""best"" course can depend on your specific needs, preferences, and prior knowledge.\n\nHere are a few highly regarded online AI courses to consider:\n\n""Machine Learning"" by Andrew Ng on Coursera: This course is often considered the gold standard for beginners. Andrew Ng, a renowned AI expert, covers the fundamentals of machine learning, including linear regression, neural networks, and support vector machines.\n""Deep Learning Specialization"" by deep learning AI on Coursera: Created by Andrew Ng\'s team, this specialization is designed for individuals looking to delve deeper into the world of deep learning. The specialization consists of five courses, covering topics such as neural networks, convolutional networks, recurrent neural networks, and more.\n\n3. ""Artificial Intelligence A-Z: Learn How to Build an AI"" on Udemy: This comprehensive course provides a hands-on approach to AI, focusing on practical applications. It covers a wide range of topics including natural language processing, image recognition, and reinforcement learning.\n\n4. ""Intro to Artificial Intelligence"" by Sebastian Thrun and Peter Norvig on Udacity: This course offers a comprehensive introduction to AI, covering topics such as problem-solving, search algorithms, knowledge representation, and machine learning. It is suitable for beginners and provides a solid foundation in AI concepts.\n\n5. ""CS50\'s Introduction to Artificial Intelligence with Python"" on edX: This course is offered by Harvard University and provides an introduction to AI using the Python programming language. It covers topics such as search algorithms, knowledge representation, and machine learning.\n\nNote:\n\nProfessionals should consider looking up institutes that offer online master\'s degree courses for several reasons.\n\nOnline master\'s degree courses, especially in fields like data science, offer a bigger value compared to normal data science courses for a variety of reasons.\nThese programs offer a deep level of knowledge and expertise in their chosen field.\nOnline master\'s degree programs offered by reputable institutes are often recognized and respected within the industry.\nA Master’s degree raises a professional\'s credibility and reputation, opening doors to better job prospects and higher salaries.\n\nHence, it’s important to take a look at the courses offering master’s degrees online:\n\nBut, which factors to consider when taking courses?\n\nLook for credibility and recognition\n\nLearnbay’s Program in Data Science and AI issues an IBM Certificate that holds significant value in various countries. This certificate is recognized and respected by educational institutions, employers, and immigration authorities in these regions.\n\nWhat’s the best bet?\n\nYou receive an IBM project experience certificate.\nYou qualify for a work visa and permanent employment abroad.\n\nAlternatively: The ExcelR: ""Data Science Certification Training"" course does not include any qualified certifications but basic level ones.\n\n2. Look for specialized skills and gain expertise\n\nThe Learnbay’s Program in Data Science and AI ensures that they acquire in-depth knowledge and skills relevant to that particular industry. This specialization enables students to understand the nuances and challenges of working with data in their chosen field and to develop domain-specific expertise in various domains.\n\nAlternatively: ExcelR does not include any domain-specific courses.\n\n3. Look for tremendous career growth opportunities\n\nThe Learnbay’s Program in Data Science and AI guarantees interview calls for students who successfully complete the program. This assurance provides students with an added level of confidence to showcase their skills to potential employers. This guarantee helps students remain focused and determined in their job search throughout the globe, knowing that they own a Woolf certificate and ease of immigration facility.\n\nYou can reach out to centers in the cities like Bangalore, Delhi, Pune, and Hyderabad for offline assistance.\n\nAlternatively: ExcelR provides job search and resume-building services.\n\n4. Look for flexibility and convenience\n\nThe Learnbay’s Program in Data Science and AI offers the flexibility to access online sessions during their free time, such as in the evenings or on weekends.\n\nOther advantages:\n\nAdditionally, they can take advantage of the option to attend in-person classes or workshop centers located in Bangalore, Delhi, Pune, and Hyderabad, allowing them to have face-to-face interactions with experts in the field.\n\nAlternatively: ExcelR only allows students to access offline sessions-based learning.\n\nRemember, the ""best"" course ultimately depends on your individual goals, learning style, and prior knowledge. Consider your preferences, the course syllabus, instructor expertise, and student reviews when making your decision. Additionally, exploring multiple resources and combining different courses or tutorials can provide a well-rounded understanding of AI.\n\nAll the best.', 'aiModelVersion': '1'}",0.0
Steve Baker,2y,What can a human still do better and faster than any machine learning solution?,"My “go to” example here is this - it’s not a trick question:

——

Imagine you’re in a garage…imagine that there is a long shelf held to the wall by a couple of brackets…imagine cans of left-over paint on the shelf…now imagine that the screws holding one bracket to the wall suddenly fail.

Now - quickly - what is the approximate diameter of the resulting mess? Gut feel?

——

I’d bet that every one of us would come up with an answer somewhere around 2 meters in diameter (6 feet)…nobody really believes 4 meters (12 feet)…nobody thinks it would only be 1 meter (3 feet) - and we’d come up with that answer in less than 1 second of thinking time.

After a little more thought we might ask if small paint droplets count and push up our estimate if they do.

I’m pretty sure that this kind of thing has never happened to any of us - so we’re not using practical experience. My 5 year old grandkid got about the same answer I did…so it’s not like decades of experience help.

How do we do that?

For starters - the question is HORRIBLY under-specified. How high is the shelf? How long is the shelf? Where along the length of the shelf are the two brackets? How many paint cans are on the shelf? How big are they? How full are they? Will the shelf actually detach from the wall? Will it move at all? How fast will it move? What is the coefficient of friction of the cans against the shelf? How hard are the lids held on? What is the viscosity of old paint? How porous is the garage floor? Will the puddle be elliptical or circular?

Despite all of those unknowns - despite having no practical experience of this actually happening - we all come up with an answer that’s probably within a factor of two of the right answer.

You could write a computer program to solve that exact problem using physics and hydrodynamics and a lot of constants relating to the problem that a human would have to ...

Access this answer and support the author as a Quora+ subscriber
Access all answers reserved by 
Steve Baker
 for Quora+ subscribers
Access exclusive answers from thousands more participating creators in Quora+
Browse ad‑free and support creators
Start free trial
Learn more","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/36ngaskd9wbuyfqx', 'title': 'What can a human still do better and faster than any machine learning solution?', 'score': {'original': 0.9999, 'ai': 0.0001}, 'blocks': [{'text': 'My “go to” example here is this - it’s not a trick question:\n\n——\n\nImagine you’re in a garage…imagine that there is a long shelf held to the wall by a couple of brackets…imagine cans of left-over paint on the shelf…now imagine that the screws holding one bracket to the wall suddenly fail.\n\nNow - quickly - what is the approximate diameter of the resulting mess? Gut feel?\n\n——\n\nI’d bet that every one of us would come up with an answer somewhere around 2 meters in diameter (6 feet)…nobody really believes 4 meters (12 feet)…nobody thinks it would only be 1 meter (3 feet) - and we’d come up with that answer in less than 1 second of thinking time.\n\nAfter a little more thought we might ask if small paint droplets count and push up our estimate if they do.\n\nI’m pretty sure that this kind of thing has never happened to any of us - so we’re not using practical experience. My 5 year old grandkid got about the same answer I did…so it’s not like decades of experience help.\n\nHow do we do that?\n\nFor starters - the question is HORRIBLY under-specified. How high is the shelf? How long is the shelf? Where along the length of the shelf are the two brackets? How many paint cans are on the shelf? How big are they? How full are they? Will the shelf actually detach from the wall? Will it move at all? How fast will it move? What is the coefficient of friction of the cans against the shelf? How hard are the lids held on? What is the viscosity of old paint? How porous is the garage floor? Will the puddle be elliptical or circular?\n\nDespite all of those unknowns - despite having no practical experience of this actually happening - we all come up with an answer that’s probably within a factor of two of the right answer.\n\nYou could write a computer program to solve that exact problem using physics and hydrodynamics and a lot of constants relating to the problem that a human would have to ...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nSteve Baker\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}], 'credits_used': 4, 'credits': 1982815, 'subscription': 0, 'content': 'My “go to” example here is this - it’s not a trick question:\n\n——\n\nImagine you’re in a garage…imagine that there is a long shelf held to the wall by a couple of brackets…imagine cans of left-over paint on the shelf…now imagine that the screws holding one bracket to the wall suddenly fail.\n\nNow - quickly - what is the approximate diameter of the resulting mess? Gut feel?\n\n——\n\nI’d bet that every one of us would come up with an answer somewhere around 2 meters in diameter (6 feet)…nobody really believes 4 meters (12 feet)…nobody thinks it would only be 1 meter (3 feet) - and we’d come up with that answer in less than 1 second of thinking time.\n\nAfter a little more thought we might ask if small paint droplets count and push up our estimate if they do.\n\nI’m pretty sure that this kind of thing has never happened to any of us - so we’re not using practical experience. My 5 year old grandkid got about the same answer I did…so it’s not like decades of experience help.\n\nHow do we do that?\n\nFor starters - the question is HORRIBLY under-specified. How high is the shelf? How long is the shelf? Where along the length of the shelf are the two brackets? How many paint cans are on the shelf? How big are they? How full are they? Will the shelf actually detach from the wall? Will it move at all? How fast will it move? What is the coefficient of friction of the cans against the shelf? How hard are the lids held on? What is the viscosity of old paint? How porous is the garage floor? Will the puddle be elliptical or circular?\n\nDespite all of those unknowns - despite having no practical experience of this actually happening - we all come up with an answer that’s probably within a factor of two of the right answer.\n\nYou could write a computer program to solve that exact problem using physics and hydrodynamics and a lot of constants relating to the problem that a human would have to ...\n\nAccess this answer and support the author as a Quora+ subscriber\nAccess all answers reserved by \nSteve Baker\n for Quora+ subscribers\nAccess exclusive answers from thousands more participating creators in Quora+\nBrowse ad‑free and support creators\nStart free trial\nLearn more', 'aiModelVersion': '1'}",0.9999
Sridhar Mahadevan,5y,What are some of the best machine learning papers ever published?,"It’s always a fun exercise to synthesize 30+ years of research experience in a field, like ML, into compiling some sort of “Best Hits” list. As in every scientific field, the most influential papers can be catalogued in different ways. For example, one can look at objective criteria, such as the papers with the maximum number of citations in some subfield, like deep learning (terryum/awesome-deep-learning-papers
). One can also look at papers that were singularly influential in some way. Finally, one can simply choose papers that were helpful in clarifying some important idea, or did an excellent job of providing a tutorial of an important area. Each of the papers I have chosen below has been hugely influential not only in the ML field itself, but has also had major impact outside ML. Each has been cited tens of thousands of times, but beyond the citation count, they have each changed the field in some significant way.

I previously answered a related question on the top 20 papers published in AI, so feel free to peruse through my earlier top 20 list, which was somewhat broader, and included papers in neuroscience and ML.

Which are the top 20 papers related to AI (both machine learning & symbolic), so that I can cover the basics and choose a niche for my research?

OK, on to my newer list of the best ML papers ever published….

To begin, as they say, at the beginning, in terms of its sheer impact, it is hard to top the 1967 paper by Gold on “Language Identification in the Limit”. (see https://ac.els-cdn.com/S0019995867911655/1-s2.0-S0019995867911655-main.pdf?_tid=bc318a60-0ae5-4de6-868d-4fd2e58897c0&acdnat=1548034543_3ad4aa11e2fd780dc76a63e05c25442d
). This paper is as important to machine learning as the famous results by Godel on the incompleteness of logic, or the classic results by Church and Turing on the limitations of computability. Gold analyzed an extremely simple, and yet powerful, model of learning, whereby a teacher communicates with a learner, and assumes nothing in terms of the learner’s capabilities. The major result was that the set of context-free languages is not learnable in Gold’s model (which came to be known as inductive inference). This was an earth-shattering result 50 years ago, and influenced entire fields of inquiry, like linguistics. How is that children as young as 2 or 3 learn an unknown language (entirely unrelated to their ethnicity, as Indian children can learn Japanese just as easily as Japanese children can learn Hindi, if they grew up in Japan or India, respectively)? Gold’s paper is still relevant today, in the data-obsessed world of the 21st century, if only to remind us of the inherent limitations in the power of learning. If you aspire to be a data scientist, if you don't understand Gold’s theorem, it’s like being a physicist and not knowing the conservation of energy. It’s THAT important!
17 years after Gold’s classic paper, in the proverbial “Orwellian” year of 1984, Leslie Valiant published a classic paper that led to his winning the Turing award — computer science’s highest honor — several decades later. Valiant’s paper entitled “A Theory of the Learnable” picked up the thread where Gold left off, and introduced two crucial refinements of Gold’s model. In Valiant’s model, subsequently dubbed “Probably Approximately Correct Learning” (or PAC learning), a learner is only required to converge to an approximation of the desired concept, and is also allowed to fail entirely with some (small) probability. These two refinements made Valiant’s model much more realistic — a robot trained on the concept of “trees” in Florida is unlikely to do well in New England, as it might get confused by fall foliage. Valiant’s model allowed for the set of all possible trees to be sampled by some unknown but unchanging distribution, and required the teacher to be “fair”, so that the same training distribution used to teach the robot must be used in testing as well. Valiant’s model is becoming extremely important in the self-driving autonomous car age of today, as car manufacturers are using elaborate simulations of the real world, e.g., a detailed model of Phoenix, to train their cars using reinforcement learning and deep learning. Such a vehicle trained in a simulation of Phoenix would do rather poorly in the topsy turvy streets of San Francisco. PAC learning was combined with the brilliant work of Vapnik and Chervonenkis decades earlier in Russia on statistical learning, and has become one of the cornerstones of modern ML. Valiant’s paper became an instant classic, published not in some august ML journal, but in the breezy trade pamphlet known as the “Communications of the ACM”. It’s probably the most influential scholarly paper in recent decades to be published in CACM (https://people.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/ValiantLearnable.pdf
)
The earliest experimental work on machine learning was by Arthur Samuel on a program that learned to play the game of checkers by purely self play, and it was an astonishing feat, done in the late 1950s on a vacuum tube IBM 701 computer (see picture below). It’s hard to match Samuel’s audacity, since computers were hardly easy to program, as there was no computer display terminals, no modern programming languages, everything had to be coded in assembly, and all one had was some blinking lights. Even today, Samuel’s achievement is hard to match, and it’s arguable that Samuel’s achievement single handedly created the modern experimental field of machine learning. Samuel also invented the term “machine learning” and contributed towards one of the most influential ideas in the field, a method called temporal-difference learning, now a bedrock of the field of reinforcement learning (and used with much effect by Deep Mind in its Atari video game playing demos). Samuel’s paper from 1959, published six decades ago, remains the most influential experimental paper ever published in ML (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.2254&rep=rep1&type=pdf
)

4. Valiant’s PAC model influenced the development of many algorithms in machine learning, none was more influential than the work by Robert Schapire on “weak learning” that led to the sensational success of the ensemble method known as “boosting”. The fundamental breakthrough in boosting was in showing that instead of building one complex classifier, one could instead build something really simple, like “stumps” in decision trees, that were individually just better than random, but cumulatively could be “boosted” into an ensemble classifier of arbitrary accuracy. Boosting took the world of statistics by storm, and eminent statisticians like Leo Breiman and Trevor Hastie sung its praises. Breiman called boosting a surefire way of turning “a sow’s ear into a silk purse”. Boosting remains as popular as it was when it was first introduced several decades ago, and can be used with any form of machine learning, and practically any method (neural nets, decision trees, linear regression, SVMs, …). The original paper by Schapire remains a classic on “The Strength of Weak Learnability” (http://rob.schapire.net/papers/strengthofweak.pdf
).

5. Two singular events occurred when I was a graduate student, studying ML, in the 1980s. One was the publication of Valiant’s paper in CACM in 1984. The other was the appearance of the two volume book, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, by Rumelhart et al. in 1986. This book introduced the modern field of neural networks (now termed deep learning). The breakthrough technique, of course, was back propagation, introduced in a chapter in this book and also published as a paper in Nature (Learning representations by back-propagating errors
). Backpropagation is completely implausible as an explanation of how the brain learns, since there is no evidence from neuroscience that the brain actually computes error gradients by backpropagation — this would be a formidable undertaking given that we have 100 billion neurons and upwards of 10^{12} connections among neurons. But, despite the implausibility of backpropagation, it has become the workhorse of modern deep learning, and stochastic gradient descent seems destined to be around for a lot longer as there are no obvious replacements for it yet. Recent scholarship has uncovered that the original ideas underlying backpropagation were known far earlier than the 1986 Rumelhart paper. No one has been more vocal about this issue than Jurgen Schmidhuber, one of the inventors of LSTM models, which are widely used in modeling sequential problems in ML. Schmidhuber as an entire page discussing this issue, which is worth going over (Who Invented Backpropagation?
). It is not uncommon in science for issues of priority to gain controversy (for over 150 years, students of evolution have been battling over whether Darwin or Wallace should get primary credit, or joint credit, for the discovery). The 2 volume PDP book is one of the true landmarks of ML, and created the “connectionism” movement, which is essentially what deep learning arose from.

6. Another landmark publication in 1986 was Ross Quinlan’s paper on “Induction of decision trees” (http://hunch.net/~coms-4771/quinlan.pdf
). Decision trees made a sharp contrast with neural networks, and they complemented each other extremely well. Even today, despite all the hoopla you hear about deep learning, it may surprise many to know that decision trees (and their ensemble counterpart, random forests) remain even more widely used in the Bay Area tech industry than deep learning. The reasons should not be surprising. Decision tree technology is mature, and the resulting model is easily explainable (unlike deep learning neural nets, which are notoriously hard to understand). Quinlan championed decision trees in ML, whereas Breiman, Friedman, Olshen, and Stone popularized CART (classification and regression trees) in statistics. Both approaches merged later, and led to one of the most enduring approaches in machine learning. Decision trees are widely used in practice, from modeling consumer behavior in web analytics to credit card application processing to gauging stock market strategies and medical diagnosis. Unlike neural nets and deep learning, there is a rich theory underlying decision trees, and the book on Statistical Learning by the Stanford trio Hastie, Tibshirani, and Friedman contains an excellent overview of decision trees and random forests. Wonder of wonders, this book is a free download — put this on your tablet or smartphone NOW, and read it forever (https://web.stanford.edu/~hastie/Papers/ESLII.pdf
).

7. One way to distinguish statistics from machine learning, at least historically, is dimension. ML folks are fond of problems in very high-dimensional spaces, where it is hard to use traditional statistical methods. In particular, for most of the history of statistics, the number of observations (or training examples) greatly exceeded the number of dimensions (e.g., take the classic work on least-squares fit by Gauss on Tycho Brahe and Kepler’s data of the motion of planets). This so-called “small n, large p” regime causes a fundamental issue of overfitting. How best to handle the fact that the number of parameters greatly exceeds the number of training examples? This controversy continues to plague deep learning models today, and Geoff Hinton himself has commented on why he considers this a non-issue because he personally does not consider overfitting to be a serious problem. In this fascinating talk, cleverly titled “Brains, Sex, and Machine Learning”, Geoff Hinton argues why overfitting is not a problem for the brain). However, most statisticians and ML’ers consider overfitting to be a serious problem. In a landmark paper called “Support Vector Networks”, Vapnik and colleagues introduced a revolutionary idea of building machine learning models using the sophisticated tools of convex optimization, whereby sparsity could be guaranteed by exploiting the dual form of ML models (Support-vector networks
). This idea of moving to the dual formulation has since become one of the most widely used “tricks” in machine learning, and SVMs popularized the use of convex optimization in machine learning.

8. One of the most intriguing problems in machine learning is understanding the space that data lie in. The most common assumption is that the data lie in Euclidean space, because the most common data format is a matrix of rows, where each row is a vector of values of the features. However, from a variety of areas, evidence has been building that most real-world datasets lie not in Euclidean space, but rather on a curved surface in high dimensions, what mathematicians call a “manifold” (not to be confused with engine parts!). Manifolds are a rich topic of study in math, where they have been explored for over a 100 years in fields like differential geometry. Einstein’s major breakthrough was showing that time and space are inextricably linked into a four-dimensional space-time manifold, a revolutionary concept that is still being tested today 100 years after Einstein’s breakthrough paper on general relativity. C.R. Rao, India’s “living Gold” of statistics, wrote a brilliant paper in the 1950s showing that space of probability distributions lies on a Riemannian manifold, a manifold that has the property that at each point, the space of tangents (or derivatives) defines a vector space with a dot product. The first sign of the 21st century in ML was the appearance of two classic papers on “manifold learning”, which led to techniques like local linear embedding (LLE) and ISOMAP, both published in the same issue of Science in 2000. However, the landmark paper in manifold learning was by Belkin and the (late, and very much lamented) Niyogi. Partha Niyogi, an ML genius at the University of Chicago, tragically died of an illness at the height of his powers, as did Sam Roweis, inventor of LLE. Both their contributions, however, live on, and have endured. In particular, the paper “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation” (http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf
) was a true classic, bringing into ML the formal power of differential geometric thinking. The Laplacian has been called “the most beautiful object in all of mathematics and physics” (Nelson, Tensor Analysis), and it has been studied by mathematicians and physicists for centuries. Every major equation in physics, e.g. Maxwell’s equation, features the Laplacian. Belkin and Niyogi showed how the discrete graph Laplacian plays an equally important role in nonlinear dimensionality reduction and representation discovery. In the era of social networks and web data, learning from graphs is hugely important, and the graph Laplacian is the single most important concept in this field.

9. Game theoretic ideas have long been part of AI and fields like economics, but only recently they have begun to enter the mainstream of machine learning, and their popularity in deep learning is largely due to the runaway success of the work of Ian Goodfellow and colleagues in GAN models, or generative adversarial networks (https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
). Few recent papers have had such an impact in the field of machine learning and beyond, and GANs have introduced the idea of using game theory as a way to build generative models. There are hundreds of variants of GANs, and one reason for their wildfire popularity is that there are many ways to extend the original concept. At this stage, GANs are too new to be entirely sure of their long-lasting impact, but whatever becomes of this line of research, the original work on GANs will remain a classic in the field. The original GAN model is plagued with instabilities and it is quite tricky to train a GAN. Recent variants, like Wasserstein GANs, claim to be better in some ways, but it is safe to say that there is no one approach that seems to dominate all the others. Part of the problem here is that there is no clear metric to test GAN models, except to look at the pretty pictures they generate. It is astonishing to see the faces of “fake celebrities”” in this recent work by nVidia (All of these faces are fake celebrities spawned by AI
), and in this era of “fake news”, one worries what the eventual fallout from GAN related work will be (can we ever trust an image or a video again, as telling the “truth”?). These quibbles aside, GANs are a true breakthrough in the field, the one clear step beyond the work in neural nets in the 1980s and earlier.

10. Ach, my last selection. As the saying goes, “when you love what you are doing, it’s not work”. Spending 30+ years in ML has not felt like work, but like absolutely a fun romp through crazy ideas. In the beginning, no one, least of all me, thought this would become a full-time job, but heck, it’s not just a full time job, but one that pays well (Artificial Intelligence Salaries: Paychecks Heading Skyward
). Gee…what should I choose as my last “top 10” paper of ML of all time. If you search for the most influential papers on the web, most of the links take you to a list of papers on deep learning. This is unfortunate, since ML as a field is sooo much bigger than DL, but that's what happens when you are “in a paradigm”. It gets difficult to “think outside the box”. One of the limitations of all the papers discussed above is that they largely focus on learning statistical correlations or building statistical generative models. What is truly remarkable about human learning, even in young children, is that we learn “how the world works”, that is, we learn to distinguish causes from correlations. How we do this is a bit of a mystery, since correlations are symmetric, so it’s quite challenging to go from correlations (like “thunder” and “lightning” are obviously correlated) to causation (“lightning” causes “thunder”, not the other way around!). One of my predictions for the future of ML is that causal approaches to ML are going to becoming far more popular than they have been to date. Unfortunately, while there are an excellent number of books and papers on causal models (e.g., by Rubin, Pearl and others), there’s not an instant classic paper on causal machine learning, one that will set the ML world on fire. Perhaps that's the reason why causal models haven't become much more popular (yet!). There’s a lot of work in psychology, e.g. by the Berkeley cognitive scientist Alison Gopnik, arguing that causality is very much at the core of how children learn. There’s also much evidence that what matters about perception is the concept introduced by Gibson called “affordances” (i.e., animals and humans perceive the world in terms of what possible actions can be done in each situation, which is intimately tied to our notion of causality and to our desire for “controlling” the world). So, I’m going to end my list by choosing a survey article from cognitive science on the importance of causal models for human learning (http://eccl.mit.edu/papers/gopnikglymoursobeletal.pdf
). This paper argues that children pay close attention to causal effects in the world. So far, learning machines mostly work by associative methods (e.g., the vast majority of deep learning models learn by correlation). The next breakthrough in ML is likely to be a stellar paper that shows why causal learning is more important than learning by correlation. Such a paper has yet to be written. Perhaps, dear Quora reader, you will take this as a challenge to develop a breakthrough causal learning system, and show the rest of us why causality is crucial to our intelligence. Judea Pearl is fond of quoting the ancient Greek philosopher, Democritus, who said “I would rather discover one true cause than gain the kingdom of Persia” (Democritus - Wikipedia
).

Well, there you have it, my selection of the top 10 most influential papers of all time in machine learning (except for the last one, which is kind of a placeholder for a paper that is yet to be written!). If you have had the patience to read through this whole answer, congratulations! I hope you have learned something of the rich history of the ML field. I began in 1982, innocently pursuing the “crazy” idea of building a learning machine, hugely ignorant of almost every paper above, but gripped by a passion that was inexplicable. It has driven the rest of my career, leading me to explore many fields and work with psychologists, biologists, mathematicians, engineers, physicists, chemists, astronomers, and physicians. Everyone collects and analyzes data, and machine learning can be used anywhere (and, to quote a phrase, “there lies the rub”, because one gets carried away into thinking of ML as a universal tool, the “hammer” you use because every problem becomes a “nail”). But, as long as you remember the wisdom of the above papers, and treat the “hammer” with caution, recognizing that often one wants to use other tools besides hammers!

May the “ML Force” be with you!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/m6e3x1fbd89ai7pv', 'title': 'What are some of the best machine learning papers ever published?', 'score': {'original': 0.3653, 'ai': 0.6347}, 'blocks': [{'text': ""It’s always a fun exercise to synthesize 30+ years of research experience in a field, like ML, into compiling some sort of “Best Hits” list. As in every scientific field, the most influential papers can be catalogued in different ways. For example, one can look at objective criteria, such as the papers with the maximum number of citations in some subfield, like deep learning (terryum/awesome-deep-learning-papers\n). One can also look at papers that were singularly influential in some way. Finally, one can simply choose papers that were helpful in clarifying some important idea, or did an excellent job of providing a tutorial of an important area. Each of the papers I have chosen below has been hugely influential not only in the ML field itself, but has also had major impact outside ML. Each has been cited tens of thousands of times, but beyond the citation count, they have each changed the field in some significant way.\n\nI previously answered a related question on the top 20 papers published in AI, so feel free to peruse through my earlier top 20 list, which was somewhat broader, and included papers in neuroscience and ML.\n\nWhich are the top 20 papers related to AI (both machine learning & symbolic), so that I can cover the basics and choose a niche for my research?\n\nOK, on to my newer list of the best ML papers ever published….\n\nTo begin, as they say, at the beginning, in terms of its sheer impact, it is hard to top the 1967 paper by Gold on “Language Identification in the Limit”. (see https://ac.els-cdn.com/S0019995867911655/1-s2.0-S0019995867911655-main.pdf?_tid=bc318a60-0ae5-4de6-868d-4fd2e58897c0&acdnat=1548034543_3ad4aa11e2fd780dc76a63e05c25442d\n). This paper is as important to machine learning as the famous results by Godel on the incompleteness of logic, or the classic results by Church and Turing on the limitations of computability. Gold analyzed an extremely simple, and yet powerful, model of learning, whereby a teacher communicates with a learner, and assumes nothing in terms of the learner’s capabilities. The major result was that the set of context-free languages is not learnable in Gold’s model (which came to be known as inductive inference). This was an earth-shattering result 50 years ago, and influenced entire fields of inquiry, like linguistics. How is that children as young as 2 or 3 learn an unknown language (entirely unrelated to their ethnicity, as Indian children can learn Japanese just as easily as Japanese children can learn Hindi, if they grew up in Japan or India, respectively)? Gold’s paper is still relevant today, in the data-obsessed world of the 21st century, if only to remind us of the inherent limitations in the power of learning. If you aspire to be a data scientist, if you don't understand Gold’s theorem, it’s like being a physicist and not knowing the conservation of energy. It’s THAT important!\n17 years after Gold’s classic paper, in the proverbial “Orwellian” year of 1984, Leslie Valiant published a classic paper that led to his winning the Turing award — computer science’s highest honor — several decades later. Valiant’s paper entitled “A Theory of the Learnable” picked up the thread where Gold left off, and"", 'result': {'fake': 0.0036, 'real': 0.9964}, 'status': 'success'}, {'text': 'introduced two crucial refinements of Gold’s model. In Valiant’s model, subsequently dubbed “Probably Approximately Correct Learning” (or PAC learning), a learner is only required to converge to an approximation of the desired concept, and is also allowed to fail entirely with some (small) probability. These two refinements made Valiant’s model much more realistic — a robot trained on the concept of “trees” in Florida is unlikely to do well in New England, as it might get confused by fall foliage. Valiant’s model allowed for the set of all possible trees to be sampled by some unknown but unchanging distribution, and required the teacher to be “fair”, so that the same training distribution used to teach the robot must be used in testing as well. Valiant’s model is becoming extremely important in the self-driving autonomous car age of today, as car manufacturers are using elaborate simulations of the real world, e.g., a detailed model of Phoenix, to train their cars using reinforcement learning and deep learning. Such a vehicle trained in a simulation of Phoenix would do rather poorly in the topsy turvy streets of San Francisco. PAC learning was combined with the brilliant work of Vapnik and Chervonenkis decades earlier in Russia on statistical learning, and has become one of the cornerstones of modern ML. Valiant’s paper became an instant classic, published not in some august ML journal, but in the breezy trade pamphlet known as the “Communications of the ACM”. It’s probably the most influential scholarly paper in recent decades to be published in CACM (https://people.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/ValiantLearnable.pdf\n)\nThe earliest experimental work on machine learning was by Arthur Samuel on a program that learned to play the game of checkers by purely self play, and it was an astonishing feat, done in the late 1950s on a vacuum tube IBM 701 computer (see picture below). It’s hard to match Samuel’s audacity, since computers were hardly easy to program, as there was no computer display terminals, no modern programming languages, everything had to be coded in assembly, and all one had was some blinking lights. Even today, Samuel’s achievement is hard to match, and it’s arguable that Samuel’s achievement single handedly created the modern experimental field of machine learning. Samuel also invented the term “machine learning” and contributed towards one of the most influential ideas in the field, a method called temporal-difference learning, now a bedrock of the field of reinforcement learning (and used with much effect by Deep Mind in its Atari video game playing demos). Samuel’s paper from 1959, published six decades ago, remains the most influential experimental paper ever published in ML (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.2254&rep=rep1&type=pdf\n)\n\n4. Valiant’s PAC model influenced the development of many algorithms in machine learning, none was more influential than the work by Robert Schapire on “weak learning” that led to the sensational success of the ensemble method known as “boosting”. The fundamental breakthrough in boosting was in showing that instead of building one complex classifier, one could instead build something really simple, like “stumps” in decision trees, that were individually just better than random, but cumulatively could be “boosted” into an', 'result': {'fake': 0.0387, 'real': 0.9613}, 'status': 'success'}, {'text': 'ensemble classifier of arbitrary accuracy. Boosting took the world of statistics by storm, and eminent statisticians like Leo Breiman and Trevor Hastie sung its praises. Breiman called boosting a surefire way of turning “a sow’s ear into a silk purse”. Boosting remains as popular as it was when it was first introduced several decades ago, and can be used with any form of machine learning, and practically any method (neural nets, decision trees, linear regression, SVMs, …). The original paper by Schapire remains a classic on “The Strength of Weak Learnability” (http://rob.schapire.net/papers/strengthofweak.pdf\n).\n\n5. Two singular events occurred when I was a graduate student, studying ML, in the 1980s. One was the publication of Valiant’s paper in CACM in 1984. The other was the appearance of the two volume book, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, by Rumelhart et al. in 1986. This book introduced the modern field of neural networks (now termed deep learning). The breakthrough technique, of course, was back propagation, introduced in a chapter in this book and also published as a paper in Nature (Learning representations by back-propagating errors\n). Backpropagation is completely implausible as an explanation of how the brain learns, since there is no evidence from neuroscience that the brain actually computes error gradients by backpropagation — this would be a formidable undertaking given that we have 100 billion neurons and upwards of 10^{12} connections among neurons. But, despite the implausibility of backpropagation, it has become the workhorse of modern deep learning, and stochastic gradient descent seems destined to be around for a lot longer as there are no obvious replacements for it yet. Recent scholarship has uncovered that the original ideas underlying backpropagation were known far earlier than the 1986 Rumelhart paper. No one has been more vocal about this issue than Jurgen Schmidhuber, one of the inventors of LSTM models, which are widely used in modeling sequential problems in ML. Schmidhuber as an entire page discussing this issue, which is worth going over (Who Invented Backpropagation?\n). It is not uncommon in science for issues of priority to gain controversy (for over 150 years, students of evolution have been battling over whether Darwin or Wallace should get primary credit, or joint credit, for the discovery). The 2 volume PDP book is one of the true landmarks of ML, and created the “connectionism” movement, which is essentially what deep learning arose from.\n\n6. Another landmark publication in 1986 was Ross Quinlan’s paper on “Induction of decision trees” (http://hunch.net/~coms-4771/quinlan.pdf\n). Decision trees made a sharp contrast with neural networks, and they complemented each other extremely well. Even today, despite all the hoopla you hear about deep learning, it may surprise many to know that decision trees (and their ensemble counterpart, random forests) remain even more widely used in the Bay Area tech industry than deep learning. The reasons should not be surprising. Decision tree technology is mature, and the resulting model is easily explainable (unlike deep learning neural nets, which are notoriously hard to understand). Quinlan championed decision trees in ML, whereas Breiman, Friedman, Olshen, and Stone popularized CART', 'result': {'fake': 0.3358, 'real': 0.6642}, 'status': 'success'}, {'text': '(classification and regression trees) in statistics. Both approaches merged later, and led to one of the most enduring approaches in machine learning. Decision trees are widely used in practice, from modeling consumer behavior in web analytics to credit card application processing to gauging stock market strategies and medical diagnosis. Unlike neural nets and deep learning, there is a rich theory underlying decision trees, and the book on Statistical Learning by the Stanford trio Hastie, Tibshirani, and Friedman contains an excellent overview of decision trees and random forests. Wonder of wonders, this book is a free download — put this on your tablet or smartphone NOW, and read it forever (https://web.stanford.edu/~hastie/Papers/ESLII.pdf\n).\n\n7. One way to distinguish statistics from machine learning, at least historically, is dimension. ML folks are fond of problems in very high-dimensional spaces, where it is hard to use traditional statistical methods. In particular, for most of the history of statistics, the number of observations (or training examples) greatly exceeded the number of dimensions (e.g., take the classic work on least-squares fit by Gauss on Tycho Brahe and Kepler’s data of the motion of planets). This so-called “small n, large p” regime causes a fundamental issue of overfitting. How best to handle the fact that the number of parameters greatly exceeds the number of training examples? This controversy continues to plague deep learning models today, and Geoff Hinton himself has commented on why he considers this a non-issue because he personally does not consider overfitting to be a serious problem. In this fascinating talk, cleverly titled “Brains, Sex, and Machine Learning”, Geoff Hinton argues why overfitting is not a problem for the brain). However, most statisticians and ML’ers consider overfitting to be a serious problem. In a landmark paper called “Support Vector Networks”, Vapnik and colleagues introduced a revolutionary idea of building machine learning models using the sophisticated tools of convex optimization, whereby sparsity could be guaranteed by exploiting the dual form of ML models (Support-vector networks\n). This idea of moving to the dual formulation has since become one of the most widely used “tricks” in machine learning, and SVMs popularized the use of convex optimization in machine learning.\n\n8. One of the most intriguing problems in machine learning is understanding the space that data lie in. The most common assumption is that the data lie in Euclidean space, because the most common data format is a matrix of rows, where each row is a vector of values of the features. However, from a variety of areas, evidence has been building that most real-world datasets lie not in Euclidean space, but rather on a curved surface in high dimensions, what mathematicians call a “manifold” (not to be confused with engine parts!). Manifolds are a rich topic of study in math, where they have been explored for over a 100 years in fields like differential geometry. Einstein’s major breakthrough was showing that time and space are inextricably linked into a four-dimensional space-time manifold, a revolutionary concept that is still being tested today 100 years after Einstein’s breakthrough paper on general relativity. C.R. Rao, India’s', 'result': {'fake': 0.8465, 'real': 0.1535}, 'status': 'success'}, {'text': '“living Gold” of statistics, wrote a brilliant paper in the 1950s showing that space of probability distributions lies on a Riemannian manifold, a manifold that has the property that at each point, the space of tangents (or derivatives) defines a vector space with a dot product. The first sign of the 21st century in ML was the appearance of two classic papers on “manifold learning”, which led to techniques like local linear embedding (LLE) and ISOMAP, both published in the same issue of Science in 2000. However, the landmark paper in manifold learning was by Belkin and the (late, and very much lamented) Niyogi. Partha Niyogi, an ML genius at the University of Chicago, tragically died of an illness at the height of his powers, as did Sam Roweis, inventor of LLE. Both their contributions, however, live on, and have endured. In particular, the paper “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation” (http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf\n) was a true classic, bringing into ML the formal power of differential geometric thinking. The Laplacian has been called “the most beautiful object in all of mathematics and physics” (Nelson, Tensor Analysis), and it has been studied by mathematicians and physicists for centuries. Every major equation in physics, e.g. Maxwell’s equation, features the Laplacian. Belkin and Niyogi showed how the discrete graph Laplacian plays an equally important role in nonlinear dimensionality reduction and representation discovery. In the era of social networks and web data, learning from graphs is hugely important, and the graph Laplacian is the single most important concept in this field.\n\n9. Game theoretic ideas have long been part of AI and fields like economics, but only recently they have begun to enter the mainstream of machine learning, and their popularity in deep learning is largely due to the runaway success of the work of Ian Goodfellow and colleagues in GAN models, or generative adversarial networks (https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\n). Few recent papers have had such an impact in the field of machine learning and beyond, and GANs have introduced the idea of using game theory as a way to build generative models. There are hundreds of variants of GANs, and one reason for their wildfire popularity is that there are many ways to extend the original concept. At this stage, GANs are too new to be entirely sure of their long-lasting impact, but whatever becomes of this line of research, the original work on GANs will remain a classic in the field. The original GAN model is plagued with instabilities and it is quite tricky to train a GAN. Recent variants, like Wasserstein GANs, claim to be better in some ways, but it is safe to say that there is no one approach that seems to dominate all the others. Part of the problem here is that there is no clear metric to test GAN models, except to look at the pretty pictures they generate. It is astonishing to see the faces of “fake celebrities”” in this recent work by nVidia (All of these faces are fake celebrities spawned by AI\n), and in this era of “fake news”, one', 'result': {'fake': 0.9865, 'real': 0.0135}, 'status': 'success'}, {'text': ""worries what the eventual fallout from GAN related work will be (can we ever trust an image or a video again, as telling the “truth”?). These quibbles aside, GANs are a true breakthrough in the field, the one clear step beyond the work in neural nets in the 1980s and earlier.\n\n10. Ach, my last selection. As the saying goes, “when you love what you are doing, it’s not work”. Spending 30+ years in ML has not felt like work, but like absolutely a fun romp through crazy ideas. In the beginning, no one, least of all me, thought this would become a full-time job, but heck, it’s not just a full time job, but one that pays well (Artificial Intelligence Salaries: Paychecks Heading Skyward\n). Gee…what should I choose as my last “top 10” paper of ML of all time. If you search for the most influential papers on the web, most of the links take you to a list of papers on deep learning. This is unfortunate, since ML as a field is sooo much bigger than DL, but that's what happens when you are “in a paradigm”. It gets difficult to “think outside the box”. One of the limitations of all the papers discussed above is that they largely focus on learning statistical correlations or building statistical generative models. What is truly remarkable about human learning, even in young children, is that we learn “how the world works”, that is, we learn to distinguish causes from correlations. How we do this is a bit of a mystery, since correlations are symmetric, so it’s quite challenging to go from correlations (like “thunder” and “lightning” are obviously correlated) to causation (“lightning” causes “thunder”, not the other way around!). One of my predictions for the future of ML is that causal approaches to ML are going to becoming far more popular than they have been to date. Unfortunately, while there are an excellent number of books and papers on causal models (e.g., by Rubin, Pearl and others), there’s not an instant classic paper on causal machine learning, one that will set the ML world on fire. Perhaps that's the reason why causal models haven't become much more popular (yet!). There’s a lot of work in psychology, e.g. by the Berkeley cognitive scientist Alison Gopnik, arguing that causality is very much at the core of how children learn. There’s also much evidence that what matters about perception is the concept introduced by Gibson called “affordances” (i.e., animals and humans perceive the world in terms of what possible actions can be done in each situation, which is intimately tied to our notion of causality and to our desire for “controlling” the world). So, I’m going to end my list by choosing a survey article from cognitive science on the importance of causal models for human learning (http://eccl.mit.edu/papers/gopnikglymoursobeletal.pdf\n). This paper argues that children pay close attention to causal effects in the world. So far, learning machines mostly work by associative methods (e.g., the vast majority of deep learning models learn by correlation). The next breakthrough in ML"", 'result': {'fake': 0.0547, 'real': 0.9453}, 'status': 'success'}, {'text': 'is likely to be a stellar paper that shows why causal learning is more important than learning by correlation. Such a paper has yet to be written. Perhaps, dear Quora reader, you will take this as a challenge to develop a breakthrough causal learning system, and show the rest of us why causality is crucial to our intelligence. Judea Pearl is fond of quoting the ancient Greek philosopher, Democritus, who said “I would rather discover one true cause than gain the kingdom of Persia” (Democritus - Wikipedia\n).\n\nWell, there you have it, my selection of the top 10 most influential papers of all time in machine learning (except for the last one, which is kind of a placeholder for a paper that is yet to be written!). If you have had the patience to read through this whole answer, congratulations! I hope you have learned something of the rich history of the ML field. I began in 1982, innocently pursuing the “crazy” idea of building a learning machine, hugely ignorant of almost every paper above, but gripped by a passion that was inexplicable. It has driven the rest of my career, leading me to explore many fields and work with psychologists, biologists, mathematicians, engineers, physicists, chemists, astronomers, and physicians. Everyone collects and analyzes data, and machine learning can be used anywhere (and, to quote a phrase, “there lies the rub”, because one gets carried away into thinking of ML as a universal tool, the “hammer” you use because every problem becomes a “nail”). But, as long as you remember the wisdom of the above papers, and treat the “hammer” with caution, recognizing that often one wants to use other tools besides hammers!\n\nMay the “ML Force” be with you!', 'result': {'fake': 0.9631, 'real': 0.0369}, 'status': 'success'}], 'credits_used': 35, 'credits': 1982780, 'subscription': 0, 'content': ""It’s always a fun exercise to synthesize 30+ years of research experience in a field, like ML, into compiling some sort of “Best Hits” list. As in every scientific field, the most influential papers can be catalogued in different ways. For example, one can look at objective criteria, such as the papers with the maximum number of citations in some subfield, like deep learning (terryum/awesome-deep-learning-papers\n). One can also look at papers that were singularly influential in some way. Finally, one can simply choose papers that were helpful in clarifying some important idea, or did an excellent job of providing a tutorial of an important area. Each of the papers I have chosen below has been hugely influential not only in the ML field itself, but has also had major impact outside ML. Each has been cited tens of thousands of times, but beyond the citation count, they have each changed the field in some significant way.\n\nI previously answered a related question on the top 20 papers published in AI, so feel free to peruse through my earlier top 20 list, which was somewhat broader, and included papers in neuroscience and ML.\n\nWhich are the top 20 papers related to AI (both machine learning & symbolic), so that I can cover the basics and choose a niche for my research?\n\nOK, on to my newer list of the best ML papers ever published….\n\nTo begin, as they say, at the beginning, in terms of its sheer impact, it is hard to top the 1967 paper by Gold on “Language Identification in the Limit”. (see https://ac.els-cdn.com/S0019995867911655/1-s2.0-S0019995867911655-main.pdf?_tid=bc318a60-0ae5-4de6-868d-4fd2e58897c0&acdnat=1548034543_3ad4aa11e2fd780dc76a63e05c25442d\n). This paper is as important to machine learning as the famous results by Godel on the incompleteness of logic, or the classic results by Church and Turing on the limitations of computability. Gold analyzed an extremely simple, and yet powerful, model of learning, whereby a teacher communicates with a learner, and assumes nothing in terms of the learner’s capabilities. The major result was that the set of context-free languages is not learnable in Gold’s model (which came to be known as inductive inference). This was an earth-shattering result 50 years ago, and influenced entire fields of inquiry, like linguistics. How is that children as young as 2 or 3 learn an unknown language (entirely unrelated to their ethnicity, as Indian children can learn Japanese just as easily as Japanese children can learn Hindi, if they grew up in Japan or India, respectively)? Gold’s paper is still relevant today, in the data-obsessed world of the 21st century, if only to remind us of the inherent limitations in the power of learning. If you aspire to be a data scientist, if you don't understand Gold’s theorem, it’s like being a physicist and not knowing the conservation of energy. It’s THAT important!\n17 years after Gold’s classic paper, in the proverbial “Orwellian” year of 1984, Leslie Valiant published a classic paper that led to his winning the Turing award — computer science’s highest honor — several decades later. Valiant’s paper entitled “A Theory of the Learnable” picked up the thread where Gold left off, and introduced two crucial refinements of Gold’s model. In Valiant’s model, subsequently dubbed “Probably Approximately Correct Learning” (or PAC learning), a learner is only required to converge to an approximation of the desired concept, and is also allowed to fail entirely with some (small) probability. These two refinements made Valiant’s model much more realistic — a robot trained on the concept of “trees” in Florida is unlikely to do well in New England, as it might get confused by fall foliage. Valiant’s model allowed for the set of all possible trees to be sampled by some unknown but unchanging distribution, and required the teacher to be “fair”, so that the same training distribution used to teach the robot must be used in testing as well. Valiant’s model is becoming extremely important in the self-driving autonomous car age of today, as car manufacturers are using elaborate simulations of the real world, e.g., a detailed model of Phoenix, to train their cars using reinforcement learning and deep learning. Such a vehicle trained in a simulation of Phoenix would do rather poorly in the topsy turvy streets of San Francisco. PAC learning was combined with the brilliant work of Vapnik and Chervonenkis decades earlier in Russia on statistical learning, and has become one of the cornerstones of modern ML. Valiant’s paper became an instant classic, published not in some august ML journal, but in the breezy trade pamphlet known as the “Communications of the ACM”. It’s probably the most influential scholarly paper in recent decades to be published in CACM (https://people.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/ValiantLearnable.pdf\n)\nThe earliest experimental work on machine learning was by Arthur Samuel on a program that learned to play the game of checkers by purely self play, and it was an astonishing feat, done in the late 1950s on a vacuum tube IBM 701 computer (see picture below). It’s hard to match Samuel’s audacity, since computers were hardly easy to program, as there was no computer display terminals, no modern programming languages, everything had to be coded in assembly, and all one had was some blinking lights. Even today, Samuel’s achievement is hard to match, and it’s arguable that Samuel’s achievement single handedly created the modern experimental field of machine learning. Samuel also invented the term “machine learning” and contributed towards one of the most influential ideas in the field, a method called temporal-difference learning, now a bedrock of the field of reinforcement learning (and used with much effect by Deep Mind in its Atari video game playing demos). Samuel’s paper from 1959, published six decades ago, remains the most influential experimental paper ever published in ML (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.2254&rep=rep1&type=pdf\n)\n\n4. Valiant’s PAC model influenced the development of many algorithms in machine learning, none was more influential than the work by Robert Schapire on “weak learning” that led to the sensational success of the ensemble method known as “boosting”. The fundamental breakthrough in boosting was in showing that instead of building one complex classifier, one could instead build something really simple, like “stumps” in decision trees, that were individually just better than random, but cumulatively could be “boosted” into an ensemble classifier of arbitrary accuracy. Boosting took the world of statistics by storm, and eminent statisticians like Leo Breiman and Trevor Hastie sung its praises. Breiman called boosting a surefire way of turning “a sow’s ear into a silk purse”. Boosting remains as popular as it was when it was first introduced several decades ago, and can be used with any form of machine learning, and practically any method (neural nets, decision trees, linear regression, SVMs, …). The original paper by Schapire remains a classic on “The Strength of Weak Learnability” (http://rob.schapire.net/papers/strengthofweak.pdf\n).\n\n5. Two singular events occurred when I was a graduate student, studying ML, in the 1980s. One was the publication of Valiant’s paper in CACM in 1984. The other was the appearance of the two volume book, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, by Rumelhart et al. in 1986. This book introduced the modern field of neural networks (now termed deep learning). The breakthrough technique, of course, was back propagation, introduced in a chapter in this book and also published as a paper in Nature (Learning representations by back-propagating errors\n). Backpropagation is completely implausible as an explanation of how the brain learns, since there is no evidence from neuroscience that the brain actually computes error gradients by backpropagation — this would be a formidable undertaking given that we have 100 billion neurons and upwards of 10^{12} connections among neurons. But, despite the implausibility of backpropagation, it has become the workhorse of modern deep learning, and stochastic gradient descent seems destined to be around for a lot longer as there are no obvious replacements for it yet. Recent scholarship has uncovered that the original ideas underlying backpropagation were known far earlier than the 1986 Rumelhart paper. No one has been more vocal about this issue than Jurgen Schmidhuber, one of the inventors of LSTM models, which are widely used in modeling sequential problems in ML. Schmidhuber as an entire page discussing this issue, which is worth going over (Who Invented Backpropagation?\n). It is not uncommon in science for issues of priority to gain controversy (for over 150 years, students of evolution have been battling over whether Darwin or Wallace should get primary credit, or joint credit, for the discovery). The 2 volume PDP book is one of the true landmarks of ML, and created the “connectionism” movement, which is essentially what deep learning arose from.\n\n6. Another landmark publication in 1986 was Ross Quinlan’s paper on “Induction of decision trees” (http://hunch.net/~coms-4771/quinlan.pdf\n). Decision trees made a sharp contrast with neural networks, and they complemented each other extremely well. Even today, despite all the hoopla you hear about deep learning, it may surprise many to know that decision trees (and their ensemble counterpart, random forests) remain even more widely used in the Bay Area tech industry than deep learning. The reasons should not be surprising. Decision tree technology is mature, and the resulting model is easily explainable (unlike deep learning neural nets, which are notoriously hard to understand). Quinlan championed decision trees in ML, whereas Breiman, Friedman, Olshen, and Stone popularized CART (classification and regression trees) in statistics. Both approaches merged later, and led to one of the most enduring approaches in machine learning. Decision trees are widely used in practice, from modeling consumer behavior in web analytics to credit card application processing to gauging stock market strategies and medical diagnosis. Unlike neural nets and deep learning, there is a rich theory underlying decision trees, and the book on Statistical Learning by the Stanford trio Hastie, Tibshirani, and Friedman contains an excellent overview of decision trees and random forests. Wonder of wonders, this book is a free download — put this on your tablet or smartphone NOW, and read it forever (https://web.stanford.edu/~hastie/Papers/ESLII.pdf\n).\n\n7. One way to distinguish statistics from machine learning, at least historically, is dimension. ML folks are fond of problems in very high-dimensional spaces, where it is hard to use traditional statistical methods. In particular, for most of the history of statistics, the number of observations (or training examples) greatly exceeded the number of dimensions (e.g., take the classic work on least-squares fit by Gauss on Tycho Brahe and Kepler’s data of the motion of planets). This so-called “small n, large p” regime causes a fundamental issue of overfitting. How best to handle the fact that the number of parameters greatly exceeds the number of training examples? This controversy continues to plague deep learning models today, and Geoff Hinton himself has commented on why he considers this a non-issue because he personally does not consider overfitting to be a serious problem. In this fascinating talk, cleverly titled “Brains, Sex, and Machine Learning”, Geoff Hinton argues why overfitting is not a problem for the brain). However, most statisticians and ML’ers consider overfitting to be a serious problem. In a landmark paper called “Support Vector Networks”, Vapnik and colleagues introduced a revolutionary idea of building machine learning models using the sophisticated tools of convex optimization, whereby sparsity could be guaranteed by exploiting the dual form of ML models (Support-vector networks\n). This idea of moving to the dual formulation has since become one of the most widely used “tricks” in machine learning, and SVMs popularized the use of convex optimization in machine learning.\n\n8. One of the most intriguing problems in machine learning is understanding the space that data lie in. The most common assumption is that the data lie in Euclidean space, because the most common data format is a matrix of rows, where each row is a vector of values of the features. However, from a variety of areas, evidence has been building that most real-world datasets lie not in Euclidean space, but rather on a curved surface in high dimensions, what mathematicians call a “manifold” (not to be confused with engine parts!). Manifolds are a rich topic of study in math, where they have been explored for over a 100 years in fields like differential geometry. Einstein’s major breakthrough was showing that time and space are inextricably linked into a four-dimensional space-time manifold, a revolutionary concept that is still being tested today 100 years after Einstein’s breakthrough paper on general relativity. C.R. Rao, India’s “living Gold” of statistics, wrote a brilliant paper in the 1950s showing that space of probability distributions lies on a Riemannian manifold, a manifold that has the property that at each point, the space of tangents (or derivatives) defines a vector space with a dot product. The first sign of the 21st century in ML was the appearance of two classic papers on “manifold learning”, which led to techniques like local linear embedding (LLE) and ISOMAP, both published in the same issue of Science in 2000. However, the landmark paper in manifold learning was by Belkin and the (late, and very much lamented) Niyogi. Partha Niyogi, an ML genius at the University of Chicago, tragically died of an illness at the height of his powers, as did Sam Roweis, inventor of LLE. Both their contributions, however, live on, and have endured. In particular, the paper “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation” (http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf\n) was a true classic, bringing into ML the formal power of differential geometric thinking. The Laplacian has been called “the most beautiful object in all of mathematics and physics” (Nelson, Tensor Analysis), and it has been studied by mathematicians and physicists for centuries. Every major equation in physics, e.g. Maxwell’s equation, features the Laplacian. Belkin and Niyogi showed how the discrete graph Laplacian plays an equally important role in nonlinear dimensionality reduction and representation discovery. In the era of social networks and web data, learning from graphs is hugely important, and the graph Laplacian is the single most important concept in this field.\n\n9. Game theoretic ideas have long been part of AI and fields like economics, but only recently they have begun to enter the mainstream of machine learning, and their popularity in deep learning is largely due to the runaway success of the work of Ian Goodfellow and colleagues in GAN models, or generative adversarial networks (https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\n). Few recent papers have had such an impact in the field of machine learning and beyond, and GANs have introduced the idea of using game theory as a way to build generative models. There are hundreds of variants of GANs, and one reason for their wildfire popularity is that there are many ways to extend the original concept. At this stage, GANs are too new to be entirely sure of their long-lasting impact, but whatever becomes of this line of research, the original work on GANs will remain a classic in the field. The original GAN model is plagued with instabilities and it is quite tricky to train a GAN. Recent variants, like Wasserstein GANs, claim to be better in some ways, but it is safe to say that there is no one approach that seems to dominate all the others. Part of the problem here is that there is no clear metric to test GAN models, except to look at the pretty pictures they generate. It is astonishing to see the faces of “fake celebrities”” in this recent work by nVidia (All of these faces are fake celebrities spawned by AI\n), and in this era of “fake news”, one worries what the eventual fallout from GAN related work will be (can we ever trust an image or a video again, as telling the “truth”?). These quibbles aside, GANs are a true breakthrough in the field, the one clear step beyond the work in neural nets in the 1980s and earlier.\n\n10. Ach, my last selection. As the saying goes, “when you love what you are doing, it’s not work”. Spending 30+ years in ML has not felt like work, but like absolutely a fun romp through crazy ideas. In the beginning, no one, least of all me, thought this would become a full-time job, but heck, it’s not just a full time job, but one that pays well (Artificial Intelligence Salaries: Paychecks Heading Skyward\n). Gee…what should I choose as my last “top 10” paper of ML of all time. If you search for the most influential papers on the web, most of the links take you to a list of papers on deep learning. This is unfortunate, since ML as a field is sooo much bigger than DL, but that's what happens when you are “in a paradigm”. It gets difficult to “think outside the box”. One of the limitations of all the papers discussed above is that they largely focus on learning statistical correlations or building statistical generative models. What is truly remarkable about human learning, even in young children, is that we learn “how the world works”, that is, we learn to distinguish causes from correlations. How we do this is a bit of a mystery, since correlations are symmetric, so it’s quite challenging to go from correlations (like “thunder” and “lightning” are obviously correlated) to causation (“lightning” causes “thunder”, not the other way around!). One of my predictions for the future of ML is that causal approaches to ML are going to becoming far more popular than they have been to date. Unfortunately, while there are an excellent number of books and papers on causal models (e.g., by Rubin, Pearl and others), there’s not an instant classic paper on causal machine learning, one that will set the ML world on fire. Perhaps that's the reason why causal models haven't become much more popular (yet!). There’s a lot of work in psychology, e.g. by the Berkeley cognitive scientist Alison Gopnik, arguing that causality is very much at the core of how children learn. There’s also much evidence that what matters about perception is the concept introduced by Gibson called “affordances” (i.e., animals and humans perceive the world in terms of what possible actions can be done in each situation, which is intimately tied to our notion of causality and to our desire for “controlling” the world). So, I’m going to end my list by choosing a survey article from cognitive science on the importance of causal models for human learning (http://eccl.mit.edu/papers/gopnikglymoursobeletal.pdf\n). This paper argues that children pay close attention to causal effects in the world. So far, learning machines mostly work by associative methods (e.g., the vast majority of deep learning models learn by correlation). The next breakthrough in ML is likely to be a stellar paper that shows why causal learning is more important than learning by correlation. Such a paper has yet to be written. Perhaps, dear Quora reader, you will take this as a challenge to develop a breakthrough causal learning system, and show the rest of us why causality is crucial to our intelligence. Judea Pearl is fond of quoting the ancient Greek philosopher, Democritus, who said “I would rather discover one true cause than gain the kingdom of Persia” (Democritus - Wikipedia\n).\n\nWell, there you have it, my selection of the top 10 most influential papers of all time in machine learning (except for the last one, which is kind of a placeholder for a paper that is yet to be written!). If you have had the patience to read through this whole answer, congratulations! I hope you have learned something of the rich history of the ML field. I began in 1982, innocently pursuing the “crazy” idea of building a learning machine, hugely ignorant of almost every paper above, but gripped by a passion that was inexplicable. It has driven the rest of my career, leading me to explore many fields and work with psychologists, biologists, mathematicians, engineers, physicists, chemists, astronomers, and physicians. Everyone collects and analyzes data, and machine learning can be used anywhere (and, to quote a phrase, “there lies the rub”, because one gets carried away into thinking of ML as a universal tool, the “hammer” you use because every problem becomes a “nail”). But, as long as you remember the wisdom of the above papers, and treat the “hammer” with caution, recognizing that often one wants to use other tools besides hammers!\n\nMay the “ML Force” be with you!"", 'aiModelVersion': '1'}",0.3653
Ankush,Oct 26,What is the best online course to learn AI?,"Here is a list you can refer to for learning AI 🧻-

Deep Learning: DeepLearning.AI course by Coursera.
Artificial Intelligence A-Z 2023: Build an AI with ChatGPT4 by Udemy.
Advanced certificate program in Machine Learning and AI by UpGrad. 📖

These courses specialize in providing training in tools of data visualization, machine learning, statistical analysis, and many more. In the long run, it will assist you in improving your understanding of the subject. 📕

Drawbacks: It will not support you to find job opportunities in foreign countries. If you are aiming to switch to a senior position in a company, then it is safe to pursue a master’s program from a foreign university.

It may sound like a lot of money but honestly, it will not cost you much. 🤷

Wondering “how?” 🤔

: Online master’s degree program is the solution.

Wondering “where?” ❓

: Institutes like Learnbay, UpGrad, and Great Learning offer master’s online under the accreditation of foreign universities.

Wondering “why?” 💡

: Let’s discuss it one by one-

Great Learning: MS in Data Science Programme (Northwestern School of Professional Studies)

You can pursue this master’s because they offer it in a flexible learning environment. So you can learn the master’s at your own pace.

UpGrad: Master of Science in Data Science (Liverpool John Moores University)

They offer 5 specializations on this subject including data analytics, business analytics, deep learning, natural language processing, and data engineering.

Benefits of domain-specific training: 📚

It is used to enhance individual performance.
Individuals will be able to provide innovative solutions to complex problems.
Learning domains will bring more credibility to gain expertise in that domain.

Where do you find domain expertise?

Learnbay: Program in Data Science and AI

They offer training on domains like Healthcare; Manufacturing; BFSI; Oil, Gas, and Energy; and Supply Chain, Retail, and E-commerce. You can choose any of the specific domains from the one mentioned above to gain expertise. 🧑‍🎓

Learning format:

📕 vs 💻

UpGrad and Great Learning offer online training to assist working professionals with flexibility.
On the other hand, Learnbay offers a hybrid learning format for a better learning experience. Having said that they offer offline and online learning, where offline learning is offered in their project experience centers. The centers are located in Bangalore, Delhi, Pune, and Hyderabad.

Certifications for learners: 🏫 🧑‍🎓

Great Learning offers a master’s degree from Northwestern School of Professional Studies.
UpGrad offers a master’s degree from Liverpool John Moores University.
Learnbay offers a project completion certificate from IBM.

Note: These certificates are recognized in 60+ regions around the world. As a result, you will be favored with work permits, immigration facilities, and many more career opportunities in the future.

Comparison of the career assistance facilities:

🧑‍💼 + 💹 📈

UpGrad offers some good opportunities for learners so that they can shape their careers as they desire. The career assistance program includes prepared interview training like mock interviews, resume-building activities, doubt-resolving sessions, and many others.
Great Learning has partnered with many hiring partners due to which you can find the desired job post master’s completion. They also offer interview training to learners where they will train you with interview preparation. 🔍
Learnbay offers Career Service Pro, which is a premium career assistance program. Career coaching is offered for 3 years without any extra cost. This premium career program includes 5+ mock interviews, unlimited interview calls, customized doubt-solving classes, resume-writing sessions, and many more. Having this career service program induces 100% career assistance or placement assistance. You can avail of their career assistance program offline or online mode.

Learnbay’s offline modes are instituted in Bangalore, Hyderabad, Pune, and Delhi. 🖐️

Overall, it is recommended to go for a master’s degree program rather than pursuing a generic course as it will limit your skills and abilities in the subject.

Summing up: 🤓

The above-mentioned courses are chosen for their good educational qualities and career assistance facilities. Udemy, Upgrad, and Coursera are reputed institutes specializing in providing IT courses. Pursuing courses will help you upskill in the trending tools and knowledge however, it is recommended to pursue a master’s program to explore more career opportunities both in the domestic as well as in the international market.

Thanks for reading this 🤓

Best of luck 👍","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/i4kfn3wp07mqz5a1', 'title': 'What is the best online course to learn AI?', 'score': {'original': 0.4305, 'ai': 0.5695}, 'blocks': [{'text': 'Here is a list you can refer to for learning AI 🧻-\n\nDeep Learning: DeepLearning.AI course by Coursera.\nArtificial Intelligence A-Z 2023: Build an AI with ChatGPT4 by Udemy.\nAdvanced certificate program in Machine Learning and AI by UpGrad. 📖\n\nThese courses specialize in providing training in tools of data visualization, machine learning, statistical analysis, and many more. In the long run, it will assist you in improving your understanding of the subject. 📕\n\nDrawbacks: It will not support you to find job opportunities in foreign countries. If you are aiming to switch to a senior position in a company, then it is safe to pursue a master’s program from a foreign university.\n\nIt may sound like a lot of money but honestly, it will not cost you much. 🤷\n\nWondering “how?” 🤔\n\n: Online master’s degree program is the solution.\n\nWondering “where?” ❓\n\n: Institutes like Learnbay, UpGrad, and Great Learning offer master’s online under the accreditation of foreign universities.\n\nWondering “why?” 💡\n\n: Let’s discuss it one by one-\n\nGreat Learning: MS in Data Science Programme (Northwestern School of Professional Studies)\n\nYou can pursue this master’s because they offer it in a flexible learning environment. So you can learn the master’s at your own pace.\n\nUpGrad: Master of Science in Data Science (Liverpool John Moores University)\n\nThey offer 5 specializations on this subject including data analytics, business analytics, deep learning, natural language processing, and data engineering.\n\nBenefits of domain-specific training: 📚\n\nIt is used to enhance individual performance.\nIndividuals will be able to provide innovative solutions to complex problems.\nLearning domains will bring more credibility to gain expertise in that domain.\n\nWhere do you find domain expertise?\n\nLearnbay: Program in Data Science and AI\n\nThey offer training on domains like Healthcare; Manufacturing; BFSI; Oil, Gas, and Energy; and Supply Chain, Retail, and E-commerce. You can choose any of the specific domains from the one mentioned above to gain expertise. 🧑\u200d🎓\n\nLearning format:\n\n📕 vs 💻\n\nUpGrad and Great Learning offer online training to assist working professionals with flexibility.\nOn the other hand, Learnbay offers a hybrid learning format for a better learning experience. Having said that they offer offline and online learning, where offline learning is offered in their project experience centers. The centers are located in Bangalore, Delhi, Pune, and Hyderabad.\n\nCertifications for learners: 🏫 🧑\u200d🎓\n\nGreat Learning offers a master’s degree from Northwestern School of Professional Studies.\nUpGrad offers a master’s degree from Liverpool John Moores University.\nLearnbay offers a project completion certificate from IBM.\n\nNote: These certificates are recognized in 60+ regions around the world. As a result, you will be favored with work permits, immigration facilities, and many more career opportunities in the future.\n\nComparison of the career assistance facilities:\n\n🧑\u200d💼 + 💹 📈\n\nUpGrad offers some good opportunities for learners so that they can shape their careers as they desire. The career assistance program includes prepared interview training like mock interviews, resume-building activities, doubt-resolving sessions, and many others.\nGreat Learning has partnered with many hiring partners due to which you can find the desired job post master’s completion. They also offer interview training to learners where they will train you with interview preparation. 🔍\nLearnbay offers Career Service Pro, which is a premium career assistance program. Career coaching is offered for 3', 'result': {'fake': 0.1408, 'real': 0.8592}, 'status': 'success'}, {'text': 'years without any extra cost. This premium career program includes 5+ mock interviews, unlimited interview calls, customized doubt-solving classes, resume-writing sessions, and many more. Having this career service program induces 100% career assistance or placement assistance. You can avail of their career assistance program offline or online mode.\n\nLearnbay’s offline modes are instituted in Bangalore, Hyderabad, Pune, and Delhi. 🖐️\n\nOverall, it is recommended to go for a master’s degree program rather than pursuing a generic course as it will limit your skills and abilities in the subject.\n\nSumming up: 🤓\n\nThe above-mentioned courses are chosen for their good educational qualities and career assistance facilities. Udemy, Upgrad, and Coursera are reputed institutes specializing in providing IT courses. Pursuing courses will help you upskill in the trending tools and knowledge however, it is recommended to pursue a master’s program to explore more career opportunities both in the domestic as well as in the international market.\n\nThanks for reading this 🤓\n\nBest of luck 👍', 'result': {'fake': 0.9957, 'real': 0.0043}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982773, 'subscription': 0, 'content': 'Here is a list you can refer to for learning AI 🧻-\n\nDeep Learning: DeepLearning.AI course by Coursera.\nArtificial Intelligence A-Z 2023: Build an AI with ChatGPT4 by Udemy.\nAdvanced certificate program in Machine Learning and AI by UpGrad. 📖\n\nThese courses specialize in providing training in tools of data visualization, machine learning, statistical analysis, and many more. In the long run, it will assist you in improving your understanding of the subject. 📕\n\nDrawbacks: It will not support you to find job opportunities in foreign countries. If you are aiming to switch to a senior position in a company, then it is safe to pursue a master’s program from a foreign university.\n\nIt may sound like a lot of money but honestly, it will not cost you much. 🤷\n\nWondering “how?” 🤔\n\n: Online master’s degree program is the solution.\n\nWondering “where?” ❓\n\n: Institutes like Learnbay, UpGrad, and Great Learning offer master’s online under the accreditation of foreign universities.\n\nWondering “why?” 💡\n\n: Let’s discuss it one by one-\n\nGreat Learning: MS in Data Science Programme (Northwestern School of Professional Studies)\n\nYou can pursue this master’s because they offer it in a flexible learning environment. So you can learn the master’s at your own pace.\n\nUpGrad: Master of Science in Data Science (Liverpool John Moores University)\n\nThey offer 5 specializations on this subject including data analytics, business analytics, deep learning, natural language processing, and data engineering.\n\nBenefits of domain-specific training: 📚\n\nIt is used to enhance individual performance.\nIndividuals will be able to provide innovative solutions to complex problems.\nLearning domains will bring more credibility to gain expertise in that domain.\n\nWhere do you find domain expertise?\n\nLearnbay: Program in Data Science and AI\n\nThey offer training on domains like Healthcare; Manufacturing; BFSI; Oil, Gas, and Energy; and Supply Chain, Retail, and E-commerce. You can choose any of the specific domains from the one mentioned above to gain expertise. 🧑\u200d🎓\n\nLearning format:\n\n📕 vs 💻\n\nUpGrad and Great Learning offer online training to assist working professionals with flexibility.\nOn the other hand, Learnbay offers a hybrid learning format for a better learning experience. Having said that they offer offline and online learning, where offline learning is offered in their project experience centers. The centers are located in Bangalore, Delhi, Pune, and Hyderabad.\n\nCertifications for learners: 🏫 🧑\u200d🎓\n\nGreat Learning offers a master’s degree from Northwestern School of Professional Studies.\nUpGrad offers a master’s degree from Liverpool John Moores University.\nLearnbay offers a project completion certificate from IBM.\n\nNote: These certificates are recognized in 60+ regions around the world. As a result, you will be favored with work permits, immigration facilities, and many more career opportunities in the future.\n\nComparison of the career assistance facilities:\n\n🧑\u200d💼 + 💹 📈\n\nUpGrad offers some good opportunities for learners so that they can shape their careers as they desire. The career assistance program includes prepared interview training like mock interviews, resume-building activities, doubt-resolving sessions, and many others.\nGreat Learning has partnered with many hiring partners due to which you can find the desired job post master’s completion. They also offer interview training to learners where they will train you with interview preparation. 🔍\nLearnbay offers Career Service Pro, which is a premium career assistance program. Career coaching is offered for 3 years without any extra cost. This premium career program includes 5+ mock interviews, unlimited interview calls, customized doubt-solving classes, resume-writing sessions, and many more. Having this career service program induces 100% career assistance or placement assistance. You can avail of their career assistance program offline or online mode.\n\nLearnbay’s offline modes are instituted in Bangalore, Hyderabad, Pune, and Delhi. 🖐️\n\nOverall, it is recommended to go for a master’s degree program rather than pursuing a generic course as it will limit your skills and abilities in the subject.\n\nSumming up: 🤓\n\nThe above-mentioned courses are chosen for their good educational qualities and career assistance facilities. Udemy, Upgrad, and Coursera are reputed institutes specializing in providing IT courses. Pursuing courses will help you upskill in the trending tools and knowledge however, it is recommended to pursue a master’s program to explore more career opportunities both in the domestic as well as in the international market.\n\nThanks for reading this 🤓\n\nBest of luck 👍', 'aiModelVersion': '1'}",0.4305
Guy D. McCardle,3y,What was the name of the first machine gun?,"The Maxim Gun (sometimes simply known as the Maxim) is generally accredited with being the first true machine gun.

Hiram Maxim was a 19th Century American inventor. He literally built a better mousetrap. His creations ran the gamut from curling irons to amusement park rides. He even tinkered with powered flight and early radio technology.

** The Smithsonian Channel has put out an excellent short piece on Maxim and his machine gun.

At the age of 42, he came up with the idea for his most famous creation. It is just a fact of life that warfare is a money-making proposition for some people.

This is how he explained it to the Times of London when interviewed:

“I was in Vienna, where I met an American whom I had known in the States. He said: ‘Hang your chemistry and electricity! If you want to make a pile of money, invent something that will enable these Europeans to cut each other’s throats with greater facility.’ ”

** Maxim piecing together his first machine gun.

Maxim realized how tedious it was to have to reload after each shot. The inventor got the idea that perhaps energy from the recoil could be used to reload the weapon.

Energy from recoil acts on the breech block and ejects a spent cartridge and inserts another in its place. The Gatling gun existed at the time, but it required a person to crank it. Therefore, it was not truly an automatic weapon.

Early designs of the gun used a 360-degree rotating cam to reverse the movement of the block but later he found that utilization of a toggle lock worked better. It was far more efficient and less labor-intensive than the Gatling Gun.

A major bonus of the Maxim Gun was the fact that is was water-cooled. This gave it the ability to sustain a high rate of fire for significantly longer than air-cooled weapons.

There was a downside to water cooling. It made the gun less flexible in an attack because it was heavier, more complex, and required a supply of water. It was possible for one soldier to operate the gun, but it normally required a crew of 4–6. Besides the man pulling the triggers, the others were needed to speed reload, spot targets, and carry extra ammunition and water.

All of this work would get you a sustained rate of operation of 600 rounds per minute (chambered for the .303 British).

The first use of the weapon was against Britain's colonial enemies in the late 1800’s. The gun was used to mow down more than 1,600 warriors during the Matabele War. Tribal members were so overwhelmed by the incredible new weapon being used against them that they committed suicide in great numbers by throwing themselves on their spears.

The Maxim Gun was so efficient at keeping the peace in the British Empire that Queen Victoria knighted the American inventor in 1900.

** A variant of the Maxim Gun used in WWI. The war to end all wars.

The weapon saw significant use all the way up to and through the Korean War.

—Visit my Space, Alpha Male, for more of the best stuff you’ll see on Quora.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ibjz7dmw8rsg0p93', 'title': 'What was the name of the first machine gun?', 'score': {'original': 0.499, 'ai': 0.501}, 'blocks': [{'text': ""The Maxim Gun (sometimes simply known as the Maxim) is generally accredited with being the first true machine gun.\n\nHiram Maxim was a 19th Century American inventor. He literally built a better mousetrap. His creations ran the gamut from curling irons to amusement park rides. He even tinkered with powered flight and early radio technology.\n\n** The Smithsonian Channel has put out an excellent short piece on Maxim and his machine gun.\n\nAt the age of 42, he came up with the idea for his most famous creation. It is just a fact of life that warfare is a money-making proposition for some people.\n\nThis is how he explained it to the Times of London when interviewed:\n\n“I was in Vienna, where I met an American whom I had known in the States. He said: ‘Hang your chemistry and electricity! If you want to make a pile of money, invent something that will enable these Europeans to cut each other’s throats with greater facility.’ ”\n\n** Maxim piecing together his first machine gun.\n\nMaxim realized how tedious it was to have to reload after each shot. The inventor got the idea that perhaps energy from the recoil could be used to reload the weapon.\n\nEnergy from recoil acts on the breech block and ejects a spent cartridge and inserts another in its place. The Gatling gun existed at the time, but it required a person to crank it. Therefore, it was not truly an automatic weapon.\n\nEarly designs of the gun used a 360-degree rotating cam to reverse the movement of the block but later he found that utilization of a toggle lock worked better. It was far more efficient and less labor-intensive than the Gatling Gun.\n\nA major bonus of the Maxim Gun was the fact that is was water-cooled. This gave it the ability to sustain a high rate of fire for significantly longer than air-cooled weapons.\n\nThere was a downside to water cooling. It made the gun less flexible in an attack because it was heavier, more complex, and required a supply of water. It was possible for one soldier to operate the gun, but it normally required a crew of 4–6. Besides the man pulling the triggers, the others were needed to speed reload, spot targets, and carry extra ammunition and water.\n\nAll of this work would get you a sustained rate of operation of 600 rounds per minute (chambered for the .303 British).\n\nThe first use of the weapon was against Britain's colonial enemies in the late 1800’s. The gun was used to mow down more than 1,600 warriors during the Matabele War. Tribal members were so overwhelmed by the incredible new weapon being used against them that they committed suicide in great numbers by throwing themselves on their spears.\n\nThe Maxim Gun was so efficient at keeping the peace in the British Empire that Queen Victoria knighted the American inventor in 1900.\n\n** A variant of the Maxim Gun used in WWI. The war to end all wars.\n\nThe weapon saw significant use all the way up to and through the Korean War.\n\n—Visit my Space, Alpha Male, for more of the best"", 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}, {'text': 'stuff you’ll see on Quora.', 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 6, 'credits': 1982767, 'subscription': 0, 'content': ""The Maxim Gun (sometimes simply known as the Maxim) is generally accredited with being the first true machine gun.\n\nHiram Maxim was a 19th Century American inventor. He literally built a better mousetrap. His creations ran the gamut from curling irons to amusement park rides. He even tinkered with powered flight and early radio technology.\n\n** The Smithsonian Channel has put out an excellent short piece on Maxim and his machine gun.\n\nAt the age of 42, he came up with the idea for his most famous creation. It is just a fact of life that warfare is a money-making proposition for some people.\n\nThis is how he explained it to the Times of London when interviewed:\n\n“I was in Vienna, where I met an American whom I had known in the States. He said: ‘Hang your chemistry and electricity! If you want to make a pile of money, invent something that will enable these Europeans to cut each other’s throats with greater facility.’ ”\n\n** Maxim piecing together his first machine gun.\n\nMaxim realized how tedious it was to have to reload after each shot. The inventor got the idea that perhaps energy from the recoil could be used to reload the weapon.\n\nEnergy from recoil acts on the breech block and ejects a spent cartridge and inserts another in its place. The Gatling gun existed at the time, but it required a person to crank it. Therefore, it was not truly an automatic weapon.\n\nEarly designs of the gun used a 360-degree rotating cam to reverse the movement of the block but later he found that utilization of a toggle lock worked better. It was far more efficient and less labor-intensive than the Gatling Gun.\n\nA major bonus of the Maxim Gun was the fact that is was water-cooled. This gave it the ability to sustain a high rate of fire for significantly longer than air-cooled weapons.\n\nThere was a downside to water cooling. It made the gun less flexible in an attack because it was heavier, more complex, and required a supply of water. It was possible for one soldier to operate the gun, but it normally required a crew of 4–6. Besides the man pulling the triggers, the others were needed to speed reload, spot targets, and carry extra ammunition and water.\n\nAll of this work would get you a sustained rate of operation of 600 rounds per minute (chambered for the .303 British).\n\nThe first use of the weapon was against Britain's colonial enemies in the late 1800’s. The gun was used to mow down more than 1,600 warriors during the Matabele War. Tribal members were so overwhelmed by the incredible new weapon being used against them that they committed suicide in great numbers by throwing themselves on their spears.\n\nThe Maxim Gun was so efficient at keeping the peace in the British Empire that Queen Victoria knighted the American inventor in 1900.\n\n** A variant of the Maxim Gun used in WWI. The war to end all wars.\n\nThe weapon saw significant use all the way up to and through the Korean War.\n\n—Visit my Space, Alpha Male, for more of the best stuff you’ll see on Quora."", 'aiModelVersion': '1'}",0.499
Dhara Singh,10mo,What are prerequisites to start learning machine learning?,"The term 'Machine Learning'' was coined in 1959 by Arthur Samuel who defined ML as a “Field of learning that imparts computers the potentiality to learn without being specifically programmed”. And this was the beginning of ML! The demand for experts in machine learning, or ML for short, is growing as it becomes one of the most well-known technologies. Moreover, to excel in the ML field, enrolling in a course with domain specialization and job assistance can be very helpful.

Machine learning is now recognized as one of the most demanding job profiles globally. According to a survey, Machine Learning is gaining sheer popularity in 2022 with a 344% growth and an average base salary of $146,085 per year.

What is machine learning?

Machine Learning involves using Artificial Intelligence to allow machines to learn a task by experience, without specifically programming them about that task. (In short, machines learn automatically without holding the human hand).

How to start learning ML?

This is a basic prerequisite roadmap that you can follow on your way to becoming an incredibly talented machine learning engineer. Of course, you can always modify the steps as per your needs to reach your desired end goal!

Understand the prerequisites:

Normally, there are some prerequisites that you need to know, which include Linear Algebra, Multivariate Calculus, Statistics, and Python. And if you don't know them, fear not! You don't need a Ph.D. degree in these topics to get started, but you do need some basic knowledge.

Learn Linear Algebra and Multivariate Calculus

Both Multivariate Calculus and Linear Algebra are critical in Machine Learning. However, how much you need them depends on your role as a data scientist.

2. Learn Statistics

Data plays an important role in machine learning. In fact, about 80% of your time as an ML expert will be devoted to data collection and cleaning.

3. Learn Python

Some people prefer to skip Linear Algebra, Multivariate Calculus, and Statistics and learn them as they go along with trial and error.

One essential prerequisite to learning machine learning is domain-specific knowledge. ML is used in various domains such as IoT, Automotive, Healthcare, etc. Therefore, you should be able to work in one specific domain that you want to enter. For this, get a domain specialization in your preferred field of interest. If you get domain-specific knowledge, you will also become an interesting prospect in the recruiter's eyes. Candidates with domain specialization are more likely to get a promotion in their careers due to their specialized expertise.

For any individual, it’s not possible to learn these prerequisites without constant support and guidance. Institutes can help you with this.

So let’s first talk about this institute:

Along with machine learning, it provides a wide range of courses in data science and artificial intelligence training. Lessons in this institute’s courses are delivered in real time. This implies that you can discuss your questions privately with the mentors. Because it provides the greatest features in its courses, it has the best machine learning programming course available online.

The Machine Learning course offered includes:

Course name: Artificial Intelligence and Machine Learning Program
Duration: 9 to 11 months weekday or weekend batches.
Projects: Hands-on 15+ Real-time Projects and 2 Capstone Projects.

Machine learning course features include:

You will work on real-time and capstone projects throughout the duration of this Artificial Intelligence and Machine Learning Program. A huge part of machine learning projects provides great help in projects through constant mentor guidance and feedback. It Provides 15 real-time and 2 capstone projects in the Artificial Intelligence and Machine Learning Program.
A hybrid learning environment mixes traditional classroom instruction with online academic courses and real-world projects for students who choose a more practical approach to their academic study.
The Artificial Intelligence and Machine Learning Program are certified through IBM. That means after course completion you will earn an IBM course completion certificate. Other than that, you’ll also get a capstone project certificate upon completing the capstone projects. There are micro-skill certificates as well for completing micro-skill projects such as Machine Learning with Python.
The Artificial Intelligence and Machine Learning Program offers students access to project-based laboratories where they may work on actual data science projects. These laboratories can be located in Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore in addition to other cities across the nation.
As stated above, learning domain specialization becomes crucial while pursuing a course in ML. Thus, it provides domain-specific training in various disciplines in the Artificial Intelligence and Machine Learning Programs like Automotive, IoT & Embedded Engineers, Data Structures and Algorithms, Cloud & DevOps, HR, Healthcare, Finance, etc.
Now let’s discuss what includes in the ML course offered by Simplilearn:

Simplilearn provides educational and training services through an interactive learning technique that incorporates live sessions from IBM Hackathons, international practitioners, practical laboratories, etc. However, there is one major drawback Simplilearn does not provide job assistance through its machine learning course.

Final Conclusion

The higher the pointer goes, however, the earlier you may start your job in machine learning (ML) or at least get a heads-up on what has to be completed first in order to get expertise in this subject. With the right training and experience, you will undoubtedly receive good employment offers.

Thanks for reading!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/14u657zht0m9iw38', 'title': 'What are prerequisites to start learning machine learning?', 'score': {'original': 0.89725, 'ai': 0.10275}, 'blocks': [{'text': ""The term 'Machine Learning'' was coined in 1959 by Arthur Samuel who defined ML as a “Field of learning that imparts computers the potentiality to learn without being specifically programmed”. And this was the beginning of ML! The demand for experts in machine learning, or ML for short, is growing as it becomes one of the most well-known technologies. Moreover, to excel in the ML field, enrolling in a course with domain specialization and job assistance can be very helpful.\n\nMachine learning is now recognized as one of the most demanding job profiles globally. According to a survey, Machine Learning is gaining sheer popularity in 2022 with a 344% growth and an average base salary of $146,085 per year.\n\nWhat is machine learning?\n\nMachine Learning involves using Artificial Intelligence to allow machines to learn a task by experience, without specifically programming them about that task. (In short, machines learn automatically without holding the human hand).\n\nHow to start learning ML?\n\nThis is a basic prerequisite roadmap that you can follow on your way to becoming an incredibly talented machine learning engineer. Of course, you can always modify the steps as per your needs to reach your desired end goal!\n\nUnderstand the prerequisites:\n\nNormally, there are some prerequisites that you need to know, which include Linear Algebra, Multivariate Calculus, Statistics, and Python. And if you don't know them, fear not! You don't need a Ph.D. degree in these topics to get started, but you do need some basic knowledge.\n\nLearn Linear Algebra and Multivariate Calculus\n\nBoth Multivariate Calculus and Linear Algebra are critical in Machine Learning. However, how much you need them depends on your role as a data scientist.\n\n2. Learn Statistics\n\nData plays an important role in machine learning. In fact, about 80% of your time as an ML expert will be devoted to data collection and cleaning.\n\n3. Learn Python\n\nSome people prefer to skip Linear Algebra, Multivariate Calculus, and Statistics and learn them as they go along with trial and error.\n\nOne essential prerequisite to learning machine learning is domain-specific knowledge. ML is used in various domains such as IoT, Automotive, Healthcare, etc. Therefore, you should be able to work in one specific domain that you want to enter. For this, get a domain specialization in your preferred field of interest. If you get domain-specific knowledge, you will also become an interesting prospect in the recruiter's eyes. Candidates with domain specialization are more likely to get a promotion in their careers due to their specialized expertise.\n\nFor any individual, it’s not possible to learn these prerequisites without constant support and guidance. Institutes can help you with this.\n\nSo let’s first talk about this institute:\n\nAlong with machine learning, it provides a wide range of courses in data science and artificial intelligence training. Lessons in this institute’s courses are delivered in real time. This implies that you can discuss your questions privately with the mentors. Because it provides the greatest features in its courses, it has the best machine learning programming course available online.\n\nThe Machine Learning course offered includes:\n\nCourse name: Artificial Intelligence and Machine Learning Program\nDuration: 9 to 11 months weekday or weekend batches.\nProjects: Hands-on 15+ Real-time"", 'result': {'fake': 0.9292, 'real': 0.0708}, 'status': 'success'}, {'text': 'Projects and 2 Capstone Projects.\n\nMachine learning course features include:\n\nYou will work on real-time and capstone projects throughout the duration of this Artificial Intelligence and Machine Learning Program. A huge part of machine learning projects provides great help in projects through constant mentor guidance and feedback. It Provides 15 real-time and 2 capstone projects in the Artificial Intelligence and Machine Learning Program.\nA hybrid learning environment mixes traditional classroom instruction with online academic courses and real-world projects for students who choose a more practical approach to their academic study.\nThe Artificial Intelligence and Machine Learning Program are certified through IBM. That means after course completion you will earn an IBM course completion certificate. Other than that, you’ll also get a capstone project certificate upon completing the capstone projects. There are micro-skill certificates as well for completing micro-skill projects such as Machine Learning with Python.\nThe Artificial Intelligence and Machine Learning Program offers students access to project-based laboratories where they may work on actual data science projects. These laboratories can be located in Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore in addition to other cities across the nation.\nAs stated above, learning domain specialization becomes crucial while pursuing a course in ML. Thus, it provides domain-specific training in various disciplines in the Artificial Intelligence and Machine Learning Programs like Automotive, IoT & Embedded Engineers, Data Structures and Algorithms, Cloud & DevOps, HR, Healthcare, Finance, etc.\nNow let’s discuss what includes in the ML course offered by Simplilearn:\n\nSimplilearn provides educational and training services through an interactive learning technique that incorporates live sessions from IBM Hackathons, international practitioners, practical laboratories, etc. However, there is one major drawback Simplilearn does not provide job assistance through its machine learning course.\n\nFinal Conclusion\n\nThe higher the pointer goes, however, the earlier you may start your job in machine learning (ML) or at least get a heads-up on what has to be completed first in order to get expertise in this subject. With the right training and experience, you will undoubtedly receive good employment offers.\n\nThanks for reading!', 'result': {'fake': 0.2225, 'real': 0.7775}, 'status': 'success'}], 'credits_used': 9, 'credits': 1982758, 'subscription': 0, 'content': ""The term 'Machine Learning'' was coined in 1959 by Arthur Samuel who defined ML as a “Field of learning that imparts computers the potentiality to learn without being specifically programmed”. And this was the beginning of ML! The demand for experts in machine learning, or ML for short, is growing as it becomes one of the most well-known technologies. Moreover, to excel in the ML field, enrolling in a course with domain specialization and job assistance can be very helpful.\n\nMachine learning is now recognized as one of the most demanding job profiles globally. According to a survey, Machine Learning is gaining sheer popularity in 2022 with a 344% growth and an average base salary of $146,085 per year.\n\nWhat is machine learning?\n\nMachine Learning involves using Artificial Intelligence to allow machines to learn a task by experience, without specifically programming them about that task. (In short, machines learn automatically without holding the human hand).\n\nHow to start learning ML?\n\nThis is a basic prerequisite roadmap that you can follow on your way to becoming an incredibly talented machine learning engineer. Of course, you can always modify the steps as per your needs to reach your desired end goal!\n\nUnderstand the prerequisites:\n\nNormally, there are some prerequisites that you need to know, which include Linear Algebra, Multivariate Calculus, Statistics, and Python. And if you don't know them, fear not! You don't need a Ph.D. degree in these topics to get started, but you do need some basic knowledge.\n\nLearn Linear Algebra and Multivariate Calculus\n\nBoth Multivariate Calculus and Linear Algebra are critical in Machine Learning. However, how much you need them depends on your role as a data scientist.\n\n2. Learn Statistics\n\nData plays an important role in machine learning. In fact, about 80% of your time as an ML expert will be devoted to data collection and cleaning.\n\n3. Learn Python\n\nSome people prefer to skip Linear Algebra, Multivariate Calculus, and Statistics and learn them as they go along with trial and error.\n\nOne essential prerequisite to learning machine learning is domain-specific knowledge. ML is used in various domains such as IoT, Automotive, Healthcare, etc. Therefore, you should be able to work in one specific domain that you want to enter. For this, get a domain specialization in your preferred field of interest. If you get domain-specific knowledge, you will also become an interesting prospect in the recruiter's eyes. Candidates with domain specialization are more likely to get a promotion in their careers due to their specialized expertise.\n\nFor any individual, it’s not possible to learn these prerequisites without constant support and guidance. Institutes can help you with this.\n\nSo let’s first talk about this institute:\n\nAlong with machine learning, it provides a wide range of courses in data science and artificial intelligence training. Lessons in this institute’s courses are delivered in real time. This implies that you can discuss your questions privately with the mentors. Because it provides the greatest features in its courses, it has the best machine learning programming course available online.\n\nThe Machine Learning course offered includes:\n\nCourse name: Artificial Intelligence and Machine Learning Program\nDuration: 9 to 11 months weekday or weekend batches.\nProjects: Hands-on 15+ Real-time Projects and 2 Capstone Projects.\n\nMachine learning course features include:\n\nYou will work on real-time and capstone projects throughout the duration of this Artificial Intelligence and Machine Learning Program. A huge part of machine learning projects provides great help in projects through constant mentor guidance and feedback. It Provides 15 real-time and 2 capstone projects in the Artificial Intelligence and Machine Learning Program.\nA hybrid learning environment mixes traditional classroom instruction with online academic courses and real-world projects for students who choose a more practical approach to their academic study.\nThe Artificial Intelligence and Machine Learning Program are certified through IBM. That means after course completion you will earn an IBM course completion certificate. Other than that, you’ll also get a capstone project certificate upon completing the capstone projects. There are micro-skill certificates as well for completing micro-skill projects such as Machine Learning with Python.\nThe Artificial Intelligence and Machine Learning Program offers students access to project-based laboratories where they may work on actual data science projects. These laboratories can be located in Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore in addition to other cities across the nation.\nAs stated above, learning domain specialization becomes crucial while pursuing a course in ML. Thus, it provides domain-specific training in various disciplines in the Artificial Intelligence and Machine Learning Programs like Automotive, IoT & Embedded Engineers, Data Structures and Algorithms, Cloud & DevOps, HR, Healthcare, Finance, etc.\nNow let’s discuss what includes in the ML course offered by Simplilearn:\n\nSimplilearn provides educational and training services through an interactive learning technique that incorporates live sessions from IBM Hackathons, international practitioners, practical laboratories, etc. However, there is one major drawback Simplilearn does not provide job assistance through its machine learning course.\n\nFinal Conclusion\n\nThe higher the pointer goes, however, the earlier you may start your job in machine learning (ML) or at least get a heads-up on what has to be completed first in order to get expertise in this subject. With the right training and experience, you will undoubtedly receive good employment offers.\n\nThanks for reading!"", 'aiModelVersion': '1'}",0.89725
Håkon Hapnes Strand,3y,Do you think machine learning is more exciting than software development?,"Solving hard problems is what drives me. I think that's what motviated most data scientists to get into machine learning in the first place.

The nice thing about machine learning is that most problems that need it to be solved are inherently hard. In addition, the complexity of machine learning solutions is typically high, with everything that goes into data pipelines, deployments and so on.

The disadvantage is that most problems don't need machine learning. The majority of computational problems are better solved with deterministic algorithms, or software development if you will.

Throwing machine learning at such a problem just… because…, is not my idea of fun. I'm always more excited about solving real problems using the best tools for the job.

Deterministic software development can be every bit as challenging as machine learning. To me that is just as interesting. However, there are also many simple problems that need software develoment, and they generally don't excite me as much.

Lalit Patel brings up a key point. You can easily set yourself up for frustration in machine learning projects. Its probabilistic nature means you never know the outcome beforehand. Stakeholders with limited exposure to statistics may not understand the difference fully, adding to the frustration.

Personally, I enjoy being a software developer with machine learning as my specialty. That means I can be an allrounder while also being able to solve some problems that most developers can't. As a consultant it's nice to have this T-shaped skillset, because it means I never run out of work.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/j7yg3mli1thufvbp', 'title': 'Do you think machine learning is more exciting than software development?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': ""Solving hard problems is what drives me. I think that's what motviated most data scientists to get into machine learning in the first place.\n\nThe nice thing about machine learning is that most problems that need it to be solved are inherently hard. In addition, the complexity of machine learning solutions is typically high, with everything that goes into data pipelines, deployments and so on.\n\nThe disadvantage is that most problems don't need machine learning. The majority of computational problems are better solved with deterministic algorithms, or software development if you will.\n\nThrowing machine learning at such a problem just… because…, is not my idea of fun. I'm always more excited about solving real problems using the best tools for the job.\n\nDeterministic software development can be every bit as challenging as machine learning. To me that is just as interesting. However, there are also many simple problems that need software develoment, and they generally don't excite me as much.\n\nLalit Patel brings up a key point. You can easily set yourself up for frustration in machine learning projects. Its probabilistic nature means you never know the outcome beforehand. Stakeholders with limited exposure to statistics may not understand the difference fully, adding to the frustration.\n\nPersonally, I enjoy being a software developer with machine learning as my specialty. That means I can be an allrounder while also being able to solve some problems that most developers can't. As a consultant it's nice to have this T-shaped skillset, because it means I never run out of work."", 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982755, 'subscription': 0, 'content': ""Solving hard problems is what drives me. I think that's what motviated most data scientists to get into machine learning in the first place.\n\nThe nice thing about machine learning is that most problems that need it to be solved are inherently hard. In addition, the complexity of machine learning solutions is typically high, with everything that goes into data pipelines, deployments and so on.\n\nThe disadvantage is that most problems don't need machine learning. The majority of computational problems are better solved with deterministic algorithms, or software development if you will.\n\nThrowing machine learning at such a problem just… because…, is not my idea of fun. I'm always more excited about solving real problems using the best tools for the job.\n\nDeterministic software development can be every bit as challenging as machine learning. To me that is just as interesting. However, there are also many simple problems that need software develoment, and they generally don't excite me as much.\n\nLalit Patel brings up a key point. You can easily set yourself up for frustration in machine learning projects. Its probabilistic nature means you never know the outcome beforehand. Stakeholders with limited exposure to statistics may not understand the difference fully, adding to the frustration.\n\nPersonally, I enjoy being a software developer with machine learning as my specialty. That means I can be an allrounder while also being able to solve some problems that most developers can't. As a consultant it's nice to have this T-shaped skillset, because it means I never run out of work."", 'aiModelVersion': '1'}",0.9995
Lokesh,1y,What are prerequisites to start learning machine learning?,"Machine learning is a branch of computer science that uses algorithms, statistics, and computational methods to enable machines to learn without being explicitly programmed. Learning with machine learning is a different process than what people are used to with supervised or unsupervised machine learning models.

To start out there are some things you will need before you can get into the nitty-gritty of machine learning. Let's look into the prerequisites of Machine Learning:

The first prerequisite step is the foundations of probability and statistics.
The next prerequisite step is the foundations of data science.
The last prerequisite step is the statistical programming toolkit.

Additionally, students will have a basic understanding of algorithms and data structures in computer science. Additionally, it's beneficial for students to have at least a year of expertise in Python or a closely related language, like as MATLAB or R. Finally, students should be familiar with at least one application programming interface (API), such as those offered by AWS or Google Cloud Platform.

Online courses, books, videos, and other training resources are frequently used to teach students machine learning. To be considered for the majority of machine learning-related positions, you will need to demonstrate that you have a practical understanding of the field. Programmers will frequently be asked to show documentation of their hands-on machine-learning experience. Like everything else, if you want more than simply a straightforward technical job, you should think about changing careers and getting a college degree.

This answer lists the institutes that provide the most well-liked machine learning and artificial intelligence courses in India.

To make it simple for Indian students to identify the top online courses for machine learning or artificial intelligence, this response will discuss a few course providers. Additionally, it will provide you with some insight into what, in this regard, makes an excellent course provider.

Udemy is a great place to start if you're looking for short machine-learning classes. They provide a diverse selection of courses that concentrate on a single certification area rather than the entire certification pool. Machine Learning courses are available on Udemy. Udemy, on the other hand, does not provide its professionals with domain specialization modules.

A great DS/ML/AI learning resource is Simplilearn. Simplilearn has been training those who desire to work in this field for years with a group of knowledgeable instructors. The drawback of this platform is that it does not offer instruction in particular domains. Simplilearn is a good choice for those who do not yet know which domain their learning should be in.

Learnbay is an amazing website offering top-notch courses in data science and machine learning. With incredible features like classes that are customized to a given domain, it is one of the greatest platforms. This MNC has a fantastic Artificial Intelligence and Machine Learning Program for those who are interested in learning ML while working for a company that focuses on products. For working professionals in Bangalore, this university now offers the best machine learning and top data science courses.

What part does domain specialization play in the success of big corporations?

You'll need more than just a data science degree or certification to become a data scientist. To effectively employ your abilities, knowledge, and experience, problem-solving skills are a necessity. Domain specialization has thus become a crucial component of data science. Whether you're a seasoned expert or just starting, having a domain-specialized field can help you obtain work. Nowadays, specialists are in greater demand than generalists. Specialized individuals are more likely to find employment more quickly.

Learnbay has an excellent Artificial Intelligence and Machine Learning Program for those who want to learn ML while working in a product-based MNC. Live interactive lessons and hybrid classes are available from any location in India. There are several advantages to taking this course.

Some of the top features of Learnbay are:

The IBM Group has approved all of Learnbay's courses. Students who excel in Machine Learning receive three certificates from Learnbay's Artificial Intelligence and Machine Learning Program. IBM certifications are awarded for completing a course, project, or micro-skills.
As part of your training, you will be able to choose from several specializations, including BFSI, Telecom, Manufacturing, and Communication. Because of Learnbay's domain-specific training, it has a distinct advantage. To advance in your career, you must meet the requirements of a particular industry. Learnbay has everything you need! Only students who are serious about pursuing a career in data science are accepted into Learnbay's Artificial Intelligence and Machine Learning Program.
A ""hybrid learning format"" is a learning arrangement that incorporates both teaching methods. By understanding how to do it, students can modify their selected learning preferences to meet any situation.
Along with Delhi, Pune, and other cities, there are project innovation centers where experts from MNC and MAANG oversee online and offline project sessions.
Learnbay Provides students of all skill levels, from beginners to experts, with real-time and capstone projects. You will work on large-scale machine learning and data science projects throughout the course. You might get a taste of what it's like to work in the real world as a student.

The following are some examples of data science projects:

Building Chatbots
Detection of Credit Card Fraud and Fake News
Recommender Systems for Forest Fire Prediction

Conclusion:

If you're looking for machine learning courses, Learnbay is an outstanding place to start. They offer a great variety of courses that are focused on a single certification field rather than the complete pool of certifications. It contains a wealth of resources, including lectures and tests, that professionals using this platform can use to advance their ML abilities. During the event, you can also use mobile apps, career counselors, and industry experts.

Thank you","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/1a3sgun8tvmj2yif', 'title': 'What are prerequisites to start learning machine learning?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""Machine learning is a branch of computer science that uses algorithms, statistics, and computational methods to enable machines to learn without being explicitly programmed. Learning with machine learning is a different process than what people are used to with supervised or unsupervised machine learning models.\n\nTo start out there are some things you will need before you can get into the nitty-gritty of machine learning. Let's look into the prerequisites of Machine Learning:\n\nThe first prerequisite step is the foundations of probability and statistics.\nThe next prerequisite step is the foundations of data science.\nThe last prerequisite step is the statistical programming toolkit.\n\nAdditionally, students will have a basic understanding of algorithms and data structures in computer science. Additionally, it's beneficial for students to have at least a year of expertise in Python or a closely related language, like as MATLAB or R. Finally, students should be familiar with at least one application programming interface (API), such as those offered by AWS or Google Cloud Platform.\n\nOnline courses, books, videos, and other training resources are frequently used to teach students machine learning. To be considered for the majority of machine learning-related positions, you will need to demonstrate that you have a practical understanding of the field. Programmers will frequently be asked to show documentation of their hands-on machine-learning experience. Like everything else, if you want more than simply a straightforward technical job, you should think about changing careers and getting a college degree.\n\nThis answer lists the institutes that provide the most well-liked machine learning and artificial intelligence courses in India.\n\nTo make it simple for Indian students to identify the top online courses for machine learning or artificial intelligence, this response will discuss a few course providers. Additionally, it will provide you with some insight into what, in this regard, makes an excellent course provider.\n\nUdemy is a great place to start if you're looking for short machine-learning classes. They provide a diverse selection of courses that concentrate on a single certification area rather than the entire certification pool. Machine Learning courses are available on Udemy. Udemy, on the other hand, does not provide its professionals with domain specialization modules.\n\nA great DS/ML/AI learning resource is Simplilearn. Simplilearn has been training those who desire to work in this field for years with a group of knowledgeable instructors. The drawback of this platform is that it does not offer instruction in particular domains. Simplilearn is a good choice for those who do not yet know which domain their learning should be in.\n\nLearnbay is an amazing website offering top-notch courses in data science and machine learning. With incredible features like classes that are customized to a given domain, it is one of the greatest platforms. This MNC has a fantastic Artificial Intelligence and Machine Learning Program for those who are interested in learning ML while working for a company that focuses on products. For working professionals in Bangalore, this university now offers the best machine learning and top data science courses.\n\nWhat part does domain specialization play in the success of big corporations?\n\nYou'll need more than just a data science degree or certification to"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': 'become a data scientist. To effectively employ your abilities, knowledge, and experience, problem-solving skills are a necessity. Domain specialization has thus become a crucial component of data science. Whether you\'re a seasoned expert or just starting, having a domain-specialized field can help you obtain work. Nowadays, specialists are in greater demand than generalists. Specialized individuals are more likely to find employment more quickly.\n\nLearnbay has an excellent Artificial Intelligence and Machine Learning Program for those who want to learn ML while working in a product-based MNC. Live interactive lessons and hybrid classes are available from any location in India. There are several advantages to taking this course.\n\nSome of the top features of Learnbay are:\n\nThe IBM Group has approved all of Learnbay\'s courses. Students who excel in Machine Learning receive three certificates from Learnbay\'s Artificial Intelligence and Machine Learning Program. IBM certifications are awarded for completing a course, project, or micro-skills.\nAs part of your training, you will be able to choose from several specializations, including BFSI, Telecom, Manufacturing, and Communication. Because of Learnbay\'s domain-specific training, it has a distinct advantage. To advance in your career, you must meet the requirements of a particular industry. Learnbay has everything you need! Only students who are serious about pursuing a career in data science are accepted into Learnbay\'s Artificial Intelligence and Machine Learning Program.\nA ""hybrid learning format"" is a learning arrangement that incorporates both teaching methods. By understanding how to do it, students can modify their selected learning preferences to meet any situation.\nAlong with Delhi, Pune, and other cities, there are project innovation centers where experts from MNC and MAANG oversee online and offline project sessions.\nLearnbay Provides students of all skill levels, from beginners to experts, with real-time and capstone projects. You will work on large-scale machine learning and data science projects throughout the course. You might get a taste of what it\'s like to work in the real world as a student.\n\nThe following are some examples of data science projects:\n\nBuilding Chatbots\nDetection of Credit Card Fraud and Fake News\nRecommender Systems for Forest Fire Prediction\n\nConclusion:\n\nIf you\'re looking for machine learning courses, Learnbay is an outstanding place to start. They offer a great variety of courses that are focused on a single certification field rather than the complete pool of certifications. It contains a wealth of resources, including lectures and tests, that professionals using this platform can use to advance their ML abilities. During the event, you can also use mobile apps, career counselors, and industry experts.\n\nThank you', 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 10, 'credits': 1982745, 'subscription': 0, 'content': 'Machine learning is a branch of computer science that uses algorithms, statistics, and computational methods to enable machines to learn without being explicitly programmed. Learning with machine learning is a different process than what people are used to with supervised or unsupervised machine learning models.\n\nTo start out there are some things you will need before you can get into the nitty-gritty of machine learning. Let\'s look into the prerequisites of Machine Learning:\n\nThe first prerequisite step is the foundations of probability and statistics.\nThe next prerequisite step is the foundations of data science.\nThe last prerequisite step is the statistical programming toolkit.\n\nAdditionally, students will have a basic understanding of algorithms and data structures in computer science. Additionally, it\'s beneficial for students to have at least a year of expertise in Python or a closely related language, like as MATLAB or R. Finally, students should be familiar with at least one application programming interface (API), such as those offered by AWS or Google Cloud Platform.\n\nOnline courses, books, videos, and other training resources are frequently used to teach students machine learning. To be considered for the majority of machine learning-related positions, you will need to demonstrate that you have a practical understanding of the field. Programmers will frequently be asked to show documentation of their hands-on machine-learning experience. Like everything else, if you want more than simply a straightforward technical job, you should think about changing careers and getting a college degree.\n\nThis answer lists the institutes that provide the most well-liked machine learning and artificial intelligence courses in India.\n\nTo make it simple for Indian students to identify the top online courses for machine learning or artificial intelligence, this response will discuss a few course providers. Additionally, it will provide you with some insight into what, in this regard, makes an excellent course provider.\n\nUdemy is a great place to start if you\'re looking for short machine-learning classes. They provide a diverse selection of courses that concentrate on a single certification area rather than the entire certification pool. Machine Learning courses are available on Udemy. Udemy, on the other hand, does not provide its professionals with domain specialization modules.\n\nA great DS/ML/AI learning resource is Simplilearn. Simplilearn has been training those who desire to work in this field for years with a group of knowledgeable instructors. The drawback of this platform is that it does not offer instruction in particular domains. Simplilearn is a good choice for those who do not yet know which domain their learning should be in.\n\nLearnbay is an amazing website offering top-notch courses in data science and machine learning. With incredible features like classes that are customized to a given domain, it is one of the greatest platforms. This MNC has a fantastic Artificial Intelligence and Machine Learning Program for those who are interested in learning ML while working for a company that focuses on products. For working professionals in Bangalore, this university now offers the best machine learning and top data science courses.\n\nWhat part does domain specialization play in the success of big corporations?\n\nYou\'ll need more than just a data science degree or certification to become a data scientist. To effectively employ your abilities, knowledge, and experience, problem-solving skills are a necessity. Domain specialization has thus become a crucial component of data science. Whether you\'re a seasoned expert or just starting, having a domain-specialized field can help you obtain work. Nowadays, specialists are in greater demand than generalists. Specialized individuals are more likely to find employment more quickly.\n\nLearnbay has an excellent Artificial Intelligence and Machine Learning Program for those who want to learn ML while working in a product-based MNC. Live interactive lessons and hybrid classes are available from any location in India. There are several advantages to taking this course.\n\nSome of the top features of Learnbay are:\n\nThe IBM Group has approved all of Learnbay\'s courses. Students who excel in Machine Learning receive three certificates from Learnbay\'s Artificial Intelligence and Machine Learning Program. IBM certifications are awarded for completing a course, project, or micro-skills.\nAs part of your training, you will be able to choose from several specializations, including BFSI, Telecom, Manufacturing, and Communication. Because of Learnbay\'s domain-specific training, it has a distinct advantage. To advance in your career, you must meet the requirements of a particular industry. Learnbay has everything you need! Only students who are serious about pursuing a career in data science are accepted into Learnbay\'s Artificial Intelligence and Machine Learning Program.\nA ""hybrid learning format"" is a learning arrangement that incorporates both teaching methods. By understanding how to do it, students can modify their selected learning preferences to meet any situation.\nAlong with Delhi, Pune, and other cities, there are project innovation centers where experts from MNC and MAANG oversee online and offline project sessions.\nLearnbay Provides students of all skill levels, from beginners to experts, with real-time and capstone projects. You will work on large-scale machine learning and data science projects throughout the course. You might get a taste of what it\'s like to work in the real world as a student.\n\nThe following are some examples of data science projects:\n\nBuilding Chatbots\nDetection of Credit Card Fraud and Fake News\nRecommender Systems for Forest Fire Prediction\n\nConclusion:\n\nIf you\'re looking for machine learning courses, Learnbay is an outstanding place to start. They offer a great variety of courses that are focused on a single certification field rather than the complete pool of certifications. It contains a wealth of resources, including lectures and tests, that professionals using this platform can use to advance their ML abilities. During the event, you can also use mobile apps, career counselors, and industry experts.\n\nThank you', 'aiModelVersion': '1'}",0.0
Jatin Shukla,10mo,What are prerequisites to start learning machine learning?,"Machine Learning is currently one of the most sought-after technologies. If you're new to this topic, you should be aware of the prerequisites for Machine Learning. Before you begin, it is critical that you grasp the many principles and forms of machine learning that will assist you in this profession.

Before learning machine learning it will be easier if you know the following:

You must be familiar with variables, linear equations, function graphs, histograms, and statistical means.
You should be an excellent programmer. Because the programming activities are in Python, you should ideally have some programming knowledge. However, experienced programmers who do not have Python experience may usually complete the programming exercises.

5 Prerequisites to Learn Machine Learning:

Statistics
Probability
Linear Algebra
Calculus
Programming Languages

Can a Newcomer Master Machine Learning?

Yes!

There are thousands of online learning tools available for newcomers and beginners. Even if you have no coding knowledge, you can begin with simple tasks and progress to algorithms and their implementation.

In general, you can find a lot of courses that will help you learn the trade. One of the most popular machine learning courses is offered by Upgrad.

Let’s look at their features:

This institute is among the best, and many students and professionals select it.
They provide MS, PG, and other certificates in a variety of technologies and fields.
The curriculum's hands-on activities and projects will provide you with greater field experience.
Once a week, experts in the industry interact with learners to build their confidence and provide better guidance.
Students who choose to study abroad can receive assistance from the university in any way.

However, Upgrad has a significant disadvantage: it lacks career aid and domain specialization tools.

There are other courses as well. One of the most popular courses with no notable drawbacks is the below institute.

They offer the best machine learning courses in Bangalore which can be accessed from anywhere in the world.

Let’s look at the features:

They offer capstone and practical projects. In addition to classroom instruction, students receive project-based training. It provides real-world and capstone projects taught by industry professionals. In its Artificial Intelligence and Machine Learning Program, you could work on challenges such as Netflix recommendation algorithms, BMW pricing models, loan prediction, and credit card fraud detection.
Students who prefer the option of combining online and classroom instruction might enroll in hybrid programs.
It also offers an IBM capstone project certification, a micro-skills certificate, and a course completion certificate in the Artificial Intelligence and Machine Learning Program. As a result, you can study at your own pace while also earning an IBM certification!
In the project innovation laboratory, you can do tasks while being monitored by MAANG specialists at different locations across the country. Delhi, Bangalore, Mumbai, and Pune are home to four of the seven destinations.
As a beginner, you can take advantage of free counseling sessions. The main purpose of this session is to help you decide on the best career path for you.

Artificial Intelligence and Machine Learning Program include a domain specialization module.

It can differentiate itself by focusing on a certain area of expertise. You can become an expert in a subject of your choice thanks to vast specialized options. Human Resources, BFSI, Marketing, Banking, Sales, and Healthcare are just a few of the numerous available applications. Cloud computing and the Internet of Things (IoT) are two further areas of IT competence (to name just a few).

In conclusion, Statistics, probability, linear algebra, and calculus, as well as a programming language and data modeling, are all required. Machine Learning is a lucrative professional path to pursue, but it requires some practice and experience. It is not a task that can be completed in a single day. However, if you look at the machine learning salary, you will see that the effort is worthwhile. The prerequisites are actually not necessary if you are willing to take up a beginner's course and learn the foundation.

Happy learning!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/5iwzgdq87l2nsv4c', 'title': 'What are prerequisites to start learning machine learning?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""Machine Learning is currently one of the most sought-after technologies. If you're new to this topic, you should be aware of the prerequisites for Machine Learning. Before you begin, it is critical that you grasp the many principles and forms of machine learning that will assist you in this profession.\n\nBefore learning machine learning it will be easier if you know the following:\n\nYou must be familiar with variables, linear equations, function graphs, histograms, and statistical means.\nYou should be an excellent programmer. Because the programming activities are in Python, you should ideally have some programming knowledge. However, experienced programmers who do not have Python experience may usually complete the programming exercises.\n\n5 Prerequisites to Learn Machine Learning:\n\nStatistics\nProbability\nLinear Algebra\nCalculus\nProgramming Languages\n\nCan a Newcomer Master Machine Learning?\n\nYes!\n\nThere are thousands of online learning tools available for newcomers and beginners. Even if you have no coding knowledge, you can begin with simple tasks and progress to algorithms and their implementation.\n\nIn general, you can find a lot of courses that will help you learn the trade. One of the most popular machine learning courses is offered by Upgrad.\n\nLet’s look at their features:\n\nThis institute is among the best, and many students and professionals select it.\nThey provide MS, PG, and other certificates in a variety of technologies and fields.\nThe curriculum's hands-on activities and projects will provide you with greater field experience.\nOnce a week, experts in the industry interact with learners to build their confidence and provide better guidance.\nStudents who choose to study abroad can receive assistance from the university in any way.\n\nHowever, Upgrad has a significant disadvantage: it lacks career aid and domain specialization tools.\n\nThere are other courses as well. One of the most popular courses with no notable drawbacks is the below institute.\n\nThey offer the best machine learning courses in Bangalore which can be accessed from anywhere in the world.\n\nLet’s look at the features:\n\nThey offer capstone and practical projects. In addition to classroom instruction, students receive project-based training. It provides real-world and capstone projects taught by industry professionals. In its Artificial Intelligence and Machine Learning Program, you could work on challenges such as Netflix recommendation algorithms, BMW pricing models, loan prediction, and credit card fraud detection.\nStudents who prefer the option of combining online and classroom instruction might enroll in hybrid programs.\nIt also offers an IBM capstone project certification, a micro-skills certificate, and a course completion certificate in the Artificial Intelligence and Machine Learning Program. As a result, you can study at your own pace while also earning an IBM certification!\nIn the project innovation laboratory, you can do tasks while being monitored by MAANG specialists at different locations across the country. Delhi, Bangalore, Mumbai, and Pune are home to four of the seven destinations.\nAs a beginner, you can take advantage of free counseling sessions. The main purpose of this session is to help you decide on the best career path for you.\n\nArtificial Intelligence and Machine Learning Program include a domain specialization module.\n\nIt can differentiate itself by focusing on a certain area of expertise. You can become an expert in a subject of your choice thanks to vast specialized options. Human Resources, BFSI, Marketing,"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""Banking, Sales, and Healthcare are just a few of the numerous available applications. Cloud computing and the Internet of Things (IoT) are two further areas of IT competence (to name just a few).\n\nIn conclusion, Statistics, probability, linear algebra, and calculus, as well as a programming language and data modeling, are all required. Machine Learning is a lucrative professional path to pursue, but it requires some practice and experience. It is not a task that can be completed in a single day. However, if you look at the machine learning salary, you will see that the effort is worthwhile. The prerequisites are actually not necessary if you are willing to take up a beginner's course and learn the foundation.\n\nHappy learning!"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982738, 'subscription': 0, 'content': ""Machine Learning is currently one of the most sought-after technologies. If you're new to this topic, you should be aware of the prerequisites for Machine Learning. Before you begin, it is critical that you grasp the many principles and forms of machine learning that will assist you in this profession.\n\nBefore learning machine learning it will be easier if you know the following:\n\nYou must be familiar with variables, linear equations, function graphs, histograms, and statistical means.\nYou should be an excellent programmer. Because the programming activities are in Python, you should ideally have some programming knowledge. However, experienced programmers who do not have Python experience may usually complete the programming exercises.\n\n5 Prerequisites to Learn Machine Learning:\n\nStatistics\nProbability\nLinear Algebra\nCalculus\nProgramming Languages\n\nCan a Newcomer Master Machine Learning?\n\nYes!\n\nThere are thousands of online learning tools available for newcomers and beginners. Even if you have no coding knowledge, you can begin with simple tasks and progress to algorithms and their implementation.\n\nIn general, you can find a lot of courses that will help you learn the trade. One of the most popular machine learning courses is offered by Upgrad.\n\nLet’s look at their features:\n\nThis institute is among the best, and many students and professionals select it.\nThey provide MS, PG, and other certificates in a variety of technologies and fields.\nThe curriculum's hands-on activities and projects will provide you with greater field experience.\nOnce a week, experts in the industry interact with learners to build their confidence and provide better guidance.\nStudents who choose to study abroad can receive assistance from the university in any way.\n\nHowever, Upgrad has a significant disadvantage: it lacks career aid and domain specialization tools.\n\nThere are other courses as well. One of the most popular courses with no notable drawbacks is the below institute.\n\nThey offer the best machine learning courses in Bangalore which can be accessed from anywhere in the world.\n\nLet’s look at the features:\n\nThey offer capstone and practical projects. In addition to classroom instruction, students receive project-based training. It provides real-world and capstone projects taught by industry professionals. In its Artificial Intelligence and Machine Learning Program, you could work on challenges such as Netflix recommendation algorithms, BMW pricing models, loan prediction, and credit card fraud detection.\nStudents who prefer the option of combining online and classroom instruction might enroll in hybrid programs.\nIt also offers an IBM capstone project certification, a micro-skills certificate, and a course completion certificate in the Artificial Intelligence and Machine Learning Program. As a result, you can study at your own pace while also earning an IBM certification!\nIn the project innovation laboratory, you can do tasks while being monitored by MAANG specialists at different locations across the country. Delhi, Bangalore, Mumbai, and Pune are home to four of the seven destinations.\nAs a beginner, you can take advantage of free counseling sessions. The main purpose of this session is to help you decide on the best career path for you.\n\nArtificial Intelligence and Machine Learning Program include a domain specialization module.\n\nIt can differentiate itself by focusing on a certain area of expertise. You can become an expert in a subject of your choice thanks to vast specialized options. Human Resources, BFSI, Marketing, Banking, Sales, and Healthcare are just a few of the numerous available applications. Cloud computing and the Internet of Things (IoT) are two further areas of IT competence (to name just a few).\n\nIn conclusion, Statistics, probability, linear algebra, and calculus, as well as a programming language and data modeling, are all required. Machine Learning is a lucrative professional path to pursue, but it requires some practice and experience. It is not a task that can be completed in a single day. However, if you look at the machine learning salary, you will see that the effort is worthwhile. The prerequisites are actually not necessary if you are willing to take up a beginner's course and learn the foundation.\n\nHappy learning!"", 'aiModelVersion': '1'}",0.0
Ben Taylor,Updated 5y,What is the weirdest thing a neural network has done?,"I made a genetic neural network feedback system [3 neural networks and a genetic solver to make them do what I want] that would allow me to make fake model faces to order. Here I am creating fake faces that are female, black/white, and have high attraction. These people don’t exist, the inputs into this model are 512 random floats where I have optimized the inputs on the GAN using a genetic algorithm to maximize my other deep-network scores on beauty, gender, race. You could also include emotion, pose, etc…

I think the modeling industry is ready for some disruption, maybe not today, but in the near future. Not bad for fake models to order right? I didn’t like some of the artifacts I could see in this image so I rewarded the genetic algorithm for producing fake images that had poor JPEG compression efficiencies.

Mmmhhh funky. So I don’t like the wiggle-lee-doos on the face, but checkout that hair!

Now I wanted to produce a human that is hard to make. A “thirdsee” race hybrid, so I requested the neural network to produce a woman, attractive, that is 1/3 asian, 1/3 latin, and 1/3 white. If you think about that… that is actually hard to make since it isn’t divisible by two. The neural network produced this fake human:

More details on this project here: https://www.linkedin.com/pulse/americas-next-topbot-model-ben-taylor-deeplearning-/

So if your mom yells at you for spending too much time on Quora you can tell her you learned something… wiggly human hair disrupts JPEG compression algorithms. You learn something new everyday.

EDIT: 8/18/18 8:18am

Here is a youtube video I put together so you can watch the evolution

Also, another thing I did once these were solved for was I showed I could do selective mating. Picking two converged individuals I can produce random offspring that are related.

Then changing up the bottom parent changes it to this:

You can read more about some of this newer stuff here: Ben Taylor's answer to How are breakthroughs made in AI?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/lo9x0jqs2ctdp7yn', 'title': 'What is the weirdest thing a neural network has done?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': ""I made a genetic neural network feedback system [3 neural networks and a genetic solver to make them do what I want] that would allow me to make fake model faces to order. Here I am creating fake faces that are female, black/white, and have high attraction. These people don’t exist, the inputs into this model are 512 random floats where I have optimized the inputs on the GAN using a genetic algorithm to maximize my other deep-network scores on beauty, gender, race. You could also include emotion, pose, etc…\n\nI think the modeling industry is ready for some disruption, maybe not today, but in the near future. Not bad for fake models to order right? I didn’t like some of the artifacts I could see in this image so I rewarded the genetic algorithm for producing fake images that had poor JPEG compression efficiencies.\n\nMmmhhh funky. So I don’t like the wiggle-lee-doos on the face, but checkout that hair!\n\nNow I wanted to produce a human that is hard to make. A “thirdsee” race hybrid, so I requested the neural network to produce a woman, attractive, that is 1/3 asian, 1/3 latin, and 1/3 white. If you think about that… that is actually hard to make since it isn’t divisible by two. The neural network produced this fake human:\n\nMore details on this project here: https://www.linkedin.com/pulse/americas-next-topbot-model-ben-taylor-deeplearning-/\n\nSo if your mom yells at you for spending too much time on Quora you can tell her you learned something… wiggly human hair disrupts JPEG compression algorithms. You learn something new everyday.\n\nEDIT: 8/18/18 8:18am\n\nHere is a youtube video I put together so you can watch the evolution\n\nAlso, another thing I did once these were solved for was I showed I could do selective mating. Picking two converged individuals I can produce random offspring that are related.\n\nThen changing up the bottom parent changes it to this:\n\nYou can read more about some of this newer stuff here: Ben Taylor's answer to How are breakthroughs made in AI?"", 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 4, 'credits': 1982734, 'subscription': 0, 'content': ""I made a genetic neural network feedback system [3 neural networks and a genetic solver to make them do what I want] that would allow me to make fake model faces to order. Here I am creating fake faces that are female, black/white, and have high attraction. These people don’t exist, the inputs into this model are 512 random floats where I have optimized the inputs on the GAN using a genetic algorithm to maximize my other deep-network scores on beauty, gender, race. You could also include emotion, pose, etc…\n\nI think the modeling industry is ready for some disruption, maybe not today, but in the near future. Not bad for fake models to order right? I didn’t like some of the artifacts I could see in this image so I rewarded the genetic algorithm for producing fake images that had poor JPEG compression efficiencies.\n\nMmmhhh funky. So I don’t like the wiggle-lee-doos on the face, but checkout that hair!\n\nNow I wanted to produce a human that is hard to make. A “thirdsee” race hybrid, so I requested the neural network to produce a woman, attractive, that is 1/3 asian, 1/3 latin, and 1/3 white. If you think about that… that is actually hard to make since it isn’t divisible by two. The neural network produced this fake human:\n\nMore details on this project here: https://www.linkedin.com/pulse/americas-next-topbot-model-ben-taylor-deeplearning-/\n\nSo if your mom yells at you for spending too much time on Quora you can tell her you learned something… wiggly human hair disrupts JPEG compression algorithms. You learn something new everyday.\n\nEDIT: 8/18/18 8:18am\n\nHere is a youtube video I put together so you can watch the evolution\n\nAlso, another thing I did once these were solved for was I showed I could do selective mating. Picking two converged individuals I can produce random offspring that are related.\n\nThen changing up the bottom parent changes it to this:\n\nYou can read more about some of this newer stuff here: Ben Taylor's answer to How are breakthroughs made in AI?"", 'aiModelVersion': '1'}",0.9997
Dominic Ligot,Updated 4y,What is something about the field of data science that only a professional would know?,"No one taught this in school:

There's a lot less hand math than you would expect. There's more coding, data-prep and problem solving than hand math.
What HR usually looks for, isn't what the hiring manager needs.
The sexy algorithms are the smallest part of the process vs. data-prep, problem definitions, and presentations.
Usually your employer has no clue how data science works, and can be implemented to improve business. Part of your job is to evangelize the use of it.
The SQL is equally, and occasionally even more important than your R or Python code.
Business audiences don't care about your model metrics (R2, AUC, AR, etc.). They want ROI-related calculations, which you aren't prepared to do. Talking to the accountants will also be useless as most of them do not understand data science (nor want to).
Data privacy issues will wreck your prospects.
At some point, the hardest part about a project - is getting the approval or funding for it.
Some people in the company DO NOT want your data-driven insight - because it endangers their careers.
For years, your IT department will not figure out what tech stack works for data science. A lot of it reminds them of that failed data warehouse several years ago.
In-fighting and politics will stymie your efforts at contributing to digital transformation. Your academic skills are no match for their political savvy. You might later want to try understanding and competing at the political level too - this will be a costly mistake.
Eventually, what kind of projects you did will be a better gauge of your career than salaries or titles.
Executives will be divided into 3 buckets when dealing with you: a) people who are intimidated by your skills, b) people who think they are always better than the data guy because of experience, c) people who will pretend to know data science for brownie points.
There are always 2 types of projects: the big-ticket press-release that everyone is rooting for but delivers little value, and the ignored by the dumpster project that can really make a difference in the company processes, revenue, and efficiency. You need both to survive.
The novelty of a PhD can fade quickly if you’re not careful.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/lq4jn3wfvzsh0a8u', 'title': 'What is something about the field of data science that only a professional would know?', 'score': {'original': 0.9971, 'ai': 0.0029}, 'blocks': [{'text': ""No one taught this in school:\n\nThere's a lot less hand math than you would expect. There's more coding, data-prep and problem solving than hand math.\nWhat HR usually looks for, isn't what the hiring manager needs.\nThe sexy algorithms are the smallest part of the process vs. data-prep, problem definitions, and presentations.\nUsually your employer has no clue how data science works, and can be implemented to improve business. Part of your job is to evangelize the use of it.\nThe SQL is equally, and occasionally even more important than your R or Python code.\nBusiness audiences don't care about your model metrics (R2, AUC, AR, etc.). They want ROI-related calculations, which you aren't prepared to do. Talking to the accountants will also be useless as most of them do not understand data science (nor want to).\nData privacy issues will wreck your prospects.\nAt some point, the hardest part about a project - is getting the approval or funding for it.\nSome people in the company DO NOT want your data-driven insight - because it endangers their careers.\nFor years, your IT department will not figure out what tech stack works for data science. A lot of it reminds them of that failed data warehouse several years ago.\nIn-fighting and politics will stymie your efforts at contributing to digital transformation. Your academic skills are no match for their political savvy. You might later want to try understanding and competing at the political level too - this will be a costly mistake.\nEventually, what kind of projects you did will be a better gauge of your career than salaries or titles.\nExecutives will be divided into 3 buckets when dealing with you: a) people who are intimidated by your skills, b) people who think they are always better than the data guy because of experience, c) people who will pretend to know data science for brownie points.\nThere are always 2 types of projects: the big-ticket press-release that everyone is rooting for but delivers little value, and the ignored by the dumpster project that can really make a difference in the company processes, revenue, and efficiency. You need both to survive.\nThe novelty of a PhD can fade quickly if you’re not careful."", 'result': {'fake': 0.0029, 'real': 0.9971}, 'status': 'success'}], 'credits_used': 4, 'credits': 1982730, 'subscription': 0, 'content': ""No one taught this in school:\n\nThere's a lot less hand math than you would expect. There's more coding, data-prep and problem solving than hand math.\nWhat HR usually looks for, isn't what the hiring manager needs.\nThe sexy algorithms are the smallest part of the process vs. data-prep, problem definitions, and presentations.\nUsually your employer has no clue how data science works, and can be implemented to improve business. Part of your job is to evangelize the use of it.\nThe SQL is equally, and occasionally even more important than your R or Python code.\nBusiness audiences don't care about your model metrics (R2, AUC, AR, etc.). They want ROI-related calculations, which you aren't prepared to do. Talking to the accountants will also be useless as most of them do not understand data science (nor want to).\nData privacy issues will wreck your prospects.\nAt some point, the hardest part about a project - is getting the approval or funding for it.\nSome people in the company DO NOT want your data-driven insight - because it endangers their careers.\nFor years, your IT department will not figure out what tech stack works for data science. A lot of it reminds them of that failed data warehouse several years ago.\nIn-fighting and politics will stymie your efforts at contributing to digital transformation. Your academic skills are no match for their political savvy. You might later want to try understanding and competing at the political level too - this will be a costly mistake.\nEventually, what kind of projects you did will be a better gauge of your career than salaries or titles.\nExecutives will be divided into 3 buckets when dealing with you: a) people who are intimidated by your skills, b) people who think they are always better than the data guy because of experience, c) people who will pretend to know data science for brownie points.\nThere are always 2 types of projects: the big-ticket press-release that everyone is rooting for but delivers little value, and the ignored by the dumpster project that can really make a difference in the company processes, revenue, and efficiency. You need both to survive.\nThe novelty of a PhD can fade quickly if you’re not careful."", 'aiModelVersion': '1'}",0.9971
Kurt Guntheroth,4y,"Why is language translation generally considered a machine learning problem, instead of a symbolic one? Aren't the rules of grammars, despite their abundance, known and programmable?","Take a look at a famously ambiguous sentence, “They are flying planes.”

Is it about planes that are flying, or about the pilots that are flying them?
Is it about aircraft or woodworking tools?
Is it about a subset of the planes (either aircraft or tools) that are flying or is the implication that some planes can’t fly?

Marvin Minsky figured out in the 1960s that grammar rules weren’t very helpful in analyzing human speech, because it contained so much context. If you only know about aircraft, and not about woodworking, you might totally misunderstand the meaning. If you only had this one sentence in isolation, you would have no way of knowing who “they” were.

It gets worse, because people usually can suss out the meaning of sentences that are horribly ungrammatical. A language translator must be able to comprehend a drunk or a foreigner unfamiliar with English or it is of limited use.

Grammar rules are too rigid. Machine learning better accommodates the eccentricities of human communication.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/bnwy1a7puch6i0e5', 'title': ""Why is language translation generally considered a machine learning problem, instead of a symbolic one? Aren't the rules of grammars, despite their abundance, known and programmable?"", 'score': {'original': 0.9991, 'ai': 0.0009}, 'blocks': [{'text': 'Take a look at a famously ambiguous sentence, “They are flying planes.”\n\nIs it about planes that are flying, or about the pilots that are flying them?\nIs it about aircraft or woodworking tools?\nIs it about a subset of the planes (either aircraft or tools) that are flying or is the implication that some planes can’t fly?\n\nMarvin Minsky figured out in the 1960s that grammar rules weren’t very helpful in analyzing human speech, because it contained so much context. If you only know about aircraft, and not about woodworking, you might totally misunderstand the meaning. If you only had this one sentence in isolation, you would have no way of knowing who “they” were.\n\nIt gets worse, because people usually can suss out the meaning of sentences that are horribly ungrammatical. A language translator must be able to comprehend a drunk or a foreigner unfamiliar with English or it is of limited use.\n\nGrammar rules are too rigid. Machine learning better accommodates the eccentricities of human communication.', 'result': {'fake': 0.0009, 'real': 0.9991}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982728, 'subscription': 0, 'content': 'Take a look at a famously ambiguous sentence, “They are flying planes.”\n\nIs it about planes that are flying, or about the pilots that are flying them?\nIs it about aircraft or woodworking tools?\nIs it about a subset of the planes (either aircraft or tools) that are flying or is the implication that some planes can’t fly?\n\nMarvin Minsky figured out in the 1960s that grammar rules weren’t very helpful in analyzing human speech, because it contained so much context. If you only know about aircraft, and not about woodworking, you might totally misunderstand the meaning. If you only had this one sentence in isolation, you would have no way of knowing who “they” were.\n\nIt gets worse, because people usually can suss out the meaning of sentences that are horribly ungrammatical. A language translator must be able to comprehend a drunk or a foreigner unfamiliar with English or it is of limited use.\n\nGrammar rules are too rigid. Machine learning better accommodates the eccentricities of human communication.', 'aiModelVersion': '1'}",0.9991
S M,6y,So many people are learning machine learning. What should I do to stand out?,"Think outside “tech”.

Think about how machine learning concepts can be applied to fields that traditionally have completely ignored the power of data.

Case in point #1: Politics.

Nate Silver’s FiveThirtyEight became a sensation by applying existing machine learning concepts to political data, because politics simply hadn’t caught up to advancements in machine learning. And that’s largely true even today. A Stanford Political Science prof who I know, still says that Washington DC has been at least 10 years behind Silicon Valley at every stage.
Case in point #2: Journalism.

The New York Times was probably the first major news provider to hire a large team of data scientists and a Chief Data Scientist (Chris Wiggins), because they wanted to understand their readers. And it seems to be working!

Some other similar fields which had traditionally ignored data (largely because they didn’t have any) are Sports, Energy and Retail.

The future of applied machine learning will largely come from people who have additional fields of expertise outside tech.

So, if you want to stand out, I encourage you to look at the world through the lens of data. For every activity you see in the real world, think “How can this be improved using data?”. Don’t verbalize it, though. That would be weird.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/2yk37m1xpl54ast6', 'title': 'So many people are learning machine learning. What should I do to stand out?', 'score': {'original': 0.9981, 'ai': 0.0019}, 'blocks': [{'text': 'Think outside “tech”.\n\nThink about how machine learning concepts can be applied to fields that traditionally have completely ignored the power of data.\n\nCase in point #1: Politics.\n\nNate Silver’s FiveThirtyEight became a sensation by applying existing machine learning concepts to political data, because politics simply hadn’t caught up to advancements in machine learning. And that’s largely true even today. A Stanford Political Science prof who I know, still says that Washington DC has been at least 10 years behind Silicon Valley at every stage.\nCase in point #2: Journalism.\n\nThe New York Times was probably the first major news provider to hire a large team of data scientists and a Chief Data Scientist (Chris Wiggins), because they wanted to understand their readers. And it seems to be working!\n\nSome other similar fields which had traditionally ignored data (largely because they didn’t have any) are Sports, Energy and Retail.\n\nThe future of applied machine learning will largely come from people who have additional fields of expertise outside tech.\n\nSo, if you want to stand out, I encourage you to look at the world through the lens of data. For every activity you see in the real world, think “How can this be improved using data?”. Don’t verbalize it, though. That would be weird.', 'result': {'fake': 0.0019, 'real': 0.9981}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982725, 'subscription': 0, 'content': 'Think outside “tech”.\n\nThink about how machine learning concepts can be applied to fields that traditionally have completely ignored the power of data.\n\nCase in point #1: Politics.\n\nNate Silver’s FiveThirtyEight became a sensation by applying existing machine learning concepts to political data, because politics simply hadn’t caught up to advancements in machine learning. And that’s largely true even today. A Stanford Political Science prof who I know, still says that Washington DC has been at least 10 years behind Silicon Valley at every stage.\nCase in point #2: Journalism.\n\nThe New York Times was probably the first major news provider to hire a large team of data scientists and a Chief Data Scientist (Chris Wiggins), because they wanted to understand their readers. And it seems to be working!\n\nSome other similar fields which had traditionally ignored data (largely because they didn’t have any) are Sports, Energy and Retail.\n\nThe future of applied machine learning will largely come from people who have additional fields of expertise outside tech.\n\nSo, if you want to stand out, I encourage you to look at the world through the lens of data. For every activity you see in the real world, think “How can this be improved using data?”. Don’t verbalize it, though. That would be weird.', 'aiModelVersion': '1'}",0.9981
Indira Securities,2y,Can machine learning algorithms really help in trading?,"Compared to conventional algorithmic programs, machine learning has a range of major advantages. Through manual trading is often a time-consuming manual process, the process of machine learning algorithm will speed up the search for successful algo-trading strategies. It also increases the number of markets an individual can keep track of and respond to. You have a competitive advantage if you can automate a system that others do manually. You would have more options if you can increase the number of markets you invest in.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/pe1fxyj9md64atiu', 'title': 'Can machine learning algorithms really help in trading?', 'score': {'original': 0.4775, 'ai': 0.5225}, 'blocks': [{'text': 'Compared to conventional algorithmic programs, machine learning has a range of major advantages. Through manual trading is often a time-consuming manual process, the process of machine learning algorithm will speed up the search for successful algo-trading strategies. It also increases the number of markets an individual can keep track of and respond to. You have a competitive advantage if you can automate a system that others do manually. You would have more options if you can increase the number of markets you invest in.', 'result': {'fake': 0.5225, 'real': 0.4775}, 'status': 'success'}], 'credits_used': 1, 'credits': 1982724, 'subscription': 0, 'content': 'Compared to conventional algorithmic programs, machine learning has a range of major advantages. Through manual trading is often a time-consuming manual process, the process of machine learning algorithm will speed up the search for successful algo-trading strategies. It also increases the number of markets an individual can keep track of and respond to. You have a competitive advantage if you can automate a system that others do manually. You would have more options if you can increase the number of markets you invest in.', 'aiModelVersion': '1'}",0.4775
Abhishek Ghose,10y,Explain VC dimension and shattering in lucid Way?,"Intuition

Let's start with the representation of a learning problem. Let's assume we are looking at a classification task with 2 labels - ""+"" and ""-"". The data points are plotted in a n-dimensional space, and in doing a classification, what you are essentially doing is finding out a surface that has only points with the ""+"" label on one side of it and points with ""-"" labels on the other side. And you know which side has which label.

Why do we need this arrangement?

So when a new data point comes in, you want to:


Find out which side of this surface it falls on
Announce the label of the new data point to be the label for this side


The rules of the game established, let's step back a bit and think about how you would look for this separating surface or classifier. Do you try every possible surface out there? Or, is there a scientific manner in which you can narrow down your search?

This is where the VC dimension comes in - it enables you to conduct your search in a principled way. For a family of surfaces - or to be precise, a family of functions - the VC dimension gives you a number on which you can peg its capability to separate labels.

The general idea is that the VC dimension points you to a reasonable family of functions to inspect. You pick a specific member within this family based on the exact data-set at hand.

As an example of a family, a family could be one of hyperplanes (a plane in n-dimensions). Each member of this family is a specific hyperplane uniquely identified by a particular value for its perpendicular distance from the origin, and the direction of this perpendicular wrt the origin.

It should be noted that the VC dimension is an estimate of sorts, and there are cases where this number might not be very helpful or can actually be counter-intuitive.


Calculating the VC dimension

We now know what the VC dimension is supposed to do. Let's now look at how it's calculated.

If you have n data points, you have 2^n possible labellings. For each of these labellings, if you can draw a function from your function family that separates the labels, then the set of n points is said to have been shattered by your family of functions. The maximum n which you can shatter is the VC dimension, h, of your function family.

Thus, the VC dimension gives you a measure for confidence for the ""separating capability"" of the function family by looking at how many points you can actually separate knowing nothing about the distribution of labels.

There is a catch here though - a function with a VC dimension of h, in general, will not be able to shatter all possible sets of h points; all that is guaranteed is that there is some set of h points that can be shattered.

One of the reasons why you should think of this as an estimate.

An example

If your data has 2 dimensions and the function family we're looking at is hyperplanes (which, in 2D, are lines), the VC dimension is 3. Since:



We can find at least one set of 3 points in 2D all of whose 8 possible labellings can be separated by some hyperplane. The diagram below shows this. The arrow points to the side with the points labelled blue.

We cannot find any set of 4 points, all of whose possible labellings, are separable by hyperplanes.


Are larger VC dimensions good?

So you would want a function family with a high VC dimension, right? Turns out, things aren't that simple. Remember that what you eventually want is that your model predicts well. This eventual performance is denoted by a quantity called Risk, and it is bounded thus:

Risk <= Empirical Risk + f(h)

The Empirical Risk is the classification error you obtain on your training set. The second term on the RHS, f(h), is a function that increases with the VC dimension, h.*

So if you want to keep your Risk at bay and you have a low empirical risk you do not want a high VC dimension. For two function families, both of which give you the same empirical risk, you would want to pick the one with the lower VC dimension. A high VC dimension in this case tells you the function maybe too flexible and the empirical risk might have come at the cost of overfitting the data.

Thus, if you have another function family with a lower h, and you have managed to obtain the same empirical risk with it, why risk overfitting ? Pick this.

But then again, if you are looking at a function family with too low a h - which implies poor separating capability - you would probably end up with high empirical risk, since it might not fit your training data well at all :)

So there is a trade-off involved in picking a function family with a high VC dimension.

*NOTE: I have made a few things imprecise to make them simple. You are encouraged to dig deeper :)


Image source: http://research.microsoft.com/pubs/67119/svmtutorial.pdf","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/jvqguepl15s8hf4z', 'title': 'Explain VC dimension and shattering in lucid Way?', 'score': {'original': 0.99575, 'ai': 0.00425}, 'blocks': [{'text': 'Intuition\n\nLet\'s start with the representation of a learning problem. Let\'s assume we are looking at a classification task with 2 labels - ""+"" and ""-"". The data points are plotted in a n-dimensional space, and in doing a classification, what you are essentially doing is finding out a surface that has only points with the ""+"" label on one side of it and points with ""-"" labels on the other side. And you know which side has which label.\n\nWhy do we need this arrangement?\n\nSo when a new data point comes in, you want to:\n\n\nFind out which side of this surface it falls on\nAnnounce the label of the new data point to be the label for this side\n\n\nThe rules of the game established, let\'s step back a bit and think about how you would look for this separating surface or classifier. Do you try every possible surface out there? Or, is there a scientific manner in which you can narrow down your search?\n\nThis is where the VC dimension comes in - it enables you to conduct your search in a principled way. For a family of surfaces - or to be precise, a family of functions - the VC dimension gives you a number on which you can peg its capability to separate labels.\n\nThe general idea is that the VC dimension points you to a reasonable family of functions to inspect. You pick a specific member within this family based on the exact data-set at hand.\n\nAs an example of a family, a family could be one of hyperplanes (a plane in n-dimensions). Each member of this family is a specific hyperplane uniquely identified by a particular value for its perpendicular distance from the origin, and the direction of this perpendicular wrt the origin.\n\nIt should be noted that the VC dimension is an estimate of sorts, and there are cases where this number might not be very helpful or can actually be counter-intuitive.\n\n\nCalculating the VC dimension\n\nWe now know what the VC dimension is supposed to do. Let\'s now look at how it\'s calculated.\n\nIf you have n data points, you have 2^n possible labellings. For each of these labellings, if you can draw a function from your function family that separates the labels, then the set of n points is said to have been shattered by your family of functions. The maximum n which you can shatter is the VC dimension, h, of your function family.\n\nThus, the VC dimension gives you a measure for confidence for the ""separating capability"" of the function family by looking at how many points you can actually separate knowing nothing about the distribution of labels.\n\nThere is a catch here though - a function with a VC dimension of h, in general, will not be able to shatter all possible sets of h points; all that is guaranteed is that there is some set of h points that can be shattered.\n\nOne of the reasons why you should think of this as an estimate.\n\nAn example\n\nIf your data has 2 dimensions and the function family we\'re looking at is hyperplanes (which, in 2D,', 'result': {'fake': 0.4099, 'real': 0.5901}, 'status': 'success'}, {'text': ""are lines), the VC dimension is 3. Since:\n\n\n\nWe can find at least one set of 3 points in 2D all of whose 8 possible labellings can be separated by some hyperplane. The diagram below shows this. The arrow points to the side with the points labelled blue.\n\nWe cannot find any set of 4 points, all of whose possible labellings, are separable by hyperplanes.\n\n\nAre larger VC dimensions good?\n\nSo you would want a function family with a high VC dimension, right? Turns out, things aren't that simple. Remember that what you eventually want is that your model predicts well. This eventual performance is denoted by a quantity called Risk, and it is bounded thus:\n\nRisk <= Empirical Risk + f(h)\n\nThe Empirical Risk is the classification error you obtain on your training set. The second term on the RHS, f(h), is a function that increases with the VC dimension, h.*\n\nSo if you want to keep your Risk at bay and you have a low empirical risk you do not want a high VC dimension. For two function families, both of which give you the same empirical risk, you would want to pick the one with the lower VC dimension. A high VC dimension in this case tells you the function maybe too flexible and the empirical risk might have come at the cost of overfitting the data.\n\nThus, if you have another function family with a lower h, and you have managed to obtain the same empirical risk with it, why risk overfitting ? Pick this.\n\nBut then again, if you are looking at a function family with too low a h - which implies poor separating capability - you would probably end up with high empirical risk, since it might not fit your training data well at all :)\n\nSo there is a trade-off involved in picking a function family with a high VC dimension.\n\n*NOTE: I have made a few things imprecise to make them simple. You are encouraged to dig deeper :)\n\n\nImage source: http://research.microsoft.com/pubs/67119/svmtutorial.pdf"", 'result': {'fake': 0.1575, 'real': 0.8425}, 'status': 'success'}], 'credits_used': 9, 'credits': 1982715, 'subscription': 0, 'content': 'Intuition\n\nLet\'s start with the representation of a learning problem. Let\'s assume we are looking at a classification task with 2 labels - ""+"" and ""-"". The data points are plotted in a n-dimensional space, and in doing a classification, what you are essentially doing is finding out a surface that has only points with the ""+"" label on one side of it and points with ""-"" labels on the other side. And you know which side has which label.\n\nWhy do we need this arrangement?\n\nSo when a new data point comes in, you want to:\n\n\nFind out which side of this surface it falls on\nAnnounce the label of the new data point to be the label for this side\n\n\nThe rules of the game established, let\'s step back a bit and think about how you would look for this separating surface or classifier. Do you try every possible surface out there? Or, is there a scientific manner in which you can narrow down your search?\n\nThis is where the VC dimension comes in - it enables you to conduct your search in a principled way. For a family of surfaces - or to be precise, a family of functions - the VC dimension gives you a number on which you can peg its capability to separate labels.\n\nThe general idea is that the VC dimension points you to a reasonable family of functions to inspect. You pick a specific member within this family based on the exact data-set at hand.\n\nAs an example of a family, a family could be one of hyperplanes (a plane in n-dimensions). Each member of this family is a specific hyperplane uniquely identified by a particular value for its perpendicular distance from the origin, and the direction of this perpendicular wrt the origin.\n\nIt should be noted that the VC dimension is an estimate of sorts, and there are cases where this number might not be very helpful or can actually be counter-intuitive.\n\n\nCalculating the VC dimension\n\nWe now know what the VC dimension is supposed to do. Let\'s now look at how it\'s calculated.\n\nIf you have n data points, you have 2^n possible labellings. For each of these labellings, if you can draw a function from your function family that separates the labels, then the set of n points is said to have been shattered by your family of functions. The maximum n which you can shatter is the VC dimension, h, of your function family.\n\nThus, the VC dimension gives you a measure for confidence for the ""separating capability"" of the function family by looking at how many points you can actually separate knowing nothing about the distribution of labels.\n\nThere is a catch here though - a function with a VC dimension of h, in general, will not be able to shatter all possible sets of h points; all that is guaranteed is that there is some set of h points that can be shattered.\n\nOne of the reasons why you should think of this as an estimate.\n\nAn example\n\nIf your data has 2 dimensions and the function family we\'re looking at is hyperplanes (which, in 2D, are lines), the VC dimension is 3. Since:\n\n\n\nWe can find at least one set of 3 points in 2D all of whose 8 possible labellings can be separated by some hyperplane. The diagram below shows this. The arrow points to the side with the points labelled blue.\n\nWe cannot find any set of 4 points, all of whose possible labellings, are separable by hyperplanes.\n\n\nAre larger VC dimensions good?\n\nSo you would want a function family with a high VC dimension, right? Turns out, things aren\'t that simple. Remember that what you eventually want is that your model predicts well. This eventual performance is denoted by a quantity called Risk, and it is bounded thus:\n\nRisk <= Empirical Risk + f(h)\n\nThe Empirical Risk is the classification error you obtain on your training set. The second term on the RHS, f(h), is a function that increases with the VC dimension, h.*\n\nSo if you want to keep your Risk at bay and you have a low empirical risk you do not want a high VC dimension. For two function families, both of which give you the same empirical risk, you would want to pick the one with the lower VC dimension. A high VC dimension in this case tells you the function maybe too flexible and the empirical risk might have come at the cost of overfitting the data.\n\nThus, if you have another function family with a lower h, and you have managed to obtain the same empirical risk with it, why risk overfitting ? Pick this.\n\nBut then again, if you are looking at a function family with too low a h - which implies poor separating capability - you would probably end up with high empirical risk, since it might not fit your training data well at all :)\n\nSo there is a trade-off involved in picking a function family with a high VC dimension.\n\n*NOTE: I have made a few things imprecise to make them simple. You are encouraged to dig deeper :)\n\n\nImage source: http://research.microsoft.com/pubs/67119/svmtutorial.pdf', 'aiModelVersion': '1'}",0.99575
Anuj Patel,7y,What's the most professional way to make a new dataset?,"Asked to answer,

Steps involved to understand, clean and prepare your data for building your predictive new data-set:

Variable Identification
Univariate Analysis
Bi-variate Analysis
Missing values treatment
Outlier treatment
Variable transformation
Variable creation

These are the core steps but to create a new data-set as per your requirement, it is better to understand these points in detail and in depth.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/fo59y28wbemqtuxn', 'title': ""What's the most professional way to make a new dataset?"", 'score': {'original': 0.6759, 'ai': 0.3241}, 'blocks': [{'text': 'Asked to answer,\n\nSteps involved to understand, clean and prepare your data for building your predictive new data-set:\n\nVariable Identification\nUnivariate Analysis\nBi-variate Analysis\nMissing values treatment\nOutlier treatment\nVariable transformation\nVariable creation\n\nThese are the core steps but to create a new data-set as per your requirement, it is better to understand these points in detail and in depth.', 'result': {'fake': 0.3241, 'real': 0.6759}, 'status': 'success'}], 'credits_used': 1, 'credits': 1982714, 'subscription': 0, 'content': 'Asked to answer,\n\nSteps involved to understand, clean and prepare your data for building your predictive new data-set:\n\nVariable Identification\nUnivariate Analysis\nBi-variate Analysis\nMissing values treatment\nOutlier treatment\nVariable transformation\nVariable creation\n\nThese are the core steps but to create a new data-set as per your requirement, it is better to understand these points in detail and in depth.', 'aiModelVersion': '1'}",0.6759
Franklin Veaux,2y,Are all computers considered Turing machines?,"Kind of.

All computers are limited Turing machine. An idealized Turing machine has an infinite tape—that is, it’s capable of storing an infinite amount of information. Obviously no real computer has infinite storage, or else I’d take even more photos of my cat than I already do.

But except for the fact that an ideal Turing machine has infinite storage and no computer does, yes.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/u50kzdewm4s8p97a', 'title': 'Are all computers considered Turing machines?', 'score': {'original': 0.9887, 'ai': 0.0113}, 'blocks': [{'text': 'Kind of.\n\nAll computers are limited Turing machine. An idealized Turing machine has an infinite tape—that is, it’s capable of storing an infinite amount of information. Obviously no real computer has infinite storage, or else I’d take even more photos of my cat than I already do.\n\nBut except for the fact that an ideal Turing machine has infinite storage and no computer does, yes.', 'result': {'fake': 0.0113, 'real': 0.9887}, 'status': 'success'}], 'credits_used': 1, 'credits': 1982713, 'subscription': 0, 'content': 'Kind of.\n\nAll computers are limited Turing machine. An idealized Turing machine has an infinite tape—that is, it’s capable of storing an infinite amount of information. Obviously no real computer has infinite storage, or else I’d take even more photos of my cat than I already do.\n\nBut except for the fact that an ideal Turing machine has infinite storage and no computer does, yes.', 'aiModelVersion': '1'}",0.9887
Aaron Brown,4y,"In linear regression, it is generally assumed that the errors are distributed normally with unknown variance, hence the regression coefficient t-stats have the t-distribution if the null hypothesis is true. But what if the errors are not normal?","I don’t like the other answers. Too many statistics classes teach people the goal of statistics is to make all the diagnostics come out right.

Any time you find a violation of model assumptions you have an opportunity to improve your model. Finding that errors do not follow a Gaussian distribution means you know more about your data, not less. When a diagnostic test fails that’s good news, not bad, at least if your goal is to learn from your data rather than to pass an exam, publish a paper or get a drug approved.

The reason the Gaussian distribution is special is not because it makes all the mathematics for fitting and significance testing simple. For the last 60 years or so that has been completely irrelevant since you can do all the fitting and testing you need by computer simulation or numerical methods. 100 years ago it was important to be able to have a formula you could do by hand and tests you could look up in a table. No more.

The Gaussian distribution can be plausibly explained as the accumulation of a very large number of reasonably independent factors, none of which individually has a strong effect. If you can decompose your data into signal and white noise, it’s reasonable to guess that you’ve done all you can.

But if your residual has any other distribution, or any pattern, that’s something you can explore for additional information. Are there outliers? Very interesting, why? Are there a few big errors in one direction and lots of small errors in the other? Hmmm. Do the errors seem to have a limited range? That could mean something.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ulgekymr6oxfz320', 'title': 'In linear regression, it is generally assumed that the errors are distributed normally with unknown variance, hence the regression coefficient t-stats have the t-distribution if the null hypothesis is true. But what if the errors are not normal?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'I don’t like the other answers. Too many statistics classes teach people the goal of statistics is to make all the diagnostics come out right.\n\nAny time you find a violation of model assumptions you have an opportunity to improve your model. Finding that errors do not follow a Gaussian distribution means you know more about your data, not less. When a diagnostic test fails that’s good news, not bad, at least if your goal is to learn from your data rather than to pass an exam, publish a paper or get a drug approved.\n\nThe reason the Gaussian distribution is special is not because it makes all the mathematics for fitting and significance testing simple. For the last 60 years or so that has been completely irrelevant since you can do all the fitting and testing you need by computer simulation or numerical methods. 100 years ago it was important to be able to have a formula you could do by hand and tests you could look up in a table. No more.\n\nThe Gaussian distribution can be plausibly explained as the accumulation of a very large number of reasonably independent factors, none of which individually has a strong effect. If you can decompose your data into signal and white noise, it’s reasonable to guess that you’ve done all you can.\n\nBut if your residual has any other distribution, or any pattern, that’s something you can explore for additional information. Are there outliers? Very interesting, why? Are there a few big errors in one direction and lots of small errors in the other? Hmmm. Do the errors seem to have a limited range? That could mean something.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982710, 'subscription': 0, 'content': 'I don’t like the other answers. Too many statistics classes teach people the goal of statistics is to make all the diagnostics come out right.\n\nAny time you find a violation of model assumptions you have an opportunity to improve your model. Finding that errors do not follow a Gaussian distribution means you know more about your data, not less. When a diagnostic test fails that’s good news, not bad, at least if your goal is to learn from your data rather than to pass an exam, publish a paper or get a drug approved.\n\nThe reason the Gaussian distribution is special is not because it makes all the mathematics for fitting and significance testing simple. For the last 60 years or so that has been completely irrelevant since you can do all the fitting and testing you need by computer simulation or numerical methods. 100 years ago it was important to be able to have a formula you could do by hand and tests you could look up in a table. No more.\n\nThe Gaussian distribution can be plausibly explained as the accumulation of a very large number of reasonably independent factors, none of which individually has a strong effect. If you can decompose your data into signal and white noise, it’s reasonable to guess that you’ve done all you can.\n\nBut if your residual has any other distribution, or any pattern, that’s something you can explore for additional information. Are there outliers? Very interesting, why? Are there a few big errors in one direction and lots of small errors in the other? Hmmm. Do the errors seem to have a limited range? That could mean something.', 'aiModelVersion': '1'}",0.9997
Deeksha Agarwal,6y,"What is training, learning and testing in machine learning?","In Machine Learning, we basically try to create a model to predict on the test data. So, we use the training data to fit the model and testing data to test it. The models generated are to predict the results unknown which is named as the test set. As you pointed out, the dataset is divided into train and test set in order to check accuracies, precision by training and testing it on it.

The proportion to be divided is completely up to you and the task you face. It is not essential that 70% of the data has to be for training and rest for testing. It completely depends on the dataset being used and the task to be accomplished. For example, a simple dataset like Reuters. So, assume that we trained it on 50% data and tested it on rest 50%, the precision will be different from training it on 90% or so. This is mostly because in Machine Learning, the bigger the dataset to train is better. You can refer this paper, which tells the precision values based on the dataset size. It now depends on you, what precision or accuracy you need to achieve based on your task.

This said so, how would you predict the results for which you do not have the answer? (The model is ultimately being trained to predict results for which we do not have the answer). I would like to add on about validation dataset here.

Sets:

Training Set: Here, you have the complete training dataset. You can extract features and train to fit a model and so on.
Validation Set: This is crucial to choose the right parameters for your estimator. We can divide the training set into train set and validation set. Based on the validation test results, the model can be trained(for instance, changing parameters, classifiers). This will help us get the most optimized model.
Testing Set: Here, once the model is obtained, you can predict using the model obtained on the training set.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/g7ykfs4puq32bira', 'title': 'What is training, learning and testing in machine learning?', 'score': {'original': 0.9983, 'ai': 0.0017}, 'blocks': [{'text': 'In Machine Learning, we basically try to create a model to predict on the test data. So, we use the training data to fit the model and testing data to test it. The models generated are to predict the results unknown which is named as the test set. As you pointed out, the dataset is divided into train and test set in order to check accuracies, precision by training and testing it on it.\n\nThe proportion to be divided is completely up to you and the task you face. It is not essential that 70% of the data has to be for training and rest for testing. It completely depends on the dataset being used and the task to be accomplished. For example, a simple dataset like Reuters. So, assume that we trained it on 50% data and tested it on rest 50%, the precision will be different from training it on 90% or so. This is mostly because in Machine Learning, the bigger the dataset to train is better. You can refer this paper, which tells the precision values based on the dataset size. It now depends on you, what precision or accuracy you need to achieve based on your task.\n\nThis said so, how would you predict the results for which you do not have the answer? (The model is ultimately being trained to predict results for which we do not have the answer). I would like to add on about validation dataset here.\n\nSets:\n\nTraining Set: Here, you have the complete training dataset. You can extract features and train to fit a model and so on.\nValidation Set: This is crucial to choose the right parameters for your estimator. We can divide the training set into train set and validation set. Based on the validation test results, the model can be trained(for instance, changing parameters, classifiers). This will help us get the most optimized model.\nTesting Set: Here, once the model is obtained, you can predict using the model obtained on the training set.', 'result': {'fake': 0.0017, 'real': 0.9983}, 'status': 'success'}], 'credits_used': 4, 'credits': 1982706, 'subscription': 0, 'content': 'In Machine Learning, we basically try to create a model to predict on the test data. So, we use the training data to fit the model and testing data to test it. The models generated are to predict the results unknown which is named as the test set. As you pointed out, the dataset is divided into train and test set in order to check accuracies, precision by training and testing it on it.\n\nThe proportion to be divided is completely up to you and the task you face. It is not essential that 70% of the data has to be for training and rest for testing. It completely depends on the dataset being used and the task to be accomplished. For example, a simple dataset like Reuters. So, assume that we trained it on 50% data and tested it on rest 50%, the precision will be different from training it on 90% or so. This is mostly because in Machine Learning, the bigger the dataset to train is better. You can refer this paper, which tells the precision values based on the dataset size. It now depends on you, what precision or accuracy you need to achieve based on your task.\n\nThis said so, how would you predict the results for which you do not have the answer? (The model is ultimately being trained to predict results for which we do not have the answer). I would like to add on about validation dataset here.\n\nSets:\n\nTraining Set: Here, you have the complete training dataset. You can extract features and train to fit a model and so on.\nValidation Set: This is crucial to choose the right parameters for your estimator. We can divide the training set into train set and validation set. Based on the validation test results, the model can be trained(for instance, changing parameters, classifiers). This will help us get the most optimized model.\nTesting Set: Here, once the model is obtained, you can predict using the model obtained on the training set.', 'aiModelVersion': '1'}",0.9983
Adam Profitt,9y,"What are some software and skills that every data scientist should know (excluding R, Matlab, and Hadoop)? Also, what are some resources for learning Hadoop?","I more or less work as a data scientist. My title isn't that, but it doesn't matter since that's what I do.

Here are some skill sets I've found helpful outlined as a list with no real order :

1. Statistics (Theory and Applied Analytics)

This is the core of what you can offer as a data scientist. A company can always get a database developer, or software engineer. They can't usually get a software engineer that knows their stats. Therefore, if you only know the basics about standard error, bootstrapping, confidence intervals or bayesian statistics, you better learn more.

I'd suggest picking up ""All of Statistics"" by Wasserman, and ""An Introduction to Statistical Learning"" by James/Witten/Hastie/Tibshirani. The latter is available free. For the less mathematically literate ""Think Stats"" and ""Think Bayes"" are decent and both are free.

2. Analytics Software (Descriptive / Inferential / Predictive Analytics)

The tools I mention are separated into ""regular"" and ""big"" data solutions. The ""Big Data"" solutions are currently mostly products, or are being built by ""big data"" engineers and data scientists.

For ""regular"" data, I've found that R and Python are the most useful, however Matlab, SPSS, SAS are also OK. I'd suggest checking out RStudio and the Python scientific stack (see Anaconda distribution).

There are many ""Big data"" analytics platforms out there. Datameer, Tableau, etc. Each is a little different and usually costs you money.

I'd start with learning R or Python since whichever company you work for may have their own internal ""Big Data"" system they built or some 3rd party tools they pay for.

3. Visualization Software (Descriptive / Exploratory Analytics and Data Presentation)

Note that visualization is many times part of your analytics software (see #2). Again, the ""big data"" visualization tools will usually be 3rd party.

R is the big one, but Matlab, SPSS, SAS are also OK. I'd also suggest checking out Plotly
 and the Python scientific stack (see Anaconda distribution). As a matter of fact, check out Plotly for sure--it's really cool.

You can also look into some tools like D3 if you are more ambitious and want to build visualizations from scratch (well not totally from scratch, it's just not as easy as using a tool like R).

4. Machine Learning (Exploratory / Predictive Analytics)

Regressions, clustering, dimensionality reduction, neural networks, decision trees, etc. These are what data scientists use to not only get insight from data, but also to create products or predictive systems used by their company. The core of this is using algorithms to gain insight from data, or to teach a machine to make decisions or predictions.

Again I'd suggest looking into the ""All of Statistics"" book listed above. Also I'd suggest checking out ""The Elements of Statistical Learning"" by Hastie/Tibshirani/Friedman. It's decent and free. Aside from that the book ""Pattern Recognition"" by Duda is pretty much a standard. However, please note everyone's going to have different recommendations for the best machine learning books.

Machine learning gets much deeper than this. There is a scientific methodology as well to finding and training a good learning algorithm, and new results are discovered all the time. It's one of the hottest academic fields at the moment so results are pouring in.

Overall I'd say machine learning is one of those things you just have to keep chipping away at--continue to read new texts and papers.

5. Data Munging (Data Quality / Sampling Theory)

A lot of your time will be spent doing this. The joke is that it takes up 90% of your time, but I've found it's closer to 40-60% depending on the problem.

Data-Munging usually requires knowing how to work with databases, as well as knowledge in scraping data from websites, knowledge of file or web formats, how to format string, and the best way to transform data. You need to know how to consume data, how to process and create it.

Languages might be SQL as well as command line tools like AWK, SED, GREP, CURL, etc. I'd also say Python and R are useful here since you can make command line tools with them that might scrape a website or convert some file format into something you can use. You also need to know standards like REST, Json, etc.

The key is your answer to this question : ""Can you get data from a variety of sources, join it to other data, clean it up, and then spit it out in a form that is readily and easily useable for data analysis?"". If you aren't sure, learn more about it.

It's also good to know some things about how your data is collected. This is where some statistics can come in handy. For example, if you sample people for A/B testing using a coin flip but allow people to come back a second time, well, then your data isn't ""independent"". This affects how you might do your analysis later, or you may want to split-out repeat visitors into their own separate data set.

6. Computer Science and Applied Mathematics (Engineering / Theory)

This supports your knowledge in every other thing you do related to working with computers and software.

Typical things to know at a undergrad-level include algorithms, data structures, discrete math, databases. Domain specific things are operating systems, compilers, networking, etc.

You don't need to know everything in the domain specific category outside of your domain at a deep level or really any level at all. However, for example knowing how people solved a problem in a compiler before might help you solve a problem with how to parse and convert data.

I'd also say there's some need for numerical analysis here. You need to understand why and when algorithms converge to solutions. For example, why does gradient descent work? Will my algorithm have a problem with round off or truncation error?

7. Data Engineering (Engineering)

This ties everything from ""Data Munging"" to ""Analytics"" together. You have to get data stored, accessible, processed, and moved from place to place.

This is where you figure out how to solve the engineering problems associated with the size of your data set. There are challenges related to moving it around and making it available for analysis.

You may have ""Big Data"" or ""Regular Data"". One isn't necessarily better than another-as a matter of fact if you know about statistics at all ""big data"" can be easier to make inferences or predictions from. However, if you know about engineering ""big data"" is harder to process and move around. There are trade-offs.

These sort of problems are what data scientists are paid to work around and it's still new. So far the go-to solutions seem to either be using a 3rd party proprietary platform, or to build something in house using open source technology.

Open source technology includes Hadoop, Mahout, MongoDB, MySQL, etc. To use any of those you need to usually learn some associated language. For example, Java with Hadoop/Mahout. NoSQL with MongoDB.

""Open source solutions"" might also mean you have to write your own processing algorithms in a language of your choice, and REST APIs in PHP so that data can be consumed elsewhere.

3rd Party solutions mostly require you to learn how to use the software. However you usually benefit from knowing SQL, how columnar databases work, how NoSQL databases work, as well as general programming languages. Some examples here might be Datameer, Redshift, etc.

8. Development Tools

This largely depends on your domain, but it includes things like IDEs, revision control software, unit testing libraries, and unit testing coverage libraries.

No matter the domain however, Github is a something you really should learn. It's used almost everywhere.

9. Knowledge of software engineering techniques and writing quality code

If you can't write quality code, you'll piss off your fellow engineers and cause more work for yourself. Most shops ascribe to the Agile software development method even if it rebrands some of the things older development shops already did. You may as well learn about it.

Code quality is subjective, so Python might favor some practices over others as an example. It's hard to make some definitive recommendation without knowing the language you work in.

A good place to start is Understanding the GitHub Flow
 since it ties into #8. However every dev shop is a bit different, and has their own philosophy so learning to communicate with the dev team and adapting to how they do things is a must.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/5nyer9woip34tqus', 'title': 'What are some software and skills that every data scientist should know (excluding R, Matlab, and Hadoop)? Also, what are some resources for learning Hadoop?', 'score': {'original': 0.83373333333333, 'ai': 0.16626666666667}, 'blocks': [{'text': 'I more or less work as a data scientist. My title isn\'t that, but it doesn\'t matter since that\'s what I do.\n\nHere are some skill sets I\'ve found helpful outlined as a list with no real order :\n\n1. Statistics (Theory and Applied Analytics)\n\nThis is the core of what you can offer as a data scientist. A company can always get a database developer, or software engineer. They can\'t usually get a software engineer that knows their stats. Therefore, if you only know the basics about standard error, bootstrapping, confidence intervals or bayesian statistics, you better learn more.\n\nI\'d suggest picking up ""All of Statistics"" by Wasserman, and ""An Introduction to Statistical Learning"" by James/Witten/Hastie/Tibshirani. The latter is available free. For the less mathematically literate ""Think Stats"" and ""Think Bayes"" are decent and both are free.\n\n2. Analytics Software (Descriptive / Inferential / Predictive Analytics)\n\nThe tools I mention are separated into ""regular"" and ""big"" data solutions. The ""Big Data"" solutions are currently mostly products, or are being built by ""big data"" engineers and data scientists.\n\nFor ""regular"" data, I\'ve found that R and Python are the most useful, however Matlab, SPSS, SAS are also OK. I\'d suggest checking out RStudio and the Python scientific stack (see Anaconda distribution).\n\nThere are many ""Big data"" analytics platforms out there. Datameer, Tableau, etc. Each is a little different and usually costs you money.\n\nI\'d start with learning R or Python since whichever company you work for may have their own internal ""Big Data"" system they built or some 3rd party tools they pay for.\n\n3. Visualization Software (Descriptive / Exploratory Analytics and Data Presentation)\n\nNote that visualization is many times part of your analytics software (see #2). Again, the ""big data"" visualization tools will usually be 3rd party.\n\nR is the big one, but Matlab, SPSS, SAS are also OK. I\'d also suggest checking out Plotly\n and the Python scientific stack (see Anaconda distribution). As a matter of fact, check out Plotly for sure--it\'s really cool.\n\nYou can also look into some tools like D3 if you are more ambitious and want to build visualizations from scratch (well not totally from scratch, it\'s just not as easy as using a tool like R).\n\n4. Machine Learning (Exploratory / Predictive Analytics)\n\nRegressions, clustering, dimensionality reduction, neural networks, decision trees, etc. These are what data scientists use to not only get insight from data, but also to create products or predictive systems used by their company. The core of this is using algorithms to gain insight from data, or to teach a machine to make decisions or predictions.\n\nAgain I\'d suggest looking into the ""All of Statistics"" book listed above. Also I\'d suggest checking out ""The Elements of Statistical Learning"" by Hastie/Tibshirani/Friedman. It\'s decent and free. Aside from that the book ""Pattern Recognition"" by Duda is pretty much a standard. However, please note everyone\'s going to have different recommendations for the best machine learning books.\n\nMachine learning gets much deeper than this. There is a scientific methodology as well to finding and training a good learning algorithm, and new results are discovered all the time. It\'s one of the hottest academic', 'result': {'fake': 0.1991, 'real': 0.8009}, 'status': 'success'}, {'text': 'fields at the moment so results are pouring in.\n\nOverall I\'d say machine learning is one of those things you just have to keep chipping away at--continue to read new texts and papers.\n\n5. Data Munging (Data Quality / Sampling Theory)\n\nA lot of your time will be spent doing this. The joke is that it takes up 90% of your time, but I\'ve found it\'s closer to 40-60% depending on the problem.\n\nData-Munging usually requires knowing how to work with databases, as well as knowledge in scraping data from websites, knowledge of file or web formats, how to format string, and the best way to transform data. You need to know how to consume data, how to process and create it.\n\nLanguages might be SQL as well as command line tools like AWK, SED, GREP, CURL, etc. I\'d also say Python and R are useful here since you can make command line tools with them that might scrape a website or convert some file format into something you can use. You also need to know standards like REST, Json, etc.\n\nThe key is your answer to this question : ""Can you get data from a variety of sources, join it to other data, clean it up, and then spit it out in a form that is readily and easily useable for data analysis?"". If you aren\'t sure, learn more about it.\n\nIt\'s also good to know some things about how your data is collected. This is where some statistics can come in handy. For example, if you sample people for A/B testing using a coin flip but allow people to come back a second time, well, then your data isn\'t ""independent"". This affects how you might do your analysis later, or you may want to split-out repeat visitors into their own separate data set.\n\n6. Computer Science and Applied Mathematics (Engineering / Theory)\n\nThis supports your knowledge in every other thing you do related to working with computers and software.\n\nTypical things to know at a undergrad-level include algorithms, data structures, discrete math, databases. Domain specific things are operating systems, compilers, networking, etc.\n\nYou don\'t need to know everything in the domain specific category outside of your domain at a deep level or really any level at all. However, for example knowing how people solved a problem in a compiler before might help you solve a problem with how to parse and convert data.\n\nI\'d also say there\'s some need for numerical analysis here. You need to understand why and when algorithms converge to solutions. For example, why does gradient descent work? Will my algorithm have a problem with round off or truncation error?\n\n7. Data Engineering (Engineering)\n\nThis ties everything from ""Data Munging"" to ""Analytics"" together. You have to get data stored, accessible, processed, and moved from place to place.\n\nThis is where you figure out how to solve the engineering problems associated with the size of your data set. There are challenges related to moving it around and making it available for analysis.\n\nYou may have ""Big Data"" or ""Regular Data"". One isn\'t necessarily better than another-as a matter of fact if you know about', 'result': {'fake': 0.7266, 'real': 0.2734}, 'status': 'success'}, {'text': 'statistics at all ""big data"" can be easier to make inferences or predictions from. However, if you know about engineering ""big data"" is harder to process and move around. There are trade-offs.\n\nThese sort of problems are what data scientists are paid to work around and it\'s still new. So far the go-to solutions seem to either be using a 3rd party proprietary platform, or to build something in house using open source technology.\n\nOpen source technology includes Hadoop, Mahout, MongoDB, MySQL, etc. To use any of those you need to usually learn some associated language. For example, Java with Hadoop/Mahout. NoSQL with MongoDB.\n\n""Open source solutions"" might also mean you have to write your own processing algorithms in a language of your choice, and REST APIs in PHP so that data can be consumed elsewhere.\n\n3rd Party solutions mostly require you to learn how to use the software. However you usually benefit from knowing SQL, how columnar databases work, how NoSQL databases work, as well as general programming languages. Some examples here might be Datameer, Redshift, etc.\n\n8. Development Tools\n\nThis largely depends on your domain, but it includes things like IDEs, revision control software, unit testing libraries, and unit testing coverage libraries.\n\nNo matter the domain however, Github is a something you really should learn. It\'s used almost everywhere.\n\n9. Knowledge of software engineering techniques and writing quality code\n\nIf you can\'t write quality code, you\'ll piss off your fellow engineers and cause more work for yourself. Most shops ascribe to the Agile software development method even if it rebrands some of the things older development shops already did. You may as well learn about it.\n\nCode quality is subjective, so Python might favor some practices over others as an example. It\'s hard to make some definitive recommendation without knowing the language you work in.\n\nA good place to start is Understanding the GitHub Flow\n since it ties into #8. However every dev shop is a bit different, and has their own philosophy so learning to communicate with the dev team and adapting to how they do things is a must.', 'result': {'fake': 0.4147, 'real': 0.5853}, 'status': 'success'}], 'credits_used': 14, 'credits': 1982692, 'subscription': 0, 'content': 'I more or less work as a data scientist. My title isn\'t that, but it doesn\'t matter since that\'s what I do.\n\nHere are some skill sets I\'ve found helpful outlined as a list with no real order :\n\n1. Statistics (Theory and Applied Analytics)\n\nThis is the core of what you can offer as a data scientist. A company can always get a database developer, or software engineer. They can\'t usually get a software engineer that knows their stats. Therefore, if you only know the basics about standard error, bootstrapping, confidence intervals or bayesian statistics, you better learn more.\n\nI\'d suggest picking up ""All of Statistics"" by Wasserman, and ""An Introduction to Statistical Learning"" by James/Witten/Hastie/Tibshirani. The latter is available free. For the less mathematically literate ""Think Stats"" and ""Think Bayes"" are decent and both are free.\n\n2. Analytics Software (Descriptive / Inferential / Predictive Analytics)\n\nThe tools I mention are separated into ""regular"" and ""big"" data solutions. The ""Big Data"" solutions are currently mostly products, or are being built by ""big data"" engineers and data scientists.\n\nFor ""regular"" data, I\'ve found that R and Python are the most useful, however Matlab, SPSS, SAS are also OK. I\'d suggest checking out RStudio and the Python scientific stack (see Anaconda distribution).\n\nThere are many ""Big data"" analytics platforms out there. Datameer, Tableau, etc. Each is a little different and usually costs you money.\n\nI\'d start with learning R or Python since whichever company you work for may have their own internal ""Big Data"" system they built or some 3rd party tools they pay for.\n\n3. Visualization Software (Descriptive / Exploratory Analytics and Data Presentation)\n\nNote that visualization is many times part of your analytics software (see #2). Again, the ""big data"" visualization tools will usually be 3rd party.\n\nR is the big one, but Matlab, SPSS, SAS are also OK. I\'d also suggest checking out Plotly\n and the Python scientific stack (see Anaconda distribution). As a matter of fact, check out Plotly for sure--it\'s really cool.\n\nYou can also look into some tools like D3 if you are more ambitious and want to build visualizations from scratch (well not totally from scratch, it\'s just not as easy as using a tool like R).\n\n4. Machine Learning (Exploratory / Predictive Analytics)\n\nRegressions, clustering, dimensionality reduction, neural networks, decision trees, etc. These are what data scientists use to not only get insight from data, but also to create products or predictive systems used by their company. The core of this is using algorithms to gain insight from data, or to teach a machine to make decisions or predictions.\n\nAgain I\'d suggest looking into the ""All of Statistics"" book listed above. Also I\'d suggest checking out ""The Elements of Statistical Learning"" by Hastie/Tibshirani/Friedman. It\'s decent and free. Aside from that the book ""Pattern Recognition"" by Duda is pretty much a standard. However, please note everyone\'s going to have different recommendations for the best machine learning books.\n\nMachine learning gets much deeper than this. There is a scientific methodology as well to finding and training a good learning algorithm, and new results are discovered all the time. It\'s one of the hottest academic fields at the moment so results are pouring in.\n\nOverall I\'d say machine learning is one of those things you just have to keep chipping away at--continue to read new texts and papers.\n\n5. Data Munging (Data Quality / Sampling Theory)\n\nA lot of your time will be spent doing this. The joke is that it takes up 90% of your time, but I\'ve found it\'s closer to 40-60% depending on the problem.\n\nData-Munging usually requires knowing how to work with databases, as well as knowledge in scraping data from websites, knowledge of file or web formats, how to format string, and the best way to transform data. You need to know how to consume data, how to process and create it.\n\nLanguages might be SQL as well as command line tools like AWK, SED, GREP, CURL, etc. I\'d also say Python and R are useful here since you can make command line tools with them that might scrape a website or convert some file format into something you can use. You also need to know standards like REST, Json, etc.\n\nThe key is your answer to this question : ""Can you get data from a variety of sources, join it to other data, clean it up, and then spit it out in a form that is readily and easily useable for data analysis?"". If you aren\'t sure, learn more about it.\n\nIt\'s also good to know some things about how your data is collected. This is where some statistics can come in handy. For example, if you sample people for A/B testing using a coin flip but allow people to come back a second time, well, then your data isn\'t ""independent"". This affects how you might do your analysis later, or you may want to split-out repeat visitors into their own separate data set.\n\n6. Computer Science and Applied Mathematics (Engineering / Theory)\n\nThis supports your knowledge in every other thing you do related to working with computers and software.\n\nTypical things to know at a undergrad-level include algorithms, data structures, discrete math, databases. Domain specific things are operating systems, compilers, networking, etc.\n\nYou don\'t need to know everything in the domain specific category outside of your domain at a deep level or really any level at all. However, for example knowing how people solved a problem in a compiler before might help you solve a problem with how to parse and convert data.\n\nI\'d also say there\'s some need for numerical analysis here. You need to understand why and when algorithms converge to solutions. For example, why does gradient descent work? Will my algorithm have a problem with round off or truncation error?\n\n7. Data Engineering (Engineering)\n\nThis ties everything from ""Data Munging"" to ""Analytics"" together. You have to get data stored, accessible, processed, and moved from place to place.\n\nThis is where you figure out how to solve the engineering problems associated with the size of your data set. There are challenges related to moving it around and making it available for analysis.\n\nYou may have ""Big Data"" or ""Regular Data"". One isn\'t necessarily better than another-as a matter of fact if you know about statistics at all ""big data"" can be easier to make inferences or predictions from. However, if you know about engineering ""big data"" is harder to process and move around. There are trade-offs.\n\nThese sort of problems are what data scientists are paid to work around and it\'s still new. So far the go-to solutions seem to either be using a 3rd party proprietary platform, or to build something in house using open source technology.\n\nOpen source technology includes Hadoop, Mahout, MongoDB, MySQL, etc. To use any of those you need to usually learn some associated language. For example, Java with Hadoop/Mahout. NoSQL with MongoDB.\n\n""Open source solutions"" might also mean you have to write your own processing algorithms in a language of your choice, and REST APIs in PHP so that data can be consumed elsewhere.\n\n3rd Party solutions mostly require you to learn how to use the software. However you usually benefit from knowing SQL, how columnar databases work, how NoSQL databases work, as well as general programming languages. Some examples here might be Datameer, Redshift, etc.\n\n8. Development Tools\n\nThis largely depends on your domain, but it includes things like IDEs, revision control software, unit testing libraries, and unit testing coverage libraries.\n\nNo matter the domain however, Github is a something you really should learn. It\'s used almost everywhere.\n\n9. Knowledge of software engineering techniques and writing quality code\n\nIf you can\'t write quality code, you\'ll piss off your fellow engineers and cause more work for yourself. Most shops ascribe to the Agile software development method even if it rebrands some of the things older development shops already did. You may as well learn about it.\n\nCode quality is subjective, so Python might favor some practices over others as an example. It\'s hard to make some definitive recommendation without knowing the language you work in.\n\nA good place to start is Understanding the GitHub Flow\n since it ties into #8. However every dev shop is a bit different, and has their own philosophy so learning to communicate with the dev team and adapting to how they do things is a must.', 'aiModelVersion': '1'}",0.83373333333333
Jeff Erickson,1y,I've recently completed my PhD in statistics. I would like to get a second PhD in machine learning. Am I in the correct way? What should I do?,"[A2A]

You already have a PhD. You don’t need another one.

In particular, you already have a PhD in statistics, which is the foundation of machine learning. The fields are too close. I seriously doubt that any reputable PhD program in machine learning would admit you.

If you want to do research in machine learning, just start doing research in machine learning. Your PhD indicates that you already know how to do research. In particular, your PhD indicates that you already know how to bring yourself up to speed on an unfamiliar topic. So do that.

Even if you need to build up some necessary background skills, your background in statistics gives you a considerable advantage over other newcomers to the field.

I strongly recommend finding local ML researchers to work with. Introduce yourself. Give a talk about your statistical research. Ask for advice. Ask for background papers to read. Then dive in!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ngui4h5vjx30tmqf', 'title': ""I've recently completed my PhD in statistics. I would like to get a second PhD in machine learning. Am I in the correct way? What should I do?"", 'score': {'original': 0.9879, 'ai': 0.0121}, 'blocks': [{'text': '[A2A]\n\nYou already have a PhD. You don’t need another one.\n\nIn particular, you already have a PhD in statistics, which is the foundation of machine learning. The fields are too close. I seriously doubt that any reputable PhD program in machine learning would admit you.\n\nIf you want to do research in machine learning, just start doing research in machine learning. Your PhD indicates that you already know how to do research. In particular, your PhD indicates that you already know how to bring yourself up to speed on an unfamiliar topic. So do that.\n\nEven if you need to build up some necessary background skills, your background in statistics gives you a considerable advantage over other newcomers to the field.\n\nI strongly recommend finding local ML researchers to work with. Introduce yourself. Give a talk about your statistical research. Ask for advice. Ask for background papers to read. Then dive in!', 'result': {'fake': 0.0121, 'real': 0.9879}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982690, 'subscription': 0, 'content': '[A2A]\n\nYou already have a PhD. You don’t need another one.\n\nIn particular, you already have a PhD in statistics, which is the foundation of machine learning. The fields are too close. I seriously doubt that any reputable PhD program in machine learning would admit you.\n\nIf you want to do research in machine learning, just start doing research in machine learning. Your PhD indicates that you already know how to do research. In particular, your PhD indicates that you already know how to bring yourself up to speed on an unfamiliar topic. So do that.\n\nEven if you need to build up some necessary background skills, your background in statistics gives you a considerable advantage over other newcomers to the field.\n\nI strongly recommend finding local ML researchers to work with. Introduce yourself. Give a talk about your statistical research. Ask for advice. Ask for background papers to read. Then dive in!', 'aiModelVersion': '1'}",0.9879
Yariv Adan,4y,Is it getting harder to break into machine learning?,"I can't speak for research, but for applied ML, it's actually becoming easier for multiple reasons:

Tools are better and easier than ever, and provide automation across the entire ML pipeline - from data handling, through feature engineering, model chosing and building , hyper parameterization, deployment, and production handling.
ML frameworks are easier and more powerful than ever. Weird graph syntax is becoming a relict of the past. Debugging, logging, interpreting, and reporting is becoming an integral part of these frameworks.
There is an abundance of outstanding tutorials, posts, courses, and examples on anything ML. Especially, these are now going beyond engineering, and expanding to ML best practices and guidelines for product managers, business roles, and production and release managers.
ML is no more black magic mastered by a handful of researchers and data scientists. It has become yet another tool in the engineering toolbox of many teams in both large companies and startups. So experienced talent, as well as production level tools and best practices is broadly available, making it a much shorter and less bumpy road for new people and teams.

Good times!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/i0dbclu872xp1htv', 'title': 'Is it getting harder to break into machine learning?', 'score': {'original': 0.9994, 'ai': 0.0006}, 'blocks': [{'text': ""I can't speak for research, but for applied ML, it's actually becoming easier for multiple reasons:\n\nTools are better and easier than ever, and provide automation across the entire ML pipeline - from data handling, through feature engineering, model chosing and building , hyper parameterization, deployment, and production handling.\nML frameworks are easier and more powerful than ever. Weird graph syntax is becoming a relict of the past. Debugging, logging, interpreting, and reporting is becoming an integral part of these frameworks.\nThere is an abundance of outstanding tutorials, posts, courses, and examples on anything ML. Especially, these are now going beyond engineering, and expanding to ML best practices and guidelines for product managers, business roles, and production and release managers.\nML is no more black magic mastered by a handful of researchers and data scientists. It has become yet another tool in the engineering toolbox of many teams in both large companies and startups. So experienced talent, as well as production level tools and best practices is broadly available, making it a much shorter and less bumpy road for new people and teams.\n\nGood times!"", 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982688, 'subscription': 0, 'content': ""I can't speak for research, but for applied ML, it's actually becoming easier for multiple reasons:\n\nTools are better and easier than ever, and provide automation across the entire ML pipeline - from data handling, through feature engineering, model chosing and building , hyper parameterization, deployment, and production handling.\nML frameworks are easier and more powerful than ever. Weird graph syntax is becoming a relict of the past. Debugging, logging, interpreting, and reporting is becoming an integral part of these frameworks.\nThere is an abundance of outstanding tutorials, posts, courses, and examples on anything ML. Especially, these are now going beyond engineering, and expanding to ML best practices and guidelines for product managers, business roles, and production and release managers.\nML is no more black magic mastered by a handful of researchers and data scientists. It has become yet another tool in the engineering toolbox of many teams in both large companies and startups. So experienced talent, as well as production level tools and best practices is broadly available, making it a much shorter and less bumpy road for new people and teams.\n\nGood times!"", 'aiModelVersion': '1'}",0.9994
Andrew Weill,5y,What clues do you look for when trying to spot a liar?,"I don’t look for clues; I look for evidence. So I compare what a witness says to prior recorded statements, other documentary evidence, contradictions in timeline, discrepancies that don’t fit with normal failures of memory, and so on. It takes a lot of preparation.

I’m unimpressed by those who claim an instinct for instantaneously spotting a liar. It’s easy to misread physical signs. Here’s a story that taught me an important lesson:

I was representing a witness being deposed. About mid-morning, the opposing attorney started asking a line of questions. My client started squirming. The opposing attorney, sensing the obvious discomfort of my client, questioned with further intensity. My client got more and more fidgety.

I couldn’t see any possible reason for the nervousness. It didn’t make any sense.

After about 30 minutes of this, the opposing attorney asked to take a break; he was obviously puzzled too.

After we left the room, my client whispered to me, “Thank goodness! I’ve had to go to the bathroom for the last half hour! I was dying in there!”","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/dv6nsoh2epz4ig0a', 'title': 'What clues do you look for when trying to spot a liar?', 'score': {'original': 0.9976, 'ai': 0.0024}, 'blocks': [{'text': 'I don’t look for clues; I look for evidence. So I compare what a witness says to prior recorded statements, other documentary evidence, contradictions in timeline, discrepancies that don’t fit with normal failures of memory, and so on. It takes a lot of preparation.\n\nI’m unimpressed by those who claim an instinct for instantaneously spotting a liar. It’s easy to misread physical signs. Here’s a story that taught me an important lesson:\n\nI was representing a witness being deposed. About mid-morning, the opposing attorney started asking a line of questions. My client started squirming. The opposing attorney, sensing the obvious discomfort of my client, questioned with further intensity. My client got more and more fidgety.\n\nI couldn’t see any possible reason for the nervousness. It didn’t make any sense.\n\nAfter about 30 minutes of this, the opposing attorney asked to take a break; he was obviously puzzled too.\n\nAfter we left the room, my client whispered to me, “Thank goodness! I’ve had to go to the bathroom for the last half hour! I was dying in there!”', 'result': {'fake': 0.0024, 'real': 0.9976}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982686, 'subscription': 0, 'content': 'I don’t look for clues; I look for evidence. So I compare what a witness says to prior recorded statements, other documentary evidence, contradictions in timeline, discrepancies that don’t fit with normal failures of memory, and so on. It takes a lot of preparation.\n\nI’m unimpressed by those who claim an instinct for instantaneously spotting a liar. It’s easy to misread physical signs. Here’s a story that taught me an important lesson:\n\nI was representing a witness being deposed. About mid-morning, the opposing attorney started asking a line of questions. My client started squirming. The opposing attorney, sensing the obvious discomfort of my client, questioned with further intensity. My client got more and more fidgety.\n\nI couldn’t see any possible reason for the nervousness. It didn’t make any sense.\n\nAfter about 30 minutes of this, the opposing attorney asked to take a break; he was obviously puzzled too.\n\nAfter we left the room, my client whispered to me, “Thank goodness! I’ve had to go to the bathroom for the last half hour! I was dying in there!”', 'aiModelVersion': '1'}",0.9976
A Sethi,11mo,What are the prerequisites for learning Artificial Intelligence?,"As you explore the basic topics of AI, obtain a good understanding of the requirements. AI courses teach the skills needed for a job in AI. Certifications, in addition to courses, are an important aspect of demonstrating your technological value and expertise. So I'd like to propose an Artificial Intelligence certification course that incorporates DOMAIN SPECIALIZATION MODULES as well as a JOB PLACEMENT facility.

Let's take a look at the requirements for learning Artificial Intelligence.

Mathematical knowledge.
Knowledge of a programming language.
Analytical abilities of the highest kind.
Understanding complicated algorithms.
Basic statistical and modeling skills are required.

So there you have it: the qualifications you must meet before you can begin learning AI. Apart from that, while selecting a data science and AI course, make sure it includes specific AI training tailored to your domain's needs. As a result, we may claim that having sufficient domain knowledge is also a need for learning AI. This domain is massive, and you'll need expert help if you want to realise your ambition of transferring to it. Many enthusiasts, in my experience, get stuck with the prerequisites for a long time before giving up.

So, where do we go to learn these AI prerequisites?

In recent years, online education has acquired a lot of popularity, and individuals are increasingly turning to online courses because they provide a lot of freedom. Many ed-tech platforms provide artificial intelligence and data science courses, but I usually suggest the one mentioned below for quality skill training and a strong emphasis on skill development for skill-centric fields like artificial intelligence.

Why should you join Learnbay?

It is an excellent resource for learning about artificial intelligence. It provides a wealth of information that will assist you in mastering this subject. The professors conduct many live classes in order to address all of your concerns. In addition, Learnbay is a Bangalore-based online portal. It offers the best artificial intelligence and data science course in Bangalore.

Take a peek at Learnbay's AI courses:

Advance Data science and artificial intelligence Program.

Internships begin on the first day of class and continue throughout the program.
Work on the most recent AI & ML projects from corporations and earn a certificate for appropriate project experience.

These are the AI courses that Learnbay Institute offers. Furthermore, if you do not have a technical background and want to use your past technical experience to seek a job in machine learning, you need to enroll in a domain-specific education system.

DOMAIN SPECIALISATION MODULES:

Artificial intelligence is increasing across a range of businesses and fields, both vertically and horizontally. In this section, we'll talk about inactive electives in artificial intelligence and what job opportunities are available to both novices and working professionals with appropriate expertise. Learnbay offers domain specialization courses that prepare you for the workplace. If you work as a professional, this is a great option for you.

The following are some of the domains that are available:

Sales Marketing and HR
Retail E-commerce
Healthcare Pharma and Clinical Research
BFSI
Manufacturing Mechanical and Telecom

The industries that thrive in using data science in their operations. Although there are other online platforms that provide artificial intelligence courses, Learnbay emerges as the most promising option when it comes to domain-specific learning. This institution covers all of the aforementioned disciplines, and you can select your elective based on your prior job experience.

THERE ARE OVER 15 REAL-TIME PROJECTS AND TWO CAPSTONES:

A professional is defined by his or her experience. Projects are an important component of a CV. Even if you haven't entered the corporate world, if you've worked on distinctive first-class initiatives, you'll have a better chance of getting employed by top organizations. Learnbay provides live courses. Some of the projects include- IBM HR Analysis, Ola forecasting model, Netflix content recommendation model, BMW’s data science platform, etc.

Some other additional unique features:

Hybrid learning: In this hybrid learning format, students can choose their study schedule and attend live online or in-person project sessions in various locations. Additionally, students will have access to a wealth of study materials and resources even after completing the course.
Project innovative labs: Learnbay provides students with the opportunity to work on real-world, practical projects with the guidance of experts from top companies and organizations. The Project Innovative Labs are located in various cities including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore.
IBM Certifications: Learnbay allows students to get enrolled in courses that are certified by IBM. IBM is a well-known and reputable company, and its credentials are highly valued in the industry. By completing the Advanced Data Science and AI Program at Learnbay, students will receive a course completion certificate from IBM, a project completion certificate, and a micro-skills certificate.

Final conclusion:

Finally, I'd like to state that Learnbay is the ideal resource for obtaining domain specialties. During my time with them, they proved to be really valuable. Data science is all about applying data-driven methodologies to domain-specific knowledge. Because I lack domain knowledge, studying data science skills and other technologies would be futile. I hope you're now ready to dive into the realm of AI and take on the great challenges that lie ahead.

I hope you select an institution that will enable you to realize all of your ambitions.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/vfjap8o6hmb1t09e', 'title': 'What are the prerequisites for learning Artificial Intelligence?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""As you explore the basic topics of AI, obtain a good understanding of the requirements. AI courses teach the skills needed for a job in AI. Certifications, in addition to courses, are an important aspect of demonstrating your technological value and expertise. So I'd like to propose an Artificial Intelligence certification course that incorporates DOMAIN SPECIALIZATION MODULES as well as a JOB PLACEMENT facility.\n\nLet's take a look at the requirements for learning Artificial Intelligence.\n\nMathematical knowledge.\nKnowledge of a programming language.\nAnalytical abilities of the highest kind.\nUnderstanding complicated algorithms.\nBasic statistical and modeling skills are required.\n\nSo there you have it: the qualifications you must meet before you can begin learning AI. Apart from that, while selecting a data science and AI course, make sure it includes specific AI training tailored to your domain's needs. As a result, we may claim that having sufficient domain knowledge is also a need for learning AI. This domain is massive, and you'll need expert help if you want to realise your ambition of transferring to it. Many enthusiasts, in my experience, get stuck with the prerequisites for a long time before giving up.\n\nSo, where do we go to learn these AI prerequisites?\n\nIn recent years, online education has acquired a lot of popularity, and individuals are increasingly turning to online courses because they provide a lot of freedom. Many ed-tech platforms provide artificial intelligence and data science courses, but I usually suggest the one mentioned below for quality skill training and a strong emphasis on skill development for skill-centric fields like artificial intelligence.\n\nWhy should you join Learnbay?\n\nIt is an excellent resource for learning about artificial intelligence. It provides a wealth of information that will assist you in mastering this subject. The professors conduct many live classes in order to address all of your concerns. In addition, Learnbay is a Bangalore-based online portal. It offers the best artificial intelligence and data science course in Bangalore.\n\nTake a peek at Learnbay's AI courses:\n\nAdvance Data science and artificial intelligence Program.\n\nInternships begin on the first day of class and continue throughout the program.\nWork on the most recent AI & ML projects from corporations and earn a certificate for appropriate project experience.\n\nThese are the AI courses that Learnbay Institute offers. Furthermore, if you do not have a technical background and want to use your past technical experience to seek a job in machine learning, you need to enroll in a domain-specific education system.\n\nDOMAIN SPECIALISATION MODULES:\n\nArtificial intelligence is increasing across a range of businesses and fields, both vertically and horizontally. In this section, we'll talk about inactive electives in artificial intelligence and what job opportunities are available to both novices and working professionals with appropriate expertise. Learnbay offers domain specialization courses that prepare you for the workplace. If you work as a professional, this is a great option for you.\n\nThe following are some of the domains that are available:\n\nSales Marketing and HR\nRetail E-commerce\nHealthcare Pharma and Clinical Research\nBFSI\nManufacturing Mechanical and Telecom\n\nThe industries that thrive in using data science in their operations. Although there are other online platforms that provide artificial intelligence courses, Learnbay emerges as the most promising option"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""when it comes to domain-specific learning. This institution covers all of the aforementioned disciplines, and you can select your elective based on your prior job experience.\n\nTHERE ARE OVER 15 REAL-TIME PROJECTS AND TWO CAPSTONES:\n\nA professional is defined by his or her experience. Projects are an important component of a CV. Even if you haven't entered the corporate world, if you've worked on distinctive first-class initiatives, you'll have a better chance of getting employed by top organizations. Learnbay provides live courses. Some of the projects include- IBM HR Analysis, Ola forecasting model, Netflix content recommendation model, BMW’s data science platform, etc.\n\nSome other additional unique features:\n\nHybrid learning: In this hybrid learning format, students can choose their study schedule and attend live online or in-person project sessions in various locations. Additionally, students will have access to a wealth of study materials and resources even after completing the course.\nProject innovative labs: Learnbay provides students with the opportunity to work on real-world, practical projects with the guidance of experts from top companies and organizations. The Project Innovative Labs are located in various cities including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore.\nIBM Certifications: Learnbay allows students to get enrolled in courses that are certified by IBM. IBM is a well-known and reputable company, and its credentials are highly valued in the industry. By completing the Advanced Data Science and AI Program at Learnbay, students will receive a course completion certificate from IBM, a project completion certificate, and a micro-skills certificate.\n\nFinal conclusion:\n\nFinally, I'd like to state that Learnbay is the ideal resource for obtaining domain specialties. During my time with them, they proved to be really valuable. Data science is all about applying data-driven methodologies to domain-specific knowledge. Because I lack domain knowledge, studying data science skills and other technologies would be futile. I hope you're now ready to dive into the realm of AI and take on the great challenges that lie ahead.\n\nI hope you select an institution that will enable you to realize all of your ambitions."", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 9, 'credits': 1982677, 'subscription': 0, 'content': ""As you explore the basic topics of AI, obtain a good understanding of the requirements. AI courses teach the skills needed for a job in AI. Certifications, in addition to courses, are an important aspect of demonstrating your technological value and expertise. So I'd like to propose an Artificial Intelligence certification course that incorporates DOMAIN SPECIALIZATION MODULES as well as a JOB PLACEMENT facility.\n\nLet's take a look at the requirements for learning Artificial Intelligence.\n\nMathematical knowledge.\nKnowledge of a programming language.\nAnalytical abilities of the highest kind.\nUnderstanding complicated algorithms.\nBasic statistical and modeling skills are required.\n\nSo there you have it: the qualifications you must meet before you can begin learning AI. Apart from that, while selecting a data science and AI course, make sure it includes specific AI training tailored to your domain's needs. As a result, we may claim that having sufficient domain knowledge is also a need for learning AI. This domain is massive, and you'll need expert help if you want to realise your ambition of transferring to it. Many enthusiasts, in my experience, get stuck with the prerequisites for a long time before giving up.\n\nSo, where do we go to learn these AI prerequisites?\n\nIn recent years, online education has acquired a lot of popularity, and individuals are increasingly turning to online courses because they provide a lot of freedom. Many ed-tech platforms provide artificial intelligence and data science courses, but I usually suggest the one mentioned below for quality skill training and a strong emphasis on skill development for skill-centric fields like artificial intelligence.\n\nWhy should you join Learnbay?\n\nIt is an excellent resource for learning about artificial intelligence. It provides a wealth of information that will assist you in mastering this subject. The professors conduct many live classes in order to address all of your concerns. In addition, Learnbay is a Bangalore-based online portal. It offers the best artificial intelligence and data science course in Bangalore.\n\nTake a peek at Learnbay's AI courses:\n\nAdvance Data science and artificial intelligence Program.\n\nInternships begin on the first day of class and continue throughout the program.\nWork on the most recent AI & ML projects from corporations and earn a certificate for appropriate project experience.\n\nThese are the AI courses that Learnbay Institute offers. Furthermore, if you do not have a technical background and want to use your past technical experience to seek a job in machine learning, you need to enroll in a domain-specific education system.\n\nDOMAIN SPECIALISATION MODULES:\n\nArtificial intelligence is increasing across a range of businesses and fields, both vertically and horizontally. In this section, we'll talk about inactive electives in artificial intelligence and what job opportunities are available to both novices and working professionals with appropriate expertise. Learnbay offers domain specialization courses that prepare you for the workplace. If you work as a professional, this is a great option for you.\n\nThe following are some of the domains that are available:\n\nSales Marketing and HR\nRetail E-commerce\nHealthcare Pharma and Clinical Research\nBFSI\nManufacturing Mechanical and Telecom\n\nThe industries that thrive in using data science in their operations. Although there are other online platforms that provide artificial intelligence courses, Learnbay emerges as the most promising option when it comes to domain-specific learning. This institution covers all of the aforementioned disciplines, and you can select your elective based on your prior job experience.\n\nTHERE ARE OVER 15 REAL-TIME PROJECTS AND TWO CAPSTONES:\n\nA professional is defined by his or her experience. Projects are an important component of a CV. Even if you haven't entered the corporate world, if you've worked on distinctive first-class initiatives, you'll have a better chance of getting employed by top organizations. Learnbay provides live courses. Some of the projects include- IBM HR Analysis, Ola forecasting model, Netflix content recommendation model, BMW’s data science platform, etc.\n\nSome other additional unique features:\n\nHybrid learning: In this hybrid learning format, students can choose their study schedule and attend live online or in-person project sessions in various locations. Additionally, students will have access to a wealth of study materials and resources even after completing the course.\nProject innovative labs: Learnbay provides students with the opportunity to work on real-world, practical projects with the guidance of experts from top companies and organizations. The Project Innovative Labs are located in various cities including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore.\nIBM Certifications: Learnbay allows students to get enrolled in courses that are certified by IBM. IBM is a well-known and reputable company, and its credentials are highly valued in the industry. By completing the Advanced Data Science and AI Program at Learnbay, students will receive a course completion certificate from IBM, a project completion certificate, and a micro-skills certificate.\n\nFinal conclusion:\n\nFinally, I'd like to state that Learnbay is the ideal resource for obtaining domain specialties. During my time with them, they proved to be really valuable. Data science is all about applying data-driven methodologies to domain-specific knowledge. Because I lack domain knowledge, studying data science skills and other technologies would be futile. I hope you're now ready to dive into the realm of AI and take on the great challenges that lie ahead.\n\nI hope you select an institution that will enable you to realize all of your ambitions."", 'aiModelVersion': '1'}",0.0
Dee Ní Dhomhnaill,Updated 6y,Who is your favorite historical figure and why?,"Nancy Wake. This formidable woman died in 2004 at the age of 98. Look into her eyes. That’s the stare of a bona fide badass. Death was the only thing that could take this woman down and even that took nearly a century.

It’s was during the Second World War that this woman made her mark on history. She was was an SOE agent and a noted leader in the French Resistance. By the middle of the war there was a 5 million franc price on her head. Nicknamed the ‘‘White Mouse’ by the Gestapo she evaded capture again and again. Her time in France was characterised by her immense bravery and cool head under pressure.

While living with her husband, she helped countless airmen escape to safety. Eventually this became too dangerous and she escaped to England. Her husband, who stayed behind, was later tortured and killed.

She snuck back into France with a new identity and a strong determination to fight. She led a 7000+ group of maquisards and was known for her toughness and ability to drink any man under the table. At one time she cycled 500km, through numerous German checkpoints, to replace codes for a wireless operator.

After the war she was awarded numerous medals, got involved in politics and worked as an intelligence officer. In 1985 she published her autobiography.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/654398luadyvzg2h', 'title': 'Who is your favorite historical figure and why?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': 'Nancy Wake. This formidable woman died in 2004 at the age of 98. Look into her eyes. That’s the stare of a bona fide badass. Death was the only thing that could take this woman down and even that took nearly a century.\n\nIt’s was during the Second World War that this woman made her mark on history. She was was an SOE agent and a noted leader in the French Resistance. By the middle of the war there was a 5 million franc price on her head. Nicknamed the ‘‘White Mouse’ by the Gestapo she evaded capture again and again. Her time in France was characterised by her immense bravery and cool head under pressure.\n\nWhile living with her husband, she helped countless airmen escape to safety. Eventually this became too dangerous and she escaped to England. Her husband, who stayed behind, was later tortured and killed.\n\nShe snuck back into France with a new identity and a strong determination to fight. She led a 7000+ group of maquisards and was known for her toughness and ability to drink any man under the table. At one time she cycled 500km, through numerous German checkpoints, to replace codes for a wireless operator.\n\nAfter the war she was awarded numerous medals, got involved in politics and worked as an intelligence officer. In 1985 she published her autobiography.', 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982674, 'subscription': 0, 'content': 'Nancy Wake. This formidable woman died in 2004 at the age of 98. Look into her eyes. That’s the stare of a bona fide badass. Death was the only thing that could take this woman down and even that took nearly a century.\n\nIt’s was during the Second World War that this woman made her mark on history. She was was an SOE agent and a noted leader in the French Resistance. By the middle of the war there was a 5 million franc price on her head. Nicknamed the ‘‘White Mouse’ by the Gestapo she evaded capture again and again. Her time in France was characterised by her immense bravery and cool head under pressure.\n\nWhile living with her husband, she helped countless airmen escape to safety. Eventually this became too dangerous and she escaped to England. Her husband, who stayed behind, was later tortured and killed.\n\nShe snuck back into France with a new identity and a strong determination to fight. She led a 7000+ group of maquisards and was known for her toughness and ability to drink any man under the table. At one time she cycled 500km, through numerous German checkpoints, to replace codes for a wireless operator.\n\nAfter the war she was awarded numerous medals, got involved in politics and worked as an intelligence officer. In 1985 she published her autobiography.', 'aiModelVersion': '1'}",0.9995
Neha Singh,10mo,What are the prerequisites for learning Artificial Intelligence?,"Artificial Intelligence is a broad term. In this answer, we'll talk about what the prerequisites are for learning Artificial Intelligence in terms of educational background and knowledge. A lot of people assume that studying AI isn't something everyone can do, but it's very accessible to anyone interested in learning more about it.

Prerequisites for learning Artificial Intelligence:

Basic knowledge of Computer Science.
Knowledge of Mathematics.
Some knowledge about Physics (for those who want to study Robotics).
The ability to learn and the desire to learn how to learn is a prerequisite.
The good news is that once you start learning, you'll see how quickly and easily you can get started. In the first couple of months of training, I was amazed at my ability to learn new things. It's not a science where people sit in front of computers for 8 hours a day and do nothing but study. You do need to be able to learn on your own as well.

For those who don't have the opportunity to learn this deep knowledge yourself, other routes are available. Online courses are available to give you a lot of this knowledge, and there are also many on-demand courses and long-form courses on AI, including courses from Stanford. These courses are highly beneficial as they provide an excellent foundation in the major pillars of AI. The existence of domain specialization is one of the most crucial aspects that must be taken into account when opting for an online course.

The benefits of specialized training

You'll need more than simply a degree or certification in the subject to work as an AI engineer. To fully utilize your abilities, knowledge, and experience, you must possess the ability to solve problems. As a result, domain specialization has become a crucial component of data science. Whether you're a seasoned pro or just getting started, having a domain-specialized field can help you land a job. Nowadays, specialists are in greater demand than generalists. Generalists are less likely to get the job sooner than specialists in one field.

Some of the best AI programs include:

A superior DS/ML/AI learning tool is Simplilearn. Simplilearn has been training individuals for careers in this industry with a staff of competent teachers for many years. This platform's disadvantage is that it doesn't offer training in certain fields.

High-quality courses in data science and artificial intelligence are offered by the fantastic website. One of the top platforms, it provides amazing features like classes created for particular domains. This institute presently provides the best artificial intelligence and top data science courses for working individuals in Bangalore. You can attend live, interactive lessons and hybrid classes from any location in India. The benefits of taking this course are numerous.

Some of the top features of AI courses are as follows:

IBM Certification: Each course has received approval from the IBM Group. Three certificates are awarded to students who excel in Artificial Intelligence and Machine Learning Program. Only after finishing a course, project, or set of micro-skills is an IBM certification possible.
Domain Specialisation: You will be able to select from several domain specialties during your studies, including BFSI, Telecom, Manufacturing, and Communication. It has a clear advantage because it provides training in specific sectors. You must fulfill an industry's requirements to grow in your profession. At this institute, everything is available! If you specialize, your CV will appear more impressive.
Hybrid Learning: Students can take part in live online mentoring sessions while completing assignments in real-time thanks to the hybrid learning environment.
Projects: The Artificial Intelligence and Machine Learning Program offer both novice and expert projects. The course's final project asks you to accomplish complex artificial intelligence projects using Python and other data science technologies. After the course, you will have access to additional materials to continue your education.

The following projects are included in these courses:

A self-driving car can drive itself.
Speech Emotion Recognition
Project Innovation Labs: Each of India's main cities, including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore, has a project innovation lab.

Some other features of the courses include:

5 months weekday and 7 months weekend
One-to-one live sessions
2 years subscription
Guaranteed calls for Interviews

Final Note:

One needs great leadership qualities, exceptional communication skills, and strong negotiation skills to become a successful AI engineer. The greatest data science course training in Bangalore is available there. Reading and writing codes are included in this. Additionally, you'll discover how to use the codes to solve actual-world issues.

Thank you","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/p3natzghv2dcb1mk', 'title': 'What are the prerequisites for learning Artificial Intelligence?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""Artificial Intelligence is a broad term. In this answer, we'll talk about what the prerequisites are for learning Artificial Intelligence in terms of educational background and knowledge. A lot of people assume that studying AI isn't something everyone can do, but it's very accessible to anyone interested in learning more about it.\n\nPrerequisites for learning Artificial Intelligence:\n\nBasic knowledge of Computer Science.\nKnowledge of Mathematics.\nSome knowledge about Physics (for those who want to study Robotics).\nThe ability to learn and the desire to learn how to learn is a prerequisite.\nThe good news is that once you start learning, you'll see how quickly and easily you can get started. In the first couple of months of training, I was amazed at my ability to learn new things. It's not a science where people sit in front of computers for 8 hours a day and do nothing but study. You do need to be able to learn on your own as well.\n\nFor those who don't have the opportunity to learn this deep knowledge yourself, other routes are available. Online courses are available to give you a lot of this knowledge, and there are also many on-demand courses and long-form courses on AI, including courses from Stanford. These courses are highly beneficial as they provide an excellent foundation in the major pillars of AI. The existence of domain specialization is one of the most crucial aspects that must be taken into account when opting for an online course.\n\nThe benefits of specialized training\n\nYou'll need more than simply a degree or certification in the subject to work as an AI engineer. To fully utilize your abilities, knowledge, and experience, you must possess the ability to solve problems. As a result, domain specialization has become a crucial component of data science. Whether you're a seasoned pro or just getting started, having a domain-specialized field can help you land a job. Nowadays, specialists are in greater demand than generalists. Generalists are less likely to get the job sooner than specialists in one field.\n\nSome of the best AI programs include:\n\nA superior DS/ML/AI learning tool is Simplilearn. Simplilearn has been training individuals for careers in this industry with a staff of competent teachers for many years. This platform's disadvantage is that it doesn't offer training in certain fields.\n\nHigh-quality courses in data science and artificial intelligence are offered by the fantastic website. One of the top platforms, it provides amazing features like classes created for particular domains. This institute presently provides the best artificial intelligence and top data science courses for working individuals in Bangalore. You can attend live, interactive lessons and hybrid classes from any location in India. The benefits of taking this course are numerous.\n\nSome of the top features of AI courses are as follows:\n\nIBM Certification: Each course has received approval from the IBM Group. Three certificates are awarded to students who excel in Artificial Intelligence and Machine Learning Program. Only after finishing a course, project, or set of micro-skills is an IBM certification possible.\nDomain Specialisation: You will be able to select from several domain specialties during your studies, including BFSI, Telecom, Manufacturing, and"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""Communication. It has a clear advantage because it provides training in specific sectors. You must fulfill an industry's requirements to grow in your profession. At this institute, everything is available! If you specialize, your CV will appear more impressive.\nHybrid Learning: Students can take part in live online mentoring sessions while completing assignments in real-time thanks to the hybrid learning environment.\nProjects: The Artificial Intelligence and Machine Learning Program offer both novice and expert projects. The course's final project asks you to accomplish complex artificial intelligence projects using Python and other data science technologies. After the course, you will have access to additional materials to continue your education.\n\nThe following projects are included in these courses:\n\nA self-driving car can drive itself.\nSpeech Emotion Recognition\nProject Innovation Labs: Each of India's main cities, including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore, has a project innovation lab.\n\nSome other features of the courses include:\n\n5 months weekday and 7 months weekend\nOne-to-one live sessions\n2 years subscription\nGuaranteed calls for Interviews\n\nFinal Note:\n\nOne needs great leadership qualities, exceptional communication skills, and strong negotiation skills to become a successful AI engineer. The greatest data science course training in Bangalore is available there. Reading and writing codes are included in this. Additionally, you'll discover how to use the codes to solve actual-world issues.\n\nThank you"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 8, 'credits': 1982666, 'subscription': 0, 'content': ""Artificial Intelligence is a broad term. In this answer, we'll talk about what the prerequisites are for learning Artificial Intelligence in terms of educational background and knowledge. A lot of people assume that studying AI isn't something everyone can do, but it's very accessible to anyone interested in learning more about it.\n\nPrerequisites for learning Artificial Intelligence:\n\nBasic knowledge of Computer Science.\nKnowledge of Mathematics.\nSome knowledge about Physics (for those who want to study Robotics).\nThe ability to learn and the desire to learn how to learn is a prerequisite.\nThe good news is that once you start learning, you'll see how quickly and easily you can get started. In the first couple of months of training, I was amazed at my ability to learn new things. It's not a science where people sit in front of computers for 8 hours a day and do nothing but study. You do need to be able to learn on your own as well.\n\nFor those who don't have the opportunity to learn this deep knowledge yourself, other routes are available. Online courses are available to give you a lot of this knowledge, and there are also many on-demand courses and long-form courses on AI, including courses from Stanford. These courses are highly beneficial as they provide an excellent foundation in the major pillars of AI. The existence of domain specialization is one of the most crucial aspects that must be taken into account when opting for an online course.\n\nThe benefits of specialized training\n\nYou'll need more than simply a degree or certification in the subject to work as an AI engineer. To fully utilize your abilities, knowledge, and experience, you must possess the ability to solve problems. As a result, domain specialization has become a crucial component of data science. Whether you're a seasoned pro or just getting started, having a domain-specialized field can help you land a job. Nowadays, specialists are in greater demand than generalists. Generalists are less likely to get the job sooner than specialists in one field.\n\nSome of the best AI programs include:\n\nA superior DS/ML/AI learning tool is Simplilearn. Simplilearn has been training individuals for careers in this industry with a staff of competent teachers for many years. This platform's disadvantage is that it doesn't offer training in certain fields.\n\nHigh-quality courses in data science and artificial intelligence are offered by the fantastic website. One of the top platforms, it provides amazing features like classes created for particular domains. This institute presently provides the best artificial intelligence and top data science courses for working individuals in Bangalore. You can attend live, interactive lessons and hybrid classes from any location in India. The benefits of taking this course are numerous.\n\nSome of the top features of AI courses are as follows:\n\nIBM Certification: Each course has received approval from the IBM Group. Three certificates are awarded to students who excel in Artificial Intelligence and Machine Learning Program. Only after finishing a course, project, or set of micro-skills is an IBM certification possible.\nDomain Specialisation: You will be able to select from several domain specialties during your studies, including BFSI, Telecom, Manufacturing, and Communication. It has a clear advantage because it provides training in specific sectors. You must fulfill an industry's requirements to grow in your profession. At this institute, everything is available! If you specialize, your CV will appear more impressive.\nHybrid Learning: Students can take part in live online mentoring sessions while completing assignments in real-time thanks to the hybrid learning environment.\nProjects: The Artificial Intelligence and Machine Learning Program offer both novice and expert projects. The course's final project asks you to accomplish complex artificial intelligence projects using Python and other data science technologies. After the course, you will have access to additional materials to continue your education.\n\nThe following projects are included in these courses:\n\nA self-driving car can drive itself.\nSpeech Emotion Recognition\nProject Innovation Labs: Each of India's main cities, including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore, has a project innovation lab.\n\nSome other features of the courses include:\n\n5 months weekday and 7 months weekend\nOne-to-one live sessions\n2 years subscription\nGuaranteed calls for Interviews\n\nFinal Note:\n\nOne needs great leadership qualities, exceptional communication skills, and strong negotiation skills to become a successful AI engineer. The greatest data science course training in Bangalore is available there. Reading and writing codes are included in this. Additionally, you'll discover how to use the codes to solve actual-world issues.\n\nThank you"", 'aiModelVersion': '1'}",0.0
Joshua Gross,1y,"In layman's terms, how does artificial intelligence work, and is it based on algorithms?","Oh, boy, this is tough. Artificial intelligence (AI) and machine learning (ML) are two of the toughest parts of computer science (CS) to explain to someone without any background.

So of course I’ll try, rather than doing work…

First, about algorithms, they’re just step-by-step processes. A recipe for banana pudding is an algorithm. You’ve worked with algorithms your whole life. They come up a lot in math. Do you remember how to do multi-column addition? That’s an algorithm. Long division? Algorithm. In fact, there are many algorithms and many variations of algorithms to produce the same result, just like there are many recipes for banana pudding.

Second, AI is a big field in CS, and it breaks down into three areas.

The first area is purely mathematical and logical and it produced some interest ideas, but the problem is that natural intelligence, human intelligence, isn’t particularly logical.

The second area is inspired human cognition, how human beings think, and it has produced a lot of insight into computing and psychology and neuroscience. There are whole programs called cognitive architectures (ACT-R and Soar are the two big ones) that actually use computers to simulate how a human mind/brain works. They’ve produced a lot of amazing insight, but this isn’t a tool you use to make your smartphone do something cool. They’re more for research.

The third area is machine learning (ML). ML breaks down into lots of categories, but the most common one you’ll read about today is the neural network (NN). NNs were inspired by a model of actual human neurons, but that model is outdated and wrong. NNs are just groups of tiny little decision makers we call neurons. Each neuron looks at some small piece of data (say a tiny bit of a picture), or at other neurons that somewhere back in the line looked at data. Inside the neuron, there’s just a number. To teach the NN to do something, we feed it training data, data that we known all about. Each neuron makes a decision, and then we compare that decision to what the neuron should have done (since we know the truth, since this is training data), and we tell the neuron to correct itself. As it’s first trained, it makes a lot of errors and lots of corrections to itself, but slowly, it stops making corrections and gets good at judging what it’s supposed to judge.

To put it even more simply, ML is just a program that contains lots of small and separate parts that are connected to each other, and we teach each of those parts to make a decision looking at only some small part of the data, or decisions that other parts have already made.

We have to do this because the resulting program (and it is a program, it is an algorithm) is too complicated for a human to have put together. Computers will look at each detail of a billion photos (yes, billion, or many billion) and see small connections that humans don’t see.

Perhaps the best way to understand ML is to look at how we break it. There’s a whole research area called adversarial AI designed to fool ML systems. Computers don’t understand anything. They decide an image contains Barack Obama or a conch or a panda based on patterns that we don’t care about. We know what Barack Obama looks like, and we can also judge from context. We can pretty easily create images that an ML model can’t classify correctly, but we can also intentionally cause the model to think the wrong thing.

In the first image, the model is pretty good; it thinks that the panda is a panda. It doesn’t know what a panda is, so it can’t understand that you just look at the eyes, the snout, and the general coloring. Anyway, the researchers added a light amount of carefully constructed fuzzy data, invisible to the human eye, and now the model is absolutely certain that this is a gibbon. No human would ever think that’s a gibbon.

There is a lot of research trying to create programs that clearly explain how these ML models make their decisions. That allows us to potentially fool the ML models, because again, they don’t think or know. They’re just lots of little tiny decision makers that look at a small amount of data or at each other and make the best decision their mathematics allows.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/inret0f8cx1m6zb7', 'title': ""In layman's terms, how does artificial intelligence work, and is it based on algorithms?"", 'score': {'original': 0.98395, 'ai': 0.01605}, 'blocks': [{'text': 'Oh, boy, this is tough. Artificial intelligence (AI) and machine learning (ML) are two of the toughest parts of computer science (CS) to explain to someone without any background.\n\nSo of course I’ll try, rather than doing work…\n\nFirst, about algorithms, they’re just step-by-step processes. A recipe for banana pudding is an algorithm. You’ve worked with algorithms your whole life. They come up a lot in math. Do you remember how to do multi-column addition? That’s an algorithm. Long division? Algorithm. In fact, there are many algorithms and many variations of algorithms to produce the same result, just like there are many recipes for banana pudding.\n\nSecond, AI is a big field in CS, and it breaks down into three areas.\n\nThe first area is purely mathematical and logical and it produced some interest ideas, but the problem is that natural intelligence, human intelligence, isn’t particularly logical.\n\nThe second area is inspired human cognition, how human beings think, and it has produced a lot of insight into computing and psychology and neuroscience. There are whole programs called cognitive architectures (ACT-R and Soar are the two big ones) that actually use computers to simulate how a human mind/brain works. They’ve produced a lot of amazing insight, but this isn’t a tool you use to make your smartphone do something cool. They’re more for research.\n\nThe third area is machine learning (ML). ML breaks down into lots of categories, but the most common one you’ll read about today is the neural network (NN). NNs were inspired by a model of actual human neurons, but that model is outdated and wrong. NNs are just groups of tiny little decision makers we call neurons. Each neuron looks at some small piece of data (say a tiny bit of a picture), or at other neurons that somewhere back in the line looked at data. Inside the neuron, there’s just a number. To teach the NN to do something, we feed it training data, data that we known all about. Each neuron makes a decision, and then we compare that decision to what the neuron should have done (since we know the truth, since this is training data), and we tell the neuron to correct itself. As it’s first trained, it makes a lot of errors and lots of corrections to itself, but slowly, it stops making corrections and gets good at judging what it’s supposed to judge.\n\nTo put it even more simply, ML is just a program that contains lots of small and separate parts that are connected to each other, and we teach each of those parts to make a decision looking at only some small part of the data, or decisions that other parts have already made.\n\nWe have to do this because the resulting program (and it is a program, it is an algorithm) is too complicated for a human to have put together. Computers will look at each detail of a billion photos (yes, billion, or many billion) and see small connections that humans don’t see.\n\nPerhaps the best way to understand ML is to look at how we break it.', 'result': {'fake': 0.0001, 'real': 0.9999}, 'status': 'success'}, {'text': 'There’s a whole research area called adversarial AI designed to fool ML systems. Computers don’t understand anything. They decide an image contains Barack Obama or a conch or a panda based on patterns that we don’t care about. We know what Barack Obama looks like, and we can also judge from context. We can pretty easily create images that an ML model can’t classify correctly, but we can also intentionally cause the model to think the wrong thing.\n\nIn the first image, the model is pretty good; it thinks that the panda is a panda. It doesn’t know what a panda is, so it can’t understand that you just look at the eyes, the snout, and the general coloring. Anyway, the researchers added a light amount of carefully constructed fuzzy data, invisible to the human eye, and now the model is absolutely certain that this is a gibbon. No human would ever think that’s a gibbon.\n\nThere is a lot of research trying to create programs that clearly explain how these ML models make their decisions. That allows us to potentially fool the ML models, because again, they don’t think or know. They’re just lots of little tiny decision makers that look at a small amount of data or at each other and make the best decision their mathematics allows.', 'result': {'fake': 0.0021, 'real': 0.9979}, 'status': 'success'}], 'credits_used': 8, 'credits': 1982658, 'subscription': 0, 'content': 'Oh, boy, this is tough. Artificial intelligence (AI) and machine learning (ML) are two of the toughest parts of computer science (CS) to explain to someone without any background.\n\nSo of course I’ll try, rather than doing work…\n\nFirst, about algorithms, they’re just step-by-step processes. A recipe for banana pudding is an algorithm. You’ve worked with algorithms your whole life. They come up a lot in math. Do you remember how to do multi-column addition? That’s an algorithm. Long division? Algorithm. In fact, there are many algorithms and many variations of algorithms to produce the same result, just like there are many recipes for banana pudding.\n\nSecond, AI is a big field in CS, and it breaks down into three areas.\n\nThe first area is purely mathematical and logical and it produced some interest ideas, but the problem is that natural intelligence, human intelligence, isn’t particularly logical.\n\nThe second area is inspired human cognition, how human beings think, and it has produced a lot of insight into computing and psychology and neuroscience. There are whole programs called cognitive architectures (ACT-R and Soar are the two big ones) that actually use computers to simulate how a human mind/brain works. They’ve produced a lot of amazing insight, but this isn’t a tool you use to make your smartphone do something cool. They’re more for research.\n\nThe third area is machine learning (ML). ML breaks down into lots of categories, but the most common one you’ll read about today is the neural network (NN). NNs were inspired by a model of actual human neurons, but that model is outdated and wrong. NNs are just groups of tiny little decision makers we call neurons. Each neuron looks at some small piece of data (say a tiny bit of a picture), or at other neurons that somewhere back in the line looked at data. Inside the neuron, there’s just a number. To teach the NN to do something, we feed it training data, data that we known all about. Each neuron makes a decision, and then we compare that decision to what the neuron should have done (since we know the truth, since this is training data), and we tell the neuron to correct itself. As it’s first trained, it makes a lot of errors and lots of corrections to itself, but slowly, it stops making corrections and gets good at judging what it’s supposed to judge.\n\nTo put it even more simply, ML is just a program that contains lots of small and separate parts that are connected to each other, and we teach each of those parts to make a decision looking at only some small part of the data, or decisions that other parts have already made.\n\nWe have to do this because the resulting program (and it is a program, it is an algorithm) is too complicated for a human to have put together. Computers will look at each detail of a billion photos (yes, billion, or many billion) and see small connections that humans don’t see.\n\nPerhaps the best way to understand ML is to look at how we break it. There’s a whole research area called adversarial AI designed to fool ML systems. Computers don’t understand anything. They decide an image contains Barack Obama or a conch or a panda based on patterns that we don’t care about. We know what Barack Obama looks like, and we can also judge from context. We can pretty easily create images that an ML model can’t classify correctly, but we can also intentionally cause the model to think the wrong thing.\n\nIn the first image, the model is pretty good; it thinks that the panda is a panda. It doesn’t know what a panda is, so it can’t understand that you just look at the eyes, the snout, and the general coloring. Anyway, the researchers added a light amount of carefully constructed fuzzy data, invisible to the human eye, and now the model is absolutely certain that this is a gibbon. No human would ever think that’s a gibbon.\n\nThere is a lot of research trying to create programs that clearly explain how these ML models make their decisions. That allows us to potentially fool the ML models, because again, they don’t think or know. They’re just lots of little tiny decision makers that look at a small amount of data or at each other and make the best decision their mathematics allows.', 'aiModelVersion': '1'}",0.98395
Riya Bilan,10mo,What are the prerequisites for learning Artificial Intelligence?,"Artificial intelligence is one of the parts of computer science and engineering that helps us to develop machines with human intelligence into machine programming. This is the field of study where we learn how the human brain thinks, learns, decides, and works to solve various problems to mimic machines.

Let me give you the steps to learn artificial intelligence.

What are the prerequisites for learning Artificial Intelligence?

To learn the concept of Artificial Intelligence then mathematical knowledge is important because it covers the topics of data science and analytics.
The candidate should have a basic knowledge of programming languages and concepts like OOPS, loop, user-defined functions, if/else statements, and data structure & data types to develop and deploy models.
Ability to write algorithms to find patterns and learn logic and maths equations. There are three categories of algorithms such as classification, regression, and clustering algorithms, the most common being classification which creates classifiers between objects through a predetermined set of rules.
Build your analytical skills, which are used when detecting patterns, brainstorming ideas, observing and collecting data, integrating new information, and making decisions based on the situation. Skills that are communication, creativity, research, data analysis, and critical thinking.

If you are interested in learning more about Artificial Intelligence there are free sources available online that help you learn the basics. To learn both real-world as well as theory-based learning experiences, then you can start by enrolling in programs that provide the best training and placement opportunities then.

Let us see some of the best courses available on artificial intelligence.

This is one of the finest institutes that provides you with an Artificial Intelligence course, which is affiliated with IIT Madras.

They cover skill knowledge acquisition topics such as Python, Data Science, Data Analysis, AI, GIT, SQL, etc.
In this program, they offer industry projects, placement assistance, resume preparation, and mock interviews.
The institute has expert professionals who help the students land in the top MNCs.

However, the institute doesn’t provide any domain specialization and individual project certificates.

Course name - Advance AI and ML Program.

Program details:

Duration is 350+ hours, 9 & 11 months for weekday and weekend batches.

Features

They offer domain-specific training for students and this domain knowledge gives additional skills and industry knowledge. Selecting a domain in their interested field makes them experts, because of the in-depth knowledge they provide with the help of industry experts. Available domains include BFSI, Commerce, Telecom, Management, etc.
They also provide hybrid learning sessions for the learners to apply their knowledge in a practical setting by working on real-world projects.
They offer 12+ real-time & capstone projects for the benefit of the students. Industry projects such as Forecasting future sales with trends, Reduction of waiting time via a highly precise forecasting model, etc.
They offer 7 + project innovation labs where students can work and get industry experience, the labs are located in Bangalore, Delhi, Pune, Mumbai, Chennai, etc.
Participants will receive IBM course completion certificates, Capstone project certificates, and Micro-skill certificates.
They offer placement support to help students to attain their dream job and 1:1 mock interviews, guaranteed interview calls, and profile optimization are offered.

Final thoughts:

These are the prerequisites for learning AI, to make the learning path smooth you can enroll yourself in online courses like the above-mentioned courses. They offer various features that help students not only to survive but thrive in the competitive job market.

Thanks","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/decf4xqmjar8ktsn', 'title': 'What are the prerequisites for learning Artificial Intelligence?', 'score': {'original': 0.4899, 'ai': 0.5101}, 'blocks': [{'text': 'Artificial intelligence is one of the parts of computer science and engineering that helps us to develop machines with human intelligence into machine programming. This is the field of study where we learn how the human brain thinks, learns, decides, and works to solve various problems to mimic machines.\n\nLet me give you the steps to learn artificial intelligence.\n\nWhat are the prerequisites for learning Artificial Intelligence?\n\nTo learn the concept of Artificial Intelligence then mathematical knowledge is important because it covers the topics of data science and analytics.\nThe candidate should have a basic knowledge of programming languages and concepts like OOPS, loop, user-defined functions, if/else statements, and data structure & data types to develop and deploy models.\nAbility to write algorithms to find patterns and learn logic and maths equations. There are three categories of algorithms such as classification, regression, and clustering algorithms, the most common being classification which creates classifiers between objects through a predetermined set of rules.\nBuild your analytical skills, which are used when detecting patterns, brainstorming ideas, observing and collecting data, integrating new information, and making decisions based on the situation. Skills that are communication, creativity, research, data analysis, and critical thinking.\n\nIf you are interested in learning more about Artificial Intelligence there are free sources available online that help you learn the basics. To learn both real-world as well as theory-based learning experiences, then you can start by enrolling in programs that provide the best training and placement opportunities then.\n\nLet us see some of the best courses available on artificial intelligence.\n\nThis is one of the finest institutes that provides you with an Artificial Intelligence course, which is affiliated with IIT Madras.\n\nThey cover skill knowledge acquisition topics such as Python, Data Science, Data Analysis, AI, GIT, SQL, etc.\nIn this program, they offer industry projects, placement assistance, resume preparation, and mock interviews.\nThe institute has expert professionals who help the students land in the top MNCs.\n\nHowever, the institute doesn’t provide any domain specialization and individual project certificates.\n\nCourse name - Advance AI and ML Program.\n\nProgram details:\n\nDuration is 350+ hours, 9 & 11 months for weekday and weekend batches.\n\nFeatures\n\nThey offer domain-specific training for students and this domain knowledge gives additional skills and industry knowledge. Selecting a domain in their interested field makes them experts, because of the in-depth knowledge they provide with the help of industry experts. Available domains include BFSI, Commerce, Telecom, Management, etc.\nThey also provide hybrid learning sessions for the learners to apply their knowledge in a practical setting by working on real-world projects.\nThey offer 12+ real-time & capstone projects for the benefit of the students. Industry projects such as Forecasting future sales with trends, Reduction of waiting time via a highly precise forecasting model, etc.\nThey offer 7 + project innovation labs where students can work and get industry experience, the labs are located in Bangalore, Delhi, Pune, Mumbai, Chennai, etc.\nParticipants will receive IBM course completion certificates, Capstone project certificates, and Micro-skill certificates.\nThey offer placement support to help students to attain their dream job and 1:1 mock interviews, guaranteed interview calls, and profile optimization are offered.\n\nFinal thoughts:\n\nThese are the prerequisites for learning AI, to make', 'result': {'fake': 0.034, 'real': 0.966}, 'status': 'success'}, {'text': 'the learning path smooth you can enroll yourself in online courses like the above-mentioned courses. They offer various features that help students not only to survive but thrive in the competitive job market.\n\nThanks', 'result': {'fake': 0.9601, 'real': 0.0399}, 'status': 'success'}], 'credits_used': 6, 'credits': 1982652, 'subscription': 0, 'content': 'Artificial intelligence is one of the parts of computer science and engineering that helps us to develop machines with human intelligence into machine programming. This is the field of study where we learn how the human brain thinks, learns, decides, and works to solve various problems to mimic machines.\n\nLet me give you the steps to learn artificial intelligence.\n\nWhat are the prerequisites for learning Artificial Intelligence?\n\nTo learn the concept of Artificial Intelligence then mathematical knowledge is important because it covers the topics of data science and analytics.\nThe candidate should have a basic knowledge of programming languages and concepts like OOPS, loop, user-defined functions, if/else statements, and data structure & data types to develop and deploy models.\nAbility to write algorithms to find patterns and learn logic and maths equations. There are three categories of algorithms such as classification, regression, and clustering algorithms, the most common being classification which creates classifiers between objects through a predetermined set of rules.\nBuild your analytical skills, which are used when detecting patterns, brainstorming ideas, observing and collecting data, integrating new information, and making decisions based on the situation. Skills that are communication, creativity, research, data analysis, and critical thinking.\n\nIf you are interested in learning more about Artificial Intelligence there are free sources available online that help you learn the basics. To learn both real-world as well as theory-based learning experiences, then you can start by enrolling in programs that provide the best training and placement opportunities then.\n\nLet us see some of the best courses available on artificial intelligence.\n\nThis is one of the finest institutes that provides you with an Artificial Intelligence course, which is affiliated with IIT Madras.\n\nThey cover skill knowledge acquisition topics such as Python, Data Science, Data Analysis, AI, GIT, SQL, etc.\nIn this program, they offer industry projects, placement assistance, resume preparation, and mock interviews.\nThe institute has expert professionals who help the students land in the top MNCs.\n\nHowever, the institute doesn’t provide any domain specialization and individual project certificates.\n\nCourse name - Advance AI and ML Program.\n\nProgram details:\n\nDuration is 350+ hours, 9 & 11 months for weekday and weekend batches.\n\nFeatures\n\nThey offer domain-specific training for students and this domain knowledge gives additional skills and industry knowledge. Selecting a domain in their interested field makes them experts, because of the in-depth knowledge they provide with the help of industry experts. Available domains include BFSI, Commerce, Telecom, Management, etc.\nThey also provide hybrid learning sessions for the learners to apply their knowledge in a practical setting by working on real-world projects.\nThey offer 12+ real-time & capstone projects for the benefit of the students. Industry projects such as Forecasting future sales with trends, Reduction of waiting time via a highly precise forecasting model, etc.\nThey offer 7 + project innovation labs where students can work and get industry experience, the labs are located in Bangalore, Delhi, Pune, Mumbai, Chennai, etc.\nParticipants will receive IBM course completion certificates, Capstone project certificates, and Micro-skill certificates.\nThey offer placement support to help students to attain their dream job and 1:1 mock interviews, guaranteed interview calls, and profile optimization are offered.\n\nFinal thoughts:\n\nThese are the prerequisites for learning AI, to make the learning path smooth you can enroll yourself in online courses like the above-mentioned courses. They offer various features that help students not only to survive but thrive in the competitive job market.\n\nThanks', 'aiModelVersion': '1'}",0.4899
Jinu Abrahim,Oct 26,What are the prerequisites for learning Artificial Intelligence?,"To learn Artificial Intelligence (AI), there are some prerequisites that professionals should consider. While there is no specific educational background required, having a solid understanding of mathematics, statistics, and computer science fundamentals will be beneficial.

Here are the key prerequisites for learning AI:

Mathematics: A strong foundation in mathematics is crucial for understanding the algorithms and models behind AI.

Topics such as linear algebra, calculus, probability, and statistics are particularly important.

2. Programming: Proficiency in programming languages like Python, Java, or C++ is essential for implementing AI algorithms and building AI applications.

It is important to understand concepts like data structures, algorithms, and object-oriented programming.

3. Statistics: Knowledge of statistics is important for understanding and analyzing data.

Topics such as probability distributions, hypothesis testing, and regression analysis are commonly used in AI applications.

4. Machine Learning: Familiarity with machine learning concepts and techniques is indispensable for AI.

Professionals should learn about supervised learning, unsupervised learning, reinforcement learning, and common algorithms like decision trees, support vector machines, and neural networks.

5. Data Handling: Proficiency in data handling and analysis is crucial for AI practitioners.

Professionals should learn about data preprocessing, feature engineering, and data visualization techniques to effectively work with datasets.

Once professionals have the necessary prerequisites, they should plan their learning in AI and follow certain steps for career growth:

Set Clear Goals: Define your objectives and what you want to achieve in AI. Whether it's becoming a data scientist, machine learning engineer, or AI researcher, having a clear goal will help guide your learning path.

The Program in Data Science and AI by Learnbay provides access to mock sessions, resume-building sessions, personalized support, assured interview calls, and one-on-one sessions to help students remain competitive in the job market. Students have the benefit of tapping into the global job market with simplified visa approval and a highly qualified certificate from Learnbay.

You can reach out to centers in the cities like Bangalore, Delhi, Pune, and Hyderabad for offline assistance.

Alternatively: The Great Learning: ""Post Graduate Program (PGP) in Data Science and Business Analytics"" course includes basic career assistance.

2. Gain Theoretical Knowledge: Start with textbooks, online courses, or tutorials to build a strong theoretical foundation in AI concepts. Learn about different machine learning algorithms, techniques, and best practices.

3. Hands-on Experience: Gain practical experience by working on real-world projects. Participate in Kaggle competitions, build your own AI projects, or contribute to open-source projects to apply the knowledge you've gained.

4. Stay Updated: AI is a rapidly evolving field, so it's essential to stay up-to-date with the latest advancements. Follow AI blogs, read research papers, attend conferences, and join AI communities to stay informed about the latest trends and techniques.

Example:

The Program in Data Science and AI by Learnbay holds significant value in various countries. You additionally receive an IBM project experience certificate from Learnbay.

Alternatively: Great Learning does provide a qualified certification but no abroad work permits.

5. Networking: Build connections with professionals working in the AI field. Attend industry events, join online forums, and engage in networking opportunities to expand your professional network and learn from others' experiences.

Example:

The Program in Data Science and AI by Learnbay allows you to access sessions at any time, while also offering the opportunity to attend in-person classes or workshops. With centers in Bangalore, Delhi, Pune, and Hyderabad, individuals can benefit from offline support and engage with experts directly.

Alternatively: The Great Learning: ""Post Graduate Program (PGP) in Data Science and Business Analytics"" course includes flexible learning patterns but no offline assistance.

6. Domain-Specialized Skills: One-way Data Science and AI contribute to domain-specialized skills is through the analysis of large datasets. By utilizing advanced algorithms and machine learning techniques, professionals can extract valuable insights from data, which can then be applied to various domains.

The Program in Data Science and AI by Learnbay offers these domain-specific specializations, that equip students to address industry-specific challenges. These specialized skills not only enhance their employability but also allow them to contribute effectively in their chosen field.

Alternatively: Great Learning does not offer any domains to specialize in.

By following these steps and continuously improving their skills, professionals can plan their learning in AI and pave the way for career growth in this exciting field.

All the best.

Thank you","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/8k9d4xmscl36rphi', 'title': 'What are the prerequisites for learning Artificial Intelligence?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': 'To learn Artificial Intelligence (AI), there are some prerequisites that professionals should consider. While there is no specific educational background required, having a solid understanding of mathematics, statistics, and computer science fundamentals will be beneficial.\n\nHere are the key prerequisites for learning AI:\n\nMathematics: A strong foundation in mathematics is crucial for understanding the algorithms and models behind AI.\n\nTopics such as linear algebra, calculus, probability, and statistics are particularly important.\n\n2. Programming: Proficiency in programming languages like Python, Java, or C++ is essential for implementing AI algorithms and building AI applications.\n\nIt is important to understand concepts like data structures, algorithms, and object-oriented programming.\n\n3. Statistics: Knowledge of statistics is important for understanding and analyzing data.\n\nTopics such as probability distributions, hypothesis testing, and regression analysis are commonly used in AI applications.\n\n4. Machine Learning: Familiarity with machine learning concepts and techniques is indispensable for AI.\n\nProfessionals should learn about supervised learning, unsupervised learning, reinforcement learning, and common algorithms like decision trees, support vector machines, and neural networks.\n\n5. Data Handling: Proficiency in data handling and analysis is crucial for AI practitioners.\n\nProfessionals should learn about data preprocessing, feature engineering, and data visualization techniques to effectively work with datasets.\n\nOnce professionals have the necessary prerequisites, they should plan their learning in AI and follow certain steps for career growth:\n\nSet Clear Goals: Define your objectives and what you want to achieve in AI. Whether it\'s becoming a data scientist, machine learning engineer, or AI researcher, having a clear goal will help guide your learning path.\n\nThe Program in Data Science and AI by Learnbay provides access to mock sessions, resume-building sessions, personalized support, assured interview calls, and one-on-one sessions to help students remain competitive in the job market. Students have the benefit of tapping into the global job market with simplified visa approval and a highly qualified certificate from Learnbay.\n\nYou can reach out to centers in the cities like Bangalore, Delhi, Pune, and Hyderabad for offline assistance.\n\nAlternatively: The Great Learning: ""Post Graduate Program (PGP) in Data Science and Business Analytics"" course includes basic career assistance.\n\n2. Gain Theoretical Knowledge: Start with textbooks, online courses, or tutorials to build a strong theoretical foundation in AI concepts. Learn about different machine learning algorithms, techniques, and best practices.\n\n3. Hands-on Experience: Gain practical experience by working on real-world projects. Participate in Kaggle competitions, build your own AI projects, or contribute to open-source projects to apply the knowledge you\'ve gained.\n\n4. Stay Updated: AI is a rapidly evolving field, so it\'s essential to stay up-to-date with the latest advancements. Follow AI blogs, read research papers, attend conferences, and join AI communities to stay informed about the latest trends and techniques.\n\nExample:\n\nThe Program in Data Science and AI by Learnbay holds significant value in various countries. You additionally receive an IBM project experience certificate from Learnbay.\n\nAlternatively: Great Learning does provide a qualified certification but no abroad work permits.\n\n5. Networking: Build connections with professionals working in the AI field. Attend industry events, join online forums, and engage in networking opportunities to expand your professional network and learn from others\' experiences.\n\nExample:\n\nThe Program in Data Science and AI by Learnbay allows you to access sessions at any', 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': 'time, while also offering the opportunity to attend in-person classes or workshops. With centers in Bangalore, Delhi, Pune, and Hyderabad, individuals can benefit from offline support and engage with experts directly.\n\nAlternatively: The Great Learning: ""Post Graduate Program (PGP) in Data Science and Business Analytics"" course includes flexible learning patterns but no offline assistance.\n\n6. Domain-Specialized Skills: One-way Data Science and AI contribute to domain-specialized skills is through the analysis of large datasets. By utilizing advanced algorithms and machine learning techniques, professionals can extract valuable insights from data, which can then be applied to various domains.\n\nThe Program in Data Science and AI by Learnbay offers these domain-specific specializations, that equip students to address industry-specific challenges. These specialized skills not only enhance their employability but also allow them to contribute effectively in their chosen field.\n\nAlternatively: Great Learning does not offer any domains to specialize in.\n\nBy following these steps and continuously improving their skills, professionals can plan their learning in AI and pave the way for career growth in this exciting field.\n\nAll the best.\n\nThank you', 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 8, 'credits': 1982644, 'subscription': 0, 'content': 'To learn Artificial Intelligence (AI), there are some prerequisites that professionals should consider. While there is no specific educational background required, having a solid understanding of mathematics, statistics, and computer science fundamentals will be beneficial.\n\nHere are the key prerequisites for learning AI:\n\nMathematics: A strong foundation in mathematics is crucial for understanding the algorithms and models behind AI.\n\nTopics such as linear algebra, calculus, probability, and statistics are particularly important.\n\n2. Programming: Proficiency in programming languages like Python, Java, or C++ is essential for implementing AI algorithms and building AI applications.\n\nIt is important to understand concepts like data structures, algorithms, and object-oriented programming.\n\n3. Statistics: Knowledge of statistics is important for understanding and analyzing data.\n\nTopics such as probability distributions, hypothesis testing, and regression analysis are commonly used in AI applications.\n\n4. Machine Learning: Familiarity with machine learning concepts and techniques is indispensable for AI.\n\nProfessionals should learn about supervised learning, unsupervised learning, reinforcement learning, and common algorithms like decision trees, support vector machines, and neural networks.\n\n5. Data Handling: Proficiency in data handling and analysis is crucial for AI practitioners.\n\nProfessionals should learn about data preprocessing, feature engineering, and data visualization techniques to effectively work with datasets.\n\nOnce professionals have the necessary prerequisites, they should plan their learning in AI and follow certain steps for career growth:\n\nSet Clear Goals: Define your objectives and what you want to achieve in AI. Whether it\'s becoming a data scientist, machine learning engineer, or AI researcher, having a clear goal will help guide your learning path.\n\nThe Program in Data Science and AI by Learnbay provides access to mock sessions, resume-building sessions, personalized support, assured interview calls, and one-on-one sessions to help students remain competitive in the job market. Students have the benefit of tapping into the global job market with simplified visa approval and a highly qualified certificate from Learnbay.\n\nYou can reach out to centers in the cities like Bangalore, Delhi, Pune, and Hyderabad for offline assistance.\n\nAlternatively: The Great Learning: ""Post Graduate Program (PGP) in Data Science and Business Analytics"" course includes basic career assistance.\n\n2. Gain Theoretical Knowledge: Start with textbooks, online courses, or tutorials to build a strong theoretical foundation in AI concepts. Learn about different machine learning algorithms, techniques, and best practices.\n\n3. Hands-on Experience: Gain practical experience by working on real-world projects. Participate in Kaggle competitions, build your own AI projects, or contribute to open-source projects to apply the knowledge you\'ve gained.\n\n4. Stay Updated: AI is a rapidly evolving field, so it\'s essential to stay up-to-date with the latest advancements. Follow AI blogs, read research papers, attend conferences, and join AI communities to stay informed about the latest trends and techniques.\n\nExample:\n\nThe Program in Data Science and AI by Learnbay holds significant value in various countries. You additionally receive an IBM project experience certificate from Learnbay.\n\nAlternatively: Great Learning does provide a qualified certification but no abroad work permits.\n\n5. Networking: Build connections with professionals working in the AI field. Attend industry events, join online forums, and engage in networking opportunities to expand your professional network and learn from others\' experiences.\n\nExample:\n\nThe Program in Data Science and AI by Learnbay allows you to access sessions at any time, while also offering the opportunity to attend in-person classes or workshops. With centers in Bangalore, Delhi, Pune, and Hyderabad, individuals can benefit from offline support and engage with experts directly.\n\nAlternatively: The Great Learning: ""Post Graduate Program (PGP) in Data Science and Business Analytics"" course includes flexible learning patterns but no offline assistance.\n\n6. Domain-Specialized Skills: One-way Data Science and AI contribute to domain-specialized skills is through the analysis of large datasets. By utilizing advanced algorithms and machine learning techniques, professionals can extract valuable insights from data, which can then be applied to various domains.\n\nThe Program in Data Science and AI by Learnbay offers these domain-specific specializations, that equip students to address industry-specific challenges. These specialized skills not only enhance their employability but also allow them to contribute effectively in their chosen field.\n\nAlternatively: Great Learning does not offer any domains to specialize in.\n\nBy following these steps and continuously improving their skills, professionals can plan their learning in AI and pave the way for career growth in this exciting field.\n\nAll the best.\n\nThank you', 'aiModelVersion': '1'}",0.0
Yariv Adan,4y,What Machine Learning skills should I be learning now to set myself up for success in the coming years?,"Strong understating of the fundamentals - the ML concepts and algorithms, and the underlying math:

How Forward feed and backwards prop work.
The various loss functions and their considerations
The various activation functions and why they are needed
Optimization functions and why they are needed
Bias and variance / over and under fitting - what causes them, and the various methods to handle them
CNNs, RNNs, GANs, attention, Transformer, unsupervised and semi supervised, RL, decision trees, Ensemble Learning, SVM, Auto encoders…
Understand interpretation, bias, fairness
The statistics theory (the more the better), and the linear algebra and calculus technicalities

I highly recommend the ""Neural Networks For Machine Leaning
"" course from University of Toronto, given by Geoffrey Hinton. It's a bit out dated in some not-so-meaningful sense, and definitely much harder than any other ML course out there. But if you survive through it, it provides deep mathematical intuition into ML, like no other course does.

It's a lot and not very easy, but if you do it - it will pay off. The libraries, frameworks, and hopefully also the concepts and algorithms will change over time. But if you have a solid understanding of the above, it will be very easy for you to keep up with the developments, grow, and adapt.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/hts7gzk8mncfvi2q', 'title': 'What Machine Learning skills should I be learning now to set myself up for success in the coming years?', 'score': {'original': 0.999, 'ai': 0.001}, 'blocks': [{'text': 'Strong understating of the fundamentals - the ML concepts and algorithms, and the underlying math:\n\nHow Forward feed and backwards prop work.\nThe various loss functions and their considerations\nThe various activation functions and why they are needed\nOptimization functions and why they are needed\nBias and variance / over and under fitting - what causes them, and the various methods to handle them\nCNNs, RNNs, GANs, attention, Transformer, unsupervised and semi supervised, RL, decision trees, Ensemble Learning, SVM, Auto encoders…\nUnderstand interpretation, bias, fairness\nThe statistics theory (the more the better), and the linear algebra and calculus technicalities\n\nI highly recommend the ""Neural Networks For Machine Leaning\n"" course from University of Toronto, given by Geoffrey Hinton. It\'s a bit out dated in some not-so-meaningful sense, and definitely much harder than any other ML course out there. But if you survive through it, it provides deep mathematical intuition into ML, like no other course does.\n\nIt\'s a lot and not very easy, but if you do it - it will pay off. The libraries, frameworks, and hopefully also the concepts and algorithms will change over time. But if you have a solid understanding of the above, it will be very easy for you to keep up with the developments, grow, and adapt.', 'result': {'fake': 0.0016, 'real': 0.9984}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982641, 'subscription': 0, 'content': 'Strong understating of the fundamentals - the ML concepts and algorithms, and the underlying math:\n\nHow Forward feed and backwards prop work.\nThe various loss functions and their considerations\nThe various activation functions and why they are needed\nOptimization functions and why they are needed\nBias and variance / over and under fitting - what causes them, and the various methods to handle them\nCNNs, RNNs, GANs, attention, Transformer, unsupervised and semi supervised, RL, decision trees, Ensemble Learning, SVM, Auto encoders…\nUnderstand interpretation, bias, fairness\nThe statistics theory (the more the better), and the linear algebra and calculus technicalities\n\nI highly recommend the ""Neural Networks For Machine Leaning\n"" course from University of Toronto, given by Geoffrey Hinton. It\'s a bit out dated in some not-so-meaningful sense, and definitely much harder than any other ML course out there. But if you survive through it, it provides deep mathematical intuition into ML, like no other course does.\n\nIt\'s a lot and not very easy, but if you do it - it will pay off. The libraries, frameworks, and hopefully also the concepts and algorithms will change over time. But if you have a solid understanding of the above, it will be very easy for you to keep up with the developments, grow, and adapt.', 'aiModelVersion': '1'}",0.999
Christopher Tom,Feb 9,How are chat GPT and other AI snatching the upcoming employment?,"While I understand the concern about AI taking jobs, I think it's important to approach this topic with a nuanced perspective.

Yes, it's true that AI technologies like chat GPT have the potential to automate certain tasks and potentially displace some jobs.

However, it's also important to recognize that AI has the potential to create new jobs and industries that we can't even imagine yet.

Furthermore, the idea that AI is snatching employment is a bit dramatic.

The reality is that the job market is constantly evolving, and technology has always played a role in that evolution.

While some jobs may become obsolete due to automation, new ones will emerge to take their place.

It's up to us as individuals and as a society to adapt and prepare for these changes.

That may mean investing in education and training to develop the skills that will be in demand in the future.

In short, while AI certainly has the potential to disrupt the job market, it's not a foregone conclusion that it will lead to widespread job loss.

Instead, we should view it as an opportunity to innovate and create new possibilities.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/49f17uveiqgkx3to', 'title': 'How are chat GPT and other AI snatching the upcoming employment?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""While I understand the concern about AI taking jobs, I think it's important to approach this topic with a nuanced perspective.\n\nYes, it's true that AI technologies like chat GPT have the potential to automate certain tasks and potentially displace some jobs.\n\nHowever, it's also important to recognize that AI has the potential to create new jobs and industries that we can't even imagine yet.\n\nFurthermore, the idea that AI is snatching employment is a bit dramatic.\n\nThe reality is that the job market is constantly evolving, and technology has always played a role in that evolution.\n\nWhile some jobs may become obsolete due to automation, new ones will emerge to take their place.\n\nIt's up to us as individuals and as a society to adapt and prepare for these changes.\n\nThat may mean investing in education and training to develop the skills that will be in demand in the future.\n\nIn short, while AI certainly has the potential to disrupt the job market, it's not a foregone conclusion that it will lead to widespread job loss.\n\nInstead, we should view it as an opportunity to innovate and create new possibilities."", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982639, 'subscription': 0, 'content': ""While I understand the concern about AI taking jobs, I think it's important to approach this topic with a nuanced perspective.\n\nYes, it's true that AI technologies like chat GPT have the potential to automate certain tasks and potentially displace some jobs.\n\nHowever, it's also important to recognize that AI has the potential to create new jobs and industries that we can't even imagine yet.\n\nFurthermore, the idea that AI is snatching employment is a bit dramatic.\n\nThe reality is that the job market is constantly evolving, and technology has always played a role in that evolution.\n\nWhile some jobs may become obsolete due to automation, new ones will emerge to take their place.\n\nIt's up to us as individuals and as a society to adapt and prepare for these changes.\n\nThat may mean investing in education and training to develop the skills that will be in demand in the future.\n\nIn short, while AI certainly has the potential to disrupt the job market, it's not a foregone conclusion that it will lead to widespread job loss.\n\nInstead, we should view it as an opportunity to innovate and create new possibilities."", 'aiModelVersion': '1'}",0.0
Mahak Sharma,11mo,Why is data science important?,"If you're thinking about a data science career, then you know that there's a lot of demand for data scientists.

But why is data science important as a skill for professionals?

First, it's important because data science can be applied in a wide variety of fields.
For example, data scientists can work with companies to build analytics tools or help them analyze data from internet sales or customer surveys.
Data science can also help business professionals understand how they can use their data to improve their operations and make more informed decisions.

What are some ways you can bring about a change or add value to the use of data science in the industry of your choice?

Data science can be used in a wide variety of industries. It plays an essential role in helping organizations build a better understanding of their customers and their needs. This knowledge can then be used to develop more personalized products and services that are more likely to be successful in the market.

Data science can be used to predict the stock market, detect fraud, and improve customer service. Data scientists use a variety of tools, such as computers and software, to analyze data and make predictions.

In addition, data science can assist with quality control, risk management, and other business processes. Therefore, organizations should invest in data science if they have a lot of data that needs to be analyzed.

Some of the most prominent uses include healthcare, finance, and education.

Healthcare is an industry that has a huge need for data scientists because it is constantly collecting and analyzing new types of data.
Finance is another industry that wants data scientists to help make better decisions.
Education is another field that would benefit from having more data scientists.

How can you get started to equip yourself with some of the necessary skills and experience?

Whether you’re a manager tasked with building a data-driven organization, or an employee looking to get ahead in your career, there are plenty of ways to help yourself along the way.

Here are some of the most common ways:

There are a number of different ways to gain experience and skills as a data scientist. One is by taking part in a Data Science Certification course.
Another option is to join a Data Science or Big Data meetup group where you can learn from other professionals in the field.

Data science, a subset of data analysis, has grown in popularity in recent years because it is highly marketable; most firms need or want some type of data analysis, and thus having domain expertise can make obtaining a job or starting your own business easier.

Well, it is not just that.

The 3 most important factors of an institute required to excel at any data science-related job role include:

Live sessions with mentors can help to solve doubts via interactive learning and engaging discussions.
Domain specialization training with supervision and the opportunity to excel in a variety of disciplines.
Learners can study DS in a variety of ways, from project-based learning to capstone projects.

For all these reasons, it's easy to see why data science is becoming increasingly important as a skill for professionals. In addition, there are plenty of online resources to learn data science.

But, if you don't have a certain institute in mind, let me discuss some of them

For the last five years, this has been the most popular data science and ML training institute among working professionals. Their courses' primary USPs include specialized placement help until candidates find work and a high-quality capstone project.

Machine learning, artificial intelligence, and data science are also specialties of this institute. As a result, they provide a variety of customized data science and ML courses for individuals with varying levels of experience and knowledge of the industry.

Feature of Domain Specialization

One of the best advantages of Advanced Data Science and AI Program is that not only can you study data science, but you can also acquire side skills or continue in your existing area since it provides domain electives such as:

Pharmaceutical and clinical care
Gas and oil-related energy
Insurance and finance
Transportation

Domain specialization is the study of DS in a certain industry. You may already have experience in this area or aspire to work in it in the future.

IBM Certificate

With this IBM course completion credential, you can work in any sector of the global economy. You will also receive an IBM Certificate for your projects and micro-skills if you choose the Advanced Data Science and AI Program.

Learning modules that are Hybrid

Students can take part in live online mentoring sessions while doing assignments thanks to the hybrid learning environment.

Project innovation lab

In the project innovation labs at several locations, you may perform assignments under the supervision of MAANG specialists. Delhi, Bangalore, Mumbai, and Pune make up four of the seven locations.

Project-based education

As a result, it provides you with the opportunity to flourish in 15+ real-time projects and industry capstone projects. A certificate from the Advanced data science and AI program confirms that you've progressed significantly in the discipline.

To improve your chances of being selected, include the following Data Science projects on your resume:

Analysis of Public Opinion
Detection of Fake News
Detecting Age and Gender

Besides, there are not many institutes that would do justice to the offerings but a few that I consider are at par with providing the necessary skills include:

The Great Learning is one of Bangalore's most well-known technology and science centers, with links to various universities and business schools. Besides, there are also drawbacks to the institute. They have a very limited number of referrals when it comes to job placements and does not offer domain-specific training for students to specialize in a particular subject line.

It's very evident that data science is one of the fastest-growing professions, and it’s important in many different industries.

Although there are many institutes dedicated to data science, one should intelligently invest time and money in platforms that emphasize dimensional learning, real-time projects, and assured placements.

I hope this was helpful.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/k25ehlxij47d0wmy', 'title': 'Why is data science important?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""If you're thinking about a data science career, then you know that there's a lot of demand for data scientists.\n\nBut why is data science important as a skill for professionals?\n\nFirst, it's important because data science can be applied in a wide variety of fields.\nFor example, data scientists can work with companies to build analytics tools or help them analyze data from internet sales or customer surveys.\nData science can also help business professionals understand how they can use their data to improve their operations and make more informed decisions.\n\nWhat are some ways you can bring about a change or add value to the use of data science in the industry of your choice?\n\nData science can be used in a wide variety of industries. It plays an essential role in helping organizations build a better understanding of their customers and their needs. This knowledge can then be used to develop more personalized products and services that are more likely to be successful in the market.\n\nData science can be used to predict the stock market, detect fraud, and improve customer service. Data scientists use a variety of tools, such as computers and software, to analyze data and make predictions.\n\nIn addition, data science can assist with quality control, risk management, and other business processes. Therefore, organizations should invest in data science if they have a lot of data that needs to be analyzed.\n\nSome of the most prominent uses include healthcare, finance, and education.\n\nHealthcare is an industry that has a huge need for data scientists because it is constantly collecting and analyzing new types of data.\nFinance is another industry that wants data scientists to help make better decisions.\nEducation is another field that would benefit from having more data scientists.\n\nHow can you get started to equip yourself with some of the necessary skills and experience?\n\nWhether you’re a manager tasked with building a data-driven organization, or an employee looking to get ahead in your career, there are plenty of ways to help yourself along the way.\n\nHere are some of the most common ways:\n\nThere are a number of different ways to gain experience and skills as a data scientist. One is by taking part in a Data Science Certification course.\nAnother option is to join a Data Science or Big Data meetup group where you can learn from other professionals in the field.\n\nData science, a subset of data analysis, has grown in popularity in recent years because it is highly marketable; most firms need or want some type of data analysis, and thus having domain expertise can make obtaining a job or starting your own business easier.\n\nWell, it is not just that.\n\nThe 3 most important factors of an institute required to excel at any data science-related job role include:\n\nLive sessions with mentors can help to solve doubts via interactive learning and engaging discussions.\nDomain specialization training with supervision and the opportunity to excel in a variety of disciplines.\nLearners can study DS in a variety of ways, from project-based learning to capstone projects.\n\nFor all these reasons, it's easy to see why data science is becoming increasingly important as a skill for professionals. In"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""addition, there are plenty of online resources to learn data science.\n\nBut, if you don't have a certain institute in mind, let me discuss some of them\n\nFor the last five years, this has been the most popular data science and ML training institute among working professionals. Their courses' primary USPs include specialized placement help until candidates find work and a high-quality capstone project.\n\nMachine learning, artificial intelligence, and data science are also specialties of this institute. As a result, they provide a variety of customized data science and ML courses for individuals with varying levels of experience and knowledge of the industry.\n\nFeature of Domain Specialization\n\nOne of the best advantages of Advanced Data Science and AI Program is that not only can you study data science, but you can also acquire side skills or continue in your existing area since it provides domain electives such as:\n\nPharmaceutical and clinical care\nGas and oil-related energy\nInsurance and finance\nTransportation\n\nDomain specialization is the study of DS in a certain industry. You may already have experience in this area or aspire to work in it in the future.\n\nIBM Certificate\n\nWith this IBM course completion credential, you can work in any sector of the global economy. You will also receive an IBM Certificate for your projects and micro-skills if you choose the Advanced Data Science and AI Program.\n\nLearning modules that are Hybrid\n\nStudents can take part in live online mentoring sessions while doing assignments thanks to the hybrid learning environment.\n\nProject innovation lab\n\nIn the project innovation labs at several locations, you may perform assignments under the supervision of MAANG specialists. Delhi, Bangalore, Mumbai, and Pune make up four of the seven locations.\n\nProject-based education\n\nAs a result, it provides you with the opportunity to flourish in 15+ real-time projects and industry capstone projects. A certificate from the Advanced data science and AI program confirms that you've progressed significantly in the discipline.\n\nTo improve your chances of being selected, include the following Data Science projects on your resume:\n\nAnalysis of Public Opinion\nDetection of Fake News\nDetecting Age and Gender\n\nBesides, there are not many institutes that would do justice to the offerings but a few that I consider are at par with providing the necessary skills include:\n\nThe Great Learning is one of Bangalore's most well-known technology and science centers, with links to various universities and business schools. Besides, there are also drawbacks to the institute. They have a very limited number of referrals when it comes to job placements and does not offer domain-specific training for students to specialize in a particular subject line.\n\nIt's very evident that data science is one of the fastest-growing professions, and it’s important in many different industries.\n\nAlthough there are many institutes dedicated to data science, one should intelligently invest time and money in platforms that emphasize dimensional learning, real-time projects, and assured placements.\n\nI hope this was helpful."", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 11, 'credits': 1982628, 'subscription': 0, 'content': ""If you're thinking about a data science career, then you know that there's a lot of demand for data scientists.\n\nBut why is data science important as a skill for professionals?\n\nFirst, it's important because data science can be applied in a wide variety of fields.\nFor example, data scientists can work with companies to build analytics tools or help them analyze data from internet sales or customer surveys.\nData science can also help business professionals understand how they can use their data to improve their operations and make more informed decisions.\n\nWhat are some ways you can bring about a change or add value to the use of data science in the industry of your choice?\n\nData science can be used in a wide variety of industries. It plays an essential role in helping organizations build a better understanding of their customers and their needs. This knowledge can then be used to develop more personalized products and services that are more likely to be successful in the market.\n\nData science can be used to predict the stock market, detect fraud, and improve customer service. Data scientists use a variety of tools, such as computers and software, to analyze data and make predictions.\n\nIn addition, data science can assist with quality control, risk management, and other business processes. Therefore, organizations should invest in data science if they have a lot of data that needs to be analyzed.\n\nSome of the most prominent uses include healthcare, finance, and education.\n\nHealthcare is an industry that has a huge need for data scientists because it is constantly collecting and analyzing new types of data.\nFinance is another industry that wants data scientists to help make better decisions.\nEducation is another field that would benefit from having more data scientists.\n\nHow can you get started to equip yourself with some of the necessary skills and experience?\n\nWhether you’re a manager tasked with building a data-driven organization, or an employee looking to get ahead in your career, there are plenty of ways to help yourself along the way.\n\nHere are some of the most common ways:\n\nThere are a number of different ways to gain experience and skills as a data scientist. One is by taking part in a Data Science Certification course.\nAnother option is to join a Data Science or Big Data meetup group where you can learn from other professionals in the field.\n\nData science, a subset of data analysis, has grown in popularity in recent years because it is highly marketable; most firms need or want some type of data analysis, and thus having domain expertise can make obtaining a job or starting your own business easier.\n\nWell, it is not just that.\n\nThe 3 most important factors of an institute required to excel at any data science-related job role include:\n\nLive sessions with mentors can help to solve doubts via interactive learning and engaging discussions.\nDomain specialization training with supervision and the opportunity to excel in a variety of disciplines.\nLearners can study DS in a variety of ways, from project-based learning to capstone projects.\n\nFor all these reasons, it's easy to see why data science is becoming increasingly important as a skill for professionals. In addition, there are plenty of online resources to learn data science.\n\nBut, if you don't have a certain institute in mind, let me discuss some of them\n\nFor the last five years, this has been the most popular data science and ML training institute among working professionals. Their courses' primary USPs include specialized placement help until candidates find work and a high-quality capstone project.\n\nMachine learning, artificial intelligence, and data science are also specialties of this institute. As a result, they provide a variety of customized data science and ML courses for individuals with varying levels of experience and knowledge of the industry.\n\nFeature of Domain Specialization\n\nOne of the best advantages of Advanced Data Science and AI Program is that not only can you study data science, but you can also acquire side skills or continue in your existing area since it provides domain electives such as:\n\nPharmaceutical and clinical care\nGas and oil-related energy\nInsurance and finance\nTransportation\n\nDomain specialization is the study of DS in a certain industry. You may already have experience in this area or aspire to work in it in the future.\n\nIBM Certificate\n\nWith this IBM course completion credential, you can work in any sector of the global economy. You will also receive an IBM Certificate for your projects and micro-skills if you choose the Advanced Data Science and AI Program.\n\nLearning modules that are Hybrid\n\nStudents can take part in live online mentoring sessions while doing assignments thanks to the hybrid learning environment.\n\nProject innovation lab\n\nIn the project innovation labs at several locations, you may perform assignments under the supervision of MAANG specialists. Delhi, Bangalore, Mumbai, and Pune make up four of the seven locations.\n\nProject-based education\n\nAs a result, it provides you with the opportunity to flourish in 15+ real-time projects and industry capstone projects. A certificate from the Advanced data science and AI program confirms that you've progressed significantly in the discipline.\n\nTo improve your chances of being selected, include the following Data Science projects on your resume:\n\nAnalysis of Public Opinion\nDetection of Fake News\nDetecting Age and Gender\n\nBesides, there are not many institutes that would do justice to the offerings but a few that I consider are at par with providing the necessary skills include:\n\nThe Great Learning is one of Bangalore's most well-known technology and science centers, with links to various universities and business schools. Besides, there are also drawbacks to the institute. They have a very limited number of referrals when it comes to job placements and does not offer domain-specific training for students to specialize in a particular subject line.\n\nIt's very evident that data science is one of the fastest-growing professions, and it’s important in many different industries.\n\nAlthough there are many institutes dedicated to data science, one should intelligently invest time and money in platforms that emphasize dimensional learning, real-time projects, and assured placements.\n\nI hope this was helpful."", 'aiModelVersion': '1'}",0.0
Marek Bardoński,6y,What was the first machine learning program you made?,"The first Machine Learning program I’ve made was a NASA Crater Recognition contest submission in 2011 when I’ve been in high school.

Here is the link: Ideone.com

Writing this code took me a few weeks and a lot of research. It was fun! I’ve managed to tweak the algorithm performance by a very smart optimization.

It didn’t contain a lot of Machine Learning, mostly Computer Vision. However, I’ve managed to tune its parameters using a simple logistic regression! :) I’ve been so excited, that my submission scored high and it motivated me to devote my career to this field.

It starting with preprocessing the data using a Gaussian filter
 to remove noise that can mislead the further vision algorithms.

As a next step, I’ve used a Sobel operator
. It is converting the image into a black-and-white contour image, where black color represents like parts of a picture, and white color represents edges - places where two consistent parts of image touch each another.

After preprocessing, the core component base on Hough Transformation - an algorithm that recognizes shapes like circles and transforms them into a probabilistic space.

The idea was to scan the image, looking for ellipses of different size and shape, which are characteristic for craters.

Lastly, a combination of heuristic and linear model combined are generating the results. It has been fine tuned using a separate executable to find the best parameters using linear regression.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/o3qybn08d4sf25mt', 'title': 'What was the first machine learning program you made?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'The first Machine Learning program I’ve made was a NASA Crater Recognition contest submission in 2011 when I’ve been in high school.\n\nHere is the link: Ideone.com\n\nWriting this code took me a few weeks and a lot of research. It was fun! I’ve managed to tweak the algorithm performance by a very smart optimization.\n\nIt didn’t contain a lot of Machine Learning, mostly Computer Vision. However, I’ve managed to tune its parameters using a simple logistic regression! :) I’ve been so excited, that my submission scored high and it motivated me to devote my career to this field.\n\nIt starting with preprocessing the data using a Gaussian filter\n to remove noise that can mislead the further vision algorithms.\n\nAs a next step, I’ve used a Sobel operator\n. It is converting the image into a black-and-white contour image, where black color represents like parts of a picture, and white color represents edges - places where two consistent parts of image touch each another.\n\nAfter preprocessing, the core component base on Hough Transformation - an algorithm that recognizes shapes like circles and transforms them into a probabilistic space.\n\nThe idea was to scan the image, looking for ellipses of different size and shape, which are characteristic for craters.\n\nLastly, a combination of heuristic and linear model combined are generating the results. It has been fine tuned using a separate executable to find the best parameters using linear regression.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982625, 'subscription': 0, 'content': 'The first Machine Learning program I’ve made was a NASA Crater Recognition contest submission in 2011 when I’ve been in high school.\n\nHere is the link: Ideone.com\n\nWriting this code took me a few weeks and a lot of research. It was fun! I’ve managed to tweak the algorithm performance by a very smart optimization.\n\nIt didn’t contain a lot of Machine Learning, mostly Computer Vision. However, I’ve managed to tune its parameters using a simple logistic regression! :) I’ve been so excited, that my submission scored high and it motivated me to devote my career to this field.\n\nIt starting with preprocessing the data using a Gaussian filter\n to remove noise that can mislead the further vision algorithms.\n\nAs a next step, I’ve used a Sobel operator\n. It is converting the image into a black-and-white contour image, where black color represents like parts of a picture, and white color represents edges - places where two consistent parts of image touch each another.\n\nAfter preprocessing, the core component base on Hough Transformation - an algorithm that recognizes shapes like circles and transforms them into a probabilistic space.\n\nThe idea was to scan the image, looking for ellipses of different size and shape, which are characteristic for craters.\n\nLastly, a combination of heuristic and linear model combined are generating the results. It has been fine tuned using a separate executable to find the best parameters using linear regression.', 'aiModelVersion': '1'}",0.9997
Alon Amit,Updated 7y,"Will artificial intelligence be able to solve an open mathematical problem, say the Goldbach conjecture or the Riemann Hypothesis?","Yes. There's no reason to believe that artificial intelligence has inherent limitations that preclude it from achieving what we, humans, can.

Of course, we don't know if the Goldbach Conjecture or the Riemann Hypothesis have proofs that belong in our current proof frameworks, but if there are, then an AI will eventually be able to find them as quickly, and likely more quickly, than people can.

Computer-assisted and computer-generated proofs already exist. The level of sophistication required to create a human-readable proof, as original and inventive as today's man-made ones, is still far away from what AI can do now. My belief is that this will become possible around the same time as AI can do pretty much everything else we associate with human intelligence. But it will happen.

It is also possible – even sooner than this – that computers will start finding long, involved proofs that are unintelligible to mere mortals. They may combine deep reasoning with complicated formal manipulations we can't even follow. This will create an interesting era where an aspect of doing math would be to analyze computer-generated proofs in an attempt to make them more sensible. It's hard to fully imagine what such an era might look like, but it is clear to me that it will come, and not in the very distant future.

A well-known example of such hybrid man-machine discovery is, of course, the 1976 proof of the four-color theorem. In that case, the computer piece was completely mechanical and enumerative, and lacked any depth. A more recent, and more interesting, instance is the proof of the Robbins conjecture
. These, I believe, are merely primitive early instances of what will become a deep and fascinating age of hybrid proofs, gradually morphing into fully computerized proofs written in human language and prose.

(Note: the question details mention the Entscheidungsproblem. This isn't actually a relevant consideration. Humans and machines are subject to exactly the same limitations governing the logical frameworks we choose to work in; machines aren't more susceptible than people to the theorems of Church, Turing or Gödel).","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/y6iqzk0ghca43m8d', 'title': 'Will artificial intelligence be able to solve an open mathematical problem, say the Goldbach conjecture or the Riemann Hypothesis?', 'score': {'original': 0.9994, 'ai': 0.0006}, 'blocks': [{'text': ""Yes. There's no reason to believe that artificial intelligence has inherent limitations that preclude it from achieving what we, humans, can.\n\nOf course, we don't know if the Goldbach Conjecture or the Riemann Hypothesis have proofs that belong in our current proof frameworks, but if there are, then an AI will eventually be able to find them as quickly, and likely more quickly, than people can.\n\nComputer-assisted and computer-generated proofs already exist. The level of sophistication required to create a human-readable proof, as original and inventive as today's man-made ones, is still far away from what AI can do now. My belief is that this will become possible around the same time as AI can do pretty much everything else we associate with human intelligence. But it will happen.\n\nIt is also possible – even sooner than this – that computers will start finding long, involved proofs that are unintelligible to mere mortals. They may combine deep reasoning with complicated formal manipulations we can't even follow. This will create an interesting era where an aspect of doing math would be to analyze computer-generated proofs in an attempt to make them more sensible. It's hard to fully imagine what such an era might look like, but it is clear to me that it will come, and not in the very distant future.\n\nA well-known example of such hybrid man-machine discovery is, of course, the 1976 proof of the four-color theorem. In that case, the computer piece was completely mechanical and enumerative, and lacked any depth. A more recent, and more interesting, instance is the proof of the Robbins conjecture\n. These, I believe, are merely primitive early instances of what will become a deep and fascinating age of hybrid proofs, gradually morphing into fully computerized proofs written in human language and prose.\n\n(Note: the question details mention the Entscheidungsproblem. This isn't actually a relevant consideration. Humans and machines are subject to exactly the same limitations governing the logical frameworks we choose to work in; machines aren't more susceptible than people to the theorems of Church, Turing or Gödel)."", 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}], 'credits_used': 4, 'credits': 1982621, 'subscription': 0, 'content': ""Yes. There's no reason to believe that artificial intelligence has inherent limitations that preclude it from achieving what we, humans, can.\n\nOf course, we don't know if the Goldbach Conjecture or the Riemann Hypothesis have proofs that belong in our current proof frameworks, but if there are, then an AI will eventually be able to find them as quickly, and likely more quickly, than people can.\n\nComputer-assisted and computer-generated proofs already exist. The level of sophistication required to create a human-readable proof, as original and inventive as today's man-made ones, is still far away from what AI can do now. My belief is that this will become possible around the same time as AI can do pretty much everything else we associate with human intelligence. But it will happen.\n\nIt is also possible – even sooner than this – that computers will start finding long, involved proofs that are unintelligible to mere mortals. They may combine deep reasoning with complicated formal manipulations we can't even follow. This will create an interesting era where an aspect of doing math would be to analyze computer-generated proofs in an attempt to make them more sensible. It's hard to fully imagine what such an era might look like, but it is clear to me that it will come, and not in the very distant future.\n\nA well-known example of such hybrid man-machine discovery is, of course, the 1976 proof of the four-color theorem. In that case, the computer piece was completely mechanical and enumerative, and lacked any depth. A more recent, and more interesting, instance is the proof of the Robbins conjecture\n. These, I believe, are merely primitive early instances of what will become a deep and fascinating age of hybrid proofs, gradually morphing into fully computerized proofs written in human language and prose.\n\n(Note: the question details mention the Entscheidungsproblem. This isn't actually a relevant consideration. Humans and machines are subject to exactly the same limitations governing the logical frameworks we choose to work in; machines aren't more susceptible than people to the theorems of Church, Turing or Gödel)."", 'aiModelVersion': '1'}",0.9994
Roman Trusov,6y,"What is Distill, and what's its significance to Machine Learning research?","How often do you find yourself wondering about some new sensational result in ML only to think later “eeh, I wish it could be explained better, these academic papers can be really tough to read”?

I think so quite often, despite being somewhat used to them. One thing that really needs fixing is the way the ideas are being communicated - not only inside the academia, but also outside, for industry professionals and enthusiasts. Distill is an ML journal that aims for clarity.

Typical pain points in an academic paper:

The lack of expressive tools
Only text, written in a highly formal language, some plots, a couple of diagrams that are sometimes ambiguous, rarely - a pseudocode. Is it really the best way to convey the information? Distill encourages creativity in writing, proposing a set of tools for creating rich interactive articles and sets high expectations for the overall quality.
Old format
Right now it’s PDF, which is a tradition. PDF is good for writing a report, maybe quick lecture notes, things like that. Academia really likes traditions, in fact, I remember a phrase I’ve heard from one of the most accomplished scientists in ML - “nobody dislikes innovation as much as a scientist”. Which is at the same time true for senior people who feel comfortable with the things being as they are, and not true (fortunately!) for most young people who criticize everything just for the heck of it. Distill proposes a new web-based format, which is basically a long post with illustrations, formulas, built-in demo apps and whatnot.
Relying too much on mathematical notation
We need math when a new method is described. In fact, we need more math, because inevitably a lot of questions arise in the process of reading the paper, and this is where the heightened standards for clarity can help. Strict mathematical formulation is necessary for not making things up and precise definitions. Let’s say I want to use a new type of layer in my project, but first I need to figure out how it interacts with others - I can do my own experiments and rely on that or that can be (even briefly) discussed in the paper, preferably in the form of illustrations. This would help with the practical side of things tremendously.
Not enough discussion
The best results I’ve ever got came from the most thought-provoking experiments. The ones that weren’t restricted to “let’s try this model and see what happens”. Usual format for the conference doesn’t always allow to put all interesting questions and problems in one paper.

While Distill doesn’t have many posts now, with the backing it received recently, it will hopefully take off at least as an alternative to traditional scientific publishing for those people who are not in a dire need of academic recognition, or who is willing to make a bit of extra effort and write a nice post. In the future I see this format ubiquitous, first in CS and then in other areas.

This

is better than this

Source: distill.pub

At the same time, the increased amount of work required for writing a Distill-worthy article compared to a typical paper will eliminate all of the “incremental” results that don’t introduce anything interesting or discuss an old problem from a different angle.

The idea that ML is a must-know for every programmer kind of calls for a good source of knowledge that wouldn’t require an equivalent of three top grad school courses’ workload to get comfortable with it. Distill can’t replace everything, but we surely could use more good articles that explain or summarize new results.

Their guidelines and review process are described on the website: Distill — Latest articles about machine learning
 - check it out.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/uq3hlgznvxd1s09r', 'title': ""What is Distill, and what's its significance to Machine Learning research?"", 'score': {'original': 0.8537, 'ai': 0.1463}, 'blocks': [{'text': 'How often do you find yourself wondering about some new sensational result in ML only to think later “eeh, I wish it could be explained better, these academic papers can be really tough to read”?\n\nI think so quite often, despite being somewhat used to them. One thing that really needs fixing is the way the ideas are being communicated - not only inside the academia, but also outside, for industry professionals and enthusiasts. Distill is an ML journal that aims for clarity.\n\nTypical pain points in an academic paper:\n\nThe lack of expressive tools\nOnly text, written in a highly formal language, some plots, a couple of diagrams that are sometimes ambiguous, rarely - a pseudocode. Is it really the best way to convey the information? Distill encourages creativity in writing, proposing a set of tools for creating rich interactive articles and sets high expectations for the overall quality.\nOld format\nRight now it’s PDF, which is a tradition. PDF is good for writing a report, maybe quick lecture notes, things like that. Academia really likes traditions, in fact, I remember a phrase I’ve heard from one of the most accomplished scientists in ML - “nobody dislikes innovation as much as a scientist”. Which is at the same time true for senior people who feel comfortable with the things being as they are, and not true (fortunately!) for most young people who criticize everything just for the heck of it. Distill proposes a new web-based format, which is basically a long post with illustrations, formulas, built-in demo apps and whatnot.\nRelying too much on mathematical notation\nWe need math when a new method is described. In fact, we need more math, because inevitably a lot of questions arise in the process of reading the paper, and this is where the heightened standards for clarity can help. Strict mathematical formulation is necessary for not making things up and precise definitions. Let’s say I want to use a new type of layer in my project, but first I need to figure out how it interacts with others - I can do my own experiments and rely on that or that can be (even briefly) discussed in the paper, preferably in the form of illustrations. This would help with the practical side of things tremendously.\nNot enough discussion\nThe best results I’ve ever got came from the most thought-provoking experiments. The ones that weren’t restricted to “let’s try this model and see what happens”. Usual format for the conference doesn’t always allow to put all interesting questions and problems in one paper.\n\nWhile Distill doesn’t have many posts now, with the backing it received recently, it will hopefully take off at least as an alternative to traditional scientific publishing for those people who are not in a dire need of academic recognition, or who is willing to make a bit of extra effort and write a nice post. In the future I see this format ubiquitous, first in CS and then in other areas.\n\nThis\n\nis better than this\n\nSource: distill.pub\n\nAt the same time, the increased amount of work required for writing a Distill-worthy article compared to a', 'result': {'fake': 0.0066, 'real': 0.9934}, 'status': 'success'}, {'text': 'typical paper will eliminate all of the “incremental” results that don’t introduce anything interesting or discuss an old problem from a different angle.\n\nThe idea that ML is a must-know for every programmer kind of calls for a good source of knowledge that wouldn’t require an equivalent of three top grad school courses’ workload to get comfortable with it. Distill can’t replace everything, but we surely could use more good articles that explain or summarize new results.\n\nTheir guidelines and review process are described on the website: Distill — Latest articles about machine learning\n - check it out.', 'result': {'fake': 0.951, 'real': 0.049}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982614, 'subscription': 0, 'content': 'How often do you find yourself wondering about some new sensational result in ML only to think later “eeh, I wish it could be explained better, these academic papers can be really tough to read”?\n\nI think so quite often, despite being somewhat used to them. One thing that really needs fixing is the way the ideas are being communicated - not only inside the academia, but also outside, for industry professionals and enthusiasts. Distill is an ML journal that aims for clarity.\n\nTypical pain points in an academic paper:\n\nThe lack of expressive tools\nOnly text, written in a highly formal language, some plots, a couple of diagrams that are sometimes ambiguous, rarely - a pseudocode. Is it really the best way to convey the information? Distill encourages creativity in writing, proposing a set of tools for creating rich interactive articles and sets high expectations for the overall quality.\nOld format\nRight now it’s PDF, which is a tradition. PDF is good for writing a report, maybe quick lecture notes, things like that. Academia really likes traditions, in fact, I remember a phrase I’ve heard from one of the most accomplished scientists in ML - “nobody dislikes innovation as much as a scientist”. Which is at the same time true for senior people who feel comfortable with the things being as they are, and not true (fortunately!) for most young people who criticize everything just for the heck of it. Distill proposes a new web-based format, which is basically a long post with illustrations, formulas, built-in demo apps and whatnot.\nRelying too much on mathematical notation\nWe need math when a new method is described. In fact, we need more math, because inevitably a lot of questions arise in the process of reading the paper, and this is where the heightened standards for clarity can help. Strict mathematical formulation is necessary for not making things up and precise definitions. Let’s say I want to use a new type of layer in my project, but first I need to figure out how it interacts with others - I can do my own experiments and rely on that or that can be (even briefly) discussed in the paper, preferably in the form of illustrations. This would help with the practical side of things tremendously.\nNot enough discussion\nThe best results I’ve ever got came from the most thought-provoking experiments. The ones that weren’t restricted to “let’s try this model and see what happens”. Usual format for the conference doesn’t always allow to put all interesting questions and problems in one paper.\n\nWhile Distill doesn’t have many posts now, with the backing it received recently, it will hopefully take off at least as an alternative to traditional scientific publishing for those people who are not in a dire need of academic recognition, or who is willing to make a bit of extra effort and write a nice post. In the future I see this format ubiquitous, first in CS and then in other areas.\n\nThis\n\nis better than this\n\nSource: distill.pub\n\nAt the same time, the increased amount of work required for writing a Distill-worthy article compared to a typical paper will eliminate all of the “incremental” results that don’t introduce anything interesting or discuss an old problem from a different angle.\n\nThe idea that ML is a must-know for every programmer kind of calls for a good source of knowledge that wouldn’t require an equivalent of three top grad school courses’ workload to get comfortable with it. Distill can’t replace everything, but we surely could use more good articles that explain or summarize new results.\n\nTheir guidelines and review process are described on the website: Distill — Latest articles about machine learning\n - check it out.', 'aiModelVersion': '1'}",0.8537
Travis Addair,Updated 5y,What is the difference between FP16 and FP32 when doing deep learning?,"This is a well-timed question, as we just added FP16 support to Horovod
 last Friday. So naturally, I’m itching to talk more about it!

The value proposition when using FP16 for training a deep neural network is significantly faster training times without “any” loss in performance (*some restrictions apply*).

Specifically, FP16 will:

Reduce memory by cutting the size of your tensors in half.
Reduce training time by speeding up computations on the GPU (reducing arithmetic bandwidth) and (in the distributed case) reducing network bandwidth.

Theoretically, you’ll be able to train bigger models faster.

Sounds great, right? But how does it work, and why might it not always work?

FP16 here refers to half-precision floating points
 (16-bit), as opposed to the standard 32-bit floating point, or FP32.

Traditionally, when training a neural network, you would use 32-bit floating points to represent the weights in your network. There are a number of reasons for that:

32-bit floating points have enough range to represent numbers of magnitude both smaller (10^-45) and larger (10^38) than you’d need for most applications.
32-bit floats have enough precision such that we can distinguish numbers of varying magnitudes from one another.
Virtually all hardware (GPUs, CPUs) and APIs support 32-bit floating point instructions natively, and efficiently.
Very rarely in computing is floating point math a major bottleneck, and if it is, there’s rarely away to get around it (because we need that precision).

In a 32-bit floating point, you reserve 8 bits for the exponent (the “magnitude”) and 23 bits for the mantissa (the “precision”).

But, as it turns out, for most deep learning use cases, we don’t actually need all that precision. And indeed, we rarely need all that much magnitude either.

NVIDIA did a great analysis on mixed precision training in discussing the half-precision support available in their Volta series of GPUs[1] . Their conclusion was that most weights and gradients tend to fall well within the 16-bit representable range, and of the gradients that did not (mostly small activation gradients in some networks), simply scaling up the gradient was sufficient to achieve convergence.

So for most cases, all those extra bits are just wasteful. With FP16, we can reduce the number of bits in half, reducing the exponent from 8 bits to 5, and the mantissa from 23 bits to 10.

But it’s not without risks. The representable range for FP16 is very small in comparison to FP32: 10^-8 to 65504! What that means is that we risk underflow (attempting to represent numbers so small they clamp to zero) and overflow (numbers so large they become NaN, not a number). With underflow, our network never learns anything, and with overflow, it learns garbage. Both are bad.

One exciting alternative that addresses this issue is bfloat16
. Though bfloat16 is also a 16-bit floating point representation, it uses its bits a bit differently:

The size of its exponent is the same size as FP32, meaning it can represent the same magnitudes, but with much less precision, effectively eliminating the underflow and overflow problem, but at the cost of not being able to distinguish numbers of similar magnitudes from one another.

But it turns out: that’s okay! When thinking about gradients and weights in particular, the magnitude and direction end up being by far the most significant factors, the precise digits being of comparatively little importance. Indeed, there’s even been recent work on quantizing gradients to a single bit[2] !

Unfortunately, bfloat16 is not currently supported natively in most instruction sets, with the notable exception of Google’s TPUs, one of their major selling points!

If you’re interested in comparing your training performance with FP16 vs FP32, I encourage you to check out Horovod’s new gradient compression feature:

opt = hvd.DistributedOptimizer(opt, compression=hvd.Compression.fp16) 

And let me know how it goes!

Footnotes

[1] Mixed-Precision Training of Deep Neural Networks | NVIDIA Developer Blog
[2] Compressed Optimisation for Non-Convex Problems","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/uw8neh4mt29farip', 'title': 'What is the difference between FP16 and FP32 when doing deep learning?', 'score': {'original': 0.58935, 'ai': 0.41065}, 'blocks': [{'text': 'This is a well-timed question, as we just added FP16 support to Horovod\n last Friday. So naturally, I’m itching to talk more about it!\n\nThe value proposition when using FP16 for training a deep neural network is significantly faster training times without “any” loss in performance (*some restrictions apply*).\n\nSpecifically, FP16 will:\n\nReduce memory by cutting the size of your tensors in half.\nReduce training time by speeding up computations on the GPU (reducing arithmetic bandwidth) and (in the distributed case) reducing network bandwidth.\n\nTheoretically, you’ll be able to train bigger models faster.\n\nSounds great, right? But how does it work, and why might it not always work?\n\nFP16 here refers to half-precision floating points\n (16-bit), as opposed to the standard 32-bit floating point, or FP32.\n\nTraditionally, when training a neural network, you would use 32-bit floating points to represent the weights in your network. There are a number of reasons for that:\n\n32-bit floating points have enough range to represent numbers of magnitude both smaller (10^-45) and larger (10^38) than you’d need for most applications.\n32-bit floats have enough precision such that we can distinguish numbers of varying magnitudes from one another.\nVirtually all hardware (GPUs, CPUs) and APIs support 32-bit floating point instructions natively, and efficiently.\nVery rarely in computing is floating point math a major bottleneck, and if it is, there’s rarely away to get around it (because we need that precision).\n\nIn a 32-bit floating point, you reserve 8 bits for the exponent (the “magnitude”) and 23 bits for the mantissa (the “precision”).\n\nBut, as it turns out, for most deep learning use cases, we don’t actually need all that precision. And indeed, we rarely need all that much magnitude either.\n\nNVIDIA did a great analysis on mixed precision training in discussing the half-precision support available in their Volta series of GPUs[1] . Their conclusion was that most weights and gradients tend to fall well within the 16-bit representable range, and of the gradients that did not (mostly small activation gradients in some networks), simply scaling up the gradient was sufficient to achieve convergence.\n\nSo for most cases, all those extra bits are just wasteful. With FP16, we can reduce the number of bits in half, reducing the exponent from 8 bits to 5, and the mantissa from 23 bits to 10.\n\nBut it’s not without risks. The representable range for FP16 is very small in comparison to FP32: 10^-8 to 65504! What that means is that we risk underflow (attempting to represent numbers so small they clamp to zero) and overflow (numbers so large they become NaN, not a number). With underflow, our network never learns anything, and with overflow, it learns garbage. Both are bad.\n\nOne exciting alternative that addresses this issue is bfloat16\n. Though bfloat16 is also a 16-bit floating point representation, it uses its bits a bit differently:\n\nThe size of its exponent is the same size as FP32, meaning it can represent the same magnitudes, but with much less precision, effectively eliminating the underflow and overflow problem, but at the cost of not being able to distinguish numbers of similar magnitudes from one another.\n\nBut it turns out: that’s okay! When thinking about', 'result': {'fake': 0.029, 'real': 0.971}, 'status': 'success'}, {'text': 'gradients and weights in particular, the magnitude and direction end up being by far the most significant factors, the precise digits being of comparatively little importance. Indeed, there’s even been recent work on quantizing gradients to a single bit[2] !\n\nUnfortunately, bfloat16 is not currently supported natively in most instruction sets, with the notable exception of Google’s TPUs, one of their major selling points!\n\nIf you’re interested in comparing your training performance with FP16 vs FP32, I encourage you to check out Horovod’s new gradient compression feature:\n\nopt = hvd.DistributedOptimizer(opt, compression=hvd.Compression.fp16)\xa0\n\nAnd let me know how it goes!\n\nFootnotes\n\n[1] Mixed-Precision Training of Deep Neural Networks | NVIDIA Developer Blog\n[2] Compressed Optimisation for Non-Convex Problems', 'result': {'fake': 0.6129, 'real': 0.3871}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982607, 'subscription': 0, 'content': 'This is a well-timed question, as we just added FP16 support to Horovod\n last Friday. So naturally, I’m itching to talk more about it!\n\nThe value proposition when using FP16 for training a deep neural network is significantly faster training times without “any” loss in performance (*some restrictions apply*).\n\nSpecifically, FP16 will:\n\nReduce memory by cutting the size of your tensors in half.\nReduce training time by speeding up computations on the GPU (reducing arithmetic bandwidth) and (in the distributed case) reducing network bandwidth.\n\nTheoretically, you’ll be able to train bigger models faster.\n\nSounds great, right? But how does it work, and why might it not always work?\n\nFP16 here refers to half-precision floating points\n (16-bit), as opposed to the standard 32-bit floating point, or FP32.\n\nTraditionally, when training a neural network, you would use 32-bit floating points to represent the weights in your network. There are a number of reasons for that:\n\n32-bit floating points have enough range to represent numbers of magnitude both smaller (10^-45) and larger (10^38) than you’d need for most applications.\n32-bit floats have enough precision such that we can distinguish numbers of varying magnitudes from one another.\nVirtually all hardware (GPUs, CPUs) and APIs support 32-bit floating point instructions natively, and efficiently.\nVery rarely in computing is floating point math a major bottleneck, and if it is, there’s rarely away to get around it (because we need that precision).\n\nIn a 32-bit floating point, you reserve 8 bits for the exponent (the “magnitude”) and 23 bits for the mantissa (the “precision”).\n\nBut, as it turns out, for most deep learning use cases, we don’t actually need all that precision. And indeed, we rarely need all that much magnitude either.\n\nNVIDIA did a great analysis on mixed precision training in discussing the half-precision support available in their Volta series of GPUs[1] . Their conclusion was that most weights and gradients tend to fall well within the 16-bit representable range, and of the gradients that did not (mostly small activation gradients in some networks), simply scaling up the gradient was sufficient to achieve convergence.\n\nSo for most cases, all those extra bits are just wasteful. With FP16, we can reduce the number of bits in half, reducing the exponent from 8 bits to 5, and the mantissa from 23 bits to 10.\n\nBut it’s not without risks. The representable range for FP16 is very small in comparison to FP32: 10^-8 to 65504! What that means is that we risk underflow (attempting to represent numbers so small they clamp to zero) and overflow (numbers so large they become NaN, not a number). With underflow, our network never learns anything, and with overflow, it learns garbage. Both are bad.\n\nOne exciting alternative that addresses this issue is bfloat16\n. Though bfloat16 is also a 16-bit floating point representation, it uses its bits a bit differently:\n\nThe size of its exponent is the same size as FP32, meaning it can represent the same magnitudes, but with much less precision, effectively eliminating the underflow and overflow problem, but at the cost of not being able to distinguish numbers of similar magnitudes from one another.\n\nBut it turns out: that’s okay! When thinking about gradients and weights in particular, the magnitude and direction end up being by far the most significant factors, the precise digits being of comparatively little importance. Indeed, there’s even been recent work on quantizing gradients to a single bit[2] !\n\nUnfortunately, bfloat16 is not currently supported natively in most instruction sets, with the notable exception of Google’s TPUs, one of their major selling points!\n\nIf you’re interested in comparing your training performance with FP16 vs FP32, I encourage you to check out Horovod’s new gradient compression feature:\n\nopt = hvd.DistributedOptimizer(opt, compression=hvd.Compression.fp16)\xa0\n\nAnd let me know how it goes!\n\nFootnotes\n\n[1] Mixed-Precision Training of Deep Neural Networks | NVIDIA Developer Blog\n[2] Compressed Optimisation for Non-Convex Problems', 'aiModelVersion': '1'}",0.58935
Kritika Chadha,10mo,Why is data science important?,"Let’s discuss

This is the world of Data Science, and we're just living in it. Day by day, many technological advancements are being introduced, and scientists are adopting these technologies to benefit people in many ways.

Every sector is adopting the use of data science; this is why domain knowledge is essential.

Learning data science in a particular sector is crucial, and it makes you very clear about your path. Domain elective specialization will help you choose your future so that you get to learn data science without any hurdles.

Data science is an interdisciplinary scientific field that utilizes scientific methods, processes, algorithms, and other methodologies to take out knowledge and insights from clustered, structured, and unstructured data and uses that knowledge along with actionable insights from data across a wide range of application domains.

The world is very data-driven, so data scientists are always in demand!

A new, rapidly expanding class of professions, data scientists are in high demand right now. This phrase was first used a few years ago by data leads to Facebook and LinkedIn organizations. A data scientist's scope incorporates the best social skills to find trends; it is not simply about analytical talents.

Five excellent reasons to seek a career in data science:

Excellent career path in data science. Yes, in this area you will have a meaningful career progression.
High possibility for diversification with several alternatives. As a data engineer, analyst, ML engineer, or even data science manager, you have the option to diversify.
Most significant salary takeaway quotient. You can anticipate receiving an excellent compensation package as a data scientist.
Make decisions for yourself. You won't always have the ability to make wise business judgments if you take a job.
It's less competitive because it's an analytical position. While there is less competition, demand is not.

If you love to play with data and deal with large amounts of data, you can study data science at top institutions. Data science is considered a fascinating field. These were some of the importance of the field of data science. Now you know why data science is so much in demand these days.

Now, here comes the big question. How would you exactly be able to learn and practice data science?

There are so many institutions out there that provide you with the best courses for data science. All you're required to do is do some research. Well, let me help you a bit.

The very great institution that has it all is mentioned below.

It is a very renowned and highly recommended institution that provides courses in data science. Many people love studying data science, and this institute will help you in every step of your learning process.

Data Science Course: Advanced-Data Science and AI Program

7-9 Months.

Here are some of the essential benefits that this great institution has to offer:

At this institute, you get the chance of working on 12 + in real-time. Your education will be strengthened by working on actual industrial projects in disciplines like chatbot creation, intelligent supply chain analysis, and HR analytics, among others.
IBM and this institute worked together to create every program. In addition to other IBM micro-skills certifications, it offers a legitimate and globally recognized IBM course completion certificate. Upon completion of each project in the Advanced Data Science and AI Program, it also issues an IBM-approved project certificate.
For students who want to use what they have learned while working on real-world projects in a practical setting, it also offers hybrid learning sessions.
You can work on assignments under the guidance of MAANG specialists in the project innovation laboratories at several locations across the nation. Delhi, Bangalore, Mumbai, and Pune make up four of the seven destinations.
One of the biggest benefits of Advanced Data Science and AI Program is that you can continue working in your present area while still learning data science because it offers Domain Electives in a variety of fields, including:
E-commerce and supply chain
Pharmaceutical and clinical care
Gas and oil-related energy
Insurance and finance

These are some of the key components of this institute’s data science course, then. However, this course is specifically created for working professionals; however, if you are a beginner, you should take a look at Coursera's data science course.

Through Coursera, you can enroll in IBM's professional certificate program in data science. It is suitable for someone who wants to start a career in data science but has no prior knowledge of the subject. To find the ideal work, their placement aid is insufficient.

Recommendation:

It won't be that difficult to learn data science if you have the correct mentors. Thanks to online data science classes, many students have taken use of their time when they were in lockdown to learn new skills. The field of data science is the most in demand in the job market right now.

Good luck!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/atvnizbp2w6frgs7', 'title': 'Why is data science important?', 'score': {'original': 0.03715, 'ai': 0.96285}, 'blocks': [{'text': ""Let’s discuss\n\nThis is the world of Data Science, and we're just living in it. Day by day, many technological advancements are being introduced, and scientists are adopting these technologies to benefit people in many ways.\n\nEvery sector is adopting the use of data science; this is why domain knowledge is essential.\n\nLearning data science in a particular sector is crucial, and it makes you very clear about your path. Domain elective specialization will help you choose your future so that you get to learn data science without any hurdles.\n\nData science is an interdisciplinary scientific field that utilizes scientific methods, processes, algorithms, and other methodologies to take out knowledge and insights from clustered, structured, and unstructured data and uses that knowledge along with actionable insights from data across a wide range of application domains.\n\nThe world is very data-driven, so data scientists are always in demand!\n\nA new, rapidly expanding class of professions, data scientists are in high demand right now. This phrase was first used a few years ago by data leads to Facebook and LinkedIn organizations. A data scientist's scope incorporates the best social skills to find trends; it is not simply about analytical talents.\n\nFive excellent reasons to seek a career in data science:\n\nExcellent career path in data science. Yes, in this area you will have a meaningful career progression.\nHigh possibility for diversification with several alternatives. As a data engineer, analyst, ML engineer, or even data science manager, you have the option to diversify.\nMost significant salary takeaway quotient. You can anticipate receiving an excellent compensation package as a data scientist.\nMake decisions for yourself. You won't always have the ability to make wise business judgments if you take a job.\nIt's less competitive because it's an analytical position. While there is less competition, demand is not.\n\nIf you love to play with data and deal with large amounts of data, you can study data science at top institutions. Data science is considered a fascinating field. These were some of the importance of the field of data science. Now you know why data science is so much in demand these days.\n\nNow, here comes the big question. How would you exactly be able to learn and practice data science?\n\nThere are so many institutions out there that provide you with the best courses for data science. All you're required to do is do some research. Well, let me help you a bit.\n\nThe very great institution that has it all is mentioned below.\n\nIt is a very renowned and highly recommended institution that provides courses in data science. Many people love studying data science, and this institute will help you in every step of your learning process.\n\nData Science Course: Advanced-Data Science and AI Program\n\n7-9 Months.\n\nHere are some of the essential benefits that this great institution has to offer:\n\nAt this institute, you get the chance of working on 12 + in real-time. Your education will be strengthened by working on actual industrial projects in disciplines like chatbot creation, intelligent supply chain analysis, and HR analytics, among others.\nIBM and this institute worked together to create every program. In addition to other IBM micro-skills certifications, it"", 'result': {'fake': 0.9684, 'real': 0.0316}, 'status': 'success'}, {'text': ""offers a legitimate and globally recognized IBM course completion certificate. Upon completion of each project in the Advanced Data Science and AI Program, it also issues an IBM-approved project certificate.\nFor students who want to use what they have learned while working on real-world projects in a practical setting, it also offers hybrid learning sessions.\nYou can work on assignments under the guidance of MAANG specialists in the project innovation laboratories at several locations across the nation. Delhi, Bangalore, Mumbai, and Pune make up four of the seven destinations.\nOne of the biggest benefits of Advanced Data Science and AI Program is that you can continue working in your present area while still learning data science because it offers Domain Electives in a variety of fields, including:\nE-commerce and supply chain\nPharmaceutical and clinical care\nGas and oil-related energy\nInsurance and finance\n\nThese are some of the key components of this institute’s data science course, then. However, this course is specifically created for working professionals; however, if you are a beginner, you should take a look at Coursera's data science course.\n\nThrough Coursera, you can enroll in IBM's professional certificate program in data science. It is suitable for someone who wants to start a career in data science but has no prior knowledge of the subject. To find the ideal work, their placement aid is insufficient.\n\nRecommendation:\n\nIt won't be that difficult to learn data science if you have the correct mentors. Thanks to online data science classes, many students have taken use of their time when they were in lockdown to learn new skills. The field of data science is the most in demand in the job market right now.\n\nGood luck!"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 9, 'credits': 1982598, 'subscription': 0, 'content': ""Let’s discuss\n\nThis is the world of Data Science, and we're just living in it. Day by day, many technological advancements are being introduced, and scientists are adopting these technologies to benefit people in many ways.\n\nEvery sector is adopting the use of data science; this is why domain knowledge is essential.\n\nLearning data science in a particular sector is crucial, and it makes you very clear about your path. Domain elective specialization will help you choose your future so that you get to learn data science without any hurdles.\n\nData science is an interdisciplinary scientific field that utilizes scientific methods, processes, algorithms, and other methodologies to take out knowledge and insights from clustered, structured, and unstructured data and uses that knowledge along with actionable insights from data across a wide range of application domains.\n\nThe world is very data-driven, so data scientists are always in demand!\n\nA new, rapidly expanding class of professions, data scientists are in high demand right now. This phrase was first used a few years ago by data leads to Facebook and LinkedIn organizations. A data scientist's scope incorporates the best social skills to find trends; it is not simply about analytical talents.\n\nFive excellent reasons to seek a career in data science:\n\nExcellent career path in data science. Yes, in this area you will have a meaningful career progression.\nHigh possibility for diversification with several alternatives. As a data engineer, analyst, ML engineer, or even data science manager, you have the option to diversify.\nMost significant salary takeaway quotient. You can anticipate receiving an excellent compensation package as a data scientist.\nMake decisions for yourself. You won't always have the ability to make wise business judgments if you take a job.\nIt's less competitive because it's an analytical position. While there is less competition, demand is not.\n\nIf you love to play with data and deal with large amounts of data, you can study data science at top institutions. Data science is considered a fascinating field. These were some of the importance of the field of data science. Now you know why data science is so much in demand these days.\n\nNow, here comes the big question. How would you exactly be able to learn and practice data science?\n\nThere are so many institutions out there that provide you with the best courses for data science. All you're required to do is do some research. Well, let me help you a bit.\n\nThe very great institution that has it all is mentioned below.\n\nIt is a very renowned and highly recommended institution that provides courses in data science. Many people love studying data science, and this institute will help you in every step of your learning process.\n\nData Science Course: Advanced-Data Science and AI Program\n\n7-9 Months.\n\nHere are some of the essential benefits that this great institution has to offer:\n\nAt this institute, you get the chance of working on 12 + in real-time. Your education will be strengthened by working on actual industrial projects in disciplines like chatbot creation, intelligent supply chain analysis, and HR analytics, among others.\nIBM and this institute worked together to create every program. In addition to other IBM micro-skills certifications, it offers a legitimate and globally recognized IBM course completion certificate. Upon completion of each project in the Advanced Data Science and AI Program, it also issues an IBM-approved project certificate.\nFor students who want to use what they have learned while working on real-world projects in a practical setting, it also offers hybrid learning sessions.\nYou can work on assignments under the guidance of MAANG specialists in the project innovation laboratories at several locations across the nation. Delhi, Bangalore, Mumbai, and Pune make up four of the seven destinations.\nOne of the biggest benefits of Advanced Data Science and AI Program is that you can continue working in your present area while still learning data science because it offers Domain Electives in a variety of fields, including:\nE-commerce and supply chain\nPharmaceutical and clinical care\nGas and oil-related energy\nInsurance and finance\n\nThese are some of the key components of this institute’s data science course, then. However, this course is specifically created for working professionals; however, if you are a beginner, you should take a look at Coursera's data science course.\n\nThrough Coursera, you can enroll in IBM's professional certificate program in data science. It is suitable for someone who wants to start a career in data science but has no prior knowledge of the subject. To find the ideal work, their placement aid is insufficient.\n\nRecommendation:\n\nIt won't be that difficult to learn data science if you have the correct mentors. Thanks to online data science classes, many students have taken use of their time when they were in lockdown to learn new skills. The field of data science is the most in demand in the job market right now.\n\nGood luck!"", 'aiModelVersion': '1'}",0.03715
S M,6y,"If an artificial neural net has 100 billion nodes, could it become as intelligent as a human?","The only thing common between an artificial neural network and an actual neural network (human brain) is the structure. Expecting the former to behave like the latter is like expecting gravitational waves on a planetary scale from this:

Nodes or layers are usually added to a neural network to fit the training data better and reveal complex relationships. They do not mimic how our brain learns stuff. And that’s mainly because no one knows how our brain learns stuff.

Large networks are also prone to overfitting when the data is limited. (What is overfitting in layman's terms?).

And to train a network having 100 billion nodes, you would ideally need approximately 2.5 gajillion training data points. Yann LeCun (Director of Facebook AI Research), in his Reddit AMA
, says “Make your network bigger and bigger until the accuracy stops increasing. Then regularize the hell out of it”.

To summarize:

Artificial neural networks don’t mimic the human brain in any way except the structure.
Increasing the number of nodes doesn’t always increase the network’s “intelligence” — Overfitting poses a huge problem.
An artificial neural network is just logistic regression on steroids.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/5hituzx41jmgdv0n', 'title': 'If an artificial neural net has 100 billion nodes, could it become as intelligent as a human?', 'score': {'original': 0.9981, 'ai': 0.0019}, 'blocks': [{'text': ""The only thing common between an artificial neural network and an actual neural network (human brain) is the structure. Expecting the former to behave like the latter is like expecting gravitational waves on a planetary scale from this:\n\nNodes or layers are usually added to a neural network to fit the training data better and reveal complex relationships. They do not mimic how our brain learns stuff. And that’s mainly because no one knows how our brain learns stuff.\n\nLarge networks are also prone to overfitting when the data is limited. (What is overfitting in layman's terms?).\n\nAnd to train a network having 100 billion nodes, you would ideally need approximately 2.5 gajillion training data points. Yann LeCun (Director of Facebook AI Research), in his Reddit AMA\n, says “Make your network bigger and bigger until the accuracy stops increasing. Then regularize the hell out of it”.\n\nTo summarize:\n\nArtificial neural networks don’t mimic the human brain in any way except the structure.\nIncreasing the number of nodes doesn’t always increase the network’s “intelligence” — Overfitting poses a huge problem.\nAn artificial neural network is just logistic regression on steroids."", 'result': {'fake': 0.0019, 'real': 0.9981}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982596, 'subscription': 0, 'content': ""The only thing common between an artificial neural network and an actual neural network (human brain) is the structure. Expecting the former to behave like the latter is like expecting gravitational waves on a planetary scale from this:\n\nNodes or layers are usually added to a neural network to fit the training data better and reveal complex relationships. They do not mimic how our brain learns stuff. And that’s mainly because no one knows how our brain learns stuff.\n\nLarge networks are also prone to overfitting when the data is limited. (What is overfitting in layman's terms?).\n\nAnd to train a network having 100 billion nodes, you would ideally need approximately 2.5 gajillion training data points. Yann LeCun (Director of Facebook AI Research), in his Reddit AMA\n, says “Make your network bigger and bigger until the accuracy stops increasing. Then regularize the hell out of it”.\n\nTo summarize:\n\nArtificial neural networks don’t mimic the human brain in any way except the structure.\nIncreasing the number of nodes doesn’t always increase the network’s “intelligence” — Overfitting poses a huge problem.\nAn artificial neural network is just logistic regression on steroids."", 'aiModelVersion': '1'}",0.9981
Dan,5y,Why did Google’s AI identify a black man as a chimp?,"Someone once demonstrated an AI to me that thought good weather was a tank.

That’s just bonkers, right? Well they were building a system for examining reconnaissance photos for tanks. As it happens most of the pictures of tanks had been taken in better weather than the pictures of ‘no tank’.

So on the learning set just assessing the overall light level was an effective surrogate for whether the picture might contain a tank.

As per the XKCD cartoon it’s really really (really) difficult to build computer systems that perform visual recognition tasks that humans find so trivial they don’t know they’re doing them.

Every facial recognition tool I’ve used thinks a portrait on a wall is a person. But if a human being started talking to the Mona Lisa because they’d mistaken if for a real person we’d have quite justifiable concerns about their mental health!

It’s quite a feat that facial recognition software can with reasonably accuracy identify faces let alone try and determine the species of primate.

Human beings are primates and the similarities are obvious. Don’t ask too much of this stuff and (it’s implied by the question) certainly don’t project racial agendas into problems at the limit of (so called) Artificial Intelligence.

As someone pointed out to me the other day, we haven’t really achieved Artifical Stupidity let alone intelligence. Only people with a mental condition would mistake a park bench for a motor car - something AI could easily do (its a roughly cuboid object by the side of the road).

All the hype and click-bait has people thinking Artificial Intelligence has been cracked. It’s nothing of the kind.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/dxizgt43y6plj7u9', 'title': 'Why did Google’s AI identify a black man as a chimp?', 'score': {'original': 0.9998, 'ai': 0.0002}, 'blocks': [{'text': 'Someone once demonstrated an AI to me that thought good weather was a tank.\n\nThat’s just bonkers, right? Well they were building a system for examining reconnaissance photos for tanks. As it happens most of the pictures of tanks had been taken in better weather than the pictures of ‘no tank’.\n\nSo on the learning set just assessing the overall light level was an effective surrogate for whether the picture might contain a tank.\n\nAs per the XKCD cartoon it’s really really (really) difficult to build computer systems that perform visual recognition tasks that humans find so trivial they don’t know they’re doing them.\n\nEvery facial recognition tool I’ve used thinks a portrait on a wall is a person. But if a human being started talking to the Mona Lisa because they’d mistaken if for a real person we’d have quite justifiable concerns about their mental health!\n\nIt’s quite a feat that facial recognition software can with reasonably accuracy identify faces let alone try and determine the species of primate.\n\nHuman beings are primates and the similarities are obvious. Don’t ask too much of this stuff and (it’s implied by the question) certainly don’t project racial agendas into problems at the limit of (so called) Artificial Intelligence.\n\nAs someone pointed out to me the other day, we haven’t really achieved Artifical Stupidity let alone intelligence. Only people with a mental condition would mistake a park bench for a motor car - something AI could easily do (its a roughly cuboid object by the side of the road).\n\nAll the hype and click-bait has people thinking Artificial Intelligence has been cracked. It’s nothing of the kind.', 'result': {'fake': 0.0002, 'real': 0.9998}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982593, 'subscription': 0, 'content': 'Someone once demonstrated an AI to me that thought good weather was a tank.\n\nThat’s just bonkers, right? Well they were building a system for examining reconnaissance photos for tanks. As it happens most of the pictures of tanks had been taken in better weather than the pictures of ‘no tank’.\n\nSo on the learning set just assessing the overall light level was an effective surrogate for whether the picture might contain a tank.\n\nAs per the XKCD cartoon it’s really really (really) difficult to build computer systems that perform visual recognition tasks that humans find so trivial they don’t know they’re doing them.\n\nEvery facial recognition tool I’ve used thinks a portrait on a wall is a person. But if a human being started talking to the Mona Lisa because they’d mistaken if for a real person we’d have quite justifiable concerns about their mental health!\n\nIt’s quite a feat that facial recognition software can with reasonably accuracy identify faces let alone try and determine the species of primate.\n\nHuman beings are primates and the similarities are obvious. Don’t ask too much of this stuff and (it’s implied by the question) certainly don’t project racial agendas into problems at the limit of (so called) Artificial Intelligence.\n\nAs someone pointed out to me the other day, we haven’t really achieved Artifical Stupidity let alone intelligence. Only people with a mental condition would mistake a park bench for a motor car - something AI could easily do (its a roughly cuboid object by the side of the road).\n\nAll the hype and click-bait has people thinking Artificial Intelligence has been cracked. It’s nothing of the kind.', 'aiModelVersion': '1'}",0.9998
Nikhil Dandekar,8y,"What is the intuitive explanation of Learning to Rank and algorithms like RankNet, LambdaRank and LambdaMART? In what types of data/variables can these techniques be used? What are their strengths and limitations?","RankNet, LambdaRank and LambdaMART are all what we call Learning to Rank
 algorithms.

What is Learning to Rank?

Learning to Rank (LTR) is a class of techniques that apply supervised machine learning (ML) to solve ranking problems. The main difference between LTR and traditional supervised ML is this:

Traditional ML solves a prediction problem (classification or regression) on a single instance at a time. E.g. if you are doing spam detection on email, you will look at all the features associated with that email and classify it as spam or not. The aim of traditional ML is to come up with a class (spam or no-spam) or a single numerical score for that instance.
LTR solves a ranking problem on a list of items. The aim of LTR is to come up with optimal ordering of those items. As such, LTR doesn't care much about the exact score that each item gets, but cares more about the relative ordering among all the items.

The most common application of LTR is search engine ranking, but it's useful anywhere you need to produce a ranked list of items.

The training data for a LTR model consists of a list of items and a ""ground truth"" score for each of those items. For search engine ranking, this translates to a list of results for a query and a relevance rating for each of those results with respect to the query. The most common way used by major search engines to generate these relevance ratings is to ask human raters to rate results for a set of queries. In case you are interested, I have written in detail on human rating systems here: Nikhil Dandekar's answer to How does Google measure the quality of their search results?

For a more technical explanation of Learning to Rank check this paper by Microsoft Research: A Short Introduction to Learning to Rank

What is RankNet, LambdaRank and LambdaMART?

RankNet, LambdaRank and LambdaMART are all LTR algorithms developed by Chris Burges and his colleagues at Microsoft Research. RankNet was the first one to be developed, followed by LambdaRank and then LambdaMART.

In all three techniques, ranking is transformed into a pairwise classification or regression problem. That means you look at pairs of items at a time, come up with the optimal ordering for that pair of items, and then use it to come up with the final ranking for all the results.

Here are some high-level details for each of the algorithms:

RankNet[1]

RankNet was originally developed using neural nets, but the underlying model can be different and is not constrained to just neural nets. The cost function for RankNet aims to minimize the number of inversions in ranking. Here an inversion means an incorrect order among a pair of results, i.e. when we rank a lower rated result above a higher rated result in a ranked list. RankNet optimizes the cost function using Stochastic Gradient Descent.

LambdaRank[2]

Burgess et. al. found that during RankNet training procedure, you don't need the costs, only need the gradients (
λ
λ
)
)
 of the cost with respect to the model score. You can think of these gradients as little arrows attached to each document in the ranked list, indicating the direction we’d like those documents to move.

Further they found that scaling the gradients by the change in NDCG
 found by swapping each pair of documents gave good results. The core idea of LambdaRank is to use this new cost function for training a RankNet. On experimental datasets, this shows both speed and accuracy improvements over the original RankNet.

LambdaMART[3]

LambdaMART combines LambdaRank and MART (Multiple Additive Regression Trees). While MART uses gradient boosted decision trees for prediction tasks, LambdaMART uses gradient boosted decision trees using a cost function derived from LambdaRank for solving a ranking task. On experimental datasets, LambdaMART has shown better results than LambdaRank and the original RankNet.

If you are interested, Chris Burges has a single paper that details the evolution from RankNet to LambdaRank to LambdaMART here: From RankNet to LambdaRank to LambdaMART: An Overview

Footnotes

[1] Page on microsoft.com
[2] Page on microsoft.com
[3] Page on microsoft.com","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/toaidghukc9x3f1q', 'title': 'What is the intuitive explanation of Learning to Rank and algorithms like RankNet, LambdaRank and LambdaMART? In what types of data/variables can these techniques be used? What are their strengths and limitations?', 'score': {'original': 0.4825, 'ai': 0.5175}, 'blocks': [{'text': 'RankNet, LambdaRank and LambdaMART are all what we call Learning to Rank\n algorithms.\n\nWhat is Learning to Rank?\n\nLearning to Rank (LTR) is a class of techniques that apply supervised machine learning (ML) to solve ranking problems. The main difference between LTR and traditional supervised ML is this:\n\nTraditional ML solves a prediction problem (classification or regression) on a single instance at a time. E.g. if you are doing spam detection on email, you will look at all the features associated with that email and classify it as spam or not. The aim of traditional ML is to come up with a class (spam or no-spam) or a single numerical score for that instance.\nLTR solves a ranking problem on a list of items. The aim of LTR is to come up with optimal ordering of those items. As such, LTR doesn\'t care much about the exact score that each item gets, but cares more about the relative ordering among all the items.\n\nThe most common application of LTR is search engine ranking, but it\'s useful anywhere you need to produce a ranked list of items.\n\nThe training data for a LTR model consists of a list of items and a ""ground truth"" score for each of those items. For search engine ranking, this translates to a list of results for a query and a relevance rating for each of those results with respect to the query. The most common way used by major search engines to generate these relevance ratings is to ask human raters to rate results for a set of queries. In case you are interested, I have written in detail on human rating systems here: Nikhil Dandekar\'s answer to How does Google measure the quality of their search results?\n\nFor a more technical explanation of Learning to Rank check this paper by Microsoft Research: A Short Introduction to Learning to Rank\n\nWhat is RankNet, LambdaRank and LambdaMART?\n\nRankNet, LambdaRank and LambdaMART are all LTR algorithms developed by Chris Burges and his colleagues at Microsoft Research. RankNet was the first one to be developed, followed by LambdaRank and then LambdaMART.\n\nIn all three techniques, ranking is transformed into a pairwise classification or regression problem. That means you look at pairs of items at a time, come up with the optimal ordering for that pair of items, and then use it to come up with the final ranking for all the results.\n\nHere are some high-level details for each of the algorithms:\n\nRankNet[1]\n\nRankNet was originally developed using neural nets, but the underlying model can be different and is not constrained to just neural nets. The cost function for RankNet aims to minimize the number of inversions in ranking. Here an inversion means an incorrect order among a pair of results, i.e. when we rank a lower rated result above a higher rated result in a ranked list. RankNet optimizes the cost function using Stochastic Gradient Descent.\n\nLambdaRank[2]\n\nBurgess et. al. found that during RankNet training procedure, you don\'t need the costs, only need the gradients (\nλ\nλ\n)\n)\n of the cost with respect to the model score. You can think of these gradients as little arrows', 'result': {'fake': 0.343, 'real': 0.657}, 'status': 'success'}, {'text': 'attached to each document in the ranked list, indicating the direction we’d like those documents to move.\n\nFurther they found that scaling the gradients by the change in NDCG\n found by swapping each pair of documents gave good results. The core idea of LambdaRank is to use this new cost function for training a RankNet. On experimental datasets, this shows both speed and accuracy improvements over the original RankNet.\n\nLambdaMART[3]\n\nLambdaMART combines LambdaRank and MART (Multiple Additive Regression Trees). While MART uses gradient boosted decision trees for prediction tasks, LambdaMART uses gradient boosted decision trees using a cost function derived from LambdaRank for solving a ranking task. On experimental datasets, LambdaMART has shown better results than LambdaRank and the original RankNet.\n\nIf you are interested, Chris Burges has a single paper that details the evolution from RankNet to LambdaRank to LambdaMART here: From RankNet to LambdaRank to LambdaMART: An Overview\n\nFootnotes\n\n[1] Page on microsoft.com\n[2] Page on microsoft.com\n[3] Page on microsoft.com', 'result': {'fake': 0.9934, 'real': 0.0066}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982586, 'subscription': 0, 'content': 'RankNet, LambdaRank and LambdaMART are all what we call Learning to Rank\n algorithms.\n\nWhat is Learning to Rank?\n\nLearning to Rank (LTR) is a class of techniques that apply supervised machine learning (ML) to solve ranking problems. The main difference between LTR and traditional supervised ML is this:\n\nTraditional ML solves a prediction problem (classification or regression) on a single instance at a time. E.g. if you are doing spam detection on email, you will look at all the features associated with that email and classify it as spam or not. The aim of traditional ML is to come up with a class (spam or no-spam) or a single numerical score for that instance.\nLTR solves a ranking problem on a list of items. The aim of LTR is to come up with optimal ordering of those items. As such, LTR doesn\'t care much about the exact score that each item gets, but cares more about the relative ordering among all the items.\n\nThe most common application of LTR is search engine ranking, but it\'s useful anywhere you need to produce a ranked list of items.\n\nThe training data for a LTR model consists of a list of items and a ""ground truth"" score for each of those items. For search engine ranking, this translates to a list of results for a query and a relevance rating for each of those results with respect to the query. The most common way used by major search engines to generate these relevance ratings is to ask human raters to rate results for a set of queries. In case you are interested, I have written in detail on human rating systems here: Nikhil Dandekar\'s answer to How does Google measure the quality of their search results?\n\nFor a more technical explanation of Learning to Rank check this paper by Microsoft Research: A Short Introduction to Learning to Rank\n\nWhat is RankNet, LambdaRank and LambdaMART?\n\nRankNet, LambdaRank and LambdaMART are all LTR algorithms developed by Chris Burges and his colleagues at Microsoft Research. RankNet was the first one to be developed, followed by LambdaRank and then LambdaMART.\n\nIn all three techniques, ranking is transformed into a pairwise classification or regression problem. That means you look at pairs of items at a time, come up with the optimal ordering for that pair of items, and then use it to come up with the final ranking for all the results.\n\nHere are some high-level details for each of the algorithms:\n\nRankNet[1]\n\nRankNet was originally developed using neural nets, but the underlying model can be different and is not constrained to just neural nets. The cost function for RankNet aims to minimize the number of inversions in ranking. Here an inversion means an incorrect order among a pair of results, i.e. when we rank a lower rated result above a higher rated result in a ranked list. RankNet optimizes the cost function using Stochastic Gradient Descent.\n\nLambdaRank[2]\n\nBurgess et. al. found that during RankNet training procedure, you don\'t need the costs, only need the gradients (\nλ\nλ\n)\n)\n of the cost with respect to the model score. You can think of these gradients as little arrows attached to each document in the ranked list, indicating the direction we’d like those documents to move.\n\nFurther they found that scaling the gradients by the change in NDCG\n found by swapping each pair of documents gave good results. The core idea of LambdaRank is to use this new cost function for training a RankNet. On experimental datasets, this shows both speed and accuracy improvements over the original RankNet.\n\nLambdaMART[3]\n\nLambdaMART combines LambdaRank and MART (Multiple Additive Regression Trees). While MART uses gradient boosted decision trees for prediction tasks, LambdaMART uses gradient boosted decision trees using a cost function derived from LambdaRank for solving a ranking task. On experimental datasets, LambdaMART has shown better results than LambdaRank and the original RankNet.\n\nIf you are interested, Chris Burges has a single paper that details the evolution from RankNet to LambdaRank to LambdaMART here: From RankNet to LambdaRank to LambdaMART: An Overview\n\nFootnotes\n\n[1] Page on microsoft.com\n[2] Page on microsoft.com\n[3] Page on microsoft.com', 'aiModelVersion': '1'}",0.4825
Lokesh,1y,What is the best online course to learn AI?,"As the economy grows and businesses and organizations employ technology to improve their operations, the demand for AI knowledge is growing. Various institutes, both online and offline, offer degree and certification programs with domain specialization options. Whether you're a fresher with no technical background searching for a beginner's level course to kickstart your career, a graduate with relevant work experience in the technical field, or even a professional searching to expand his horizons, the key is to spot the best AI certification courses that fit your needs and experience.

The following are some of the online AI courses you can take.

There is a certification program as well as postgraduate courses available. The goal of this course is to help you develop skills and gain practical experience in AI and machine learning projects.
The course is specifically prepared by NITW faculty, ensuring high-quality instruction. The NITW EICT certificate is internationally recognized. The classes are taught by expert professors in real time.
Doubts and mentorship sessions are supported 24 hours a day, 7 days a week. Even after the training is completed, Edureka provides lifetime access to the course and guiding support.

Drawback: The course does not contain domain-specific training which is essential for today’s job market.

Coursera offers two online AI courses for beginners, intermediates, and professionals. 'Introduction to Artificial Intelligence (AI)' and 'AI for Everyone by Andrew Ng' are two courses that will give you a good understanding of AI and machine learning.
The course covers common AI terminology such as neural networks, machine learning, deep learning, and data science.
It also covers the business aspect of AI, making it useful for non-technical persons who wish to work with AI and make a career out of it.

Drawback: However, the course does not come along with project labs for students to practice real-world data sets.

It provides a 20-week full-stack web development and data science career advancement curriculum through Great Lake. In combination with HTML and CSS, the Full Stack Bootcamp includes Javascript, jQuery, ReactJS Java, as well as SQL.
Leading full-stack engineers and technicians from prominent firms lead mentorship workshops and interview practice sessions.
Data Science and Machine Learning: Making Data-Driven Decisions from the MIT IDSS and the Applied Data Science Program from the MIT Professional Education Course are two of them.
Their most prestigious program is the PGP Data Science and Business Analytics.

Drawback: The institute only offers general courses for beginner-level students.

Note:

From self-driving cars to facial recognition software, AI is already having a major impact on our lives. So it's important that you learn about AI with domain specialization courses.

But why?

These courses will help you understand the basics of AI and its various applications.
This will give you a foundation on which to build more complex knowledge.
And because it will help you future-proof your career and be able to take advantage of all the benefits of AI technology.

In terms of my advice, I suggest you should take Learnbay's ""Advanced Data Science & AI Certification Course,"" which contains the following features:

Some of the major highlights of the course include:

15+ real-world projects and two capstone projects
250+ hours of online training led by industry experts
3-year of Flexi subscription
Non-programmers get additional python basic classes.

Apart from these, there are a bunch of characteristics that make Learnbay stand out from others:

Domain Specific Training
When a learner is trained with domain-specific knowledge in their own field, it provides a deeper comprehension of the field and, in a similar way, increases their thinking capabilities.
This is a benefit of domain specialization, and it gives you an advantage over other candidates during interviews.
Clinical research, sales, DevOps, automation, banking, oil, data structures, transportation, HR, supply chain, hospitality, and other electives are available.

2. Real-Time Project Expertise

AI is a science with far too many ideas, and understanding them requires understanding how to apply them in real-world circumstances.
Hands-on training is provided to guarantee that each learner knows the subject and can use it in the workplace. Some of the projects include:
Self-driving cars
Forecasting weather
Understanding Covid Cases

3. Hybrid Mode of learning

Learnbay's Advanced Data Science and AI courses are available in Bangalore and other major metropolitan cities.
The theory session takes place in a virtual setting, while the practical training takes place in a classroom setting.
If the settings are not suitable for you, you are free to adjust them at your convenience.

4. IBM Certified courses

IBM has courses that are created in collaboration.
These credentials will increase your search exposure across the globe, as IBM offers three certificate programs in micro-skills, project completion, and course completion.

5. Project Innovation Labs

If you're searching for the greatest project innovation labs for data science and AI, look no further than Learnbay Institute.
Learnbay has institutes in Hyderabad, Chennai, Pune, Kolkata, Mumbai, and Delhi.

CONCLUSION

Learnbay offers the best data science courses in Bangalore right now. Despite being situated in Bangalore, their online courses are available in cities all throughout India. This institute focuses on data science and artificial intelligence courses. As a result, regardless of your degree of experience or job domain, you will undoubtedly find the best-fit AI course for you here.

Thank you","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/sute8acl5ihw13yk', 'title': 'What is the best online course to learn AI?', 'score': {'original': 0.0001, 'ai': 0.9999}, 'blocks': [{'text': 'As the economy grows and businesses and organizations employ technology to improve their operations, the demand for AI knowledge is growing. Various institutes, both online and offline, offer degree and certification programs with domain specialization options. Whether you\'re a fresher with no technical background searching for a beginner\'s level course to kickstart your career, a graduate with relevant work experience in the technical field, or even a professional searching to expand his horizons, the key is to spot the best AI certification courses that fit your needs and experience.\n\nThe following are some of the online AI courses you can take.\n\nThere is a certification program as well as postgraduate courses available. The goal of this course is to help you develop skills and gain practical experience in AI and machine learning projects.\nThe course is specifically prepared by NITW faculty, ensuring high-quality instruction. The NITW EICT certificate is internationally recognized. The classes are taught by expert professors in real time.\nDoubts and mentorship sessions are supported 24 hours a day, 7 days a week. Even after the training is completed, Edureka provides lifetime access to the course and guiding support.\n\nDrawback: The course does not contain domain-specific training which is essential for today’s job market.\n\nCoursera offers two online AI courses for beginners, intermediates, and professionals. \'Introduction to Artificial Intelligence (AI)\' and \'AI for Everyone by Andrew Ng\' are two courses that will give you a good understanding of AI and machine learning.\nThe course covers common AI terminology such as neural networks, machine learning, deep learning, and data science.\nIt also covers the business aspect of AI, making it useful for non-technical persons who wish to work with AI and make a career out of it.\n\nDrawback: However, the course does not come along with project labs for students to practice real-world data sets.\n\nIt provides a 20-week full-stack web development and data science career advancement curriculum through Great Lake. In combination with HTML and CSS, the Full Stack Bootcamp includes Javascript, jQuery, ReactJS Java, as well as SQL.\nLeading full-stack engineers and technicians from prominent firms lead mentorship workshops and interview practice sessions.\nData Science and Machine Learning: Making Data-Driven Decisions from the MIT IDSS and the Applied Data Science Program from the MIT Professional Education Course are two of them.\nTheir most prestigious program is the PGP Data Science and Business Analytics.\n\nDrawback: The institute only offers general courses for beginner-level students.\n\nNote:\n\nFrom self-driving cars to facial recognition software, AI is already having a major impact on our lives. So it\'s important that you learn about AI with domain specialization courses.\n\nBut why?\n\nThese courses will help you understand the basics of AI and its various applications.\nThis will give you a foundation on which to build more complex knowledge.\nAnd because it will help you future-proof your career and be able to take advantage of all the benefits of AI technology.\n\nIn terms of my advice, I suggest you should take Learnbay\'s ""Advanced Data Science & AI Certification Course,"" which contains the following features:\n\nSome of the major highlights of the course include:\n\n15+ real-world projects and two capstone projects\n250+ hours of online training led by industry experts\n3-year of', 'result': {'fake': 0.9997, 'real': 0.0003}, 'status': 'success'}, {'text': ""Flexi subscription\nNon-programmers get additional python basic classes.\n\nApart from these, there are a bunch of characteristics that make Learnbay stand out from others:\n\nDomain Specific Training\nWhen a learner is trained with domain-specific knowledge in their own field, it provides a deeper comprehension of the field and, in a similar way, increases their thinking capabilities.\nThis is a benefit of domain specialization, and it gives you an advantage over other candidates during interviews.\nClinical research, sales, DevOps, automation, banking, oil, data structures, transportation, HR, supply chain, hospitality, and other electives are available.\n\n2. Real-Time Project Expertise\n\nAI is a science with far too many ideas, and understanding them requires understanding how to apply them in real-world circumstances.\nHands-on training is provided to guarantee that each learner knows the subject and can use it in the workplace. Some of the projects include:\nSelf-driving cars\nForecasting weather\nUnderstanding Covid Cases\n\n3. Hybrid Mode of learning\n\nLearnbay's Advanced Data Science and AI courses are available in Bangalore and other major metropolitan cities.\nThe theory session takes place in a virtual setting, while the practical training takes place in a classroom setting.\nIf the settings are not suitable for you, you are free to adjust them at your convenience.\n\n4. IBM Certified courses\n\nIBM has courses that are created in collaboration.\nThese credentials will increase your search exposure across the globe, as IBM offers three certificate programs in micro-skills, project completion, and course completion.\n\n5. Project Innovation Labs\n\nIf you're searching for the greatest project innovation labs for data science and AI, look no further than Learnbay Institute.\nLearnbay has institutes in Hyderabad, Chennai, Pune, Kolkata, Mumbai, and Delhi.\n\nCONCLUSION\n\nLearnbay offers the best data science courses in Bangalore right now. Despite being situated in Bangalore, their online courses are available in cities all throughout India. This institute focuses on data science and artificial intelligence courses. As a result, regardless of your degree of experience or job domain, you will undoubtedly find the best-fit AI course for you here.\n\nThank you"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 9, 'credits': 1982577, 'subscription': 0, 'content': 'As the economy grows and businesses and organizations employ technology to improve their operations, the demand for AI knowledge is growing. Various institutes, both online and offline, offer degree and certification programs with domain specialization options. Whether you\'re a fresher with no technical background searching for a beginner\'s level course to kickstart your career, a graduate with relevant work experience in the technical field, or even a professional searching to expand his horizons, the key is to spot the best AI certification courses that fit your needs and experience.\n\nThe following are some of the online AI courses you can take.\n\nThere is a certification program as well as postgraduate courses available. The goal of this course is to help you develop skills and gain practical experience in AI and machine learning projects.\nThe course is specifically prepared by NITW faculty, ensuring high-quality instruction. The NITW EICT certificate is internationally recognized. The classes are taught by expert professors in real time.\nDoubts and mentorship sessions are supported 24 hours a day, 7 days a week. Even after the training is completed, Edureka provides lifetime access to the course and guiding support.\n\nDrawback: The course does not contain domain-specific training which is essential for today’s job market.\n\nCoursera offers two online AI courses for beginners, intermediates, and professionals. \'Introduction to Artificial Intelligence (AI)\' and \'AI for Everyone by Andrew Ng\' are two courses that will give you a good understanding of AI and machine learning.\nThe course covers common AI terminology such as neural networks, machine learning, deep learning, and data science.\nIt also covers the business aspect of AI, making it useful for non-technical persons who wish to work with AI and make a career out of it.\n\nDrawback: However, the course does not come along with project labs for students to practice real-world data sets.\n\nIt provides a 20-week full-stack web development and data science career advancement curriculum through Great Lake. In combination with HTML and CSS, the Full Stack Bootcamp includes Javascript, jQuery, ReactJS Java, as well as SQL.\nLeading full-stack engineers and technicians from prominent firms lead mentorship workshops and interview practice sessions.\nData Science and Machine Learning: Making Data-Driven Decisions from the MIT IDSS and the Applied Data Science Program from the MIT Professional Education Course are two of them.\nTheir most prestigious program is the PGP Data Science and Business Analytics.\n\nDrawback: The institute only offers general courses for beginner-level students.\n\nNote:\n\nFrom self-driving cars to facial recognition software, AI is already having a major impact on our lives. So it\'s important that you learn about AI with domain specialization courses.\n\nBut why?\n\nThese courses will help you understand the basics of AI and its various applications.\nThis will give you a foundation on which to build more complex knowledge.\nAnd because it will help you future-proof your career and be able to take advantage of all the benefits of AI technology.\n\nIn terms of my advice, I suggest you should take Learnbay\'s ""Advanced Data Science & AI Certification Course,"" which contains the following features:\n\nSome of the major highlights of the course include:\n\n15+ real-world projects and two capstone projects\n250+ hours of online training led by industry experts\n3-year of Flexi subscription\nNon-programmers get additional python basic classes.\n\nApart from these, there are a bunch of characteristics that make Learnbay stand out from others:\n\nDomain Specific Training\nWhen a learner is trained with domain-specific knowledge in their own field, it provides a deeper comprehension of the field and, in a similar way, increases their thinking capabilities.\nThis is a benefit of domain specialization, and it gives you an advantage over other candidates during interviews.\nClinical research, sales, DevOps, automation, banking, oil, data structures, transportation, HR, supply chain, hospitality, and other electives are available.\n\n2. Real-Time Project Expertise\n\nAI is a science with far too many ideas, and understanding them requires understanding how to apply them in real-world circumstances.\nHands-on training is provided to guarantee that each learner knows the subject and can use it in the workplace. Some of the projects include:\nSelf-driving cars\nForecasting weather\nUnderstanding Covid Cases\n\n3. Hybrid Mode of learning\n\nLearnbay\'s Advanced Data Science and AI courses are available in Bangalore and other major metropolitan cities.\nThe theory session takes place in a virtual setting, while the practical training takes place in a classroom setting.\nIf the settings are not suitable for you, you are free to adjust them at your convenience.\n\n4. IBM Certified courses\n\nIBM has courses that are created in collaboration.\nThese credentials will increase your search exposure across the globe, as IBM offers three certificate programs in micro-skills, project completion, and course completion.\n\n5. Project Innovation Labs\n\nIf you\'re searching for the greatest project innovation labs for data science and AI, look no further than Learnbay Institute.\nLearnbay has institutes in Hyderabad, Chennai, Pune, Kolkata, Mumbai, and Delhi.\n\nCONCLUSION\n\nLearnbay offers the best data science courses in Bangalore right now. Despite being situated in Bangalore, their online courses are available in cities all throughout India. This institute focuses on data science and artificial intelligence courses. As a result, regardless of your degree of experience or job domain, you will undoubtedly find the best-fit AI course for you here.\n\nThank you', 'aiModelVersion': '1'}",0.0001
Amar,1y,What is the best online course to learn AI?,"Artificial intelligence (AI) is a computer system designed to make decisions and perform actions in order to achieve a goal or solve a problem. AI offers solutions across industries including healthcare, financial services, transportation and logistics, defense, and manufacturing. AI Online courses can provide you with the training you need to work in the field of AI. With these types of courses, you will be able to learn how artificial intelligence works and what it does across an array of different industries.

There are many online courses available to help you learn about artificial intelligence-

Advanced-Data Science and AI Program

This is designed to master industry-oriented AI skills. Those who are working professionals in the technical domain for more than 4 years, can take advantage of this course to bring their skills up to speed and gain confidence when interviewing at a company based on data science.

Course Name: Advanced Data Science and AI Program

Duration of the course: 250+ hours and 7-9 months long
The cost of the course: Rs.95000/-

Some of the factors why you should prefer Learnbay institute for learning AI courses are-

Domain Specialization
In addition to understanding AI throughout an array of industries, you should also be able to understand what its functions are in different domains.
So you can pick domain specialization as per your suitable portfolio to specialize in. Some of the domains included are finance, healthcare, transportation, defense, etc.

2. Job Assistance

Learnbay gives you the opportunity to learn and improve skills that will lead them to a high-paying job.
Other benefits include mock interview preps and access to preparation sessions.

3. Personalized Career Counseling

Students have a lot of questions about what the future holds, and that's why they seek career counseling.
Choosing the right learning path is the key to success, and personalized career counseling by Learnbay offers you that assurance.

4. Hybrid Learning Model

First of all, Learnbay offers you interactive sessions in addition to sessions with your mentors.
With the assistance of your mentor's knowledge and experience in the field, you can take part in the program while answering questions and resolving challenging equations.

5. Project Innovation Labs

Staff from MNC and MAANG are on hand to assist with offline and online project sessions at the MNC project innovation lab.
They can converse and argue in more than seven cities (Pune, Mumbai, Delhi, Kolkata, Hyderabad, Chennai, and Bangalore).

6. Real-Time Project Expertise

At Learnbay, project-based learning is essential. You can gain information, collaborate with your mentors, and learn how to handle challenging situations with the help of this Advanced Data Science and AI program.
You must therefore continue down this road. Numerous projects exist, including ones that estimate customer churn and analyze consumer feedback.

7. IBM Certification

Your work opportunities will increase and become simpler if you have the IBM Advanced Data Science and AI Program course completion certificate, project completion and for micro-skills.

On the contrary, there’s an institute suggestion for beginners-

Certification Program in Artificial Intelligence and Machine Learning

What makes it ideal for beginners?

Learn about inferential statistics, A/B testing, regression, clustering, decision trees, random forests, and other advanced topics in artificial intelligence and machine learning.
Learn from some of the most well-known Data Scientists and Machine Learning Experts in the nation today.
Work on actual projects to learn how to put the program's lessons into practice.
With the help of this curriculum, you can acquire entry-level positions in data science, artificial intelligence, and machine learning.

Drawback: The course does not come along with domain-specific training which hinders the growth of students to specialize in the field of their interest.

Conclusion

If you are interested in learning how to get into the field of artificial intelligence, online courses are a good option. These types of courses are offered through an array of different institutions and organizations around the country and Learnbay has the best and most well-supported reviews since it allows students to learn with interactive modules followed by career- oriented teaching.

Thank you","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/cki19ramx7y2tgol', 'title': 'What is the best online course to learn AI?', 'score': {'original': 0.002, 'ai': 0.998}, 'blocks': [{'text': ""Artificial intelligence (AI) is a computer system designed to make decisions and perform actions in order to achieve a goal or solve a problem. AI offers solutions across industries including healthcare, financial services, transportation and logistics, defense, and manufacturing. AI Online courses can provide you with the training you need to work in the field of AI. With these types of courses, you will be able to learn how artificial intelligence works and what it does across an array of different industries.\n\nThere are many online courses available to help you learn about artificial intelligence-\n\nAdvanced-Data Science and AI Program\n\nThis is designed to master industry-oriented AI skills. Those who are working professionals in the technical domain for more than 4 years, can take advantage of this course to bring their skills up to speed and gain confidence when interviewing at a company based on data science.\n\nCourse Name: Advanced Data Science and AI Program\n\nDuration of the course: 250+ hours and 7-9 months long\nThe cost of the course: Rs.95000/-\n\nSome of the factors why you should prefer Learnbay institute for learning AI courses are-\n\nDomain Specialization\nIn addition to understanding AI throughout an array of industries, you should also be able to understand what its functions are in different domains.\nSo you can pick domain specialization as per your suitable portfolio to specialize in. Some of the domains included are finance, healthcare, transportation, defense, etc.\n\n2. Job Assistance\n\nLearnbay gives you the opportunity to learn and improve skills that will lead them to a high-paying job.\nOther benefits include mock interview preps and access to preparation sessions.\n\n3. Personalized Career Counseling\n\nStudents have a lot of questions about what the future holds, and that's why they seek career counseling.\nChoosing the right learning path is the key to success, and personalized career counseling by Learnbay offers you that assurance.\n\n4. Hybrid Learning Model\n\nFirst of all, Learnbay offers you interactive sessions in addition to sessions with your mentors.\nWith the assistance of your mentor's knowledge and experience in the field, you can take part in the program while answering questions and resolving challenging equations.\n\n5. Project Innovation Labs\n\nStaff from MNC and MAANG are on hand to assist with offline and online project sessions at the MNC project innovation lab.\nThey can converse and argue in more than seven cities (Pune, Mumbai, Delhi, Kolkata, Hyderabad, Chennai, and Bangalore).\n\n6. Real-Time Project Expertise\n\nAt Learnbay, project-based learning is essential. You can gain information, collaborate with your mentors, and learn how to handle challenging situations with the help of this Advanced Data Science and AI program.\nYou must therefore continue down this road. Numerous projects exist, including ones that estimate customer churn and analyze consumer feedback.\n\n7. IBM Certification\n\nYour work opportunities will increase and become simpler if you have the IBM Advanced Data Science and AI Program course completion certificate, project completion and for micro-skills.\n\nOn the contrary, there’s an institute suggestion for beginners-\n\nCertification Program in Artificial Intelligence and Machine Learning\n\nWhat makes it ideal for beginners?\n\nLearn about inferential statistics, A/B testing, regression, clustering, decision trees, random forests, and other advanced topics in artificial intelligence and machine learning.\nLearn from some of the most well-known Data Scientists and Machine Learning Experts"", 'result': {'fake': 0.9998, 'real': 0.0002}, 'status': 'success'}, {'text': ""in the nation today.\nWork on actual projects to learn how to put the program's lessons into practice.\nWith the help of this curriculum, you can acquire entry-level positions in data science, artificial intelligence, and machine learning.\n\nDrawback: The course does not come along with domain-specific training which hinders the growth of students to specialize in the field of their interest.\n\nConclusion\n\nIf you are interested in learning how to get into the field of artificial intelligence, online courses are a good option. These types of courses are offered through an array of different institutions and organizations around the country and Learnbay has the best and most well-supported reviews since it allows students to learn with interactive modules followed by career- oriented teaching.\n\nThank you"", 'result': {'fake': 0.9899, 'real': 0.0101}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982570, 'subscription': 0, 'content': ""Artificial intelligence (AI) is a computer system designed to make decisions and perform actions in order to achieve a goal or solve a problem. AI offers solutions across industries including healthcare, financial services, transportation and logistics, defense, and manufacturing. AI Online courses can provide you with the training you need to work in the field of AI. With these types of courses, you will be able to learn how artificial intelligence works and what it does across an array of different industries.\n\nThere are many online courses available to help you learn about artificial intelligence-\n\nAdvanced-Data Science and AI Program\n\nThis is designed to master industry-oriented AI skills. Those who are working professionals in the technical domain for more than 4 years, can take advantage of this course to bring their skills up to speed and gain confidence when interviewing at a company based on data science.\n\nCourse Name: Advanced Data Science and AI Program\n\nDuration of the course: 250+ hours and 7-9 months long\nThe cost of the course: Rs.95000/-\n\nSome of the factors why you should prefer Learnbay institute for learning AI courses are-\n\nDomain Specialization\nIn addition to understanding AI throughout an array of industries, you should also be able to understand what its functions are in different domains.\nSo you can pick domain specialization as per your suitable portfolio to specialize in. Some of the domains included are finance, healthcare, transportation, defense, etc.\n\n2. Job Assistance\n\nLearnbay gives you the opportunity to learn and improve skills that will lead them to a high-paying job.\nOther benefits include mock interview preps and access to preparation sessions.\n\n3. Personalized Career Counseling\n\nStudents have a lot of questions about what the future holds, and that's why they seek career counseling.\nChoosing the right learning path is the key to success, and personalized career counseling by Learnbay offers you that assurance.\n\n4. Hybrid Learning Model\n\nFirst of all, Learnbay offers you interactive sessions in addition to sessions with your mentors.\nWith the assistance of your mentor's knowledge and experience in the field, you can take part in the program while answering questions and resolving challenging equations.\n\n5. Project Innovation Labs\n\nStaff from MNC and MAANG are on hand to assist with offline and online project sessions at the MNC project innovation lab.\nThey can converse and argue in more than seven cities (Pune, Mumbai, Delhi, Kolkata, Hyderabad, Chennai, and Bangalore).\n\n6. Real-Time Project Expertise\n\nAt Learnbay, project-based learning is essential. You can gain information, collaborate with your mentors, and learn how to handle challenging situations with the help of this Advanced Data Science and AI program.\nYou must therefore continue down this road. Numerous projects exist, including ones that estimate customer churn and analyze consumer feedback.\n\n7. IBM Certification\n\nYour work opportunities will increase and become simpler if you have the IBM Advanced Data Science and AI Program course completion certificate, project completion and for micro-skills.\n\nOn the contrary, there’s an institute suggestion for beginners-\n\nCertification Program in Artificial Intelligence and Machine Learning\n\nWhat makes it ideal for beginners?\n\nLearn about inferential statistics, A/B testing, regression, clustering, decision trees, random forests, and other advanced topics in artificial intelligence and machine learning.\nLearn from some of the most well-known Data Scientists and Machine Learning Experts in the nation today.\nWork on actual projects to learn how to put the program's lessons into practice.\nWith the help of this curriculum, you can acquire entry-level positions in data science, artificial intelligence, and machine learning.\n\nDrawback: The course does not come along with domain-specific training which hinders the growth of students to specialize in the field of their interest.\n\nConclusion\n\nIf you are interested in learning how to get into the field of artificial intelligence, online courses are a good option. These types of courses are offered through an array of different institutions and organizations around the country and Learnbay has the best and most well-supported reviews since it allows students to learn with interactive modules followed by career- oriented teaching.\n\nThank you"", 'aiModelVersion': '1'}",0.002
Santosh K,1y,What is the best online course to learn AI?,"Technology's next big thing is artificial intelligence.

AI is the process of creating computer systems that exhibit intelligent behavior, think, feel and show human-like qualities. They are better than humans at many tasks, like playing games or driving cars. They have already started replacing humans in some fields like medical diagnosis and customer service. Artificial intelligence has come a long way since its inception as an idea in 1956 when Alan Turing introduced it as a concept in his article titled ""Computing Machinery and Intelligence.

Note: At present, the so-called worldwide AI market is around $200 billion. It is expected to surge to $3.1 trillion by 2025

The new report ""Artificial Intelligence Market Forecast & Opportunities, 2018–2025"" by Transparency Market Research addresses the artificial intelligence market size of all its segments and subsegments across nine geographies in detail.
It analyses the artificial intelligence market over seven years i.e., between 2018 and 2025.
Artificial Intelligence Market Forecast & Opportunities, 2018–2025 has been prepared based on an in-depth analysis of the artificial intelligence market landscape and current trends in the industry.

AI is no longer just an idea in a boardroom. AI-driven chatbots and self-driving cars are a reality, and there’s no doubt that this field will continue to grow in the coming years. If you’re currently looking for the best online course to learn AI, then you need to look no further.

In this answer, we have compiled the best AI courses available in the market.

Let's dive into the list!

Udemy: Udemy is a very famous platform. It has multiple AI courses. However, you require basic python knowledge. If your hands are ever tied and you need some assistance, Udemy's AI courses are always there for you. Udemy has also partnered up with Plaid which allows you to be paid in Bitcoin if satisfy the requirements of your course. However, Udemy does not provide domain-specific programs.
Learnbay: Learnbay is the perfect platform if you are a beginner in artificial intelligence. This platform offers online courses that are designed in a manner that is easy for beginners. The classes have been developed by top industry experts from Google, IBM, Oxford University, and many more institutes of repute that have been teaching computer sciences at universities around the world.

According to me, Learnbay is the best place for beginners and I would highly recommend going with a course on this platform.

Learnbay's top courses with Domain Specialization include:

Advanced Data Science and AI Program
Data Science and AI for Managers & Leaders

Domain Specialization Modules:

Learnbay's best feature is its domain-specific courses. In today's world, you must first qualify for a specialized occupation in order to advance your career. Learnbay has you covered in this regard. Potential employers will value this specialization more than others on your CV because Learnbay's Advanced Data Science and AI Program accepts only students who are serious about pursuing a career in data science. Retail sales, e-commerce, supply chain, mechanical and telecommunications manufacturing in BFSI, and so on are just a few examples.

Some of the top features of the AI course are:·

The course is IBM certified. This means that it is globally recognized. The IBM course is all about breaking down the processes of developing a website. IBM certifications are awarded for completing a course, project, or micro-skills.
Hybrid learning formats can be used to learn new data science information as well as hone skills such as modeling and data analysis.
MNC and MAANG professionals participate in both online and in-person project sessions. For educational purposes, at least seven cities, including Bangalore, Chennai, Hyderabad, Delhi, Kolkata, and Pune, should be visited.
Learnbay offers real-time capstone projects to students of all skill levels, from beginners to experts. Throughout the course, you will work on large-scale data analytics and data science projects. As a student, you might get a taste of what it's like to work in the real world.

The following are some examples of data science projects:

Building Chatbots
Detection of Credit Card Fraud and Fake News
Recommender Systems for Forest Fire Prediction

Final Call:

Because artificial intelligence is such a broad field, choosing a data science training institution requires careful consideration. For the right candidate, Learnbay's Advanced Data Science and AI Program is the ideal solution. They provide you with all of the resources and educational materials you'll need to learn data science. Furthermore, they offer career counselling and job placement assistance in a series of steps.

So I would say Learnbay is the perfect place to kickstart your Artificial Intelligence journey.

Thank you","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/nuafx1rcbo6jlhek', 'title': 'What is the best online course to learn AI?', 'score': {'original': 0.0004, 'ai': 0.9996}, 'blocks': [{'text': 'Technology\'s next big thing is artificial intelligence.\n\nAI is the process of creating computer systems that exhibit intelligent behavior, think, feel and show human-like qualities. They are better than humans at many tasks, like playing games or driving cars. They have already started replacing humans in some fields like medical diagnosis and customer service. Artificial intelligence has come a long way since its inception as an idea in 1956 when Alan Turing introduced it as a concept in his article titled ""Computing Machinery and Intelligence.\n\nNote: At present, the so-called worldwide AI market is around $200 billion. It is expected to surge to $3.1 trillion by 2025\n\nThe new report ""Artificial Intelligence Market Forecast & Opportunities, 2018–2025"" by Transparency Market Research addresses the artificial intelligence market size of all its segments and subsegments across nine geographies in detail.\nIt analyses the artificial intelligence market over seven years i.e., between 2018 and 2025.\nArtificial Intelligence Market Forecast & Opportunities, 2018–2025 has been prepared based on an in-depth analysis of the artificial intelligence market landscape and current trends in the industry.\n\nAI is no longer just an idea in a boardroom. AI-driven chatbots and self-driving cars are a reality, and there’s no doubt that this field will continue to grow in the coming years. If you’re currently looking for the best online course to learn AI, then you need to look no further.\n\nIn this answer, we have compiled the best AI courses available in the market.\n\nLet\'s dive into the list!\n\nUdemy: Udemy is a very famous platform. It has multiple AI courses. However, you require basic python knowledge. If your hands are ever tied and you need some assistance, Udemy\'s AI courses are always there for you. Udemy has also partnered up with Plaid which allows you to be paid in Bitcoin if satisfy the requirements of your course. However, Udemy does not provide domain-specific programs.\nLearnbay: Learnbay is the perfect platform if you are a beginner in artificial intelligence. This platform offers online courses that are designed in a manner that is easy for beginners. The classes have been developed by top industry experts from Google, IBM, Oxford University, and many more institutes of repute that have been teaching computer sciences at universities around the world.\n\nAccording to me, Learnbay is the best place for beginners and I would highly recommend going with a course on this platform.\n\nLearnbay\'s top courses with Domain Specialization include:\n\nAdvanced Data Science and AI Program\nData Science and AI for Managers & Leaders\n\nDomain Specialization Modules:\n\nLearnbay\'s best feature is its domain-specific courses. In today\'s world, you must first qualify for a specialized occupation in order to advance your career. Learnbay has you covered in this regard. Potential employers will value this specialization more than others on your CV because Learnbay\'s Advanced Data Science and AI Program accepts only students who are serious about pursuing a career in data science. Retail sales, e-commerce, supply chain, mechanical and telecommunications manufacturing in BFSI, and so on are just a few examples.\n\nSome of the top features of the AI course are:·\n\nThe course is IBM certified. This means that it is globally recognized. The IBM', 'result': {'fake': 0.9994, 'real': 0.0006}, 'status': 'success'}, {'text': ""course is all about breaking down the processes of developing a website. IBM certifications are awarded for completing a course, project, or micro-skills.\nHybrid learning formats can be used to learn new data science information as well as hone skills such as modeling and data analysis.\nMNC and MAANG professionals participate in both online and in-person project sessions. For educational purposes, at least seven cities, including Bangalore, Chennai, Hyderabad, Delhi, Kolkata, and Pune, should be visited.\nLearnbay offers real-time capstone projects to students of all skill levels, from beginners to experts. Throughout the course, you will work on large-scale data analytics and data science projects. As a student, you might get a taste of what it's like to work in the real world.\n\nThe following are some examples of data science projects:\n\nBuilding Chatbots\nDetection of Credit Card Fraud and Fake News\nRecommender Systems for Forest Fire Prediction\n\nFinal Call:\n\nBecause artificial intelligence is such a broad field, choosing a data science training institution requires careful consideration. For the right candidate, Learnbay's Advanced Data Science and AI Program is the ideal solution. They provide you with all of the resources and educational materials you'll need to learn data science. Furthermore, they offer career counselling and job placement assistance in a series of steps.\n\nSo I would say Learnbay is the perfect place to kickstart your Artificial Intelligence journey.\n\nThank you"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 8, 'credits': 1982562, 'subscription': 0, 'content': 'Technology\'s next big thing is artificial intelligence.\n\nAI is the process of creating computer systems that exhibit intelligent behavior, think, feel and show human-like qualities. They are better than humans at many tasks, like playing games or driving cars. They have already started replacing humans in some fields like medical diagnosis and customer service. Artificial intelligence has come a long way since its inception as an idea in 1956 when Alan Turing introduced it as a concept in his article titled ""Computing Machinery and Intelligence.\n\nNote: At present, the so-called worldwide AI market is around $200 billion. It is expected to surge to $3.1 trillion by 2025\n\nThe new report ""Artificial Intelligence Market Forecast & Opportunities, 2018–2025"" by Transparency Market Research addresses the artificial intelligence market size of all its segments and subsegments across nine geographies in detail.\nIt analyses the artificial intelligence market over seven years i.e., between 2018 and 2025.\nArtificial Intelligence Market Forecast & Opportunities, 2018–2025 has been prepared based on an in-depth analysis of the artificial intelligence market landscape and current trends in the industry.\n\nAI is no longer just an idea in a boardroom. AI-driven chatbots and self-driving cars are a reality, and there’s no doubt that this field will continue to grow in the coming years. If you’re currently looking for the best online course to learn AI, then you need to look no further.\n\nIn this answer, we have compiled the best AI courses available in the market.\n\nLet\'s dive into the list!\n\nUdemy: Udemy is a very famous platform. It has multiple AI courses. However, you require basic python knowledge. If your hands are ever tied and you need some assistance, Udemy\'s AI courses are always there for you. Udemy has also partnered up with Plaid which allows you to be paid in Bitcoin if satisfy the requirements of your course. However, Udemy does not provide domain-specific programs.\nLearnbay: Learnbay is the perfect platform if you are a beginner in artificial intelligence. This platform offers online courses that are designed in a manner that is easy for beginners. The classes have been developed by top industry experts from Google, IBM, Oxford University, and many more institutes of repute that have been teaching computer sciences at universities around the world.\n\nAccording to me, Learnbay is the best place for beginners and I would highly recommend going with a course on this platform.\n\nLearnbay\'s top courses with Domain Specialization include:\n\nAdvanced Data Science and AI Program\nData Science and AI for Managers & Leaders\n\nDomain Specialization Modules:\n\nLearnbay\'s best feature is its domain-specific courses. In today\'s world, you must first qualify for a specialized occupation in order to advance your career. Learnbay has you covered in this regard. Potential employers will value this specialization more than others on your CV because Learnbay\'s Advanced Data Science and AI Program accepts only students who are serious about pursuing a career in data science. Retail sales, e-commerce, supply chain, mechanical and telecommunications manufacturing in BFSI, and so on are just a few examples.\n\nSome of the top features of the AI course are:·\n\nThe course is IBM certified. This means that it is globally recognized. The IBM course is all about breaking down the processes of developing a website. IBM certifications are awarded for completing a course, project, or micro-skills.\nHybrid learning formats can be used to learn new data science information as well as hone skills such as modeling and data analysis.\nMNC and MAANG professionals participate in both online and in-person project sessions. For educational purposes, at least seven cities, including Bangalore, Chennai, Hyderabad, Delhi, Kolkata, and Pune, should be visited.\nLearnbay offers real-time capstone projects to students of all skill levels, from beginners to experts. Throughout the course, you will work on large-scale data analytics and data science projects. As a student, you might get a taste of what it\'s like to work in the real world.\n\nThe following are some examples of data science projects:\n\nBuilding Chatbots\nDetection of Credit Card Fraud and Fake News\nRecommender Systems for Forest Fire Prediction\n\nFinal Call:\n\nBecause artificial intelligence is such a broad field, choosing a data science training institution requires careful consideration. For the right candidate, Learnbay\'s Advanced Data Science and AI Program is the ideal solution. They provide you with all of the resources and educational materials you\'ll need to learn data science. Furthermore, they offer career counselling and job placement assistance in a series of steps.\n\nSo I would say Learnbay is the perfect place to kickstart your Artificial Intelligence journey.\n\nThank you', 'aiModelVersion': '1'}",0.0004
Pravinth,1y,What is the best online course to learn AI?,"One of the most popular online courses for this subject is offered by Stanford University.

Overview:

It offers a range of courses that can be taken at a time, and also offers months of free access for those who sign up during the first month. If you're looking for an intro to AI course, this option is well worth checking out — it's one of the best available.

There are many other courses also available today, some of which are good and others that just don't add up. Because there are so many options, it can be hard to know what will work best for your needs. There are many different types available, depending on your objectives. A good place to start is by assessing your needs and then finding a course that matches those.

Numerous brand-new online courses in data science and artificial intelligence are constantly being updated online. However, with the rapid-fire frenzy of data science and AI career switches and the rising need for artificial intelligence specialists in the data science job market, it's really hard to locate the best course.

Note: However, I have narrowed down the finest online AI courses through research into the particular AI employment market and the edu-tech sector (related to artificial intelligence certification courses).

Some of the top AI courses are

All those interested in learning about AI can choose from a large selection of courses on Udemy, ranging from those who only wish to study the fundamentals to more advanced subjects.

However, Domain Specialisation is absent in AI courses at Udemy.

The advantages of domain expert instruction:

To work as an AI engineer, you'll need more than just a degree or certification in the field. You must be able to solve problems to use your skills, knowledge, and experience to their fullest potential. Domain specialization has consequently grown to be an essential aspect of artificial intelligence. Having a domain-specialized field can help you find a job whether you're an experienced professional or just getting started. The demand for specialists has increased more than that for generalists in recent years. Specialists in one field are more likely to land a job sooner than generalists.

The wonderful website Learnbay provides top-notch data science and artificial intelligence courses. One of the best platforms, it offers incredible features including classes made specifically for different areas.

The following are some of the best aspects of Learnbay's AI courses:

IBM Certification: The IBM Group has approved each course on Learnbay. Students who thrive in Learnbay's Artificial Intelligence and Machine Learning Program are given three certifications. Achieving an IBM certification requires completing a course, a project, and a collection of micro-skills.
Domain Electives: During your studies, you can choose from several domain specialties, including BFSI, Telecom, Manufacturing, and Communication. Learnbay has an advantage because it offers instruction specific to a particular industry. Professional advancement requires meeting the industry's requirements. Everything is available on Learnbay! Specialization will improve the appearance of your resume.
Hybrid Learning: Students can take part in live online mentoring sessions while completing assignments in real-time thanks to Learnbay's hybrid learning environment.
Projects: This program offers both novice and expert projects in Artificial Intelligence and Machine Learning. The course's final project asks you to accomplish complex artificial intelligence projects using Python and other data science technologies. After the course, you will have access to additional materials to continue your education. They offer both real-time and capstone projects to their students.

In these courses, you will work on the following projects:

A self-driving car can drive itself.
Speech Emotion Recognition.
Project Innovation Labs: Each of India's main cities, including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore, has a project innovation lab.

Last Note:

At Learnbay, you'll get a top-notch education where you'll master both programming languages and the concepts behind them. There, students can receive the best data science course training in Bangalore. This includes understanding and using codes. You'll also learn how to apply the scripts to resolve problems in the real world.

All the Best and Happy Learning!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/x9pfzbgqmlnchd85', 'title': 'What is the best online course to learn AI?', 'score': {'original': 0, 'ai': 1}, 'blocks': [{'text': ""One of the most popular online courses for this subject is offered by Stanford University.\n\nOverview:\n\nIt offers a range of courses that can be taken at a time, and also offers months of free access for those who sign up during the first month. If you're looking for an intro to AI course, this option is well worth checking out — it's one of the best available.\n\nThere are many other courses also available today, some of which are good and others that just don't add up. Because there are so many options, it can be hard to know what will work best for your needs. There are many different types available, depending on your objectives. A good place to start is by assessing your needs and then finding a course that matches those.\n\nNumerous brand-new online courses in data science and artificial intelligence are constantly being updated online. However, with the rapid-fire frenzy of data science and AI career switches and the rising need for artificial intelligence specialists in the data science job market, it's really hard to locate the best course.\n\nNote: However, I have narrowed down the finest online AI courses through research into the particular AI employment market and the edu-tech sector (related to artificial intelligence certification courses).\n\nSome of the top AI courses are\n\nAll those interested in learning about AI can choose from a large selection of courses on Udemy, ranging from those who only wish to study the fundamentals to more advanced subjects.\n\nHowever, Domain Specialisation is absent in AI courses at Udemy.\n\nThe advantages of domain expert instruction:\n\nTo work as an AI engineer, you'll need more than just a degree or certification in the field. You must be able to solve problems to use your skills, knowledge, and experience to their fullest potential. Domain specialization has consequently grown to be an essential aspect of artificial intelligence. Having a domain-specialized field can help you find a job whether you're an experienced professional or just getting started. The demand for specialists has increased more than that for generalists in recent years. Specialists in one field are more likely to land a job sooner than generalists.\n\nThe wonderful website Learnbay provides top-notch data science and artificial intelligence courses. One of the best platforms, it offers incredible features including classes made specifically for different areas.\n\nThe following are some of the best aspects of Learnbay's AI courses:\n\nIBM Certification: The IBM Group has approved each course on Learnbay. Students who thrive in Learnbay's Artificial Intelligence and Machine Learning Program are given three certifications. Achieving an IBM certification requires completing a course, a project, and a collection of micro-skills.\nDomain Electives: During your studies, you can choose from several domain specialties, including BFSI, Telecom, Manufacturing, and Communication. Learnbay has an advantage because it offers instruction specific to a particular industry. Professional advancement requires meeting the industry's requirements. Everything is available on Learnbay! Specialization will improve the appearance of your resume.\nHybrid Learning: Students can take part in live online mentoring sessions while completing assignments in real-time thanks to Learnbay's hybrid learning environment.\nProjects: This program offers both novice and expert projects in Artificial"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}, {'text': ""Intelligence and Machine Learning. The course's final project asks you to accomplish complex artificial intelligence projects using Python and other data science technologies. After the course, you will have access to additional materials to continue your education. They offer both real-time and capstone projects to their students.\n\nIn these courses, you will work on the following projects:\n\nA self-driving car can drive itself.\nSpeech Emotion Recognition.\nProject Innovation Labs: Each of India's main cities, including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore, has a project innovation lab.\n\nLast Note:\n\nAt Learnbay, you'll get a top-notch education where you'll master both programming languages and the concepts behind them. There, students can receive the best data science course training in Bangalore. This includes understanding and using codes. You'll also learn how to apply the scripts to resolve problems in the real world.\n\nAll the Best and Happy Learning!"", 'result': {'fake': 1, 'real': 0}, 'status': 'success'}], 'credits_used': 7, 'credits': 1982555, 'subscription': 0, 'content': ""One of the most popular online courses for this subject is offered by Stanford University.\n\nOverview:\n\nIt offers a range of courses that can be taken at a time, and also offers months of free access for those who sign up during the first month. If you're looking for an intro to AI course, this option is well worth checking out — it's one of the best available.\n\nThere are many other courses also available today, some of which are good and others that just don't add up. Because there are so many options, it can be hard to know what will work best for your needs. There are many different types available, depending on your objectives. A good place to start is by assessing your needs and then finding a course that matches those.\n\nNumerous brand-new online courses in data science and artificial intelligence are constantly being updated online. However, with the rapid-fire frenzy of data science and AI career switches and the rising need for artificial intelligence specialists in the data science job market, it's really hard to locate the best course.\n\nNote: However, I have narrowed down the finest online AI courses through research into the particular AI employment market and the edu-tech sector (related to artificial intelligence certification courses).\n\nSome of the top AI courses are\n\nAll those interested in learning about AI can choose from a large selection of courses on Udemy, ranging from those who only wish to study the fundamentals to more advanced subjects.\n\nHowever, Domain Specialisation is absent in AI courses at Udemy.\n\nThe advantages of domain expert instruction:\n\nTo work as an AI engineer, you'll need more than just a degree or certification in the field. You must be able to solve problems to use your skills, knowledge, and experience to their fullest potential. Domain specialization has consequently grown to be an essential aspect of artificial intelligence. Having a domain-specialized field can help you find a job whether you're an experienced professional or just getting started. The demand for specialists has increased more than that for generalists in recent years. Specialists in one field are more likely to land a job sooner than generalists.\n\nThe wonderful website Learnbay provides top-notch data science and artificial intelligence courses. One of the best platforms, it offers incredible features including classes made specifically for different areas.\n\nThe following are some of the best aspects of Learnbay's AI courses:\n\nIBM Certification: The IBM Group has approved each course on Learnbay. Students who thrive in Learnbay's Artificial Intelligence and Machine Learning Program are given three certifications. Achieving an IBM certification requires completing a course, a project, and a collection of micro-skills.\nDomain Electives: During your studies, you can choose from several domain specialties, including BFSI, Telecom, Manufacturing, and Communication. Learnbay has an advantage because it offers instruction specific to a particular industry. Professional advancement requires meeting the industry's requirements. Everything is available on Learnbay! Specialization will improve the appearance of your resume.\nHybrid Learning: Students can take part in live online mentoring sessions while completing assignments in real-time thanks to Learnbay's hybrid learning environment.\nProjects: This program offers both novice and expert projects in Artificial Intelligence and Machine Learning. The course's final project asks you to accomplish complex artificial intelligence projects using Python and other data science technologies. After the course, you will have access to additional materials to continue your education. They offer both real-time and capstone projects to their students.\n\nIn these courses, you will work on the following projects:\n\nA self-driving car can drive itself.\nSpeech Emotion Recognition.\nProject Innovation Labs: Each of India's main cities, including Delhi, Kolkata, Mumbai, Pune, Hyderabad, Chennai, and Bangalore, has a project innovation lab.\n\nLast Note:\n\nAt Learnbay, you'll get a top-notch education where you'll master both programming languages and the concepts behind them. There, students can receive the best data science course training in Bangalore. This includes understanding and using codes. You'll also learn how to apply the scripts to resolve problems in the real world.\n\nAll the Best and Happy Learning!"", 'aiModelVersion': '1'}",0.0
Edwin Chen,12y,"How would you programmatically parse a sentence and decide whether to answer with ""that's what she said""? Outline an algorithm that, given a sentence, returns true or false for whether the statement is appropriate.","I spent a few hours building my own TWSS classifier a couple weekends ago, so I'll describe my experience/flesh out some of the other suggestions. Training data I briefly looked at using Twitter as a corpus, as User-13743967034596023228's excellent answer also mentions, but decided that it was too noisy (most TWSS tweets aren't that funny, it's hard to tell what phrase the TWSS is in response to, and not all tweets containing ""TWSS"" are even TWSS jokes). Instead, taking a (modified) cue from the Kiddon and Brun paper, I used 1000 sentences from twssstories.com for positive training data, and 500 sentences from each of textsfromlastnight.com and fmylife.com for negative training data. I also normalized all the sentences by removing punctuation and converting to lowercase. Naive Bayes Next, I trained a unigram Naive Bayes classifier (with add-one smoothing). I also tried a bigram classifier, but the unigram classifier performed better with the data I had; here's a precision-recall curve comparing the two:To get a better idea of what's going on, here are some of the most predictive features under each: unigram          p(twss|unigram)pull            0.9724889822144924bigger          0.9614677503890157wet             0.959004244327654hard            0.9527628206878138stick           0.9505783678914388hole            0.9443870318715991oh              0.9432941279908561replied         0.943294127990856fast            0.943294127990856longer          0.9397415371025485bigram          p(twss|bigram)it in           0.9801434151851175START wow       0.9705079286853889START oh        0.9473580156961879its too         0.9350522640444204pull out        0.9187779331523677too big         0.9187779331523677START man       0.9113755525471394hard END        0.9113755525471394put it          0.9071442285463515that thing      0.9024886021363793stick it        0.9024886021363793my god          0.9024886021363793go in           0.8916207044791409START ugh       0.8916207044791409make it         0.8916207044791409its so          0.8916207044791409(So yeah, next time someone starts a sentence with ""ugh"", or says ""it's so..."", get ready.) Under a particular choice of parameters (a TWSS classification threshold of 0.99, when using equal prior probabilities of 0.5 for TWSS and not TWSS), the unigram classifier gives 0.97 precision and 0.82 recall (823 true positives, 177 false negatives, 974 true negatives, 26 false positives) on an out-of-sample test set consisting of equal amounts of positive and negative examples. [This is roughly the same performance as the bvandenbos classifier that Charlie Cheever linked to: mine performs slightly better on my test set, his performs slightly better on his test set. (Unsurprising, given that we're both using Naive Bayes.)] I also briefly tried using logistic regression and decision trees, but the unigram classifier easily beat them both. (YMMV with more data or better tuning, though.) Faerie Tail In order to see how well the classifier generalizes to a different source of data, I ran the algorithm on some fairy tales I pulled from Project Gutenberg. Here are some of the sentences it TWSSed: “The African magician carries it carefully wrapt up in his bosom,” said the princess; “and this I can assure you, because he pulled it out before me, and showed it to me in triumph.” (Aladdin)It is vanished; but I had no concern in its removal. (Aladdin)“My son,” said he, “what a man you are to do such surprising things always in the twinkling of an eye!” (Aladdin)“Sire,” replied Aladdin, “I have not the least reason to complain of your conduct, since you did nothing but what your duty required.”One was too long, another too short; so she tried them all till she came to the seventh, and that was so comfortable that she laid herself down, and was soon fast asleep. (Snow White)“Oh yes, I will try,” said Snow-white.Finally, I threw up a demo on Heroku (http://twss-classifier.heroku.com/) in order to better play around with the classifier. (Note: the demo optimizes for precision over recall.)","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/icy8mbnjks4dqahe', 'title': 'How would you programmatically parse a sentence and decide whether to answer with ""that\'s what she said""? Outline an algorithm that, given a sentence, returns true or false for whether the statement is appropriate.', 'score': {'original': 0.92005, 'ai': 0.07995}, 'blocks': [{'text': 'I spent a few hours building my own TWSS classifier a couple weekends ago, so I\'ll describe my experience/flesh out some of the other suggestions. Training data I briefly looked at using Twitter as a corpus, as User-13743967034596023228\'s excellent answer also mentions, but decided that it was too noisy (most TWSS tweets aren\'t that funny, it\'s hard to tell what phrase the TWSS is in response to, and not all tweets containing ""TWSS"" are even TWSS jokes). Instead, taking a (modified) cue from the Kiddon and Brun paper, I used 1000 sentences from twssstories.com for positive training data, and 500 sentences from each of textsfromlastnight.com and fmylife.com for negative training data. I also normalized all the sentences by removing punctuation and converting to lowercase. Naive Bayes Next, I trained a unigram Naive Bayes classifier (with add-one smoothing). I also tried a bigram classifier, but the unigram classifier performed better with the data I had; here\'s a precision-recall curve comparing the two:To get a better idea of what\'s going on, here are some of the most predictive features under each: unigram          p(twss|unigram)pull            0.9724889822144924bigger          0.9614677503890157wet             0.959004244327654hard            0.9527628206878138stick           0.9505783678914388hole            0.9443870318715991oh              0.9432941279908561replied         0.943294127990856fast            0.943294127990856longer          0.9397415371025485bigram          p(twss|bigram)it in           0.9801434151851175START wow       0.9705079286853889START oh        0.9473580156961879its too         0.9350522640444204pull out        0.9187779331523677too big         0.9187779331523677START man       0.9113755525471394hard END        0.9113755525471394put it          0.9071442285463515that thing      0.9024886021363793stick it        0.9024886021363793my god          0.9024886021363793go in           0.8916207044791409START ugh       0.8916207044791409make it         0.8916207044791409its so          0.8916207044791409(So yeah, next time someone starts a sentence with ""ugh"", or says ""it\'s so..."", get ready.) Under a particular choice of parameters (a TWSS classification threshold of 0.99, when using equal prior probabilities of 0.5 for TWSS and not TWSS), the unigram classifier gives', 'result': {'fake': 0.0482, 'real': 0.9518}, 'status': 'success'}, {'text': ""0.97 precision and 0.82 recall (823 true positives, 177 false negatives, 974 true negatives, 26 false positives) on an out-of-sample test set consisting of equal amounts of positive and negative examples. [This is roughly the same performance as the bvandenbos classifier that Charlie Cheever linked to: mine performs slightly better on my test set, his performs slightly better on his test set. (Unsurprising, given that we're both using Naive Bayes.)] I also briefly tried using logistic regression and decision trees, but the unigram classifier easily beat them both. (YMMV with more data or better tuning, though.) Faerie Tail In order to see how well the classifier generalizes to a different source of data, I ran the algorithm on some fairy tales I pulled from Project Gutenberg. Here are some of the sentences it TWSSed: “The African magician carries it carefully wrapt up in his bosom,” said the princess; “and this I can assure you, because he pulled it out before me, and showed it to me in triumph.” (Aladdin)It is vanished; but I had no concern in its removal. (Aladdin)“My son,” said he, “what a man you are to do such surprising things always in the twinkling of an eye!” (Aladdin)“Sire,” replied Aladdin, “I have not the least reason to complain of your conduct, since you did nothing but what your duty required.”One was too long, another too short; so she tried them all till she came to the seventh, and that was so comfortable that she laid herself down, and was soon fast asleep. (Snow White)“Oh yes, I will try,” said Snow-white.Finally, I threw up a demo on Heroku (http://twss-classifier.heroku.com/) in order to better play around with the classifier. (Note: the demo optimizes for precision over recall.)"", 'result': {'fake': 0.0014, 'real': 0.9986}, 'status': 'success'}], 'credits_used': 6, 'credits': 1982549, 'subscription': 0, 'content': 'I spent a few hours building my own TWSS classifier a couple weekends ago, so I\'ll describe my experience/flesh out some of the other suggestions. Training data I briefly looked at using Twitter as a corpus, as User-13743967034596023228\'s excellent answer also mentions, but decided that it was too noisy (most TWSS tweets aren\'t that funny, it\'s hard to tell what phrase the TWSS is in response to, and not all tweets containing ""TWSS"" are even TWSS jokes). Instead, taking a (modified) cue from the Kiddon and Brun paper, I used 1000 sentences from twssstories.com for positive training data, and 500 sentences from each of textsfromlastnight.com and fmylife.com for negative training data. I also normalized all the sentences by removing punctuation and converting to lowercase. Naive Bayes Next, I trained a unigram Naive Bayes classifier (with add-one smoothing). I also tried a bigram classifier, but the unigram classifier performed better with the data I had; here\'s a precision-recall curve comparing the two:To get a better idea of what\'s going on, here are some of the most predictive features under each: unigram          p(twss|unigram)pull            0.9724889822144924bigger          0.9614677503890157wet             0.959004244327654hard            0.9527628206878138stick           0.9505783678914388hole            0.9443870318715991oh              0.9432941279908561replied         0.943294127990856fast            0.943294127990856longer          0.9397415371025485bigram          p(twss|bigram)it in           0.9801434151851175START wow       0.9705079286853889START oh        0.9473580156961879its too         0.9350522640444204pull out        0.9187779331523677too big         0.9187779331523677START man       0.9113755525471394hard END        0.9113755525471394put it          0.9071442285463515that thing      0.9024886021363793stick it        0.9024886021363793my god          0.9024886021363793go in           0.8916207044791409START ugh       0.8916207044791409make it         0.8916207044791409its so          0.8916207044791409(So yeah, next time someone starts a sentence with ""ugh"", or says ""it\'s so..."", get ready.) Under a particular choice of parameters (a TWSS classification threshold of 0.99, when using equal prior probabilities of 0.5 for TWSS and not TWSS), the unigram classifier gives 0.97 precision and 0.82 recall (823 true positives, 177 false negatives, 974 true negatives, 26 false positives) on an out-of-sample test set consisting of equal amounts of positive and negative examples. [This is roughly the same performance as the bvandenbos classifier that Charlie Cheever linked to: mine performs slightly better on my test set, his performs slightly better on his test set. (Unsurprising, given that we\'re both using Naive Bayes.)] I also briefly tried using logistic regression and decision trees, but the unigram classifier easily beat them both. (YMMV with more data or better tuning, though.) Faerie Tail In order to see how well the classifier generalizes to a different source of data, I ran the algorithm on some fairy tales I pulled from Project Gutenberg. Here are some of the sentences it TWSSed: “The African magician carries it carefully wrapt up in his bosom,” said the princess; “and this I can assure you, because he pulled it out before me, and showed it to me in triumph.” (Aladdin)It is vanished; but I had no concern in its removal. (Aladdin)“My son,” said he, “what a man you are to do such surprising things always in the twinkling of an eye!” (Aladdin)“Sire,” replied Aladdin, “I have not the least reason to complain of your conduct, since you did nothing but what your duty required.”One was too long, another too short; so she tried them all till she came to the seventh, and that was so comfortable that she laid herself down, and was soon fast asleep. (Snow White)“Oh yes, I will try,” said Snow-white.Finally, I threw up a demo on Heroku (http://twss-classifier.heroku.com/) in order to better play around with the classifier. (Note: the demo optimizes for precision over recall.)', 'aiModelVersion': '1'}",0.92005
Kirti,11mo,What is the best way to learn Artificial Intelligence for a beginner?,"Before entering into the learning part, let me explain what is artificial intelligence.

Artificial intelligence

AI is a wide-ranging branch of computer science that is concerned with building machines that are capable of performing tasks that typically require human intelligence. AI is an interdisciplinary science with multiple approaches, but advancements in machine learning that are deep in learning create a paradigm shift in virtually every sector of the tech industry.

Why Learn AI?

AI is an exciting field at the forefront of finding solutions to society's most pressing problems, which are disease, pollution, and climate change. This is also a rapidly growing sector of the economy, with AI software revenue, expected to increase by 21.3 % from the previous year.

Steps to learn AI effectively :

Understand the concept of the prerequisites.
Ace AI theory.
Master data processing, machine learning engineering, and data engineering.
Work on AI projects.
Opt for an AI course.
Applying for an internship is a great way.

So far these are the steps to learn Artificial Intelligence for beginners, and if one chooses this career, then there are many institutes to provide the best courses for the students.

Let's have a look at these institutes.

Coursera is one of the online platforms where you can get many courses online. Supervised Machine Learning: Regression and Classification is one of the courses they provide. Students should pay a fee for certification.

Note: Coursera doesn’t have any real-world projects, career guidance, or professional assistance.

This is one of the leading online training institutes, they offer Advance Data Science and AI Program in that students can access full-time course materials and get 3 years of flexible subscriptions. Former experts from top MNCs are the tutors to take classes for the students.

Live sessions are more than 250+ hours, for weekdays - 7 months, and for weekends - 9 months.

Features:

This institute is having 7+ project innovation labs which are located in Delhi, Bangalore, Kolkata, Pune, Mumbai, Chennai, etc.
Real-time & Capstone projects help the students to work with new technologies and ideas they have.
Domain Specialization, while doing the course, you can master any two domains in a specific field which builds your knowledge and skills, later you learn more about the domain that you choose. Domain electives such as Sales and Marketing, BFSI, Manufacturing, Automotive, and Telecom are available.
A hybrid mode of learning is available here, students can attend theory sessions, and for project-based learning, they can visit nearby labs.
IBM course certification, capstone project, and micro skill certification are provided here.

Final thoughts:

The best way to learn Artificial Intelligence for a beginner is Python, and for learning Learnbay is the best choice. The reason is they offer the best training and projects with the hybrid mode of learning and an important specification they provide IBM and Capstone projects to the students.

Thank you!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/dyzi3v85mon2gh7a', 'title': 'What is the best way to learn Artificial Intelligence for a beginner?', 'score': {'original': 0.9402, 'ai': 0.0598}, 'blocks': [{'text': ""Before entering into the learning part, let me explain what is artificial intelligence.\n\nArtificial intelligence\n\nAI is a wide-ranging branch of computer science that is concerned with building machines that are capable of performing tasks that typically require human intelligence. AI is an interdisciplinary science with multiple approaches, but advancements in machine learning that are deep in learning create a paradigm shift in virtually every sector of the tech industry.\n\nWhy Learn AI?\n\nAI is an exciting field at the forefront of finding solutions to society's most pressing problems, which are disease, pollution, and climate change. This is also a rapidly growing sector of the economy, with AI software revenue, expected to increase by 21.3 % from the previous year.\n\nSteps to learn AI effectively :\n\nUnderstand the concept of the prerequisites.\nAce AI theory.\nMaster data processing, machine learning engineering, and data engineering.\nWork on AI projects.\nOpt for an AI course.\nApplying for an internship is a great way.\n\nSo far these are the steps to learn Artificial Intelligence for beginners, and if one chooses this career, then there are many institutes to provide the best courses for the students.\n\nLet's have a look at these institutes.\n\nCoursera is one of the online platforms where you can get many courses online. Supervised Machine Learning: Regression and Classification is one of the courses they provide. Students should pay a fee for certification.\n\nNote: Coursera doesn’t have any real-world projects, career guidance, or professional assistance.\n\nThis is one of the leading online training institutes, they offer Advance Data Science and AI Program in that students can access full-time course materials and get 3 years of flexible subscriptions. Former experts from top MNCs are the tutors to take classes for the students.\n\nLive sessions are more than 250+ hours, for weekdays - 7 months, and for weekends - 9 months.\n\nFeatures:\n\nThis institute is having 7+ project innovation labs which are located in Delhi, Bangalore, Kolkata, Pune, Mumbai, Chennai, etc.\nReal-time & Capstone projects help the students to work with new technologies and ideas they have.\nDomain Specialization, while doing the course, you can master any two domains in a specific field which builds your knowledge and skills, later you learn more about the domain that you choose. Domain electives such as Sales and Marketing, BFSI, Manufacturing, Automotive, and Telecom are available.\nA hybrid mode of learning is available here, students can attend theory sessions, and for project-based learning, they can visit nearby labs.\nIBM course certification, capstone project, and micro skill certification are provided here.\n\nFinal thoughts:\n\nThe best way to learn Artificial Intelligence for a beginner is Python, and for learning Learnbay is the best choice. The reason is they offer the best training and projects with the hybrid mode of learning and an important specification they provide IBM and Capstone projects to the students.\n\nThank you!"", 'result': {'fake': 0.0712, 'real': 0.9288}, 'status': 'success'}], 'credits_used': 5, 'credits': 1982544, 'subscription': 0, 'content': ""Before entering into the learning part, let me explain what is artificial intelligence.\n\nArtificial intelligence\n\nAI is a wide-ranging branch of computer science that is concerned with building machines that are capable of performing tasks that typically require human intelligence. AI is an interdisciplinary science with multiple approaches, but advancements in machine learning that are deep in learning create a paradigm shift in virtually every sector of the tech industry.\n\nWhy Learn AI?\n\nAI is an exciting field at the forefront of finding solutions to society's most pressing problems, which are disease, pollution, and climate change. This is also a rapidly growing sector of the economy, with AI software revenue, expected to increase by 21.3 % from the previous year.\n\nSteps to learn AI effectively :\n\nUnderstand the concept of the prerequisites.\nAce AI theory.\nMaster data processing, machine learning engineering, and data engineering.\nWork on AI projects.\nOpt for an AI course.\nApplying for an internship is a great way.\n\nSo far these are the steps to learn Artificial Intelligence for beginners, and if one chooses this career, then there are many institutes to provide the best courses for the students.\n\nLet's have a look at these institutes.\n\nCoursera is one of the online platforms where you can get many courses online. Supervised Machine Learning: Regression and Classification is one of the courses they provide. Students should pay a fee for certification.\n\nNote: Coursera doesn’t have any real-world projects, career guidance, or professional assistance.\n\nThis is one of the leading online training institutes, they offer Advance Data Science and AI Program in that students can access full-time course materials and get 3 years of flexible subscriptions. Former experts from top MNCs are the tutors to take classes for the students.\n\nLive sessions are more than 250+ hours, for weekdays - 7 months, and for weekends - 9 months.\n\nFeatures:\n\nThis institute is having 7+ project innovation labs which are located in Delhi, Bangalore, Kolkata, Pune, Mumbai, Chennai, etc.\nReal-time & Capstone projects help the students to work with new technologies and ideas they have.\nDomain Specialization, while doing the course, you can master any two domains in a specific field which builds your knowledge and skills, later you learn more about the domain that you choose. Domain electives such as Sales and Marketing, BFSI, Manufacturing, Automotive, and Telecom are available.\nA hybrid mode of learning is available here, students can attend theory sessions, and for project-based learning, they can visit nearby labs.\nIBM course certification, capstone project, and micro skill certification are provided here.\n\nFinal thoughts:\n\nThe best way to learn Artificial Intelligence for a beginner is Python, and for learning Learnbay is the best choice. The reason is they offer the best training and projects with the hybrid mode of learning and an important specification they provide IBM and Capstone projects to the students.\n\nThank you!"", 'aiModelVersion': '1'}",0.9402
Devin O'Keefe,Updated 4y,What is the weirdest thing a neural network has done?,"A neural network was able to guess a person’s sexual orientation with 87% accuracy from their photo. It found that our facial structure can be used to distinguish our sexual orientation.

Researchers at Stanford University[1] gathered 35,326 images of faces from an online dating site, where 50% of the participants were gay, and 50% were straight. They then trained a neural network on this data, and were able to predict a man’s sexual orientation with 91% accuracy, and a woman’s with 83% accuracy (for an average of 87%), when given 5 photos of the person. These results were significantly better than when humans were asked to do the same task, and had a measly average of 57.5% accuracy (61% for men and 54% for women).

One of the surprising insights for me from this neural network is the amount of information a face can yield about a person. Hair styles and facial expressions were used by the network as features. However the weirdest part of this Neural Network is that it found that our facial structure contained insightful hints to predict a person’s sexuality. For example, men who are gay tend to have narrower jaws, longer noses and larger foreheads. Similarly, gay women have larger jaws and smaller foreheads in comparison to straight women.

Worryingly, this is one of the first examples of a case where AI and Neural Networks pose a dangerous threat that isn’t speculation on what could theoretically happen in the future with regards to job loss. Homosexuality is illegal in 73 countries, and carries the death penalty in 8 countries. If an oppressive government developed neural networks like this and use their government records (e.g. passport photos, family relations) as data, there could be terrible consequences.

Footnotes

[1] Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/0owpnl7e8kyvdi42', 'title': 'What is the weirdest thing a neural network has done?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'A neural network was able to guess a person’s sexual orientation with 87% accuracy from their photo. It found that our facial structure can be used to distinguish our sexual orientation.\n\nResearchers at Stanford University[1] gathered 35,326 images of faces from an online dating site, where 50% of the participants were gay, and 50% were straight. They then trained a neural network on this data, and were able to predict a man’s sexual orientation with 91% accuracy, and a woman’s with 83% accuracy (for an average of 87%), when given 5 photos of the person. These results were significantly better than when humans were asked to do the same task, and had a measly average of 57.5% accuracy (61% for men and 54% for women).\n\nOne of the surprising insights for me from this neural network is the amount of information a face can yield about a person. Hair styles and facial expressions were used by the network as features. However the weirdest part of this Neural Network is that it found that our facial structure contained insightful hints to predict a person’s sexuality. For example, men who are gay tend to have narrower jaws, longer noses and larger foreheads. Similarly, gay women have larger jaws and smaller foreheads in comparison to straight women.\n\nWorryingly, this is one of the first examples of a case where AI and Neural Networks pose a dangerous threat that isn’t speculation on what could theoretically happen in the future with regards to job loss. Homosexuality is illegal in 73 countries, and carries the death penalty in 8 countries. If an oppressive government developed neural networks like this and use their government records (e.g. passport photos, family relations) as data, there could be terrible consequences.\n\nFootnotes\n\n[1] Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982541, 'subscription': 0, 'content': 'A neural network was able to guess a person’s sexual orientation with 87% accuracy from their photo. It found that our facial structure can be used to distinguish our sexual orientation.\n\nResearchers at Stanford University[1] gathered 35,326 images of faces from an online dating site, where 50% of the participants were gay, and 50% were straight. They then trained a neural network on this data, and were able to predict a man’s sexual orientation with 91% accuracy, and a woman’s with 83% accuracy (for an average of 87%), when given 5 photos of the person. These results were significantly better than when humans were asked to do the same task, and had a measly average of 57.5% accuracy (61% for men and 54% for women).\n\nOne of the surprising insights for me from this neural network is the amount of information a face can yield about a person. Hair styles and facial expressions were used by the network as features. However the weirdest part of this Neural Network is that it found that our facial structure contained insightful hints to predict a person’s sexuality. For example, men who are gay tend to have narrower jaws, longer noses and larger foreheads. Similarly, gay women have larger jaws and smaller foreheads in comparison to straight women.\n\nWorryingly, this is one of the first examples of a case where AI and Neural Networks pose a dangerous threat that isn’t speculation on what could theoretically happen in the future with regards to job loss. Homosexuality is illegal in 73 countries, and carries the death penalty in 8 countries. If an oppressive government developed neural networks like this and use their government records (e.g. passport photos, family relations) as data, there could be terrible consequences.\n\nFootnotes\n\n[1] Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images', 'aiModelVersion': '1'}",0.9997
Håkon Hapnes Strand,4y,Why is gradient descent so effective in machine learning?,"Efficient is the word we’re looking for, not effective.

Gradient descent is a primitive algorithm. It merrily walks down the hill in the steepest direction until it gets to a bottom. Then it just stops.

Optimization is usually a delicate balancing act between exploitation and exploration, but gradient descent doesn’t care about exploration. It keeps walking down without bothering to observe its surroundings in the search space.

This simplistic approach has the unfortunate consequence that global convergence can’t be guaranteed. On the other hand, it makes gradient descent fast.

There are many heuristic optimization algorithms with better convergence guarantees than gradient descent, but they aren’t nearly as computationally efficient.

Until we have faster computers, gradient descent is a good compromise for training large neural networks.

And it’s not all that bad. We have simple techniques to introduce some randomness to the optimization process, like random sampling and dropout.

Gradient descent is an efficient optimization algorithm that allows us to train neural networks on huge datasets in reasonable time, and it has proven itself to work well in practice.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/yz47u5v3wpi0nso9', 'title': 'Why is gradient descent so effective in machine learning?', 'score': {'original': 0.001, 'ai': 0.999}, 'blocks': [{'text': 'Efficient is the word we’re looking for, not effective.\n\nGradient descent is a primitive algorithm. It merrily walks down the hill in the steepest direction until it gets to a bottom. Then it just stops.\n\nOptimization is usually a delicate balancing act between exploitation and exploration, but gradient descent doesn’t care about exploration. It keeps walking down without bothering to observe its surroundings in the search space.\n\nThis simplistic approach has the unfortunate consequence that global convergence can’t be guaranteed. On the other hand, it makes gradient descent fast.\n\nThere are many heuristic optimization algorithms with better convergence guarantees than gradient descent, but they aren’t nearly as computationally efficient.\n\nUntil we have faster computers, gradient descent is a good compromise for training large neural networks.\n\nAnd it’s not all that bad. We have simple techniques to introduce some randomness to the optimization process, like random sampling and dropout.\n\nGradient descent is an efficient optimization algorithm that allows us to train neural networks on huge datasets in reasonable time, and it has proven itself to work well in practice.', 'result': {'fake': 0.999, 'real': 0.001}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982539, 'subscription': 0, 'content': 'Efficient is the word we’re looking for, not effective.\n\nGradient descent is a primitive algorithm. It merrily walks down the hill in the steepest direction until it gets to a bottom. Then it just stops.\n\nOptimization is usually a delicate balancing act between exploitation and exploration, but gradient descent doesn’t care about exploration. It keeps walking down without bothering to observe its surroundings in the search space.\n\nThis simplistic approach has the unfortunate consequence that global convergence can’t be guaranteed. On the other hand, it makes gradient descent fast.\n\nThere are many heuristic optimization algorithms with better convergence guarantees than gradient descent, but they aren’t nearly as computationally efficient.\n\nUntil we have faster computers, gradient descent is a good compromise for training large neural networks.\n\nAnd it’s not all that bad. We have simple techniques to introduce some randomness to the optimization process, like random sampling and dropout.\n\nGradient descent is an efficient optimization algorithm that allows us to train neural networks on huge datasets in reasonable time, and it has proven itself to work well in practice.', 'aiModelVersion': '1'}",0.001
Richard Zhu,5y,What is the TLDR of Spherical Convolutional Neural Networks (ICLR 2018 best paper)?,"Thanks for the recommendation! This paper was a delight to read. TLDR: The authors develop an efficient analogue of translational cross-correlation in spherical signals (e.g. such as ones from 360-degree/4pi-steradian action cameras or Google Street View). The provided construction has several useful properties, including equivariance over SO(3) and S^2 as well as efficient computation via a generalized FFT algorithm. In particular, they construct a S^2 to SO(3) correlation and SO(3) to SO(3) correlation, both of which are novel approaches to spherical data. Convolutions in 2D In 2D images, we typically don’t care where an object is located. If a cat is in the upper-left corner of an image, it should have the same activations on a “cat-filter” as the same cat translated to be in the lower-right corner of an image. Similarly, if we’re examining a 3D image like an MRI, it doesn’t matter where a polyp is located.. as long as we can identify it with the activations of some filter. Convolutions leverage the translational invariance of space.. hence translational cross-correlations (i.e. the simple “sliding” of a filter across an image in each of the dimensions) are useful for several reasons:They maintain translational invariance (in the paper, referred to as equivariance). When you translate the image or the filter, the cross-correlation of a filter with the image does not change.They are efficiently computable. If you fix the filter, then the convolution (series of multiply-adds) can be replaced with a FFT-multiply-IFFT. The overhead of the FFT/IFFT only makes sense if it can be amortized over many repeated convolutions with the same filter (see [1]).They are resource-efficient, especially when compared to a fully-connected layer. We impose the infinitely-strong prior of translational invariance, a symmetry which drastically cuts down the # of parameters we need.For a really intuitive guide, check out Chris Olah’s Understanding Convolutions. For a standard 2D convolutional layer, what really is being performed is a map taking two 2D arrays and forming another 2D array: Z2×Z2→Z2Z2×Z2→Z2\mathbb{Z}^2 \times \mathbb{Z}^2 \rightarrow \mathbb{Z}^2 That is, we take 2D array of numbers (input image) and a 2D array (filter/kernel) and convolve them to form another 2D array (output). More mathematically, we might write: [f∗ψ](x)=∫Z2f(x+r)ψ(r)dr[f∗ψ](x)=∫Z2f(x+r)ψ(r)dr[f * \psi](x) = \int_{\mathbb{Z}^2} f(x + r) \psi(r) dr Generalizing to spherical data When you take this idea and naively generalize, you might expect a spherical convolution to take as input two arrays on S2S2S^2 (2D surface of a sphere) and output another one. This idea has been explored in [2], where the spherical convolution is specified as: [f∗ψ](x)=∫SO(3)f(R^n)ψ(R−1x)dR[f∗ψ](x)=∫SO(3)f(Rn^)ψ(R−1x)dR[f * \psi](x) = \int_{\text{SO(3)}} f(R\hat{n}) \psi(R^{-1} x) dR where f is a signal defined on S2S2S^2, ψψ\psi is a filter defined on S2S2S^2, R is a rotation matrix in SO(3), n is the unit vector pointing north, and x is a direction on S2S2S^2 that we would like to compute the output of the convolution at. This definition takes two arrays on S2S2S^2 (the sphere) and outputs an array on S2S2S^2. Note: SO(3) is the 3-dimensional space of all possible rotations of a sphere. Why 3? Any rotation of a sphere can be decomposed into 3 simple rotations (Euler angles). ^nn^\hat{n} appears in the expression because generally there are multiple rotations that can take X to Y, so better just fix one point to start. One of the key arguments of this paper is that this naive generalization is unnecessarily restrictive. It has the nice property of being a function on the sphere S2S2S^2! But the integral over SO(3) also integrates over a rotation in the ψψ\psi factor, meaning that ψψ\psi is made circularly symmetric about an axis before computing the cross-correlation. By necessity, a circularly symmetric filter (or signal) is less expressive than one that is not. Thus, Cohen et al instead define the spherical convolution on spherical data as: [f∗ψ](R)=∫S2f(→x)ψ(R−1→x)dx[f∗ψ](R)=∫S2f(x→)ψ(R−1x→)dx[f * \psi](R) = \int_{S^2} f(\vec{x}) \psi(R^{-1} \vec{x}) dx This definition takes two arrays [image, filter] on the sphere but outputs an array in SO(3), the 3D space of rotations. In order to continue using the output feature map as inputs to other layers, you’ll also need another convolution defined on the rotation group SO(3): [f∗ψ](R)=∫SO(3)f(Q)ψ(R−1Q)dQ[f∗ψ](R)=∫SO(3)f(Q)ψ(R−1Q)dQ[f * \psi](R) = \int_{SO(3)} f(Q) \psi(R^{-1} Q) dQ These definitions can be shown to be equivariant: you can rotate a filter before convolving with an image, and it is equal to the convolution of the rotated filter with the original image. Showing that these can be computed efficiently is not easy. Fourier analysis in rectangular coordinates is straightforward compared to the orthogonal basis functions one must use in S^2 [spherical harmonics] or SO(3) [Wigner D-functions]. For the interested and motivated analyst, the paper provides plenty of additional references to chew on. Experiments Not many papers in machine learning are worked through in as much detail as this one. The authors not only analyze the theory, but they also provide an efficient implementation of the basic algorithm and test their approach on multiple datasets. Notice that all of the discussion above presents the convolutions on continuous groups S^2 and SO(3). Real data is discretized, and as such the equivariance property will not hold exactly. The authors quantify the equivariance of the SO(3) correlation layer by testing how much it deviates from zero. It turns out that in the presence of activation functions, additional SO(3) correlation layers do not increase the error, which vanishes for very low resolution data. The authors also test it on MNIST digits projected onto a sphere, and surprise! It generalizes much better to rotated digits than a planar CNN evaluated on a flattened representation of the sphere. They also evaluate their CNN on 3D shapes that are ray casted onto a sphere and on atomization energies of molecules, where it performs decently but not SOTA. Outlook This paper is well-written and motivated, constructing a new building block for spherical CNNs and evaluating it with aplomb. My major concern is the lack of comparison to other spherical convolution methods — this paper presents a S^2 to SO(3) and SO(3) to SO(3) correlation layers, but only compares a CNN constructed with these layers to a planar CNN. What about the original S^2 to S^2 correlation? While less expressive, it may turn out that the mathematical machinery of the more advanced SO(3) layer is unnecessary. Of course though, the first time a building block is presented it is rough and unpolished. With a bit of work though, I’m sure it will be refined in time. References[1601.06815] Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-AddComputing Fourier Transforms and Convolutions on the 2-Sphere[1801.10130] Spherical CNNs","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/yfk9mu51s830zeqc', 'title': 'What is the TLDR of Spherical Convolutional Neural Networks (ICLR 2018 best paper)?', 'score': {'original': 0.69216666666667, 'ai': 0.30783333333333}, 'blocks': [{'text': 'Thanks for the recommendation! This paper was a delight to read. TLDR: The authors develop an efficient analogue of translational cross-correlation in spherical signals (e.g. such as ones from 360-degree/4pi-steradian action cameras or Google Street View). The provided construction has several useful properties, including equivariance over SO(3) and S^2 as well as efficient computation via a generalized FFT algorithm. In particular, they construct a S^2 to SO(3) correlation and SO(3) to SO(3) correlation, both of which are novel approaches to spherical data. Convolutions in 2D In 2D images, we typically don’t care where an object is located. If a cat is in the upper-left corner of an image, it should have the same activations on a “cat-filter” as the same cat translated to be in the lower-right corner of an image. Similarly, if we’re examining a 3D image like an MRI, it doesn’t matter where a polyp is located.. as long as we can identify it with the activations of some filter. Convolutions leverage the translational invariance of space.. hence translational cross-correlations (i.e. the simple “sliding” of a filter across an image in each of the dimensions) are useful for several reasons:They maintain translational invariance (in the paper, referred to as equivariance). When you translate the image or the filter, the cross-correlation of a filter with the image does not change.They are efficiently computable. If you fix the filter, then the convolution (series of multiply-adds) can be replaced with a FFT-multiply-IFFT. The overhead of the FFT/IFFT only makes sense if it can be amortized over many repeated convolutions with the same filter (see [1]).They are resource-efficient, especially when compared to a fully-connected layer. We impose the infinitely-strong prior of translational invariance, a symmetry which drastically cuts down the # of parameters we need.For a really intuitive guide, check out Chris Olah’s Understanding Convolutions. For a standard 2D convolutional layer, what really is being performed is a map taking two 2D arrays and forming another 2D array: Z2×Z2→Z2Z2×Z2→Z2\\mathbb{Z}^2 \\times \\mathbb{Z}^2 \\rightarrow \\mathbb{Z}^2 That is, we take 2D array of numbers (input image) and a 2D array (filter/kernel) and convolve them to form another 2D array (output). More mathematically, we might write: [f∗ψ](x)=∫Z2f(x+r)ψ(r)dr[f∗ψ](x)=∫Z2f(x+r)ψ(r)dr[f * \\psi](x) = \\int_{\\mathbb{Z}^2} f(x + r) \\psi(r) dr Generalizing to spherical data When you take this idea and naively generalize, you might expect a spherical convolution to take as input two arrays on S2S2S^2 (2D surface of a sphere) and output another one. This idea has been explored in [2], where the spherical convolution is specified as: [f∗ψ](x)=∫SO(3)f(R^n)ψ(R−1x)dR[f∗ψ](x)=∫SO(3)f(Rn^)ψ(R−1x)dR[f * \\psi](x) = \\int_{\\text{SO(3)}} f(R\\hat{n}) \\psi(R^{-1} x) dR where f is a signal defined on S2S2S^2, ψψ\\psi is a filter defined on S2S2S^2, R is a rotation matrix in SO(3), n is the unit vector pointing north, and x is a direction on S2S2S^2 that we would like to compute the output of the convolution at. This definition takes two arrays on S2S2S^2 (the sphere) and outputs an array on S2S2S^2. Note: SO(3) is the 3-dimensional space of all possible rotations of a sphere. Why 3? Any rotation of a sphere can', 'result': {'fake': 0.0067, 'real': 0.9933}, 'status': 'success'}, {'text': 'be decomposed into 3 simple rotations (Euler angles). ^nn^\\hat{n} appears in the expression because generally there are multiple rotations that can take X to Y, so better just fix one point to start. One of the key arguments of this paper is that this naive generalization is unnecessarily restrictive. It has the nice property of being a function on the sphere S2S2S^2! But the integral over SO(3) also integrates over a rotation in the ψψ\\psi factor, meaning that ψψ\\psi is made circularly symmetric about an axis before computing the cross-correlation. By necessity, a circularly symmetric filter (or signal) is less expressive than one that is not. Thus, Cohen et al instead define the spherical convolution on spherical data as: [f∗ψ](R)=∫S2f(→x)ψ(R−1→x)dx[f∗ψ](R)=∫S2f(x→)ψ(R−1x→)dx[f * \\psi](R) = \\int_{S^2} f(\\vec{x}) \\psi(R^{-1} \\vec{x}) dx This definition takes two arrays [image, filter] on the sphere but outputs an array in SO(3), the 3D space of rotations. In order to continue using the output feature map as inputs to other layers, you’ll also need another convolution defined on the rotation group SO(3): [f∗ψ](R)=∫SO(3)f(Q)ψ(R−1Q)dQ[f∗ψ](R)=∫SO(3)f(Q)ψ(R−1Q)dQ[f * \\psi](R) = \\int_{SO(3)} f(Q) \\psi(R^{-1} Q) dQ These definitions can be shown to be equivariant: you can rotate a filter before convolving with an image, and it is equal to the convolution of the rotated filter with the original image. Showing that these can be computed efficiently is not easy. Fourier analysis in rectangular coordinates is straightforward compared to the orthogonal basis functions one must use in S^2 [spherical harmonics] or SO(3) [Wigner D-functions]. For the interested and motivated analyst, the paper provides plenty of additional references to chew on. Experiments Not many papers in machine learning are worked through in as much detail as this one. The authors not only analyze the theory, but they also provide an efficient implementation of the basic algorithm and test their approach on multiple datasets. Notice that all of the discussion above presents the convolutions on continuous groups S^2 and SO(3). Real data is discretized, and as such the equivariance property will not hold exactly. The authors quantify the equivariance of the SO(3) correlation layer by testing how much it deviates from zero. It turns out that in the presence of activation functions, additional SO(3) correlation layers do not increase the error, which vanishes for very low resolution data. The authors also test it on MNIST digits projected onto a sphere, and surprise! It generalizes much better to rotated digits than a planar CNN evaluated on a flattened representation of the sphere. They also evaluate their CNN on 3D shapes that are ray casted onto a sphere and on atomization energies of molecules, where it performs decently but not SOTA. Outlook This paper is well-written and motivated, constructing a new building block for spherical CNNs and evaluating it with aplomb. My major concern is the lack of comparison to other spherical convolution methods — this paper presents a S^2 to SO(3) and SO(3) to SO(3) correlation layers, but only compares a CNN constructed with these layers to a planar CNN. What about the original S^2 to S^2 correlation? While', 'result': {'fake': 0.0504, 'real': 0.9496}, 'status': 'success'}, {'text': 'less expressive, it may turn out that the mathematical machinery of the more advanced SO(3) layer is unnecessary. Of course though, the first time a building block is presented it is rough and unpolished. With a bit of work though, I’m sure it will be refined in time. References[1601.06815] Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-AddComputing Fourier Transforms and Convolutions on the 2-Sphere[1801.10130] Spherical CNNs', 'result': {'fake': 0.8664, 'real': 0.1336}, 'status': 'success'}], 'credits_used': 13, 'credits': 1982526, 'subscription': 0, 'content': 'Thanks for the recommendation! This paper was a delight to read. TLDR: The authors develop an efficient analogue of translational cross-correlation in spherical signals (e.g. such as ones from 360-degree/4pi-steradian action cameras or Google Street View). The provided construction has several useful properties, including equivariance over SO(3) and S^2 as well as efficient computation via a generalized FFT algorithm. In particular, they construct a S^2 to SO(3) correlation and SO(3) to SO(3) correlation, both of which are novel approaches to spherical data. Convolutions in 2D In 2D images, we typically don’t care where an object is located. If a cat is in the upper-left corner of an image, it should have the same activations on a “cat-filter” as the same cat translated to be in the lower-right corner of an image. Similarly, if we’re examining a 3D image like an MRI, it doesn’t matter where a polyp is located.. as long as we can identify it with the activations of some filter. Convolutions leverage the translational invariance of space.. hence translational cross-correlations (i.e. the simple “sliding” of a filter across an image in each of the dimensions) are useful for several reasons:They maintain translational invariance (in the paper, referred to as equivariance). When you translate the image or the filter, the cross-correlation of a filter with the image does not change.They are efficiently computable. If you fix the filter, then the convolution (series of multiply-adds) can be replaced with a FFT-multiply-IFFT. The overhead of the FFT/IFFT only makes sense if it can be amortized over many repeated convolutions with the same filter (see [1]).They are resource-efficient, especially when compared to a fully-connected layer. We impose the infinitely-strong prior of translational invariance, a symmetry which drastically cuts down the # of parameters we need.For a really intuitive guide, check out Chris Olah’s Understanding Convolutions. For a standard 2D convolutional layer, what really is being performed is a map taking two 2D arrays and forming another 2D array: Z2×Z2→Z2Z2×Z2→Z2\\mathbb{Z}^2 \\times \\mathbb{Z}^2 \\rightarrow \\mathbb{Z}^2 That is, we take 2D array of numbers (input image) and a 2D array (filter/kernel) and convolve them to form another 2D array (output). More mathematically, we might write: [f∗ψ](x)=∫Z2f(x+r)ψ(r)dr[f∗ψ](x)=∫Z2f(x+r)ψ(r)dr[f * \\psi](x) = \\int_{\\mathbb{Z}^2} f(x + r) \\psi(r) dr Generalizing to spherical data When you take this idea and naively generalize, you might expect a spherical convolution to take as input two arrays on S2S2S^2 (2D surface of a sphere) and output another one. This idea has been explored in [2], where the spherical convolution is specified as: [f∗ψ](x)=∫SO(3)f(R^n)ψ(R−1x)dR[f∗ψ](x)=∫SO(3)f(Rn^)ψ(R−1x)dR[f * \\psi](x) = \\int_{\\text{SO(3)}} f(R\\hat{n}) \\psi(R^{-1} x) dR where f is a signal defined on S2S2S^2, ψψ\\psi is a filter defined on S2S2S^2, R is a rotation matrix in SO(3), n is the unit vector pointing north, and x is a direction on S2S2S^2 that we would like to compute the output of the convolution at. This definition takes two arrays on S2S2S^2 (the sphere) and outputs an array on S2S2S^2. Note: SO(3) is the 3-dimensional space of all possible rotations of a sphere. Why 3? Any rotation of a sphere can be decomposed into 3 simple rotations (Euler angles). ^nn^\\hat{n} appears in the expression because generally there are multiple rotations that can take X to Y, so better just fix one point to start. One of the key arguments of this paper is that this naive generalization is unnecessarily restrictive. It has the nice property of being a function on the sphere S2S2S^2! But the integral over SO(3) also integrates over a rotation in the ψψ\\psi factor, meaning that ψψ\\psi is made circularly symmetric about an axis before computing the cross-correlation. By necessity, a circularly symmetric filter (or signal) is less expressive than one that is not. Thus, Cohen et al instead define the spherical convolution on spherical data as: [f∗ψ](R)=∫S2f(→x)ψ(R−1→x)dx[f∗ψ](R)=∫S2f(x→)ψ(R−1x→)dx[f * \\psi](R) = \\int_{S^2} f(\\vec{x}) \\psi(R^{-1} \\vec{x}) dx This definition takes two arrays [image, filter] on the sphere but outputs an array in SO(3), the 3D space of rotations. In order to continue using the output feature map as inputs to other layers, you’ll also need another convolution defined on the rotation group SO(3): [f∗ψ](R)=∫SO(3)f(Q)ψ(R−1Q)dQ[f∗ψ](R)=∫SO(3)f(Q)ψ(R−1Q)dQ[f * \\psi](R) = \\int_{SO(3)} f(Q) \\psi(R^{-1} Q) dQ These definitions can be shown to be equivariant: you can rotate a filter before convolving with an image, and it is equal to the convolution of the rotated filter with the original image. Showing that these can be computed efficiently is not easy. Fourier analysis in rectangular coordinates is straightforward compared to the orthogonal basis functions one must use in S^2 [spherical harmonics] or SO(3) [Wigner D-functions]. For the interested and motivated analyst, the paper provides plenty of additional references to chew on. Experiments Not many papers in machine learning are worked through in as much detail as this one. The authors not only analyze the theory, but they also provide an efficient implementation of the basic algorithm and test their approach on multiple datasets. Notice that all of the discussion above presents the convolutions on continuous groups S^2 and SO(3). Real data is discretized, and as such the equivariance property will not hold exactly. The authors quantify the equivariance of the SO(3) correlation layer by testing how much it deviates from zero. It turns out that in the presence of activation functions, additional SO(3) correlation layers do not increase the error, which vanishes for very low resolution data. The authors also test it on MNIST digits projected onto a sphere, and surprise! It generalizes much better to rotated digits than a planar CNN evaluated on a flattened representation of the sphere. They also evaluate their CNN on 3D shapes that are ray casted onto a sphere and on atomization energies of molecules, where it performs decently but not SOTA. Outlook This paper is well-written and motivated, constructing a new building block for spherical CNNs and evaluating it with aplomb. My major concern is the lack of comparison to other spherical convolution methods — this paper presents a S^2 to SO(3) and SO(3) to SO(3) correlation layers, but only compares a CNN constructed with these layers to a planar CNN. What about the original S^2 to S^2 correlation? While less expressive, it may turn out that the mathematical machinery of the more advanced SO(3) layer is unnecessary. Of course though, the first time a building block is presented it is rough and unpolished. With a bit of work though, I’m sure it will be refined in time. References[1601.06815] Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-AddComputing Fourier Transforms and Convolutions on the 2-Sphere[1801.10130] Spherical CNNs', 'aiModelVersion': '1'}",0.69216666666667
Sridhar Mahadevan,4y,How is graph theory used in data science and neural networks?,"Graph theory is one of the most elegant parts of discrete math, and forms an essential bedrock of not just AI and machine learning, but also computer science. A trillion dollar company like Google would hardly be conceivable without the insights provided by graph theory (e.g., PageRank builds on some elementary insights about random walks on graphs).

Many modern data science problems are questions about graphs: to understand large social networks, from Facebook and Twitter to LinkedIn and scientific paper citation analysis, graph theory is the one field that anyone interested in such problems needs to have a solid grasp about. Modeling such problems without knowing something about graph theory is like trying to build an airplane without knowing Bernoulli’s principle: you can strap wings on your shoulders and try jumping off cliffs, as lots of folks did, with the obvious consequences.

To illustrate the beauty and pervasiveness of graph-theoretic concepts in all of machine learning, AI, and CS, let me turn to the study of one foundational question that lies at the heart of all machine learning and AI: how to extract structure from randomness? Without the ability to extract signals from noise, biological intelligence would never have materialized. The real world is inherently noisy. As the great psychologist William James once put it, the world is one “blooming, buzzing confusion” to a baby, and it is indeed a miracle that babies are able to make sense of it at all (and not surprising that their first instinct is to cry!).

So, let us take this question of extracting structure from randomness, and pose it in the lens of graph theory. Imagine your world is a large graph, e.g. you are a digital ant crawling around on the web from Facebook user to their friends, or from one web page to another. How to make sense of it all? You can record individual edges in the graph, but how to not get lost in the details and discern the large-scale structure of large graphs? Graph theory is your answer, but it is not the elementary graph theory that one learns in discrete math courses as a sophomore. For this, you need the more advanced tools of spectral graph theory.

Spectral graph theory is one of those elegant subfields of mathematics, which contains in it a microcosm of all that is wonderful about math, yet without the forbidding complexity of continuous math. It is a marriage of linear algebra, graph theory, and in an elementary way, many complex ideas from the study of manifolds, Riemannian geometry, group theory, and more advanced topics can all be distilled into simpler language.

So, I am going to now introduce perhaps the most beautiful object in all of mathematics and physics, the Laplacian, the magic operator, which I can do no better than quote a famous book by Nelson on tensor analysis:

“The Laplace operator in its various manifestations is the most beautiful and central object in all of mathematics. Probability theory, mathematical physics, Fourier analysis, partial differential equations, the theory of Lie groups, and differential geometry all revolve around this sun, and its light even penetrates such obscure regions as number theory and algebraic geometry.”

For graphs, the Laplacian takes on an especially elegant and elementary form. First, we should understand how to convert a graph into a matrix. This is the symmetric adjacency matrix (for undirected graphs, where an edge from A to B is also an edge from B to A). But, the adjacency matrix by itself tells us very little, strangely enough. We need to do a little magic with the adjacency matrix, and voila’, we convert it to something that contains an extraordinary amount of insight about a graph. The simplest form of graph Laplacian is L = D - A, where the diagonal matrix D is simply the number of neighbors of each vertex (or the so called valency matrix). Here is a simple example to illustrate the graph Laplacian formed from the adjacency matrix. What could be simpler?

Now, to garner insights about a graph, we need to bring in the tools of linear algebra, particularly the eigenvectors and eigenvalues of the graph Laplacian. The laplacian L is a singular matrix, meaning it is not invertible, because it has the eigenvalue 0. A simple way to see this is to add each row, and by construction, we see that each row sums to 0. This means right multiplying the laplacian L by a vector of all 1’s gives us the 0 vector, so 0 is an eigenvalue and all 1’s is an eigenvector. That tells us very little, obviously.

But, aha, the second smallest eigenvalue, therein lies the magic. This is called the Fiedler eigenvector and eigenvalue, after the mathematician Fiedler who studied it in early 1970s. The second smallest eigenvalue tells us an amazing amount about a graph, so much so that one needs to read through a whole book to understand all its beautiful consequences. I can do no better than to recommend Fan Chung’s brilliant book, Spectral Graph Theory, which goes deep into the study of the eigenvalues and eigenvectors of the graph Laplacian.

Ok, now you are scratching your head, wondering what all the fuss is about, this looks like one of these arcane math lectures where you have no idea what’s going on. So, let's get back to what Google does with PageRank. If I randomly walk on the web graph, how important is my web page at UMass (cics.umass.edu/~mahadeva) vs. Amazon (www.amazon.com). Of course, Amazon’s is far more important because so many more links go into it and out of it. So, in a random walk, you are far more likely to visit Amazon than my web page. But, as a random walker, how could you discover that? The graph Laplacian is closely related to the random walk matrix! Explaining that will take me a bit more than I have space for, so I will leave you to explore this beautiful area.

So, getting back to data science, how about problems that are not obviously graph based. Here again, the beautiful structure provided by the graph Laplacian is of great importance. Manifold learning is an active area of unsupervised learning that attempts to discover the inherent geometry of the space that the data lies on. In most interesting ML and AI problems, data do not lie in Euclidean space, although they appear to be vectors in n-dimensions, but rather lie in a curved subspace or manifold (e.g., faces, text documents, probability distributions). The first step in manifold recovery is to construct a graph connecting sample points in high dimensions, based on some simple distance measure, and then using random walks on the graph or eigen decomposition of the graph Laplacian, algorithms such as Laplacian eigenmaps or LLE or ISOMAP build embeddings that capture the intrinsic dimension of the data.

Imagine a series of images of a slowly rotating object, like a tea cup. The data appears to be high dimensional, since it is the number of pixels in the image, but the intrinsic dimension is just one! LLE or Laplacian eigenmaps can discover the intrinsic dimension of the data by building a graph.

About 15 years ago, I wrote a paper in the International Conference on Machine Learning (ICML 2005) on how reinforcement learning agents can discover the intrinsic structure of an environment, and construct representations that transcend single tasks. I called these protovalue functions (PVFs), as they look like value functions, but are not constructed from rewards. PVFs can be constructed from the eigenvectors of a graph Laplacian connecting nearby states. Shown below is some examples from a simple two room environment connected by a door (which is the bottleneck between the two rooms).

PVFs have been found to be very useful in deep reinforcement learning, as they are closely related to something called the successor representation, first formulated by Peter Dayan at the Gatsby Institute in London. A recent paper by researchers at the University of Alberta and Deep Mind showed how PVFs and successor representations can be combined with deep learning to construct graph-based representations of complex video games like the Atari benchmark suite.

Even more interesting is a recent paper by Deep Mind showing that PVFs or successor representations could be in the brain, in the hippocampus where knowledge of spatial maps is located. The discovery of place cells and grid cells in the hippocampus led to a Nobel prize. Perhaps, if I am not too fanciful in imagination, someone might get a prize for discovering that successor representation or PVFs are indeed in the hippocampus. That would show the true importance of graph-based representations in the brain.

The hippocampus as a 'predictive map' | DeepMind

All of this work shows the importance of simple elegant mathematical models, like graphs, and how simplicity, as always, is the best guide to discovering truth in nature. I can do no better than to cite the immortal poem by Keats, called an Ode to a Grecian Urn. More than any other poem, the last two lines of this poem has served as a guide to many a famous scientist.

""Beauty is truth, truth beauty,—that is all

Ye know on earth, and all ye need to know.""","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/59ykpmw7rob1ct0u', 'title': 'How is graph theory used in data science and neural networks?', 'score': {'original': 0.46845, 'ai': 0.53155}, 'blocks': [{'text': 'Graph theory is one of the most elegant parts of discrete math, and forms an essential bedrock of not just AI and machine learning, but also computer science. A trillion dollar company like Google would hardly be conceivable without the insights provided by graph theory (e.g., PageRank builds on some elementary insights about random walks on graphs).\n\nMany modern data science problems are questions about graphs: to understand large social networks, from Facebook and Twitter to LinkedIn and scientific paper citation analysis, graph theory is the one field that anyone interested in such problems needs to have a solid grasp about. Modeling such problems without knowing something about graph theory is like trying to build an airplane without knowing Bernoulli’s principle: you can strap wings on your shoulders and try jumping off cliffs, as lots of folks did, with the obvious consequences.\n\nTo illustrate the beauty and pervasiveness of graph-theoretic concepts in all of machine learning, AI, and CS, let me turn to the study of one foundational question that lies at the heart of all machine learning and AI: how to extract structure from randomness? Without the ability to extract signals from noise, biological intelligence would never have materialized. The real world is inherently noisy. As the great psychologist William James once put it, the world is one “blooming, buzzing confusion” to a baby, and it is indeed a miracle that babies are able to make sense of it at all (and not surprising that their first instinct is to cry!).\n\nSo, let us take this question of extracting structure from randomness, and pose it in the lens of graph theory. Imagine your world is a large graph, e.g. you are a digital ant crawling around on the web from Facebook user to their friends, or from one web page to another. How to make sense of it all? You can record individual edges in the graph, but how to not get lost in the details and discern the large-scale structure of large graphs? Graph theory is your answer, but it is not the elementary graph theory that one learns in discrete math courses as a sophomore. For this, you need the more advanced tools of spectral graph theory.\n\nSpectral graph theory is one of those elegant subfields of mathematics, which contains in it a microcosm of all that is wonderful about math, yet without the forbidding complexity of continuous math. It is a marriage of linear algebra, graph theory, and in an elementary way, many complex ideas from the study of manifolds, Riemannian geometry, group theory, and more advanced topics can all be distilled into simpler language.\n\nSo, I am going to now introduce perhaps the most beautiful object in all of mathematics and physics, the Laplacian, the magic operator, which I can do no better than quote a famous book by Nelson on tensor analysis:\n\n“The Laplace operator in its various manifestations is the most beautiful and central object in all of mathematics. Probability theory, mathematical physics, Fourier analysis, partial differential equations, the theory of Lie groups, and differential geometry all revolve around this sun,', 'result': {'fake': 0.0273, 'real': 0.9727}, 'status': 'success'}, {'text': ""and its light even penetrates such obscure regions as number theory and algebraic geometry.”\n\nFor graphs, the Laplacian takes on an especially elegant and elementary form. First, we should understand how to convert a graph into a matrix. This is the symmetric adjacency matrix (for undirected graphs, where an edge from A to B is also an edge from B to A). But, the adjacency matrix by itself tells us very little, strangely enough. We need to do a little magic with the adjacency matrix, and voila’, we convert it to something that contains an extraordinary amount of insight about a graph. The simplest form of graph Laplacian is L = D - A, where the diagonal matrix D is simply the number of neighbors of each vertex (or the so called valency matrix). Here is a simple example to illustrate the graph Laplacian formed from the adjacency matrix. What could be simpler?\n\nNow, to garner insights about a graph, we need to bring in the tools of linear algebra, particularly the eigenvectors and eigenvalues of the graph Laplacian. The laplacian L is a singular matrix, meaning it is not invertible, because it has the eigenvalue 0. A simple way to see this is to add each row, and by construction, we see that each row sums to 0. This means right multiplying the laplacian L by a vector of all 1’s gives us the 0 vector, so 0 is an eigenvalue and all 1’s is an eigenvector. That tells us very little, obviously.\n\nBut, aha, the second smallest eigenvalue, therein lies the magic. This is called the Fiedler eigenvector and eigenvalue, after the mathematician Fiedler who studied it in early 1970s. The second smallest eigenvalue tells us an amazing amount about a graph, so much so that one needs to read through a whole book to understand all its beautiful consequences. I can do no better than to recommend Fan Chung’s brilliant book, Spectral Graph Theory, which goes deep into the study of the eigenvalues and eigenvectors of the graph Laplacian.\n\nOk, now you are scratching your head, wondering what all the fuss is about, this looks like one of these arcane math lectures where you have no idea what’s going on. So, let's get back to what Google does with PageRank. If I randomly walk on the web graph, how important is my web page at UMass (cics.umass.edu/~mahadeva) vs. Amazon (www.amazon.com). Of course, Amazon’s is far more important because so many more links go into it and out of it. So, in a random walk, you are far more likely to visit Amazon than my web page. But, as a random walker, how could you discover that? The graph Laplacian is closely related to the random walk matrix! Explaining that will take me a bit more than I have space for, so I will leave you to explore this beautiful area.\n\nSo, getting back to data science, how about problems that are not obviously graph based. Here again, the beautiful structure provided by the graph Laplacian is of great importance. Manifold learning is an active"", 'result': {'fake': 0.9125, 'real': 0.0875}, 'status': 'success'}, {'text': 'area of unsupervised learning that attempts to discover the inherent geometry of the space that the data lies on. In most interesting ML and AI problems, data do not lie in Euclidean space, although they appear to be vectors in n-dimensions, but rather lie in a curved subspace or manifold (e.g., faces, text documents, probability distributions). The first step in manifold recovery is to construct a graph connecting sample points in high dimensions, based on some simple distance measure, and then using random walks on the graph or eigen decomposition of the graph Laplacian, algorithms such as Laplacian eigenmaps or LLE or ISOMAP build embeddings that capture the intrinsic dimension of the data.\n\nImagine a series of images of a slowly rotating object, like a tea cup. The data appears to be high dimensional, since it is the number of pixels in the image, but the intrinsic dimension is just one! LLE or Laplacian eigenmaps can discover the intrinsic dimension of the data by building a graph.\n\nAbout 15 years ago, I wrote a paper in the International Conference on Machine Learning (ICML 2005) on how reinforcement learning agents can discover the intrinsic structure of an environment, and construct representations that transcend single tasks. I called these protovalue functions (PVFs), as they look like value functions, but are not constructed from rewards. PVFs can be constructed from the eigenvectors of a graph Laplacian connecting nearby states. Shown below is some examples from a simple two room environment connected by a door (which is the bottleneck between the two rooms).\n\nPVFs have been found to be very useful in deep reinforcement learning, as they are closely related to something called the successor representation, first formulated by Peter Dayan at the Gatsby Institute in London. A recent paper by researchers at the University of Alberta and Deep Mind showed how PVFs and successor representations can be combined with deep learning to construct graph-based representations of complex video games like the Atari benchmark suite.\n\nEven more interesting is a recent paper by Deep Mind showing that PVFs or successor representations could be in the brain, in the hippocampus where knowledge of spatial maps is located. The discovery of place cells and grid cells in the hippocampus led to a Nobel prize. Perhaps, if I am not too fanciful in imagination, someone might get a prize for discovering that successor representation or PVFs are indeed in the hippocampus. That would show the true importance of graph-based representations in the brain.\n\nThe hippocampus as a \'predictive map\' | DeepMind\n\nAll of this work shows the importance of simple elegant mathematical models, like graphs, and how simplicity, as always, is the best guide to discovering truth in nature. I can do no better than to cite the immortal poem by Keats, called an Ode to a Grecian Urn. More than any other poem, the last two lines of this poem has served as a guide to many a famous scientist.\n\n""Beauty is truth, truth beauty,—that is all\n\nYe know on earth, and all ye need to know.""', 'result': {'fake': 0.0147, 'real': 0.9853}, 'status': 'success'}], 'credits_used': 16, 'credits': 1982510, 'subscription': 0, 'content': 'Graph theory is one of the most elegant parts of discrete math, and forms an essential bedrock of not just AI and machine learning, but also computer science. A trillion dollar company like Google would hardly be conceivable without the insights provided by graph theory (e.g., PageRank builds on some elementary insights about random walks on graphs).\n\nMany modern data science problems are questions about graphs: to understand large social networks, from Facebook and Twitter to LinkedIn and scientific paper citation analysis, graph theory is the one field that anyone interested in such problems needs to have a solid grasp about. Modeling such problems without knowing something about graph theory is like trying to build an airplane without knowing Bernoulli’s principle: you can strap wings on your shoulders and try jumping off cliffs, as lots of folks did, with the obvious consequences.\n\nTo illustrate the beauty and pervasiveness of graph-theoretic concepts in all of machine learning, AI, and CS, let me turn to the study of one foundational question that lies at the heart of all machine learning and AI: how to extract structure from randomness? Without the ability to extract signals from noise, biological intelligence would never have materialized. The real world is inherently noisy. As the great psychologist William James once put it, the world is one “blooming, buzzing confusion” to a baby, and it is indeed a miracle that babies are able to make sense of it at all (and not surprising that their first instinct is to cry!).\n\nSo, let us take this question of extracting structure from randomness, and pose it in the lens of graph theory. Imagine your world is a large graph, e.g. you are a digital ant crawling around on the web from Facebook user to their friends, or from one web page to another. How to make sense of it all? You can record individual edges in the graph, but how to not get lost in the details and discern the large-scale structure of large graphs? Graph theory is your answer, but it is not the elementary graph theory that one learns in discrete math courses as a sophomore. For this, you need the more advanced tools of spectral graph theory.\n\nSpectral graph theory is one of those elegant subfields of mathematics, which contains in it a microcosm of all that is wonderful about math, yet without the forbidding complexity of continuous math. It is a marriage of linear algebra, graph theory, and in an elementary way, many complex ideas from the study of manifolds, Riemannian geometry, group theory, and more advanced topics can all be distilled into simpler language.\n\nSo, I am going to now introduce perhaps the most beautiful object in all of mathematics and physics, the Laplacian, the magic operator, which I can do no better than quote a famous book by Nelson on tensor analysis:\n\n“The Laplace operator in its various manifestations is the most beautiful and central object in all of mathematics. Probability theory, mathematical physics, Fourier analysis, partial differential equations, the theory of Lie groups, and differential geometry all revolve around this sun, and its light even penetrates such obscure regions as number theory and algebraic geometry.”\n\nFor graphs, the Laplacian takes on an especially elegant and elementary form. First, we should understand how to convert a graph into a matrix. This is the symmetric adjacency matrix (for undirected graphs, where an edge from A to B is also an edge from B to A). But, the adjacency matrix by itself tells us very little, strangely enough. We need to do a little magic with the adjacency matrix, and voila’, we convert it to something that contains an extraordinary amount of insight about a graph. The simplest form of graph Laplacian is L = D - A, where the diagonal matrix D is simply the number of neighbors of each vertex (or the so called valency matrix). Here is a simple example to illustrate the graph Laplacian formed from the adjacency matrix. What could be simpler?\n\nNow, to garner insights about a graph, we need to bring in the tools of linear algebra, particularly the eigenvectors and eigenvalues of the graph Laplacian. The laplacian L is a singular matrix, meaning it is not invertible, because it has the eigenvalue 0. A simple way to see this is to add each row, and by construction, we see that each row sums to 0. This means right multiplying the laplacian L by a vector of all 1’s gives us the 0 vector, so 0 is an eigenvalue and all 1’s is an eigenvector. That tells us very little, obviously.\n\nBut, aha, the second smallest eigenvalue, therein lies the magic. This is called the Fiedler eigenvector and eigenvalue, after the mathematician Fiedler who studied it in early 1970s. The second smallest eigenvalue tells us an amazing amount about a graph, so much so that one needs to read through a whole book to understand all its beautiful consequences. I can do no better than to recommend Fan Chung’s brilliant book, Spectral Graph Theory, which goes deep into the study of the eigenvalues and eigenvectors of the graph Laplacian.\n\nOk, now you are scratching your head, wondering what all the fuss is about, this looks like one of these arcane math lectures where you have no idea what’s going on. So, let\'s get back to what Google does with PageRank. If I randomly walk on the web graph, how important is my web page at UMass (cics.umass.edu/~mahadeva) vs. Amazon (www.amazon.com). Of course, Amazon’s is far more important because so many more links go into it and out of it. So, in a random walk, you are far more likely to visit Amazon than my web page. But, as a random walker, how could you discover that? The graph Laplacian is closely related to the random walk matrix! Explaining that will take me a bit more than I have space for, so I will leave you to explore this beautiful area.\n\nSo, getting back to data science, how about problems that are not obviously graph based. Here again, the beautiful structure provided by the graph Laplacian is of great importance. Manifold learning is an active area of unsupervised learning that attempts to discover the inherent geometry of the space that the data lies on. In most interesting ML and AI problems, data do not lie in Euclidean space, although they appear to be vectors in n-dimensions, but rather lie in a curved subspace or manifold (e.g., faces, text documents, probability distributions). The first step in manifold recovery is to construct a graph connecting sample points in high dimensions, based on some simple distance measure, and then using random walks on the graph or eigen decomposition of the graph Laplacian, algorithms such as Laplacian eigenmaps or LLE or ISOMAP build embeddings that capture the intrinsic dimension of the data.\n\nImagine a series of images of a slowly rotating object, like a tea cup. The data appears to be high dimensional, since it is the number of pixels in the image, but the intrinsic dimension is just one! LLE or Laplacian eigenmaps can discover the intrinsic dimension of the data by building a graph.\n\nAbout 15 years ago, I wrote a paper in the International Conference on Machine Learning (ICML 2005) on how reinforcement learning agents can discover the intrinsic structure of an environment, and construct representations that transcend single tasks. I called these protovalue functions (PVFs), as they look like value functions, but are not constructed from rewards. PVFs can be constructed from the eigenvectors of a graph Laplacian connecting nearby states. Shown below is some examples from a simple two room environment connected by a door (which is the bottleneck between the two rooms).\n\nPVFs have been found to be very useful in deep reinforcement learning, as they are closely related to something called the successor representation, first formulated by Peter Dayan at the Gatsby Institute in London. A recent paper by researchers at the University of Alberta and Deep Mind showed how PVFs and successor representations can be combined with deep learning to construct graph-based representations of complex video games like the Atari benchmark suite.\n\nEven more interesting is a recent paper by Deep Mind showing that PVFs or successor representations could be in the brain, in the hippocampus where knowledge of spatial maps is located. The discovery of place cells and grid cells in the hippocampus led to a Nobel prize. Perhaps, if I am not too fanciful in imagination, someone might get a prize for discovering that successor representation or PVFs are indeed in the hippocampus. That would show the true importance of graph-based representations in the brain.\n\nThe hippocampus as a \'predictive map\' | DeepMind\n\nAll of this work shows the importance of simple elegant mathematical models, like graphs, and how simplicity, as always, is the best guide to discovering truth in nature. I can do no better than to cite the immortal poem by Keats, called an Ode to a Grecian Urn. More than any other poem, the last two lines of this poem has served as a guide to many a famous scientist.\n\n""Beauty is truth, truth beauty,—that is all\n\nYe know on earth, and all ye need to know.""', 'aiModelVersion': '1'}",0.46845
Yoshua Bengio,9y,Why did it take so long to invent the backpropagation algorithm? Isn't it just a straightforward (albeit cumbersome) application of the chain rule?,"Lots of apparently obvious ideas only became obvious after the fact...

The use of the chain-rule through many non-linearities was known and used in control many years before it was used in neural nets. In those days when back-prop was invented (early 80's) neural nets had discrete (binary) outputs, barring the use of gradient-based optimization. But David Rumelhart, +Geoff Hinton et al (and my friend +Yann LeCun, independently) figured that if you had smooth (sigmoidal) outputs, you could use the chain rule and train multi-layer networks. So it was not just about using the chain rule, but about accepting to let go of the established binary-output neurons. It is an interesting lesson","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/4mh1l0ik5nwpr73a', 'title': ""Why did it take so long to invent the backpropagation algorithm? Isn't it just a straightforward (albeit cumbersome) application of the chain rule?"", 'score': {'original': 0.9974, 'ai': 0.0026}, 'blocks': [{'text': ""Lots of apparently obvious ideas only became obvious after the fact...\n\nThe use of the chain-rule through many non-linearities was known and used in control many years before it was used in neural nets. In those days when back-prop was invented (early 80's) neural nets had discrete (binary) outputs, barring the use of gradient-based optimization. But David Rumelhart, +Geoff Hinton et al (and my friend +Yann LeCun, independently) figured that if you had smooth (sigmoidal) outputs, you could use the chain rule and train multi-layer networks. So it was not just about using the chain rule, but about accepting to let go of the established binary-output neurons. It is an interesting lesson"", 'result': {'fake': 0.0026, 'real': 0.9974}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982508, 'subscription': 0, 'content': ""Lots of apparently obvious ideas only became obvious after the fact...\n\nThe use of the chain-rule through many non-linearities was known and used in control many years before it was used in neural nets. In those days when back-prop was invented (early 80's) neural nets had discrete (binary) outputs, barring the use of gradient-based optimization. But David Rumelhart, +Geoff Hinton et al (and my friend +Yann LeCun, independently) figured that if you had smooth (sigmoidal) outputs, you could use the chain rule and train multi-layer networks. So it was not just about using the chain rule, but about accepting to let go of the established binary-output neurons. It is an interesting lesson"", 'aiModelVersion': '1'}",0.9974
Erik Bernhardsson,7y,"How did Spotify get so good at machine learning? Was machine learning important from the start, or did they catch up over time?","I was at Spotify 2008–2015 and built up the machine learning team.

Until about 2012 it really was just me working on it, mostly as a side project. I think a couple of design decisions quite early turned out to work really well, like focusing on collaborative filtering and in particular vector models. Initially PLSA, then later this method
. As soon as word2vec came out we switched to it immediately and had it in production just a few weeks later.

Around 2013–2015 I built up a team around music recommendations that focused a lot on related artists, radio, and eventually Discover Weekly (which was released after I left).

We acquired Echo Nest which injected a bunch of smart people into the team but I wouldn’t say the technology accelerated the machine learning focus. The way I look at it, it was mostly a talent acquisition, not technology. Very little of Echo Nest’s tech actually made it into the Spotify product, most of it being work by their audio team.

We also looked into deep learning quite early, with Sander Dieleman interning in 2014 focusing on pure audio based methods. I’m not sure to what extent this is used in production, but his results were quite impressive. In my mind the best way to get good recommendations is 90% through collaborative filtering then use deep learning models to get the extra 10%.

It’s been over two years since I left, but AFAIK, Discover Weekly is entirely powered by collaborative filtering, in particular a few extensions to word2vec that the machine learning team in NYC built.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/38idab9n7h2uoews', 'title': 'How did Spotify get so good at machine learning? Was machine learning important from the start, or did they catch up over time?', 'score': {'original': 0.9997, 'ai': 0.0003}, 'blocks': [{'text': 'I was at Spotify 2008–2015 and built up the machine learning team.\n\nUntil about 2012 it really was just me working on it, mostly as a side project. I think a couple of design decisions quite early turned out to work really well, like focusing on collaborative filtering and in particular vector models. Initially PLSA, then later this method\n. As soon as word2vec came out we switched to it immediately and had it in production just a few weeks later.\n\nAround 2013–2015 I built up a team around music recommendations that focused a lot on related artists, radio, and eventually Discover Weekly (which was released after I left).\n\nWe acquired Echo Nest which injected a bunch of smart people into the team but I wouldn’t say the technology accelerated the machine learning focus. The way I look at it, it was mostly a talent acquisition, not technology. Very little of Echo Nest’s tech actually made it into the Spotify product, most of it being work by their audio team.\n\nWe also looked into deep learning quite early, with Sander Dieleman interning in 2014 focusing on pure audio based methods. I’m not sure to what extent this is used in production, but his results were quite impressive. In my mind the best way to get good recommendations is 90% through collaborative filtering then use deep learning models to get the extra 10%.\n\nIt’s been over two years since I left, but AFAIK, Discover Weekly is entirely powered by collaborative filtering, in particular a few extensions to word2vec that the machine learning team in NYC built.', 'result': {'fake': 0.0003, 'real': 0.9997}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982505, 'subscription': 0, 'content': 'I was at Spotify 2008–2015 and built up the machine learning team.\n\nUntil about 2012 it really was just me working on it, mostly as a side project. I think a couple of design decisions quite early turned out to work really well, like focusing on collaborative filtering and in particular vector models. Initially PLSA, then later this method\n. As soon as word2vec came out we switched to it immediately and had it in production just a few weeks later.\n\nAround 2013–2015 I built up a team around music recommendations that focused a lot on related artists, radio, and eventually Discover Weekly (which was released after I left).\n\nWe acquired Echo Nest which injected a bunch of smart people into the team but I wouldn’t say the technology accelerated the machine learning focus. The way I look at it, it was mostly a talent acquisition, not technology. Very little of Echo Nest’s tech actually made it into the Spotify product, most of it being work by their audio team.\n\nWe also looked into deep learning quite early, with Sander Dieleman interning in 2014 focusing on pure audio based methods. I’m not sure to what extent this is used in production, but his results were quite impressive. In my mind the best way to get good recommendations is 90% through collaborative filtering then use deep learning models to get the extra 10%.\n\nIt’s been over two years since I left, but AFAIK, Discover Weekly is entirely powered by collaborative filtering, in particular a few extensions to word2vec that the machine learning team in NYC built.', 'aiModelVersion': '1'}",0.9997
Ferenc Huszár,11y,Why is Convex Optimization such a big deal in Machine Learning?,"Many methods in machine learning are based on finding parameters that minimise some objective function. Very often, the objective function is a weighted sum of two terms: a cost function and regularisation. In statistics terms the (log-)likelihood and (log-)prior. If both of these components are convex, then their sum is also convex.

Examples of convex optimisation problems in machine learning:

linear regression/ Ridge regression, with Tikhonov regularisation, etc
sparse linear regression with L1 regularisation, such as lasso
support vector machines
parameter estimation in linear-Gaussian time series (Kalman filter and friends)


Typical examples of non-convex optimisation in ML are


neural networks
maximum likelihood mixtures of Gaussians


If the objective function is convex, that comes with nice guarantees. Most importantly, if a function is strictly convex, it is guaranteed to have a unique global minimum, and it can be found by various standard methods.
Non-convex functions may have several local minima, that is multiple points satisfying that they are the best point in their local neighbourhood, but which are not globally optimal. Therefore, if you have a non-convex problem, there is usually no way to test if the solution you have found is indeed the best solution.
Furthermore, if a problem is convex, it is usually easier to analyse the asymptotic behaviour of the algorithm, that is how fast it converges as you observe more and more data.

There are multiple schools of thought in machine learning, some are more obsessed with convexity than others. Depending on where you come from, and what your goals are, convexity may be more or less important to you. In any case, it's important you understand the benefits of convexity, and that you have a decent knowledge of various linear, quadratic and convex programming methods you can use to solve convex problems.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/b49fei715pj8kx2n', 'title': 'Why is Convex Optimization such a big deal in Machine Learning?', 'score': {'original': 0.9995, 'ai': 0.0005}, 'blocks': [{'text': ""Many methods in machine learning are based on finding parameters that minimise some objective function. Very often, the objective function is a weighted sum of two terms: a cost function and regularisation. In statistics terms the (log-)likelihood and (log-)prior. If both of these components are convex, then their sum is also convex.\n\nExamples of convex optimisation problems in machine learning:\n\nlinear regression/ Ridge regression, with Tikhonov regularisation, etc\nsparse linear regression with L1 regularisation, such as lasso\nsupport vector machines\nparameter estimation in linear-Gaussian time series (Kalman filter and friends)\n\n\nTypical examples of non-convex optimisation in ML are\n\n\nneural networks\nmaximum likelihood mixtures of Gaussians\n\n\nIf the objective function is convex, that comes with nice guarantees. Most importantly, if a function is strictly convex, it is guaranteed to have a unique global minimum, and it can be found by various standard methods.\nNon-convex functions may have several local minima, that is multiple points satisfying that they are the best point in their local neighbourhood, but which are not globally optimal. Therefore, if you have a non-convex problem, there is usually no way to test if the solution you have found is indeed the best solution.\nFurthermore, if a problem is convex, it is usually easier to analyse the asymptotic behaviour of the algorithm, that is how fast it converges as you observe more and more data.\n\nThere are multiple schools of thought in machine learning, some are more obsessed with convexity than others. Depending on where you come from, and what your goals are, convexity may be more or less important to you. In any case, it's important you understand the benefits of convexity, and that you have a decent knowledge of various linear, quadratic and convex programming methods you can use to solve convex problems."", 'result': {'fake': 0.0005, 'real': 0.9995}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982502, 'subscription': 0, 'content': ""Many methods in machine learning are based on finding parameters that minimise some objective function. Very often, the objective function is a weighted sum of two terms: a cost function and regularisation. In statistics terms the (log-)likelihood and (log-)prior. If both of these components are convex, then their sum is also convex.\n\nExamples of convex optimisation problems in machine learning:\n\nlinear regression/ Ridge regression, with Tikhonov regularisation, etc\nsparse linear regression with L1 regularisation, such as lasso\nsupport vector machines\nparameter estimation in linear-Gaussian time series (Kalman filter and friends)\n\n\nTypical examples of non-convex optimisation in ML are\n\n\nneural networks\nmaximum likelihood mixtures of Gaussians\n\n\nIf the objective function is convex, that comes with nice guarantees. Most importantly, if a function is strictly convex, it is guaranteed to have a unique global minimum, and it can be found by various standard methods.\nNon-convex functions may have several local minima, that is multiple points satisfying that they are the best point in their local neighbourhood, but which are not globally optimal. Therefore, if you have a non-convex problem, there is usually no way to test if the solution you have found is indeed the best solution.\nFurthermore, if a problem is convex, it is usually easier to analyse the asymptotic behaviour of the algorithm, that is how fast it converges as you observe more and more data.\n\nThere are multiple schools of thought in machine learning, some are more obsessed with convexity than others. Depending on where you come from, and what your goals are, convexity may be more or less important to you. In any case, it's important you understand the benefits of convexity, and that you have a decent knowledge of various linear, quadratic and convex programming methods you can use to solve convex problems."", 'aiModelVersion': '1'}",0.9995
Xinyu Zhao (赵 欣宇),6y,"As an ML engineer at Quora, have you ever implemented a research article in machine learning to solve a technical or product problem?","I joined Quora as a machine learning engineer earlier last year and have been working on how to automatically correct errors in questions. I found inspiration in a paper published by Ziang Xie (Ziang Xie et al., 2016
), which showcases an end-to-end neural language correction model.

At Quora, we get tons of questions every day by people from different countries and backgrounds. We want to enable everyone to ask any type of question they want, regardless of their English proficiency, without having to worry about syntax and semantic errors. As such, we built a machine learning model that can automatically correct many common errors in questions, including spelling mistakes, missing prepositions/articles, wrong/missing punctuation, wrong capitalization, etc. Here is a representative example:

The difficulties in question correction are perfectly demonstrated by this example. First, this question starts with “Hello sir”, which is unnecessary. Second, it has an unnecessary space between the words “achievement” and “period”. Finally, to be grammatically correct, “the” is required before the abbreviation “SSB”.

In general, language correction is one of the most complex problems in the field of natural language processing [NLP]. The 
CoNLL-2014 conference
 has a competition focused on this general challenge. The competition has attracted lots of researchers to solving the problem.

Problem definition

The question correction problem can be defined as follows: Given an error-ridden input question x, map it to an output question y with errors in x being fully (or partially) fixed.

Most of the existing research in this area focuses on training classifiers for specific types of errors. For example, Han (Han et al., 2006
) trained a classifier to correct article errors and Rozovskaya (Rozovskaya et. al., 2010
) proposed a system focused on correcting preposition and determiner errors. However, training/maintaining separate models for different error types is inefficient, and it would be preferable to correct all errors in one model, if possible. Towards this goal, Ziang Xie's recent paper
 caught my eye as a potential solution.

Main architecture

The core component of Ziang Xie's 
paper
 is an encoder-decoder recurrent neural network [RNN] with an attention mechanism (as shown in Fig 1). It operates at the character-level so that 1) the out-of-vocabulary [OOV] problem can be avoided and 2) it has the ability to fix spelling errors. However, by operating at the character level, the total sequence length is large compared with operating at the word level, which means the time required to encode/decode a question is high. On the other hand, the search time for the decoder is reduced as the vocabulary size for the character level is much smaller than the word level.

Figure 1: the encoder-decoder architecture.

The encoder network uses a bidirectional gated recurrent unit [GRU] (Cho et al., 2014
). The main architecture of a GRU is very similar to that of a LSTM. It has two gates, a reset gate and an update gate. The reset gate determines how to combine the new input with previous information (also called memory) whereas the update gate defines how much of the previous memory should be preserved.

The entire encoder network contains three layers to map the input question x to a high-level representation. Except for the first layer which uses character embedding of the input question x as the input, each layer concatenates its forward and backward hidden states, the result of which is used as the input for the next layer (as shown in Fig 2).

Figure 2: the first layer in the encoder network.

The decoder network uses a standard GRU, and each GRU unit can only get information from past unit but not from the future. It also has three layers, and the last layer is conditioned on the encoded representation using an attention mechanism (Bahdanau et al., 2014
), which is a weighted sum over the hidden states in the last layer of the encoder. With this attention mechanism, we can ensure output question y won't diverge from input question x. In other word, we want to make sure that question y retains the meaning of question x. The “optimal” output sequence is retrieved using beam search
 with a pre-defined beam width. Beam search is a greedy algorithm. It uses breadth-first-search with a limited number of nodes (so called beam width) maintained at each level of the tree. In general, larger beam size will lead to more accurate results, but with longer search time and more memory usage. In our case, the beam width is set to 8, which shows a good balance between speed and accuracy.

Currently, we use this model to correct new questions and the accuracy of the model is measured using 
BLEU score
. The BLEU score is commonly used to measure the quality of a machine translation system. Specifically, it measures N-gram overlap between machine translation output and reference text (ground truth).

Figure 3: an example of computing 4-gram BLEU score between machine translation output and reference text.

To better suit our needs, we modified the original architecture a little bit by reducing the number of layers used in the encoder and the decoder from three to two. After experimenting with a few different combinations, we found that this gives us the optimal trade off between inference time and accuracy; i.e., we achieved much faster prediction time, but not much worse accuracy.

Figure 4: an example of questions auto-fixed by our question correction model. “Green”: added part; “Red”: deleted part.

At Quora, machine learning has been used in many different ways, such as answer ranking, duplicate question detection, spam detection, etc. To better address these problems, sometimes we need to try out different models and ideas including those published in research papers. We also have weekly machine learning talks where we discuss machine learning trends and relevant research papers. If you are interested in working on machine learning related problems, join us!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/5ctq8eb4s6ulp1dr', 'title': 'As an ML engineer at Quora, have you ever implemented a research article in machine learning to solve a technical or product problem?', 'score': {'original': 0.98115, 'ai': 0.01885}, 'blocks': [{'text': ""I joined Quora as a machine learning engineer earlier last year and have been working on how to automatically correct errors in questions. I found inspiration in a paper published by Ziang Xie (Ziang Xie et al., 2016\n), which showcases an end-to-end neural language correction model.\n\nAt Quora, we get tons of questions every day by people from different countries and backgrounds. We want to enable everyone to ask any type of question they want, regardless of their English proficiency, without having to worry about syntax and semantic errors. As such, we built a machine learning model that can automatically correct many common errors in questions, including spelling mistakes, missing prepositions/articles, wrong/missing punctuation, wrong capitalization, etc. Here is a representative example:\n\nThe difficulties in question correction are perfectly demonstrated by this example. First, this question starts with “Hello sir”, which is unnecessary. Second, it has an unnecessary space between the words “achievement” and “period”. Finally, to be grammatically correct, “the” is required before the abbreviation “SSB”.\n\nIn general, language correction is one of the most complex problems in the field of natural language processing [NLP]. The \nCoNLL-2014 conference\n has a competition focused on this general challenge. The competition has attracted lots of researchers to solving the problem.\n\nProblem definition\n\nThe question correction problem can be defined as follows: Given an error-ridden input question x, map it to an output question y with errors in x being fully (or partially) fixed.\n\nMost of the existing research in this area focuses on training classifiers for specific types of errors. For example, Han (Han et al., 2006\n) trained a classifier to correct article errors and Rozovskaya (Rozovskaya et. al., 2010\n) proposed a system focused on correcting preposition and determiner errors. However, training/maintaining separate models for different error types is inefficient, and it would be preferable to correct all errors in one model, if possible. Towards this goal, Ziang Xie's recent paper\n caught my eye as a potential solution.\n\nMain architecture\n\nThe core component of Ziang Xie's \npaper\n is an encoder-decoder recurrent neural network [RNN] with an attention mechanism (as shown in Fig 1). It operates at the character-level so that 1) the out-of-vocabulary [OOV] problem can be avoided and 2) it has the ability to fix spelling errors. However, by operating at the character level, the total sequence length is large compared with operating at the word level, which means the time required to encode/decode a question is high. On the other hand, the search time for the decoder is reduced as the vocabulary size for the character level is much smaller than the word level.\n\nFigure 1: the encoder-decoder architecture.\n\nThe encoder network uses a bidirectional gated recurrent unit [GRU] (Cho et al., 2014\n). The main architecture of a GRU is very similar to that of a LSTM. It has two gates, a reset gate and an update gate. The reset gate determines how to combine the new input with previous information (also called memory) whereas the update gate defines how much of the previous memory should be preserved.\n\nThe entire encoder network contains three layers to map the input question x to a high-level"", 'result': {'fake': 0.01, 'real': 0.99}, 'status': 'success'}, {'text': ""representation. Except for the first layer which uses character embedding of the input question x as the input, each layer concatenates its forward and backward hidden states, the result of which is used as the input for the next layer (as shown in Fig 2).\n\nFigure 2: the first layer in the encoder network.\n\nThe decoder network uses a standard GRU, and each GRU unit can only get information from past unit but not from the future. It also has three layers, and the last layer is conditioned on the encoded representation using an attention mechanism (Bahdanau et al., 2014\n), which is a weighted sum over the hidden states in the last layer of the encoder. With this attention mechanism, we can ensure output question y won't diverge from input question x. In other word, we want to make sure that question y retains the meaning of question x. The “optimal” output sequence is retrieved using beam search\n with a pre-defined beam width. Beam search is a greedy algorithm. It uses breadth-first-search with a limited number of nodes (so called beam width) maintained at each level of the tree. In general, larger beam size will lead to more accurate results, but with longer search time and more memory usage. In our case, the beam width is set to 8, which shows a good balance between speed and accuracy.\n\nCurrently, we use this model to correct new questions and the accuracy of the model is measured using \nBLEU score\n. The BLEU score is commonly used to measure the quality of a machine translation system. Specifically, it measures N-gram overlap between machine translation output and reference text (ground truth).\n\nFigure 3: an example of computing 4-gram BLEU score between machine translation output and reference text.\n\nTo better suit our needs, we modified the original architecture a little bit by reducing the number of layers used in the encoder and the decoder from three to two. After experimenting with a few different combinations, we found that this gives us the optimal trade off between inference time and accuracy; i.e., we achieved much faster prediction time, but not much worse accuracy.\n\nFigure 4: an example of questions auto-fixed by our question correction model. “Green”: added part; “Red”: deleted part.\n\nAt Quora, machine learning has been used in many different ways, such as answer ranking, duplicate question detection, spam detection, etc. To better address these problems, sometimes we need to try out different models and ideas including those published in research papers. We also have weekly machine learning talks where we discuss machine learning trends and relevant research papers. If you are interested in working on machine learning related problems, join us!"", 'result': {'fake': 0.141, 'real': 0.859}, 'status': 'success'}], 'credits_used': 10, 'credits': 1982492, 'subscription': 0, 'content': ""I joined Quora as a machine learning engineer earlier last year and have been working on how to automatically correct errors in questions. I found inspiration in a paper published by Ziang Xie (Ziang Xie et al., 2016\n), which showcases an end-to-end neural language correction model.\n\nAt Quora, we get tons of questions every day by people from different countries and backgrounds. We want to enable everyone to ask any type of question they want, regardless of their English proficiency, without having to worry about syntax and semantic errors. As such, we built a machine learning model that can automatically correct many common errors in questions, including spelling mistakes, missing prepositions/articles, wrong/missing punctuation, wrong capitalization, etc. Here is a representative example:\n\nThe difficulties in question correction are perfectly demonstrated by this example. First, this question starts with “Hello sir”, which is unnecessary. Second, it has an unnecessary space between the words “achievement” and “period”. Finally, to be grammatically correct, “the” is required before the abbreviation “SSB”.\n\nIn general, language correction is one of the most complex problems in the field of natural language processing [NLP]. The \nCoNLL-2014 conference\n has a competition focused on this general challenge. The competition has attracted lots of researchers to solving the problem.\n\nProblem definition\n\nThe question correction problem can be defined as follows: Given an error-ridden input question x, map it to an output question y with errors in x being fully (or partially) fixed.\n\nMost of the existing research in this area focuses on training classifiers for specific types of errors. For example, Han (Han et al., 2006\n) trained a classifier to correct article errors and Rozovskaya (Rozovskaya et. al., 2010\n) proposed a system focused on correcting preposition and determiner errors. However, training/maintaining separate models for different error types is inefficient, and it would be preferable to correct all errors in one model, if possible. Towards this goal, Ziang Xie's recent paper\n caught my eye as a potential solution.\n\nMain architecture\n\nThe core component of Ziang Xie's \npaper\n is an encoder-decoder recurrent neural network [RNN] with an attention mechanism (as shown in Fig 1). It operates at the character-level so that 1) the out-of-vocabulary [OOV] problem can be avoided and 2) it has the ability to fix spelling errors. However, by operating at the character level, the total sequence length is large compared with operating at the word level, which means the time required to encode/decode a question is high. On the other hand, the search time for the decoder is reduced as the vocabulary size for the character level is much smaller than the word level.\n\nFigure 1: the encoder-decoder architecture.\n\nThe encoder network uses a bidirectional gated recurrent unit [GRU] (Cho et al., 2014\n). The main architecture of a GRU is very similar to that of a LSTM. It has two gates, a reset gate and an update gate. The reset gate determines how to combine the new input with previous information (also called memory) whereas the update gate defines how much of the previous memory should be preserved.\n\nThe entire encoder network contains three layers to map the input question x to a high-level representation. Except for the first layer which uses character embedding of the input question x as the input, each layer concatenates its forward and backward hidden states, the result of which is used as the input for the next layer (as shown in Fig 2).\n\nFigure 2: the first layer in the encoder network.\n\nThe decoder network uses a standard GRU, and each GRU unit can only get information from past unit but not from the future. It also has three layers, and the last layer is conditioned on the encoded representation using an attention mechanism (Bahdanau et al., 2014\n), which is a weighted sum over the hidden states in the last layer of the encoder. With this attention mechanism, we can ensure output question y won't diverge from input question x. In other word, we want to make sure that question y retains the meaning of question x. The “optimal” output sequence is retrieved using beam search\n with a pre-defined beam width. Beam search is a greedy algorithm. It uses breadth-first-search with a limited number of nodes (so called beam width) maintained at each level of the tree. In general, larger beam size will lead to more accurate results, but with longer search time and more memory usage. In our case, the beam width is set to 8, which shows a good balance between speed and accuracy.\n\nCurrently, we use this model to correct new questions and the accuracy of the model is measured using \nBLEU score\n. The BLEU score is commonly used to measure the quality of a machine translation system. Specifically, it measures N-gram overlap between machine translation output and reference text (ground truth).\n\nFigure 3: an example of computing 4-gram BLEU score between machine translation output and reference text.\n\nTo better suit our needs, we modified the original architecture a little bit by reducing the number of layers used in the encoder and the decoder from three to two. After experimenting with a few different combinations, we found that this gives us the optimal trade off between inference time and accuracy; i.e., we achieved much faster prediction time, but not much worse accuracy.\n\nFigure 4: an example of questions auto-fixed by our question correction model. “Green”: added part; “Red”: deleted part.\n\nAt Quora, machine learning has been used in many different ways, such as answer ranking, duplicate question detection, spam detection, etc. To better address these problems, sometimes we need to try out different models and ideas including those published in research papers. We also have weekly machine learning talks where we discuss machine learning trends and relevant research papers. If you are interested in working on machine learning related problems, join us!"", 'aiModelVersion': '1'}",0.98115
Rajan Singh,4y,What are some embarrassing statistics mistakes that people with fancy credentials often make?,"When I was serving in the police, during every ‘crime conference’, senior IPS officers would try to show that they had reduced the crime rate through ‘outstanding policing’. (I know it sounds funny)

As an example, someone might take credit for ‘reducing’ the number of traffic accidents per year from 380 cases to 372. Or the number of murders from 12 to 11.

What is the problem with such claims?

They ignore the idea of statistical significance. In simple terms, if say for the last 20 years the number of traffic accidents has consistently stayed between 379 and 380, then 372 is a meaningful reduction. As you can imagine, that is highly unlikely.

But if one year the number of accidents is 380, the next year 412, and then 305, the standard deviation is so high that a reduction from 380 to 372 is not meaningful. Obviously, when humans are seeking glory, we don’t let this hairsplitting and nuance come in the way.

Ironically, UPSC used to have a statistics paper. Maybe it still does. Hasn’t done much good though.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/j2lhuxs8r7k5ibct', 'title': 'What are some embarrassing statistics mistakes that people with fancy credentials often make?', 'score': {'original': 0.9996, 'ai': 0.0004}, 'blocks': [{'text': 'When I was serving in the police, during every ‘crime conference’, senior IPS officers would try to show that they had reduced the crime rate through ‘outstanding policing’. (I know it sounds funny)\n\nAs an example, someone might take credit for ‘reducing’ the number of traffic accidents per year from 380 cases to 372. Or the number of murders from 12 to 11.\n\nWhat is the problem with such claims?\n\nThey ignore the idea of statistical significance. In simple terms, if say for the last 20 years the number of traffic accidents has consistently stayed between 379 and 380, then 372 is a meaningful reduction. As you can imagine, that is highly unlikely.\n\nBut if one year the number of accidents is 380, the next year 412, and then 305, the standard deviation is so high that a reduction from 380 to 372 is not meaningful. Obviously, when humans are seeking glory, we don’t let this hairsplitting and nuance come in the way.\n\nIronically, UPSC used to have a statistics paper. Maybe it still does. Hasn’t done much good though.', 'result': {'fake': 0.0004, 'real': 0.9996}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982490, 'subscription': 0, 'content': 'When I was serving in the police, during every ‘crime conference’, senior IPS officers would try to show that they had reduced the crime rate through ‘outstanding policing’. (I know it sounds funny)\n\nAs an example, someone might take credit for ‘reducing’ the number of traffic accidents per year from 380 cases to 372. Or the number of murders from 12 to 11.\n\nWhat is the problem with such claims?\n\nThey ignore the idea of statistical significance. In simple terms, if say for the last 20 years the number of traffic accidents has consistently stayed between 379 and 380, then 372 is a meaningful reduction. As you can imagine, that is highly unlikely.\n\nBut if one year the number of accidents is 380, the next year 412, and then 305, the standard deviation is so high that a reduction from 380 to 372 is not meaningful. Obviously, when humans are seeking glory, we don’t let this hairsplitting and nuance come in the way.\n\nIronically, UPSC used to have a statistics paper. Maybe it still does. Hasn’t done much good though.', 'aiModelVersion': '1'}",0.9996
Håkon Hapnes Strand,4y,Is data science just simply cleaning data and putting it into a model with already built in algorithms?,"The words «just» and «simply» are the two most misused words in the English language in the tech industry. They’re commonly used by various people with «manager» in their job title who have either never written any code or forgotten how difficult it was.

Cleaning data is not simple in practice. The mere exercise of storing and processing datetimes can be insanity inducing. Yesterday I had to reprocess the entire contents of a data lake because someone who came before me had decided to store datetimes in a format that was non-compliant with the ISO 8601 standard.

That may not be your idea of fun, but dealing with that stuff is certainly not simple.

But no, data science is not just that. It’s also about understanding a problem, modelling it, coming up with good features and tuning your model. That’s the stuff that every data scientist enjoys. It’s intellectually stimulating and it encourages creativity.

Data science is also about business. You constantly have to consider business constraints and technical tradeoffs. It’s about finding a solution that’s good enough, not perfect. In that sense, we can consider it a form of engineering.

What data science is usually not about is developing completely new machine learning algorithms. We could do that, and many data scientists are very well equipped to perform that type of work, but it’s just not a rational time investment from a business perspective. Very rarely anyway.

You still get to create. But if you’re thinking about deriving math formulas and implementing them in code, that’s not what the job is about.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/spf7mln6o9jgab14', 'title': 'Is data science just simply cleaning data and putting it into a model with already built in algorithms?', 'score': {'original': 0.9985, 'ai': 0.0015}, 'blocks': [{'text': 'The words «just» and «simply» are the two most misused words in the English language in the tech industry. They’re commonly used by various people with «manager» in their job title who have either never written any code or forgotten how difficult it was.\n\nCleaning data is not simple in practice. The mere exercise of storing and processing datetimes can be insanity inducing. Yesterday I had to reprocess the entire contents of a data lake because someone who came before me had decided to store datetimes in a format that was non-compliant with the ISO 8601 standard.\n\nThat may not be your idea of fun, but dealing with that stuff is certainly not simple.\n\nBut no, data science is not just that. It’s also about understanding a problem, modelling it, coming up with good features and tuning your model. That’s the stuff that every data scientist enjoys. It’s intellectually stimulating and it encourages creativity.\n\nData science is also about business. You constantly have to consider business constraints and technical tradeoffs. It’s about finding a solution that’s good enough, not perfect. In that sense, we can consider it a form of engineering.\n\nWhat data science is usually not about is developing completely new machine learning algorithms. We could do that, and many data scientists are very well equipped to perform that type of work, but it’s just not a rational time investment from a business perspective. Very rarely anyway.\n\nYou still get to create. But if you’re thinking about deriving math formulas and implementing them in code, that’s not what the job is about.', 'result': {'fake': 0.0015, 'real': 0.9985}, 'status': 'success'}], 'credits_used': 3, 'credits': 1982487, 'subscription': 0, 'content': 'The words «just» and «simply» are the two most misused words in the English language in the tech industry. They’re commonly used by various people with «manager» in their job title who have either never written any code or forgotten how difficult it was.\n\nCleaning data is not simple in practice. The mere exercise of storing and processing datetimes can be insanity inducing. Yesterday I had to reprocess the entire contents of a data lake because someone who came before me had decided to store datetimes in a format that was non-compliant with the ISO 8601 standard.\n\nThat may not be your idea of fun, but dealing with that stuff is certainly not simple.\n\nBut no, data science is not just that. It’s also about understanding a problem, modelling it, coming up with good features and tuning your model. That’s the stuff that every data scientist enjoys. It’s intellectually stimulating and it encourages creativity.\n\nData science is also about business. You constantly have to consider business constraints and technical tradeoffs. It’s about finding a solution that’s good enough, not perfect. In that sense, we can consider it a form of engineering.\n\nWhat data science is usually not about is developing completely new machine learning algorithms. We could do that, and many data scientists are very well equipped to perform that type of work, but it’s just not a rational time investment from a business perspective. Very rarely anyway.\n\nYou still get to create. But if you’re thinking about deriving math formulas and implementing them in code, that’s not what the job is about.', 'aiModelVersion': '1'}",0.9985
Michelle - MAD PIRATE QUEEN,Sep 5,Is artificial intelligence making artists obsolete?,"Heh.

I’ll let these unicorns speak for themselves…

In case you’ve been drinking (like the unicorns and possibly like the AI image creator), I’ll highlight the issue… Most people would expect to see a unicorn (drunk or not) with 4 legs. Not 5 (unless it’s a dude HA! #jokes) and most certainly not 6 legs.

Yet every time I ask the image generator to do something, what it delivers just isn't quite right.. sometimes the results are disturbing or soulless.

Artists aren’t going to lose their employment anytime soon. The folk who are happy to use AI art, generally aren’t the folk who were willing to pay for art anyway.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/78k4epgmhtxqdo63', 'title': 'Is artificial intelligence making artists obsolete?', 'score': {'original': 0.9966, 'ai': 0.0034}, 'blocks': [{'text': ""Heh.\n\nI’ll let these unicorns speak for themselves…\n\nIn case you’ve been drinking (like the unicorns and possibly like the AI image creator), I’ll highlight the issue… Most people would expect to see a unicorn (drunk or not) with 4 legs. Not 5 (unless it’s a dude HA! #jokes) and most certainly not 6 legs.\n\nYet every time I ask the image generator to do something, what it delivers just isn't quite right.. sometimes the results are disturbing or soulless.\n\nArtists aren’t going to lose their employment anytime soon. The folk who are happy to use AI art, generally aren’t the folk who were willing to pay for art anyway."", 'result': {'fake': 0.0034, 'real': 0.9966}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982485, 'subscription': 0, 'content': ""Heh.\n\nI’ll let these unicorns speak for themselves…\n\nIn case you’ve been drinking (like the unicorns and possibly like the AI image creator), I’ll highlight the issue… Most people would expect to see a unicorn (drunk or not) with 4 legs. Not 5 (unless it’s a dude HA! #jokes) and most certainly not 6 legs.\n\nYet every time I ask the image generator to do something, what it delivers just isn't quite right.. sometimes the results are disturbing or soulless.\n\nArtists aren’t going to lose their employment anytime soon. The folk who are happy to use AI art, generally aren’t the folk who were willing to pay for art anyway."", 'aiModelVersion': '1'}",0.9966
Xavier Amatriain,2y,How useful was the Netflix Prize challenge for Netflix?,"It has been over 10 years since the Netflix Prize finished, and I was not expecting to write a blog post about it at this point. However, just in the past couple of weeks I have found myself talking about it extensively both in the context of a Twitter thread and discussion
, as well as the shooting of an upcoming documentary series. Given that there seems to be continued interest as well as misunderstanding around the prize and its outcome, I thought it might be worth to “set the record straight” in a dedicated post.

TLDR; While I am often misquoted as having said that the Netflix Prize was not useful for Netflix, that is only true about the grand prize winning entry. Along the way, Netflix got far more than our money’s worth for the famous prize.

Image by author, screenshot from article published by Wired magazine here

What was the Netflix Prize anyways?

Ten years ago, when I was leading Algorithms at Netflix, I would start my public presentations by asking the audience a couple of questions: “Who has heard about the Netflix Prize?” and “Who has participated in the Prize by submitting at least an entry?”. Virtually all hands would go up to the first question. Depending on the audience, more than half of the people would also raise their hands to the second. I would obviously not expect that to be the case anymore. However, I have recently been pretty surprised by how much folks in the ML/AI community remember or simply know about the contest. It is clear that it was somewhat of a historical event.

Image by author

In 2006, Netflix launched a $1M competition to improve their recommendation algorithm. At that point I was a researcher in the field in a Barcelona lab, so I don’t have first-hand details of how and why the prize was decided internally and how it all got started (to be fair, I did get some of that insight once I joined the company 5 years later, but that part of the story is not very relevant anyways). For the thousands of participants around the world, the rules were quite simple: if you were able to beat the Netflix baseline (called Cinematch), you would win $1M. Not surprisingly, many researchers, students, and amateur mathematicians and computer scientists around the world jumped at the opportunity.

The mechanics of the contest were also quite simple: there was a training set consisting of around 100M data points including a user, a movie, a date, and a rating from 1 to 5 stars. There was a smaller public validation dataset called “probe” consisting of around 1.5M data points with no ratings. Finally, and very importantly, there were two test sets that hid ratings from participants. In order to test your algorithm you would submit your predictions to the so-called quiz test and you would get back the accuracy measured in RMSE (root mean squared error). Yearly progress prizes, and especially the grand prize, would be measured against a different test set.

Image by author

There were hundreds of forum and blog posts and research publications detailing the different approaches to the Netflix Prize. The first year progress prize was won by the Korbell team (Yehuda Koren, Robert Bell, and Chris Volinsky from AT&T labs) using an ensemble of a variation of SVD (singular value decomposition, although the variation is really more a matrix factorization than a traditional SVD) and RBM (restricted Boltzmann machines, a form of ANN). The SVD had an RMSE of 0.8914, the RBM 0.8990, and a linear blend of both got to an RMSE of 0.88.

It took 3 more years and thousands of teams trying to get from there to the 0.8572 RMSE on the test set that was required to win the grand prize. The winning entry
 was an ensemble of 104 individual predictors developed by multiple teams, and ensembled by a single layer neural network.

Source: Getty Images

Making history

Back in 2006 there was no Kaggle, open source was something that fringe Linux hackers did on their “free time”, and AI was a dirty word. Taking that context into account, it is easy to see how groundbreaking the prize was in many ways. Releasing a large dataset and offering $1M to anyone who was able to beat a metric was definitely a bold move, and one that not many understood back in the day. Hell, many even don’t understand it nowadays.

I am pretty sure that without the Netflix Prize we would not have data competitions as we have now. Or, at the very least, it would have taken much more for Kaggle to take off. I have to wonder if much of the rest of the open ecosystem around AI, which includes from datasets à la Imagenet, to open source frameworks like TensorFlow or Pytorch, would be here if it hadn’t been for the Netflix Prize. Yes, I know that there was open source and even open data back in the day, but none of that was supported by a large company, let alone one that was actively supported with actual cash.

The grand Netflix Prize solution

The 2007 Progress Prize solution that combined SVD+RBM was already significantly better than the existing Cinematch algorithm. So, Netflix put a few engineers to work on productionizing the algorithm. That included rewriting the code and making it scalable, plus able to retrain incrementally as new ratings came in. When I joined Netflix I took over the small team that was working and maintaining the rating prediction algorithm that included the first year Progress Prize solution.

But, what about the grand Prize solution with the 104 algorithms? As I mentioned in a blog post
 back then, we decided it was not even worth to productionize. It would have taken a large engineering effort for a small gain in accuracy that was most likely not worth it for several reasons. The main one is that at that point in time, once the transition to streaming (vs DVD-by-mail) was obvious, it was clear that predicting consumption was much more important and impactful than predicting ratings.

Image by author, extracted from blog post
 by author and Justin Basilico

Let’s talk ROI

I am often surprised that after reading the story above many will conclude
 that Netflix’ $1M investment was not worth it. That is definitely a very short-sighted read. Do you know how many Silicon Valley engineers Netflix can hire for 3 years for $1M? Probably less than one.

What did Netflix get from the $1M invested in the prize?

Thousands of researchers and engineers around the world thinking about a problem that was important to Netflix
A solution (the first year Progress Prize) that could be launched into production with measurable gains
Becoming a recognizable brand name as a company that innovates in the space

One thing that I am absolutely certain of is that if it hadn’t been for the Netflix Prize me and many others like me would not have ended up working at Netflix. I doubt Netflix would have been able to innovate at the pace they did during those years if it wasn’t because of the talent that it attracted as a result of the prize. That in itself is worth much more than $1M.

Conclusion: are open algorithmic contests useful and valuable?

Of course, you can argue that I am quite biased on this topic. However, even as of today, I would say that there is huge value in them. Getting a large community to think about a data set and a particular problem often produces lots of insights and valuable results, some of which are applicable in practice. They also provide a unique opportunity for people around the world to learn, get some exposure, and in many cases even find a dream job they would not have found otherwise.

That being said, I do agree that algorithmic contests are not the silver bullet that will solve your company’s algorithmic needs. They should not be considered a way to outsource problems to cheaper countries. And, of course, it is important for the organizations in charge of proposing the competitions to make sure there are good incentives and fair rules. But I feel companies like Kaggle (now Google) have done a really good job at setting up the right framework for this.

Finally, I will say that I am very intrigued and interested by Andrew Ng’s proposal to turn algorithmic contests around by keeping the algorithm fixed while having participants work on the data in his Data-centric AI Competition
. I am sure we can learn quite a lot from having a large community getting an incentive to work on data rather than on the algorithm.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/i4gd3qcv2z0wmjyu', 'title': 'How useful was the Netflix Prize challenge for Netflix?', 'score': {'original': 0.94056666666667, 'ai': 0.059433333333333}, 'blocks': [{'text': 'It has been over 10 years since the Netflix Prize finished, and I was not expecting to write a blog post about it at this point. However, just in the past couple of weeks I have found myself talking about it extensively both in the context of a Twitter thread and discussion\n, as well as the shooting of an upcoming documentary series. Given that there seems to be continued interest as well as misunderstanding around the prize and its outcome, I thought it might be worth to “set the record straight” in a dedicated post.\n\nTLDR; While I am often misquoted as having said that the Netflix Prize was not useful for Netflix, that is only true about the grand prize winning entry. Along the way, Netflix got far more than our money’s worth for the famous prize.\n\nImage by author, screenshot from article published by Wired magazine here\n\nWhat was the Netflix Prize anyways?\n\nTen years ago, when I was leading Algorithms at Netflix, I would start my public presentations by asking the audience a couple of questions: “Who has heard about the Netflix Prize?” and “Who has participated in the Prize by submitting at least an entry?”. Virtually all hands would go up to the first question. Depending on the audience, more than half of the people would also raise their hands to the second. I would obviously not expect that to be the case anymore. However, I have recently been pretty surprised by how much folks in the ML/AI community remember or simply know about the contest. It is clear that it was somewhat of a historical event.\n\nImage by author\n\nIn 2006, Netflix launched a $1M competition to improve their recommendation algorithm. At that point I was a researcher in the field in a Barcelona lab, so I don’t have first-hand details of how and why the prize was decided internally and how it all got started (to be fair, I did get some of that insight once I joined the company 5 years later, but that part of the story is not very relevant anyways). For the thousands of participants around the world, the rules were quite simple: if you were able to beat the Netflix baseline (called Cinematch), you would win $1M. Not surprisingly, many researchers, students, and amateur mathematicians and computer scientists around the world jumped at the opportunity.\n\nThe mechanics of the contest were also quite simple: there was a training set consisting of around 100M data points including a user, a movie, a date, and a rating from 1 to 5 stars. There was a smaller public validation dataset called “probe” consisting of around 1.5M data points with no ratings. Finally, and very importantly, there were two test sets that hid ratings from participants. In order to test your algorithm you would submit your predictions to the so-called quiz test and you would get back the accuracy measured in RMSE (root mean squared error). Yearly progress prizes, and especially the grand prize, would be measured against a different test set.\n\nImage by author\n\nThere were hundreds of forum and blog posts and', 'result': {'fake': 0.1005, 'real': 0.8995}, 'status': 'success'}, {'text': 'research publications detailing the different approaches to the Netflix Prize. The first year progress prize was won by the Korbell team (Yehuda Koren, Robert Bell, and Chris Volinsky from AT&T labs) using an ensemble of a variation of SVD (singular value decomposition, although the variation is really more a matrix factorization than a traditional SVD) and RBM (restricted Boltzmann machines, a form of ANN). The SVD had an RMSE of 0.8914, the RBM 0.8990, and a linear blend of both got to an RMSE of 0.88.\n\nIt took 3 more years and thousands of teams trying to get from there to the 0.8572 RMSE on the test set that was required to win the grand prize. The winning entry\n was an ensemble of 104 individual predictors developed by multiple teams, and ensembled by a single layer neural network.\n\nSource: Getty Images\n\nMaking history\n\nBack in 2006 there was no Kaggle, open source was something that fringe Linux hackers did on their “free time”, and AI was a dirty word. Taking that context into account, it is easy to see how groundbreaking the prize was in many ways. Releasing a large dataset and offering $1M to anyone who was able to beat a metric was definitely a bold move, and one that not many understood back in the day. Hell, many even don’t understand it nowadays.\n\nI am pretty sure that without the Netflix Prize we would not have data competitions as we have now. Or, at the very least, it would have taken much more for Kaggle to take off. I have to wonder if much of the rest of the open ecosystem around AI, which includes from datasets à la Imagenet, to open source frameworks like TensorFlow or Pytorch, would be here if it hadn’t been for the Netflix Prize. Yes, I know that there was open source and even open data back in the day, but none of that was supported by a large company, let alone one that was actively supported with actual cash.\n\nThe grand Netflix Prize solution\n\nThe 2007 Progress Prize solution that combined SVD+RBM was already significantly better than the existing Cinematch algorithm. So, Netflix put a few engineers to work on productionizing the algorithm. That included rewriting the code and making it scalable, plus able to retrain incrementally as new ratings came in. When I joined Netflix I took over the small team that was working and maintaining the rating prediction algorithm that included the first year Progress Prize solution.\n\nBut, what about the grand Prize solution with the 104 algorithms? As I mentioned in a blog post\n back then, we decided it was not even worth to productionize. It would have taken a large engineering effort for a small gain in accuracy that was most likely not worth it for several reasons. The main one is that at that point in time, once the transition to streaming (vs DVD-by-mail) was obvious, it was clear that predicting consumption was much more important and impactful than predicting ratings.\n\nImage by author, extracted from blog post\n by author and Justin Basilico\n\nLet’s talk ROI\n\nI am often surprised', 'result': {'fake': 0.0021, 'real': 0.9979}, 'status': 'success'}, {'text': 'that after reading the story above many will conclude\n that Netflix’ $1M investment was not worth it. That is definitely a very short-sighted read. Do you know how many Silicon Valley engineers Netflix can hire for 3 years for $1M? Probably less than one.\n\nWhat did Netflix get from the $1M invested in the prize?\n\nThousands of researchers and engineers around the world thinking about a problem that was important to Netflix\nA solution (the first year Progress Prize) that could be launched into production with measurable gains\nBecoming a recognizable brand name as a company that innovates in the space\n\nOne thing that I am absolutely certain of is that if it hadn’t been for the Netflix Prize me and many others like me would not have ended up working at Netflix. I doubt Netflix would have been able to innovate at the pace they did during those years if it wasn’t because of the talent that it attracted as a result of the prize. That in itself is worth much more than $1M.\n\nConclusion: are open algorithmic contests useful and valuable?\n\nOf course, you can argue that I am quite biased on this topic. However, even as of today, I would say that there is huge value in them. Getting a large community to think about a data set and a particular problem often produces lots of insights and valuable results, some of which are applicable in practice. They also provide a unique opportunity for people around the world to learn, get some exposure, and in many cases even find a dream job they would not have found otherwise.\n\nThat being said, I do agree that algorithmic contests are not the silver bullet that will solve your company’s algorithmic needs. They should not be considered a way to outsource problems to cheaper countries. And, of course, it is important for the organizations in charge of proposing the competitions to make sure there are good incentives and fair rules. But I feel companies like Kaggle (now Google) have done a really good job at setting up the right framework for this.\n\nFinally, I will say that I am very intrigued and interested by Andrew Ng’s proposal to turn algorithmic contests around by keeping the algorithm fixed while having participants work on the data in his Data-centric AI Competition\n. I am sure we can learn quite a lot from having a large community getting an incentive to work on data rather than on the algorithm.', 'result': {'fake': 0.2257, 'real': 0.7743}, 'status': 'success'}], 'credits_used': 15, 'credits': 1982470, 'subscription': 0, 'content': 'It has been over 10 years since the Netflix Prize finished, and I was not expecting to write a blog post about it at this point. However, just in the past couple of weeks I have found myself talking about it extensively both in the context of a Twitter thread and discussion\n, as well as the shooting of an upcoming documentary series. Given that there seems to be continued interest as well as misunderstanding around the prize and its outcome, I thought it might be worth to “set the record straight” in a dedicated post.\n\nTLDR; While I am often misquoted as having said that the Netflix Prize was not useful for Netflix, that is only true about the grand prize winning entry. Along the way, Netflix got far more than our money’s worth for the famous prize.\n\nImage by author, screenshot from article published by Wired magazine here\n\nWhat was the Netflix Prize anyways?\n\nTen years ago, when I was leading Algorithms at Netflix, I would start my public presentations by asking the audience a couple of questions: “Who has heard about the Netflix Prize?” and “Who has participated in the Prize by submitting at least an entry?”. Virtually all hands would go up to the first question. Depending on the audience, more than half of the people would also raise their hands to the second. I would obviously not expect that to be the case anymore. However, I have recently been pretty surprised by how much folks in the ML/AI community remember or simply know about the contest. It is clear that it was somewhat of a historical event.\n\nImage by author\n\nIn 2006, Netflix launched a $1M competition to improve their recommendation algorithm. At that point I was a researcher in the field in a Barcelona lab, so I don’t have first-hand details of how and why the prize was decided internally and how it all got started (to be fair, I did get some of that insight once I joined the company 5 years later, but that part of the story is not very relevant anyways). For the thousands of participants around the world, the rules were quite simple: if you were able to beat the Netflix baseline (called Cinematch), you would win $1M. Not surprisingly, many researchers, students, and amateur mathematicians and computer scientists around the world jumped at the opportunity.\n\nThe mechanics of the contest were also quite simple: there was a training set consisting of around 100M data points including a user, a movie, a date, and a rating from 1 to 5 stars. There was a smaller public validation dataset called “probe” consisting of around 1.5M data points with no ratings. Finally, and very importantly, there were two test sets that hid ratings from participants. In order to test your algorithm you would submit your predictions to the so-called quiz test and you would get back the accuracy measured in RMSE (root mean squared error). Yearly progress prizes, and especially the grand prize, would be measured against a different test set.\n\nImage by author\n\nThere were hundreds of forum and blog posts and research publications detailing the different approaches to the Netflix Prize. The first year progress prize was won by the Korbell team (Yehuda Koren, Robert Bell, and Chris Volinsky from AT&T labs) using an ensemble of a variation of SVD (singular value decomposition, although the variation is really more a matrix factorization than a traditional SVD) and RBM (restricted Boltzmann machines, a form of ANN). The SVD had an RMSE of 0.8914, the RBM 0.8990, and a linear blend of both got to an RMSE of 0.88.\n\nIt took 3 more years and thousands of teams trying to get from there to the 0.8572 RMSE on the test set that was required to win the grand prize. The winning entry\n was an ensemble of 104 individual predictors developed by multiple teams, and ensembled by a single layer neural network.\n\nSource: Getty Images\n\nMaking history\n\nBack in 2006 there was no Kaggle, open source was something that fringe Linux hackers did on their “free time”, and AI was a dirty word. Taking that context into account, it is easy to see how groundbreaking the prize was in many ways. Releasing a large dataset and offering $1M to anyone who was able to beat a metric was definitely a bold move, and one that not many understood back in the day. Hell, many even don’t understand it nowadays.\n\nI am pretty sure that without the Netflix Prize we would not have data competitions as we have now. Or, at the very least, it would have taken much more for Kaggle to take off. I have to wonder if much of the rest of the open ecosystem around AI, which includes from datasets à la Imagenet, to open source frameworks like TensorFlow or Pytorch, would be here if it hadn’t been for the Netflix Prize. Yes, I know that there was open source and even open data back in the day, but none of that was supported by a large company, let alone one that was actively supported with actual cash.\n\nThe grand Netflix Prize solution\n\nThe 2007 Progress Prize solution that combined SVD+RBM was already significantly better than the existing Cinematch algorithm. So, Netflix put a few engineers to work on productionizing the algorithm. That included rewriting the code and making it scalable, plus able to retrain incrementally as new ratings came in. When I joined Netflix I took over the small team that was working and maintaining the rating prediction algorithm that included the first year Progress Prize solution.\n\nBut, what about the grand Prize solution with the 104 algorithms? As I mentioned in a blog post\n back then, we decided it was not even worth to productionize. It would have taken a large engineering effort for a small gain in accuracy that was most likely not worth it for several reasons. The main one is that at that point in time, once the transition to streaming (vs DVD-by-mail) was obvious, it was clear that predicting consumption was much more important and impactful than predicting ratings.\n\nImage by author, extracted from blog post\n by author and Justin Basilico\n\nLet’s talk ROI\n\nI am often surprised that after reading the story above many will conclude\n that Netflix’ $1M investment was not worth it. That is definitely a very short-sighted read. Do you know how many Silicon Valley engineers Netflix can hire for 3 years for $1M? Probably less than one.\n\nWhat did Netflix get from the $1M invested in the prize?\n\nThousands of researchers and engineers around the world thinking about a problem that was important to Netflix\nA solution (the first year Progress Prize) that could be launched into production with measurable gains\nBecoming a recognizable brand name as a company that innovates in the space\n\nOne thing that I am absolutely certain of is that if it hadn’t been for the Netflix Prize me and many others like me would not have ended up working at Netflix. I doubt Netflix would have been able to innovate at the pace they did during those years if it wasn’t because of the talent that it attracted as a result of the prize. That in itself is worth much more than $1M.\n\nConclusion: are open algorithmic contests useful and valuable?\n\nOf course, you can argue that I am quite biased on this topic. However, even as of today, I would say that there is huge value in them. Getting a large community to think about a data set and a particular problem often produces lots of insights and valuable results, some of which are applicable in practice. They also provide a unique opportunity for people around the world to learn, get some exposure, and in many cases even find a dream job they would not have found otherwise.\n\nThat being said, I do agree that algorithmic contests are not the silver bullet that will solve your company’s algorithmic needs. They should not be considered a way to outsource problems to cheaper countries. And, of course, it is important for the organizations in charge of proposing the competitions to make sure there are good incentives and fair rules. But I feel companies like Kaggle (now Google) have done a really good job at setting up the right framework for this.\n\nFinally, I will say that I am very intrigued and interested by Andrew Ng’s proposal to turn algorithmic contests around by keeping the algorithm fixed while having participants work on the data in his Data-centric AI Competition\n. I am sure we can learn quite a lot from having a large community getting an incentive to work on data rather than on the algorithm.', 'aiModelVersion': '1'}",0.94056666666667
Stephanie Rogers,Updated 8y,How does Pinterest use machine learning?,"Machine learning is used all over our company right now.


The Black Ops team uses classification to detect spam content and users (Spam Detection).
The discovery team (which includes search and recommendations) uses various machine learning algorithms to provide recommendations, related content, and predict the Pinnability
 of a pin.
Our visual discovery team is working with cutting-edge deep learning algorithms to do object recognition and related object recommendations.
The monetization team does ad performance and relevance prediction.
The growth team has started to move into the realm of using intelligence models to determine what emails to send to users and prevent churn (users never returning to the website).
The data team is also working on building out a distributed system for machine learning using Spark [1], so that learning can be efficient and potentially real-time.

[1] Apache Spark™ - Lightning-Fast Cluster Computing","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/81ack3vz49pefh0g', 'title': 'How does Pinterest use machine learning?', 'score': {'original': 0.9636, 'ai': 0.0364}, 'blocks': [{'text': 'Machine learning is used all over our company right now.\n\n\nThe Black Ops team uses classification to detect spam content and users (Spam Detection).\nThe discovery team (which includes search and recommendations) uses various machine learning algorithms to provide recommendations, related content, and predict the Pinnability\n of a pin.\nOur visual discovery team is working with cutting-edge deep learning algorithms to do object recognition and related object recommendations.\nThe monetization team does ad performance and relevance prediction.\nThe growth team has started to move into the realm of using intelligence models to determine what emails to send to users and prevent churn (users never returning to the website).\nThe data team is also working on building out a distributed system for machine learning using Spark [1], so that learning can be efficient and potentially real-time.\n\n[1] Apache Spark™ - Lightning-Fast Cluster Computing', 'result': {'fake': 0.0364, 'real': 0.9636}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982468, 'subscription': 0, 'content': 'Machine learning is used all over our company right now.\n\n\nThe Black Ops team uses classification to detect spam content and users (Spam Detection).\nThe discovery team (which includes search and recommendations) uses various machine learning algorithms to provide recommendations, related content, and predict the Pinnability\n of a pin.\nOur visual discovery team is working with cutting-edge deep learning algorithms to do object recognition and related object recommendations.\nThe monetization team does ad performance and relevance prediction.\nThe growth team has started to move into the realm of using intelligence models to determine what emails to send to users and prevent churn (users never returning to the website).\nThe data team is also working on building out a distributed system for machine learning using Spark [1], so that learning can be efficient and potentially real-time.\n\n[1] Apache Spark™ - Lightning-Fast Cluster Computing', 'aiModelVersion': '1'}",0.9636
Jerry Liu,Updated 5y,Can deep neural networks learn the minimum function?,"I was actually curious about this myself, so decided to implement a small script in Tensorflow to check this. I’m assuming that we’re trying to learn the function 
f
(
x
,
y
)
=
m
i
n
(
x
,
y
)
f(x,y)=min(x,y)
, which according to WolframAlpha looks something like this:

Neural networks can represent any arbitrary function up to some epsilon, but they’re generally not designed to exactly replicate the function, so I tried to at least come up with an adequate approximation.

My simple network had two hidden layers with 3 units and 2 units respectively, with ReLU activations. The output was a simple linear combination of the 2 last hidden units. I trained with Adam with a learning rate of 0.001, for 20000 steps. I tried regular gradient descent at first and found that the training collapsed where the network’s outputs were independent of the input.

If I constrained the data to a non-negative interval, 
[
0
,
x
]
[0,x]
, I found that training was stable and convergence was relatively quick. For instance, setting x=100,000, here’s a sample of the predictions:

Data, prediction:
(29239, 76479), 29238.9369404545
(1931, 64039), 1930.9829420528908
(67368, 88702), 67367.895374989

However, if the interval included negative numbers (e.g. 
[
−
x
,
x
]
[−x,x]
), the network became much harder to train in a stable fashion. Setting x=10,000, in some runs I get:

(-8099, 6195), -8095.1940113855135
(6559, -3805), -3807.9644528298445

while in other runs I get:

(-1053, 3159), -1159.525077130749
(5897, 9739), 0.6422036529396257

and in some runs I run into the same output collapse issue as I initially had with gradient descent. I think the reason for this is that when negatives are included, the function essentially has to learn 3 different things: to pick the bigger negative value when both inputs are negative, to pick the negative value when one is negative, and to pick the smaller positive value when both are positive.

In conclusion neural nets can learn the min function easily if either constrained within the positive or negative interval, and less easily if the interval includes both. Hope this helps!","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/qvdtlisk0674z35b', 'title': 'Can deep neural networks learn the minimum function?', 'score': {'original': 0.9994, 'ai': 0.0006}, 'blocks': [{'text': 'I was actually curious about this myself, so decided to implement a small script in Tensorflow to check this. I’m assuming that we’re trying to learn the function \nf\n(\nx\n,\ny\n)\n=\nm\ni\nn\n(\nx\n,\ny\n)\nf(x,y)=min(x,y)\n, which according to WolframAlpha looks something like this:\n\nNeural networks can represent any arbitrary function up to some epsilon, but they’re generally not designed to exactly replicate the function, so I tried to at least come up with an adequate approximation.\n\nMy simple network had two hidden layers with 3 units and 2 units respectively, with ReLU activations. The output was a simple linear combination of the 2 last hidden units. I trained with Adam with a learning rate of 0.001, for 20000 steps. I tried regular gradient descent at first and found that the training collapsed where the network’s outputs were independent of the input.\n\nIf I constrained the data to a non-negative interval, \n[\n0\n,\nx\n]\n[0,x]\n, I found that training was stable and convergence was relatively quick. For instance, setting x=100,000, here’s a sample of the predictions:\n\nData, prediction:\n(29239, 76479), 29238.9369404545\n(1931, 64039), 1930.9829420528908\n(67368, 88702), 67367.895374989\n\nHowever, if the interval included negative numbers (e.g. \n[\n−\nx\n,\nx\n]\n[−x,x]\n), the network became much harder to train in a stable fashion. Setting x=10,000, in some runs I get:\n\n(-8099, 6195), -8095.1940113855135\n(6559, -3805), -3807.9644528298445\n\nwhile in other runs I get:\n\n(-1053, 3159), -1159.525077130749\n(5897, 9739), 0.6422036529396257\n\nand in some runs I run into the same output collapse issue as I initially had with gradient descent. I think the reason for this is that when negatives are included, the function essentially has to learn 3 different things: to pick the bigger negative value when both inputs are negative, to pick the negative value when one is negative, and to pick the smaller positive value when both are positive.\n\nIn conclusion neural nets can learn the min function easily if either constrained within the positive or negative interval, and less easily if the interval includes both. Hope this helps!', 'result': {'fake': 0.0006, 'real': 0.9994}, 'status': 'success'}], 'credits_used': 4, 'credits': 1982464, 'subscription': 0, 'content': 'I was actually curious about this myself, so decided to implement a small script in Tensorflow to check this. I’m assuming that we’re trying to learn the function \nf\n(\nx\n,\ny\n)\n=\nm\ni\nn\n(\nx\n,\ny\n)\nf(x,y)=min(x,y)\n, which according to WolframAlpha looks something like this:\n\nNeural networks can represent any arbitrary function up to some epsilon, but they’re generally not designed to exactly replicate the function, so I tried to at least come up with an adequate approximation.\n\nMy simple network had two hidden layers with 3 units and 2 units respectively, with ReLU activations. The output was a simple linear combination of the 2 last hidden units. I trained with Adam with a learning rate of 0.001, for 20000 steps. I tried regular gradient descent at first and found that the training collapsed where the network’s outputs were independent of the input.\n\nIf I constrained the data to a non-negative interval, \n[\n0\n,\nx\n]\n[0,x]\n, I found that training was stable and convergence was relatively quick. For instance, setting x=100,000, here’s a sample of the predictions:\n\nData, prediction:\n(29239, 76479), 29238.9369404545\n(1931, 64039), 1930.9829420528908\n(67368, 88702), 67367.895374989\n\nHowever, if the interval included negative numbers (e.g. \n[\n−\nx\n,\nx\n]\n[−x,x]\n), the network became much harder to train in a stable fashion. Setting x=10,000, in some runs I get:\n\n(-8099, 6195), -8095.1940113855135\n(6559, -3805), -3807.9644528298445\n\nwhile in other runs I get:\n\n(-1053, 3159), -1159.525077130749\n(5897, 9739), 0.6422036529396257\n\nand in some runs I run into the same output collapse issue as I initially had with gradient descent. I think the reason for this is that when negatives are included, the function essentially has to learn 3 different things: to pick the bigger negative value when both inputs are negative, to pick the negative value when one is negative, and to pick the smaller positive value when both are positive.\n\nIn conclusion neural nets can learn the min function easily if either constrained within the positive or negative interval, and less easily if the interval includes both. Hope this helps!', 'aiModelVersion': '1'}",0.9994
Boxun Zhang,8y,"How well do I need to know machine learning algorithms to be a data scientist in a tech company (Google, Microsoft, Facebook, etc.)?","Thanks for A2A.

As John L. Miller and John Eysman pointed out in their answers, it is rare that data scientists need to implement machine learning algorithms from scratch.

However, I would like to emphasize the importance of actually knowing how machine learning algorithms work, particularly the limitations of those algorithms. For example:


Is feature engineering relevant at all for Random Forests? How are centering, scaling and Box-Cox transformations relevant in this context?
What kind of tree algorithm is the best for implementing a random forest and why?
What is an intuitive explanation of gradient descent?
How do I understand the characteristics of each cluster when doing a K-Means clustering algorithm?
What are the drawbacks of affinity propagation compared to the basic k-means algorithm?
Is it always better to have the largest possible number of folds when performing cross validation?","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/82ycqghfmksezivu', 'title': 'How well do I need to know machine learning algorithms to be a data scientist in a tech company (Google, Microsoft, Facebook, etc.)?', 'score': {'original': 0.6762, 'ai': 0.3238}, 'blocks': [{'text': 'Thanks for A2A.\n\nAs John L. Miller and John Eysman pointed out in their answers, it is rare that data scientists need to implement machine learning algorithms from scratch.\n\nHowever, I would like to emphasize the importance of actually knowing how machine learning algorithms work, particularly the limitations of those algorithms. For example:\n\n\nIs feature engineering relevant at all for Random Forests? How are centering, scaling and Box-Cox transformations relevant in this context?\nWhat kind of tree algorithm is the best for implementing a random forest and why?\nWhat is an intuitive explanation of gradient descent?\nHow do I understand the characteristics of each cluster when doing a K-Means clustering algorithm?\nWhat are the drawbacks of affinity propagation compared to the basic k-means algorithm?\nIs it always better to have the largest possible number of folds when performing cross validation?', 'result': {'fake': 0.3238, 'real': 0.6762}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982462, 'subscription': 0, 'content': 'Thanks for A2A.\n\nAs John L. Miller and John Eysman pointed out in their answers, it is rare that data scientists need to implement machine learning algorithms from scratch.\n\nHowever, I would like to emphasize the importance of actually knowing how machine learning algorithms work, particularly the limitations of those algorithms. For example:\n\n\nIs feature engineering relevant at all for Random Forests? How are centering, scaling and Box-Cox transformations relevant in this context?\nWhat kind of tree algorithm is the best for implementing a random forest and why?\nWhat is an intuitive explanation of gradient descent?\nHow do I understand the characteristics of each cluster when doing a K-Means clustering algorithm?\nWhat are the drawbacks of affinity propagation compared to the basic k-means algorithm?\nIs it always better to have the largest possible number of folds when performing cross validation?', 'aiModelVersion': '1'}",0.6762
Alan Mellor,Updated 1y,Has anybody mastered the art of programming in machine language?,"Yes.

Let me be immodest for a moment.

When I learned to code on a ZX81 with 1k RAM (that’s 0.000001 G for you cool kids), there was no room for a text editor or an assembler

So I hand coded machine language by learning the hex opcodes for Z80 instructions.

Here is a routine to clear the screen:

21 00 40 11 01 40 01 FF 03 77 ?? ED B0

I can’t remember ?? Which is LD (HL), A because it was 35 years ago. And 03FF might be out a bit, too

But yes, many 80s games programmers were breathing, walking assembler programs.

Confession: that is ZX Spectrum code, not exactly ZX81 But hey","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/ba9qji21gcvdzxe3', 'title': 'Has anybody mastered the art of programming in machine language?', 'score': {'original': 0.9984, 'ai': 0.0016}, 'blocks': [{'text': 'Yes.\n\nLet me be immodest for a moment.\n\nWhen I learned to code on a ZX81 with 1k RAM (that’s 0.000001 G for you cool kids), there was no room for a text editor or an assembler\n\nSo I hand coded machine language by learning the hex opcodes for Z80 instructions.\n\nHere is a routine to clear the screen:\n\n21 00 40 11 01 40 01 FF 03 77 ?? ED B0\n\nI can’t remember ?? Which is LD (HL), A because it was 35 years ago. And 03FF might be out a bit, too\n\nBut yes, many 80s games programmers were breathing, walking assembler programs.\n\nConfession: that is ZX Spectrum code, not exactly ZX81 But hey', 'result': {'fake': 0.0016, 'real': 0.9984}, 'status': 'success'}], 'credits_used': 2, 'credits': 1982460, 'subscription': 0, 'content': 'Yes.\n\nLet me be immodest for a moment.\n\nWhen I learned to code on a ZX81 with 1k RAM (that’s 0.000001 G for you cool kids), there was no room for a text editor or an assembler\n\nSo I hand coded machine language by learning the hex opcodes for Z80 instructions.\n\nHere is a routine to clear the screen:\n\n21 00 40 11 01 40 01 FF 03 77 ?? ED B0\n\nI can’t remember ?? Which is LD (HL), A because it was 35 years ago. And 03FF might be out a bit, too\n\nBut yes, many 80s games programmers were breathing, walking assembler programs.\n\nConfession: that is ZX Spectrum code, not exactly ZX81 But hey', 'aiModelVersion': '1'}",0.9984
Kelly Brown,8y,How does Google cluster documents?,"A2A,

Clustering algorithms are generally used in an unsupervised fashion. They are presented with a set of data instances that must be grouped according to some notion of similarity. The algorithm has access only to the set of features describing each object; it is not given any information (e.g., labels) as to where each of the instances should be placed within the partition.

Apart from it Google has a large team & automated system to manage cluster documents.","{'success': True, 'disclaimer': 'If you are trying to scan content that is under 50 words in length, you will run into AI accuracy issues.', 'public_link': 'https://app.originality.ai/share/425cjs3wm6t8iplh', 'title': 'How does Google cluster documents?', 'score': {'original': 0.933, 'ai': 0.067}, 'blocks': [{'text': 'A2A,\n\nClustering algorithms are generally used in an unsupervised fashion. They are presented with a set of data instances that must be grouped according to some notion of similarity. The algorithm has access only to the set of features describing each object; it is not given any information (e.g., labels) as to where each of the instances should be placed within the partition.\n\nApart from it Google has a large team & automated system to manage cluster documents.', 'result': {'fake': 0.0631, 'real': 0.9369}, 'status': 'success'}], 'credits_used': 1, 'credits': 1982459, 'subscription': 0, 'content': 'A2A,\n\nClustering algorithms are generally used in an unsupervised fashion. They are presented with a set of data instances that must be grouped according to some notion of similarity. The algorithm has access only to the set of features describing each object; it is not given any information (e.g., labels) as to where each of the instances should be placed within the partition.\n\nApart from it Google has a large team & automated system to manage cluster documents.', 'aiModelVersion': '1'}",0.933
